{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nn_builder\n# !pip uninstall torch -y\n# !pip install torch==1.4.0\nfrom gym.spaces import Discrete\nimport numpy as np\nimport gym\nimport matplotlib.pyplot as plt\nfrom gym import spaces\n# from google.colab import files\nimport torch\nimport random\nfrom collections import deque\nimport pandas as pd\nfrom nn_builder.pytorch.CNN import CNN   \nfrom nn_builder.pytorch.RNN import RNN   \nimport pandas as pd\nimport seaborn as sns\nfrom torch.distributions import Categorical\nimport torch.nn.functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-10c8163a4281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnn_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCNN\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnn_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRNN\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nn_builder'"],"ename":"ModuleNotFoundError","evalue":"No module named 'nn_builder'","output_type":"error"}]},{"cell_type":"markdown","source":"# HELPER FUNCTIONS","metadata":{}},{"cell_type":"code","source":"##### HELPER FUNCTIONS #####\nimport time\nimport datetime\n\ndef copy_model_over(from_model, to_model):\n        \"\"\"Copies model parameters from from_model to to_model\"\"\"\n        for to_model, from_model in zip(to_model.parameters(), from_model.parameters()):\n            to_model.data.copy_(from_model.data.clone())\n            \ndef plot_results(results, rolling, download=False, filename=\"\"):\n    \"\"\"\n    Plotting the results\n    :param list_rewards: list of cumulative rewards of all episodes\n    :param list_cumulative_steps: list of number of actions of all episodes\n    :param algo_name: the name of the learning algorithm\n    :param download_plots: \"True\" if this notebook is running on Google Colab and you want to save the figures to your computer, \"False\" otherwise\n    \"\"\"\n    pd_results = pd.DataFrame()\n    for r in results:\n        pd_results = pd.concat([pd_results, r], axis=1)\n        \n    list_agent_names = pd_results.columns.to_list()\n    pd_data = pd_results.reset_index()\n    bines = [(i) * rolling for i in list(range(201))]\n    bines_lab = [(i) * rolling for i in list(range(200))]\n    pd_data['binned_episode'] = pd.cut(pd_data['index'], bins=bines, labels=bines_lab)\n    pd_data = pd_data.fillna(0)\n    \n    for agent in list_agent_names:\n        sns.lineplot(data=pd_data, x=\"binned_episode\", y=agent, label=agent)\n   \n    plt.title(filename)\n    plt.xlabel('Episode')\n    plt.ylabel('Reward')\n    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n    plt.subplots_adjust(0,0,1,1,0,0)\n    \n    date = datetime.datetime.now().strftime(\"%Y_%m_%d-%I_%M_%S_%p\")\n    plt.savefig(filename+ \"_figure_\" + date + \".pdf\", dpi=300, bbox_inches='tight', pad_inches = 0)\n    filename = filename + \"_results_\" + date + \".csv\"\n    \n    if download:\n        pd_data.to_csv(filename, header=True, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Segment Environment","metadata":{}},{"cell_type":"code","source":"class SegmentEnv(object):\n    def __init__(self, max_steps, window, segment_length, noise, free_location, exploration_cost, pred_reward):\n        self.env_name = \"segment\"\n        self.board_size_x = 1\n        self.board_size_y = segment_length\n        self.num_state_channels = 2\n        self.num_action_channels = 2\n        \n        self.pred_reward = pred_reward # Final reward for the correct prediction\n        self.free_location = free_location\n        self.noise = noise\n        self.segment_length = segment_length\n        self.action_space = (Discrete(self.segment_length), Discrete(self.segment_length))\n        self.observation_space = Discrete(self.segment_length)\n        self.window = window\n        self.max_steps = max_steps # Maximum number of steps in one episode\n        self.to_draw = torch.zeros((self.max_steps + 1, 1, self.segment_length, 3))\n        self.exploration_cost = exploration_cost # The negative reward for each step\n        self.window = window\n        self.nA = self.num_action_channels * self.board_size_x * self.board_size_y # Size of action space n*x*y\n\n    def seed(self, seed):\n        np.random.seed(seed)\n    \n    def generate_a_segment(self, length, noise=0.0, free_location=False):\n        '''\n        Generate randomly a new bi-color segment\n        '''\n        if free_location:\n            l1 = np.random.randint(1, length-1)\n            l2 = np.random.randint(1, length-1)\n            left = min([l1, l2])\n            right = max([l1, l2])+1\n            segment = [0]*left + [1]*(right-left) + [0]*(length-right)\n        else:\n            right = np.random.randint(1, length)\n            segment = [[0]*right + [1]*(length-right)] # size [1, length]\n#         segment = np.array(segment) + noise * np.random.random(length)\n        segment = torch.tensor(segment)\n        return segment, torch.tensor(right)\n\n    def reset(self, NEXT=True):\n        self.pos = None\n        self.num_steps = 0 # Current number of steps in an episode so far\n        if NEXT:\n            self.curr_img, self.target = self.generate_a_segment(length=self.segment_length, noise=self.noise,\n                                                            free_location=self.free_location)\n\n        self.state = torch.zeros((self.num_state_channels, 1, self.segment_length))\n#         self.state[2] = torch.ones(1, self.segment_length)\n        \n#         self.state[3] = (-1) * torch.ones(1, self.segment_length)\n        \n        return self.state\n    \n    def observe(self):\n        \"\"\"\n          Return the current state\n        \"\"\"\n        return self.state\n    \n    def get_frame(self, t, pred=None):\n        true_image = np.zeros((1, self.segment_length, 3)).astype(int)\n        true_image[:, self.curr_img == 1, 2] = 255\n        true_image[:, self.curr_img == 0, 0] = 255\n\n        segment_plot = np.zeros((1, self.segment_length, 3)).astype(int) + 128\n        segment_plot[:, self.state[0] == 1, :] = true_image[:, self.state[0] == 1, :]\n        if self.pos is not None:\n            segment_plot[:, self.pos, :] = np.clip(segment_plot[:, self.pos, :] + 170, 0, 255)\n        if pred is not None:\n            segment_plot[:, pred, 1] = np.clip(segment_plot[:, pred, 1] + 170, 0, 255)\n\n        self.to_draw[t, :, :, :] = segment_plot\n\n    def step(self, action):\n        # Reveal     \n#         print(\"action=\", action)\n        if action[0] == 0:\n#             print(\"action=\", action)\n            self.pos = action[2]\n            done = False\n            reward = -self.exploration_cost\n            \n            self.state[0, 0, max(self.pos - self.window, 0):min(self.pos + self.window + 1, self.segment_length)] = torch.ones(\n                min(self.pos + self.window + 1, self.segment_length) - max(self.pos - self.window, 0))\n            \n            self.state[1, 0, max(self.pos - self.window, 0):min(self.pos + self.window + 1, self.segment_length)] = self.curr_img[0, max(\n                self.pos - self.window, 0):min(self.pos + self.window + 1, self.segment_length)]\n            pred = None\n\n        # Predict\n        else:\n            # Receive the final reward if the agent predicts correctly\n            reward = (self.target == action[2]) * self.pred_reward + (-self.exploration_cost)\n            done = self.target == action[2]\n            pred = action[2] # Predicted position\n\n#         self.get_frame(int(self.num_steps), pred=pred)\n        self.num_steps += 1\n    \n#         Remaining time = (max_steps - current_step)/max_steps\n#         self.state[2] = ((self.max_steps - self.num_steps)/self.max_steps) * self.state[2] \n        \n#         #Last action\n#         self.state[3] = (-1) * torch.ones(1, self.segment_length)\n#         self.state[3, 0, action[2]] = 1\n        \n        info = {}\n        \n        return self.state, torch.tensor(reward), torch.tensor(int(done == True)), info\n\n    def render(self, e):\n        true_image = np.zeros((1, self.segment_length, 3)).astype(int)\n        true_image[:, self.curr_img == 1, 2] = 255\n        true_image[:, self.curr_img == 0, 0] = 255\n\n        array_list = [\n            np.vstack([s_plot, true_image]) for s_plot in self.to_draw[:self.num_steps]\n            ]\n        image_list = []\n        for a in array_list:\n            fig = plt.figure()\n            plt.yticks([])\n            plt.imshow(a)\n            plt.hlines(0.5, -0.5, self.segment_length - 0.5)\n            fig.canvas.draw()\n            image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n            image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n            plt.close(fig)\n            image_list.append(image)\n        imageio.mimsave('./{}.gif'.format(e), image_list, fps=1)\n\n    def get_current_obs(self):\n        return self.state\n\n    def close(self):\n        pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"right = np.random.randint(1, 10)\nsegment = [[0]*right + [1]*(10-right)]\nsegment = torch.tensor(segment)\nsegment.size()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Batteship Environment","metadata":{}},{"cell_type":"code","source":"class BatteshipEnv(gym.Env):\n    def __init__(self, board_size_x, board_size_y, num_small_ships, num_medium_ships, num_large_ships, max_steps):\n        \"\"\"\n        Initialize a Battleship environment\n        :param board_size: With board_size = n we have a board of n*n\n        :param ships_board: Given board with placed ships\n        :return BatteshipEnv object\n        \"\"\"\n#         super(BatteshipEnv, self).__init__()\n\n        self.board_size_x = board_size_x\n        self.board_size_y = board_size_y\n        \n        self.num_state_channels = 2\n        self.num_action_channels = 1\n        \n        self.env_name = \"battleship\"\n        \n        self.EMPTY = 0 \n        self.UNKNOWN = 1\n        self.LAST_ACTION = 1\n        self.ships_board = torch.zeros(self.board_size_x, self.board_size_y)\n        self.nA = self.board_size_x * self.board_size_y # Size of action space n*n\n     \n        self.num_small_ships =  num_small_ships\n        self.num_medium_ships = num_medium_ships\n        self.num_large_ships = num_large_ships\n#         self.observation_space = spaces.Discrete(self.board_size * self.board_size) # Obeservation space\n#         self.action_space = spaces.Tuple((spaces.Discrete(self.board_size),spaces.Discrete(self.board_size))) # Action space\n#         self.nS = 2*self.board_size * self.board_size\n        \n        self.max_steps = max_steps\n        self.num_steps = 0\n            \n    def calculate_reward(self, action):\n        '''\n        Return reward for an action\n        :param action: action taken\n        :return reward \n        ''' \n        \n        x = action[0] # x coordinate of the action\n        y = action[1] # y coordinate of the action\n        \n        if (self.s[0,x,y] != self.UNKNOWN): #Already revealed\n            if (self.s[1,x,y] == self.EMPTY): #Punish for repeating hitting empty place\n                return torch.tensor(-1.0) #-1 \n            else: #Keep hitting the ship\n                self.num_hits = self.num_hits + 1\n                return torch.tensor(1.0) \n        else: #Not revealed\n            if (self.ships_board[x,y] == self.EMPTY): #Empty place\n                return torch.tensor(-1.0)\n            else: #Found a ship\n                self.num_hits = self.num_hits + 1\n                return torch.tensor(1.0)\n        return torch.tensor(-1.0)\n            \n    def update_board(self, action):\n        \"\"\"\n        Update the playing board\n        :param action: action taken\n        :return the current state after taking the action\n        \"\"\"\n        x = action[0] # x coordinate of the action\n        y = action[1] # y coordinate of the action\n        \n        if self.s[0,x,y] == self.UNKNOWN:\n            if self.ships_board[x,y] != self.EMPTY:\n                self.s[1,x,y] = self.ships_board[x,y] - 1\n            else:\n                self.s[1,x,y] = self.EMPTY\n        else:\n            self.s[1,x,y] = max(self.s[1,x,y]-1,0)\n            \n        self.s[0,x,y] = 1-self.UNKNOWN\n        \n#         Remaining time = (max_steps - current_step)/max_steps\n#         self.s[2] = ((self.max_steps - self.num_steps)/self.max_steps) * self.s[2] \n\n#         self.s[2] = (-1) * torch.ones(self.board_size_x, self.board_size_y)  \n#         self.s[2, x, y] = 1\n        \n#         \n        \n        return self.s\n\n    def step(self, action):\n        \"\"\" \n          Take a step by executing an action\n          :param actionL action taken\n          :return next_state: the state after taking the action\n          :return reward: reward for taking action from the current state\n          :return done: Whether all the ships have been destroyed \n          :return info: none\n        \"\"\"\n        reward = self.calculate_reward(action)\n        next_state = self.update_board(action)\n        if self.num_hits >= self.total_hits:\n            done = torch.tensor(1)\n        else:\n            done = torch.tensor(0)\n        info = {}\n        self.num_steps = self.num_steps + 1\n        return next_state, reward, done, info\n\n    def observe(self):\n        \"\"\"\n          Return the current state\n        \"\"\"\n        return self.s\n\n    def reset(self):\n        \"\"\"\n          Reset the environment\n          :return the initial playing board\n        \"\"\"\n        self.ships_board = self.EMPTY * torch.ones(self.board_size_x, self.board_size_y)\n        self.num_steps = 0\n        \n        self.s = torch.ones(self.num_state_channels, self.board_size_x, self.board_size_y)  \n        self.s[0] = self.UNKNOWN * self.s[0]    # Revealed position\n        self.s[1] = self.EMPTY * self.s[1]      # Ship's health\n#         self.s[2] = self.s[2]          \n        \n#         self.s[2] = (-1) * self.s[2]            # Last action\n                 \n        \n        \n        num_small_ships = self.num_small_ships\n        num_medium_ships = self.num_medium_ships\n        num_large_ships = self.num_large_ships\n\n        while num_small_ships>0:\n            i = random.randint(0, self.board_size_x-1)\n            j = random.randint(0, self.board_size_y-1)\n            if self.ships_board[i, j] == 0:\n                self.ships_board[i, j] = 1\n                num_small_ships -= 1\n\n        while num_medium_ships>0:\n            i = random.randint(0, self.board_size_x-1)\n            j = random.randint(0, self.board_size_y-1)\n            if self.ships_board[i, j] == 0:\n                self.ships_board[i, j] = 2\n                num_medium_ships -= 1\n\n        while num_large_ships>0:\n            i = random.randint(0, self.board_size_x-1)\n            j = random.randint(0, self.board_size_y-1)\n            if self.ships_board[i, j] == 0:\n                self.ships_board[i, j] = 3\n                num_large_ships -= 1\n        \n#         self.ships_board[0, 0] = 3\n#         self.ships_board[2, 2] = 2\n        \n        self.total_hits = torch.sum(self.ships_board)\n        self.num_hits = 0\n        return self.s\n\n    def render(self, mode='human'):\n        \"\"\"\n          Render the current playing board\n        \"\"\"\n        for i in range(self.board_size_x):\n          print(\"-------------------------------------------\")\n          line = \"\"\n          for j in range(self.board_size_y):     \n            line += \" | \"\n            if self.s[i,j] == self.EMPTY:\n              line += \"O\"\n            elif self.s[i,j] == self.UNKNOWN:\n              line += \" \"\n            else:\n              line += str(int(self.s[i,j]))\n          line += \" | \"\n          print(line)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# REPLAY BUFFER","metadata":{}},{"cell_type":"code","source":"class Replay():\n    \"\"\"\n    Memory for storing experience \n    \"\"\"\n    def __init__(self):\n#         self.buffer = []\n        self.buffer = deque(maxlen=100000)\n\n    def __len__(self):\n        return len(self.buffer)\n\n    def add(self, state, action, reward, next_state, done):\n        \"\"\"\n        Add a new experience to the buffer\n        :param state: The state before taking the action\n        :param action: action taken\n        :param reward: Reward for taking that action\n        :param next_state: The state that the agent enters after taking the action\n        :param done: Whether the agent finishes the game\n        \"\"\"\n        self.buffer.append((state, action, reward, next_state, done))\n\n    def sample(self, batch_size):\n        \"\"\"\n          Return a batch of samples from the experience buffer\n          :param batch_size: The number of sample that you want to take\n          :return the batch of samples but decomposed into lists of states, actions, rewards, next_states \n        \"\"\"\n        states, actions, rewards, next_states, dones = [], [], [], [], []\n\n        # Random samples\n        samples = random.sample(self.buffer, batch_size)\n\n        for s in samples:\n            state = s[0]\n            action = s[1]\n            reward = s[2]\n            next_state = s[3]\n            done = s[4]\n            \n            states.append(state)\n            actions.append(action)\n            rewards.append(reward)\n            next_states.append(next_state)\n            dones.append(done)\n        \n        return states, actions, rewards, next_states, dones","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BASE AGENT","metadata":{}},{"cell_type":"code","source":"class Base_Agent():\n    def __init__(self, agent_name, env):\n        self.agent_name = agent_name\n        self.env = env\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.list_rewards = pd.DataFrame (columns = [agent_name])\n        self.epsilon = None\n        self.epsilon_decay_rate = None\n    def select_action(self, state):\n        pass\n    \n    def train():\n        pass\n    \n    def soft_update_of_target_network(self, local_model, target_model, tau):\n        \"\"\"Updates the target network in the direction of the local network but by taking a step size\n        less than one so the target network's parameter values trail the local networks. This helps stabilise training\"\"\"\n        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n    \n    def runner(self, number_of_episodes, training=False):\n        print(\"########## \" + self.agent_name + \" is running ##########\")\n        \n        for episode in range(number_of_episodes+1):\n            ep_reward = 0.0         # Reward for this episode\n            done = False          # Whether the game is finished\n            loss = 0.0\n            self.env.reset()\n\n            # Get the current state\n            state = self.env.observe().to(self.device, dtype=torch.float)\n            \n            while not done and self.env.num_steps<self.env.max_steps:\n                with torch.no_grad():\n                    # Get and execute the next action for the current state\n                    action = self.select_action(state)\n                    next_state, reward, done, info = env.step(action)\n                    ep_reward = ep_reward + reward.item()\n                    \n                # Start training once the size of the buffer greater than the batch size\n                if training:\n                    # Save what the agent just learnt to the experience buffer.\n                    self.buffer.add(state, action, reward, next_state, done)\n                    loss = self.train()\n                    \n                state = next_state.to(self.device, dtype=torch.float)\n                \n            # Print log\n            print(f'Agent: {self.agent_name} . Episode {episode}/{number_of_episodes} . Number of steps to finish: {self.env.num_steps} . Loss: {loss} '\n                f'Reward: {ep_reward}')\n            \n            # Epsilon is decayed since the agent is getting more and more knowledge\n            if self.epsilon != None and self.epsilon_decay_rate != None:\n                print(\"Epsilon:\", self.epsilon)\n                self.epsilon = self.epsilon * self.epsilon_decay_rate\n#                 self.epsilon = max(self.epsilon * self.epsilon_decay_rate, 0.1) \n\n            self.list_rewards = self.list_rewards.append({self.agent_name:ep_reward}, ignore_index=True)\n\n        env.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RANDOM AGENT","metadata":{}},{"cell_type":"code","source":"class Random_Agent(Base_Agent):\n    \"\"\"\n    Model a random agent\n    \"\"\"\n    def __init__(self, agent_name, env):\n        super().__init__(agent_name, env)        \n\n    def select_action(self, state):\n        \"\"\"\n          Return an random action\n          :param state: the current state\n          :return action: next action to take\n        \"\"\"\n        if self.env.env_name == \"segment\": \n            random_action = torch.tensor([random.randint(0,self.env.num_action_channels-1), random.randint(0,self.env.board_size_x-1), random.randint(0,self.env.board_size_y-1)])\n        elif self.env.env_name == \"battleship\":\n            random_action = torch.tensor([random.randint(0,self.env.board_size_x-1), random.randint(0,self.env.board_size_y-1)])\n        \n        return random_action","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Battleship Baseline","metadata":{}},{"cell_type":"code","source":"class Battleship_Baseline(Base_Agent):\n    \"\"\"\n    Baseline for Battleship\n    \"\"\"\n    def __init__(self, agent_name, env):\n        super().__init__(agent_name, env)\n        self.last_action = None\n\n    def select_action(self, state):\n        \"\"\"\n          Return an random action\n          :param state: the current state\n          :return action: next action to take\n        \"\"\"\n      \n        x = random.randint(0,self.env.board_size_x-1)\n        y = random.randint(0,self.env.board_size_y-1)\n            \n        if self.last_action == None:\n            x = random.randint(0,self.env.board_size_x-1)\n            y = random.randint(0,self.env.board_size_y-1)\n            random_action = torch.tensor([x, y])\n            self.last_action = random_action\n            return random_action\n        else:\n            if state[1, self.last_action[0], self.last_action[1]] == self.env.EMPTY:\n                while state[0, x, y] != self.env.UNKNOWN and state[1, x, y] == self.env.EMPTY:\n                    x = random.randint(0,self.env.board_size_x-1)\n                    y = random.randint(0,self.env.board_size_y-1)\n                random_action = torch.tensor([x, y])\n                self.last_action = random_action\n                return random_action\n            else:\n                repeat_action = self.last_action\n                return repeat_action","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DDQN AGENT","metadata":{}},{"cell_type":"code","source":"class DDQN(Base_Agent):\n    \"\"\"\n    Model a DQN agent\n    \"\"\"\n    def __init__(self, agent_name, env, epsilon, epsilon_decay_rate, gamma, batch_size):\n        super().__init__(agent_name, env)        \n        self.epsilon = epsilon # For deciding exploitation or exploration\n        self.epsilon_decay_rate = epsilon_decay_rate # Epsilon is decayed after each episode with a fixed rate\n        self.gamma = gamma # The weight for future rewards\n        self.batch_size = batch_size\n        \n        self.buffer = Replay()             # Experience Buffer\n        \n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        \n        self.main_dqn = CNN(input_dim=(self.env.num_state_channels, self.env.board_size_x, self.env.board_size_y), \n                            layers_info=[\n                            [\"conv\", self.env.num_state_channels, 2, 1, 1], \n                            [\"linear\", 64],\n                            [\"linear\", 16],\n                            [\"linear\", self.env.nA]],\n                            hidden_activations=\"relu\", \n#                             output_activation=\"softmax\", \n#                             dropout=0.5,\n                            initialiser=\"xavier\", \n                            batch_norm=False)\n#         self.main_dqn = RNN(input_dim=2, layers_info=[[\"lstm\", 16], [\"linear\", self.env.nA]],\n#             hidden_activations=\"relu\",\n#             batch_norm=False, dropout=0.0, initialiser=\"xavier\")\n\n\n        self.target_dqn = CNN(input_dim=(self.env.num_state_channels, self.env.board_size_x, self.env.board_size_y), \n                            layers_info=[\n                            [\"conv\", self.env.num_state_channels, 2, 1, 1], \n                            [\"linear\", 64],\n                            [\"linear\", 16],\n                            [\"linear\", self.env.nA]],\n                            hidden_activations=\"relu\", \n#                             output_activation=\"softmax\", \n#                             dropout=0.5,\n                            initialiser=\"xavier\", \n                            batch_norm=False)\n    \n        # Send models to GPU\n        self.main_dqn.to(self.device)\n        self.target_dqn.to(self.device)\n        \n         # Optimizer and Loss function\n        self.optimizer = torch.optim.Adam(self.main_dqn.parameters(), lr=1e-4)\n        self.mse = torch.nn.MSELoss()\n        self.L1 = torch.nn.SmoothL1Loss()\n        \n#         self.list_number_of_actions = pd.DataFrame (columns = ['episode','length'])\n#         self.list_cumulative_loss = pd.DataFrame (columns = ['episode','loss'])\n#         self.list_number_of_actions.length = self.list_number_of_actions['length'].astype(float)\n#         self.list_cumulative_loss.length = self.list_cumulative_loss['loss'].astype(float)\n\n    def select_action(self, state):\n        \"\"\"\n          Return an action to take based on epsilon (greedy or random action)\n          :param state: the current state\n          :return action: next action to take\n        \"\"\"\n        random_number = np.random.uniform()\n        if random_number < self.epsilon:\n            # Random action\n            if self.env.env_name == \"segment\":\n                return torch.tensor([random.randint(0,self.env.num_action_channels-1), random.randint(0,self.env.board_size_x-1), random.randint(0,self.env.board_size_y-1)])\n            elif self.env.env_name == \"battleship\":\n                return torch.tensor([random.randint(0,self.env.board_size_x-1), random.randint(0,self.env.board_size_y-1)])\n        else:\n            # Greedy action\n            state = [state]\n            state = torch.stack(state)\n            state = state.to(self.device, dtype=torch.float)\n            argmax = torch.argmax(self.main_dqn(state)).item()\n            \n            if self.env.env_name == \"segment\":\n                return torch.tensor(np.unravel_index(argmax, (self.env.num_action_channels, self.env.board_size_x, self.env.board_size_y)))\n            elif self.env.env_name == \"battleship\":\n                return torch.tensor(np.unravel_index(argmax, (self.env.board_size_x, self.env.board_size_y)))\n            \n            \n       \n    def train(self):\n        \"\"\"\n        Train the network with a batch of samples\n        :param states: The state before taking the action\n        :param actions: action taken\n        :param rewards: Reward for taking that action\n        :param next_states: The state that the agent enters after taking the action\n        :return loss: the loss value after training the batch of samples\n        \"\"\"\n        if len(self.buffer) >= self.batch_size:\n            with torch.no_grad():\n                states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n                        \n            # Send data to GPU\n            states = torch.stack(states).to(self.device, dtype=torch.float) \n            actions = torch.stack(actions).to(self.device, dtype=torch.float)\n            rewards = torch.stack(rewards).to(self.device, dtype=torch.float)\n            rewards = torch.reshape(rewards, (self.batch_size, 1))\n            \n            next_states = torch.stack(next_states).to(self.device, dtype=torch.float)\n            dones = torch.stack(dones).to(self.device, dtype=torch.float)\n            \n            #TODO\n            if self.env.env_name == \"battleship\":\n                ravel = torch.tensor([[self.env.board_size_y*1.0], [1.0]], dtype=torch.float64).to(self.device, dtype=torch.float)\n            elif self.env.env_name == \"segment\":        \n                ravel = torch.tensor([[self.env.board_size_x * self.env.board_size_y * 1.0], [self.env.board_size_y*1.0], [1.0]], dtype=torch.float64).to(self.device, dtype=torch.float)\n\n            actions = torch.matmul(actions,ravel) # size([128, 1])    \n\n            # Calculate target Q values using the Target Network\n            selection = torch.argmax(self.main_dqn(next_states), dim = 1).unsqueeze(1)\n            \n            evaluation = self.target_dqn(next_states)\n            evaluation = evaluation.gather(1, selection.long()) #size [256,1]\n#             evaluation = torch.reshape(evaluation, (-1,))\n            \n            \n        \n            #Create Done mask\n            nonzero_indices = torch.nonzero(dones).reshape(-1).tolist()  \n            dones_mask = torch.eye(self.batch_size)\n            for index in nonzero_indices:\n                dones_mask[index,index] = 0\n            dones_mask = dones_mask.to(self.device, dtype=torch.float)\n\n            # Calculte target\n#             print(\"torch.matmul(dones_mask, evaluation*self.gamma)size =\", torch.matmul(dones_mask, evaluation*self.gamma).size())\n#             print(\"rewards size=\", rewards.size())\n            target = rewards + torch.matmul(dones_mask, evaluation*self.gamma)\n            target = target.detach()\n#             print(\"target size=\", target.size())\n#             print(\"target =\", target)\n\n            # Calculate Q values using the Main Network        \n            n_classes = self.env.num_action_channels * self.env.board_size_x * self.env.board_size_y\n            n_samples = self.batch_size\n            labels = torch.flatten(actions.type(torch.LongTensor), start_dim=0)\n            labels_tensor = torch.as_tensor(labels)\n            action_masks = torch.nn.functional.one_hot(labels_tensor, num_classes=n_classes).to(self.device, dtype=torch.float)\n            \n            q_value = action_masks * self.main_dqn(states)\n#             print(\"q-value size=\",q_value.size())\n#             print(\"q-value=\",q_value)\n            \n            q_value = torch.sum(q_value, dim=-1).reshape((self.batch_size, 1))\n#             print(\"q-value after size=\",q_value.size())\n#             print(\"q-value after=\",q_value)\n            \n           \n            # Calculate loss\n            loss = self.mse(target, q_value)\n#             loss = self.L1(target, q_value)\n\n            # Optimize the model\n            self.optimizer.zero_grad()\n            loss.backward()\n#             for param in self.main_dqn.parameters():\n#                 # Clip the target to avoid exploding gradients\n#                 param.grad.data.clamp_(-1e-6,1e-6)\n            self.optimizer.step()\n            \n            # Soft Copy the Main Network's weights to the Target Network \n            self.soft_update_of_target_network(self.main_dqn, self.target_dqn,tau=1e-3)\n\n            return loss\n        return 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SACD AGENT","metadata":{}},{"cell_type":"code","source":"\n            \nclass SAC_Discrete(Base_Agent):\n    '''\n    Soft Actor Critic for discrete action space\n    '''\n    def __init__(self, agent_name, env, gamma, batch_size, automatic_entropy_tuning):\n        super().__init__(agent_name, env)        \n       \n        self.gamma = gamma # The weight for future rewards\n        self.batch_size = batch_size\n        self.automatic_entropy_tuning = automatic_entropy_tuning\n        self.buffer = Replay()\n        self.L1loss = torch.nn.SmoothL1Loss()\n        self.mse = torch.nn.MSELoss()\n        \n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        \n        if self.automatic_entropy_tuning:\n            # we set the max possible entropy as the target entropy\n            self.target_entropy = -np.log((1.0 / self.env.nA)) * 0.98\n#             self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n#             self.alpha = self.log_alpha.exp()\n            self.alpha = torch.ones(1, requires_grad=True, device=self.device)\n            self.alpha_optim = torch.optim.Adam([self.alpha], lr=1e-4, eps=1e-4)\n        else:\n            self.alpha = 0.2\n            self.add_extra_noise = False\n            self.do_evaluation_iterations = False\n        \n        \n        self.critic_local = CNN(input_dim=(self.env.num_state_channels, self.env.board_size_x, self.env.board_size_y), layers_info=[\n                            [\"conv\", self.env.num_state_channels, 2, 1, 1], \n                            [\"linear\", 128],\n                            [\"linear\", 16],\n                            [\"linear\", self.env.nA]],\n                            hidden_activations=\"relu\", \n#                             output_activation=\"softmax\", \n#                             dropout=0.5,\n                            initialiser=\"xavier\", \n                            batch_norm=False)\n        self.critic_target = CNN(input_dim=(self.env.num_state_channels, self.env.board_size_x, self.env.board_size_y), layers_info=[\n                            [\"conv\", self.env.num_state_channels, 2, 1, 1], \n                            [\"linear\", 128],\n                            [\"linear\", 16],\n                            [\"linear\", self.env.nA]],\n                            hidden_activations=\"relu\", \n#                             output_activation=\"softmax\", dropout=0.0,\n                            initialiser=\"xavier\", \n                            batch_norm=False)\n        self.critic_local_2 = CNN(input_dim=(self.env.num_state_channels, self.env.board_size_x, self.env.board_size_y), layers_info=[\n                            [\"conv\", self.env.num_state_channels, 2, 1, 1], \n                            [\"linear\", 128],\n                            [\"linear\", 16],\n                            [\"linear\", self.env.nA]],\n                            hidden_activations=\"relu\", \n#                             output_activation=\"softmax\", dropout=0.0,\n                            initialiser=\"xavier\", \n                            batch_norm=False)\n        self.critic_target_2 = CNN(input_dim=(self.env.num_state_channels, self.env.board_size_x, self.env.board_size_y), layers_info=[\n                            [\"conv\", self.env.num_state_channels, 2, 1, 1], \n                            [\"linear\", 128],\n                            [\"linear\", 16],\n                            [\"linear\", self.env.nA]],\n                            hidden_activations=\"relu\", \n#                             output_activation=\"softmax\", dropout=0.0,\n                            initialiser=\"xavier\", \n                            batch_norm=False)\n        self.actor_local = CNN(input_dim=(self.env.num_state_channels, self.env.board_size_x, self.env.board_size_y), layers_info=[\n                            [\"conv\", self.env.num_state_channels, 2, 1, 1], \n                            [\"linear\", 128],\n                            [\"linear\", 16],\n                            [\"linear\", self.env.nA]],\n                            hidden_activations=\"relu\", \n                            output_activation=\"softmax\", \n#                             dropout=0.0,\n                            initialiser=\"xavier\", \n                            batch_norm=False)\n        \n        self.critic_local.to(self.device)\n        self.critic_target.to(self.device)\n        self.critic_local_2.to(self.device)\n        self.critic_target_2.to(self.device)\n        self.actor_local.to(self.device)\n        \n        self.actor_optimizer = torch.optim.Adam(self.actor_local.parameters(), lr=1e-4)\n        self.critic_optimizer = torch.optim.Adam(self.critic_local.parameters(), lr=1e-4)\n        self.critic_optimizer_2 = torch.optim.Adam(self.critic_local_2.parameters(), lr=1e-4)\n        \n        copy_model_over(self.critic_local, self.critic_target)\n        copy_model_over(self.critic_local_2, self.critic_target_2)\n\n    def create_actor_distribution(self, action_probabilities):\n        \"\"\"Creates a distribution that the actor can then use to randomly draw actions\"\"\"\n        \n        action_distribution = Categorical(action_probabilities)  # this creates a distribution to sample from\n        return action_distribution\n\n\n    def produce_action_and_action_info(self, state):\n        '''\n        Given the state, produces an action, the probability of the action, the log probability of the action, and\n        the argmax action\n        '''\n#         print(\"state size=\", state.size())\n        action_probabilities = self.actor_local(state)\n        max_probability_action = torch.argmax(action_probabilities, dim=-1)\n        action_distribution = self.create_actor_distribution(action_probabilities)\n        \n        action = action_distribution.sample().cpu()\n        # Have to deal with situation of 0.0 probabilities because we can't do log 0\n        z = action_probabilities == 0.0\n        z = z.float() * 1e-8\n        log_action_probabilities = torch.log(action_probabilities + z)\n        return action, (action_probabilities, log_action_probabilities), max_probability_action\n\n    def calculate_critic_losses(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch):\n        '''\n        Calculates the losses for the two critics. This is the ordinary Q-learning loss except the additional entropy\n        term is taken into account\n        '''\n        with torch.no_grad():\n            if self.env.env_name == \"battleship\":\n                ravel = torch.tensor([[self.env.board_size_y*1.0], [1.0]], dtype=torch.float64).to(self.device, dtype=torch.float)\n            elif self.env.env_name == \"segment\":\n                ravel = torch.tensor([[self.env.board_size_x * self.env.board_size_y * 1.0], [self.env.board_size_y*1.0], [1.0]], dtype=torch.float64).to(self.device, dtype=torch.float)\n                \n            action_batch = torch.matmul(action_batch,ravel)\n            \n            next_state_action, (action_probabilities, log_action_probabilities), _ = self.produce_action_and_action_info(next_state_batch)\n            \n            qf1_next_target = self.critic_target(next_state_batch)\n            qf2_next_target = self.critic_target_2(next_state_batch)\n           \n#             print(\"qf1_next_target=\", qf1_next_target)\n#             print(\"qf2_next_target=\", qf2_next_target)\n            \n#             print(\"torch.min(qf1_next_target, qf2_next_target) size=\", torch.min(qf1_next_target, qf2_next_target).size())\n#             print(\"log_action_probabilities size\", log_action_probabilities)\n#             print(\"log_action_probabilities size\", log_action_probabilities.size())\n#             print(\"inside term size=\", (torch.min(qf1_next_target, qf2_next_target) - self.alpha * log_action_probabilities))\n            min_qf_next_target = action_probabilities * (torch.min(qf1_next_target, qf2_next_target) - self.alpha * log_action_probabilities)\n            \n#             print(\"min_qf_next_target before =\", min_qf_next_target)\n            min_qf_next_target = min_qf_next_target.sum(dim=1).unsqueeze(-1)\n            \n#             print(\"min_qf_next_target after =\", min_qf_next_target)\n            \n            next_q_value = reward_batch + torch.matmul(done_batch, self.gamma * min_qf_next_target)\n\n        \n        qf1 = self.critic_local(state_batch).gather(1, action_batch.long())\n        qf2 = self.critic_local_2(state_batch).gather(1, action_batch.long())\n        \n#         print(\"qf1  size =\", qf1)\n#         print(\"qf2  size =\", qf2)\n#         print(\"next_q_value  size =\", next_q_value)\n        \n        qf1_loss = self.mse(qf1, next_q_value)\n        qf2_loss = self.mse(qf2, next_q_value)\n\n        return qf1_loss, qf2_loss\n\n    def calculate_actor_loss(self, state_batch):\n        '''\n        Calculates the loss for the actor. This loss includes the additional entropy term\n        '''\n        _, (action_probabilities, log_action_probabilities), _ = self.produce_action_and_action_info(state_batch)\n        \n        qf1_pi = self.critic_local(state_batch)\n        qf2_pi = self.critic_local_2(state_batch)\n        min_qf_pi = torch.min(qf1_pi, qf2_pi) #size [256, 9]\n        \n#         print(\"min_qf_pi size = \", min_qf_pi)\n#         print(\"log_action_probabilities=\",log_action_probabilities)\n        inside_term = self.alpha * log_action_probabilities - min_qf_pi   #size [256, 9]\n#         print(\"inside_term size=\", inside_term)\n        \n        entropies = torch.sum(log_action_probabilities * action_probabilities, dim=1)\n        \n        policy_loss = (action_probabilities * inside_term).sum(dim=1).mean()\n        \n        \n#         # Expectations of entropies.\n#         entropies = -torch.sum(action_probabilities * log_action_probabilities, dim=1, keepdim=True)\n\n#         # Expectations of Q.\n#         q = torch.sum(torch.min(qf1_pi, qf2_pi) * action_probabilities, dim=1, keepdim=True)\n\n#         # Policy objective is maximization of (Q + alpha * entropy) with\n#         # priority weights.\n#         policy_loss = (- q - self.alpha * entropies).mean()\n\n        return policy_loss, entropies\n    \n    \n\n    def select_action(self, state):\n        state = [state]\n        state = torch.stack(state)\n        state = state.to(self.device, dtype=torch.float)\n        action, _, _ = self.produce_action_and_action_info(state)\n        action = action #.detach().clone()\n        \n        if self.env.env_name == \"battleship\":\n            return torch.tensor(np.unravel_index(action.item(), (self.env.board_size_x, self.env.board_size_y)))\n        elif self.env.env_name == \"segment\":\n            return torch.tensor(np.unravel_index(action.item(), (self.env.num_action_channels, self.env.board_size_x, self.env.board_size_y)))\n        \n\n    def take_optimisation_step(self, optimizer, network, loss, clipping_norm=None, retain_graph=False):\n        \"\"\"Takes an optimisation step by calculating gradients given the loss and then updating the parameters\"\"\"\n        if not isinstance(network, list): network = [network]\n        with torch.autograd.set_detect_anomaly(True):\n            optimizer.zero_grad()\n            loss.backward(retain_graph=retain_graph)\n#             for net in network:\n#                 for param in net.parameters():\n#     #                 # Clip the target to avoid exploding gradients\n#                     param.grad.data.clamp_(-1e-6,1e-6)\n    \n            if clipping_norm is not None:\n                for net in network:\n                    torch.nn.utils.clip_grad_norm_(net.parameters(), clipping_norm) #clip gradients to help stabilise training\n#             optimizer.step()\n\n    def calculate_entropy_tuning_loss(self, log_pi):\n        \"\"\"Calculates the loss for the entropy temperature parameter. This is only relevant if self.automatic_entropy_tuning\n        is True.\"\"\"\n#         print(\"log_pi=\",log_pi)\n#         print(\"self.log_alpha=\",self.log_alpha)\n#         alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n        alpha_loss = -(self.alpha * (log_pi + self.target_entropy).detach()).mean()\n#         print(\"alpha_loss=\", alpha_loss)\n        return alpha_loss\n\n    def train(self):\n        \"\"\"\n        Train the network with a batch of samples\n        :param states: The state before taking the action\n        :param actions: action taken\n        :param rewards: Reward for taking that action\n        :param next_states: The state that the agent enters after taking the action\n        :return loss: the loss value after training the batch of samples\n        \"\"\"\n        if len(self.buffer) >= self.batch_size:\n            with torch.no_grad():\n                states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n                states = torch.stack(states).to(self.device, dtype=torch.float)\n                actions = torch.stack(actions).to(self.device, dtype=torch.float)\n                rewards = torch.stack(rewards).to(self.device, dtype=torch.float)\n                next_states = torch.stack(next_states).to(self.device, dtype=torch.float)\n                rewards = torch.reshape(rewards, (self.batch_size, 1))\n\n                dones = torch.stack(dones).to(self.device, dtype=torch.long)\n                nonzero_indices = torch.nonzero(dones).reshape(-1).tolist()  \n                dones_mask = torch.eye(self.batch_size)\n                for index in nonzero_indices:\n                    dones_mask[index,index] = 0\n                dones_mask = dones_mask.to(self.device, dtype=torch.float)  # size [256, 256]\n        \n            #Updates the parameters for both critics\n            qf1_loss, qf2_loss = self.calculate_critic_losses(states, actions, rewards, next_states, dones_mask)\n            self.take_optimisation_step(self.critic_optimizer, self.critic_local, qf1_loss,None)\n            self.take_optimisation_step(self.critic_optimizer_2, self.critic_local_2, qf2_loss,None)\n            self.critic_optimizer.step()\n            self.critic_optimizer_2.step()\n            self.soft_update_of_target_network(self.critic_local, self.critic_target,tau=1e-3)\n            self.soft_update_of_target_network(self.critic_local_2, self.critic_target_2,tau=1e-3)\n\n            # update_actor_parameters\n            policy_loss, log_pi = self.calculate_actor_loss(states)\n            self.take_optimisation_step(self.actor_optimizer, self.actor_local, policy_loss,None)\n            self.actor_optimizer.step()\n\n            if self.automatic_entropy_tuning:\n                alpha_loss = self.calculate_entropy_tuning_loss(log_pi)\n            else: \n                alpha_loss = None\n            \n            \n            if alpha_loss is not None:\n                self.take_optimisation_step(self.alpha_optim, None, alpha_loss, None)\n                self.alpha_optim.step()\n#                 self.alpha = self.log_alpha.exp()\n\n#         return policy_loss, qf1_loss, qf2_loss\n            print(\"policy_loss = \" + str(policy_loss.item()) + \"| qf1_loss = \" + str(qf1_loss.item()) + \"| qf2_loss = \" + str(qf2_loss.item()))\n            return qf1_loss\n        return 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run Segment Game","metadata":{}},{"cell_type":"code","source":"# #### SEGMENT EXPERIMENTS #####\n\nenv = SegmentEnv(max_steps=10, window=1, segment_length=10, noise=0.0, free_location=False, exploration_cost=1, pred_reward=1) #exploration_cost=0.05\nenv.reset(NEXT=True)\n\nagent_random = Random_Agent(agent_name=\"random_agent\", env=env)\nagent_ddqn = DDQN(agent_name=\"ddqn_agent\", env=env, epsilon=0.99, epsilon_decay_rate=0.999, gamma=0.99, batch_size=256)\nagent_sacd = SAC_Discrete(agent_name=\"sacd_agent\", env=env, gamma=0.99, batch_size=256, automatic_entropy_tuning=False)\n\nagent_ddqn.runner(10000, training=True)\nagent_sacd.runner(10000, training=True)\nagent_random.runner(10000)\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = []\nresults.append(agent_random.list_rewards)\nresults.append(agent_ddqn.list_rewards)\nresults.append(agent_sacd.list_rewards)\n# results.append(agent_battleship_optimal.list_rewards)\nplot_results(results, rolling = 200, download=True, filename = \"segment\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def optimal_reward(segment_length, reward_pred, exploration_loss):\n#     state_value = np.zeros(segment_length + 1)\n#     state_value[1] = reward_pred\n#     for i in range(2, segment_length + 1):\n#         exploration_value = [state_value[k]*k/i + (1-k/i)*state_value[i-k] - exploration_loss for k in range(1, i)]\n#         state_value[i] = max(reward_pred/i, np.max(exploration_value))\n#     return state_value\n# s = optimal_reward(segment_length=5, reward_pred=1, exploration_loss=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run Battleship game","metadata":{}},{"cell_type":"code","source":"##### BATTLE SHIP EXPERIMENTS #####\n\nenv = BatteshipEnv(board_size_x=3, board_size_y=3, num_small_ships=1, num_medium_ships=1, num_large_ships=1, max_steps=6)\n\nagent_ddqn = DDQN(agent_name=\"ddqn_agent\", env=env, epsilon=0.99, epsilon_decay_rate=0.999, gamma=0.99, batch_size=256)\nagent_sacd = SAC_Discrete(agent_name=\"sacd_agent\", env=env, gamma=0.99, batch_size=256, automatic_entropy_tuning=False)\nagent_battleship_optimal = Battleship_Baseline(agent_name=\"battleship_optimal_agent\", env=env)\nagent_random = Random_Agent(agent_name=\"random_agent\", env=env)\n\nagent_ddqn.runner(10000, training=True)\nagent_sacd.runner(10000, training=True)\nagent_battleship_optimal.runner(10000)\nagent_random.runner(10000)\n\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = []\nresults.append(agent_random.list_rewards)\nresults.append(agent_ddqn.list_rewards)\nresults.append(agent_sacd.list_rewards)\nresults.append(agent_battleship_optimal.list_rewards)\nplot_results(results, rolling = 200, download=True, filename = \"battleship\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}