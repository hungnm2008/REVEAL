{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install nn_builder\n# !pip uninstall torch -y\n# !pip install torch==1.4.0\n\nimport numpy as np\nimport gym\nimport matplotlib.pyplot as plt\nfrom gym import spaces\n# from google.colab import files\nimport torch\nimport random\nfrom collections import deque\nimport pandas as pd\nfrom nn_builder.pytorch.CNN import CNN   \nfrom nn_builder.pytorch.RNN import RNN   \nimport pandas as pd\nimport seaborn as sns\nfrom torch.distributions import Categorical\nimport torch.nn.functional as F\n","execution_count":52,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: nn_builder in /opt/conda/lib/python3.7/site-packages (1.0.5)\nRequirement already satisfied: tensorflow==2.0.0a0 in /opt/conda/lib/python3.7/site-packages (from nn_builder) (2.0.0a0)\nRequirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.0.0a0->nn_builder) (0.8.1)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.0.0a0->nn_builder) (1.1.2)\nRequirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.0.0a0->nn_builder) (1.0.8)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.0.0a0->nn_builder) (1.1.0)\nRequirement already satisfied: google-pasta>=0.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.0.0a0->nn_builder) (0.2.0)\nRequirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.0.0a0->nn_builder) (1.32.0)\nRequirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.0.0a0->nn_builder) (1.14.0a20190301)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.0.0a0->nn_builder) (0.36.2)\nRequirement already satisfied: gast>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.0.0a0->nn_builder) (0.3.3)\nRequirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.0.0a0->nn_builder) (1.14.0.dev2019030115)\nRequirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.0.0a0->nn_builder) (0.10.0)\nRequirement already satisfied: numpy<2.0,>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.0.0a0->nn_builder) (1.19.5)\nRequirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.0.0a0->nn_builder) (3.14.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.0.0a0->nn_builder) (1.15.0)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow==2.0.0a0->nn_builder) (2.10.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0a0->nn_builder) (1.0.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0a0->nn_builder) (3.3.3)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0a0->nn_builder) (3.3.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0a0->nn_builder) (3.4.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0a0->nn_builder) (3.7.4.3)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# HELPER FUNCTIONS"},{"metadata":{"trusted":true},"cell_type":"code","source":"##### HELPER FUNCTIONS #####\ndef copy_model_over(from_model, to_model):\n        \"\"\"Copies model parameters from from_model to to_model\"\"\"\n        for to_model, from_model in zip(to_model.parameters(), from_model.parameters()):\n            to_model.data.copy_(from_model.data.clone())\n            \ndef plot_results(results, rolling, download=False):\n    \"\"\"\n    Plotting the results\n    :param list_rewards: list of cumulative rewards of all episodes\n    :param list_cumulative_steps: list of number of actions of all episodes\n    :param algo_name: the name of the learning algorithm\n    :param download_plots: \"True\" if this notebook is running on Google Colab and you want to save the figures to your computer, \"False\" otherwise\n    \"\"\"\n    pd_results = pd.DataFrame()\n    for r in results:\n        pd_results = pd.concat([pd_results, r], axis=1)\n        \n    list_agent_names = pd_results.columns.to_list()\n    pd_data = pd_results.reset_index()\n    bines = [(i) * rolling for i in list(range(101))]\n    bines_lab = [(i) * rolling for i in list(range(100))]\n    pd_data['binned_episode'] = pd.cut(pd_data['index'], bins=bines, labels=bines_lab)\n    pd_data = pd_data.fillna(0)\n    \n    for agent in list_agent_names:\n        sns.lineplot(data=pd_data, x=\"binned_episode\", y=agent, label=agent)\n   \n    plt.title('Battleship agents')\n    plt.xlabel('Episode')\n    plt.ylabel('Reward')","execution_count":53,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Batteship Environment"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BatteshipEnv(gym.Env):\n    def __init__(self, grid_size, num_small_ships, num_medium_ships, num_large_ships, max_steps):\n        \"\"\"\n        Initialize a Battleship environment\n        :param grid_size: With grid_size = n we have a board of n*n\n        :param ships_board: Given board with placed ships\n        :return BatteshipEnv object\n        \"\"\"\n        super(BatteshipEnv, self).__init__()\n        \n        self.grid_size = grid_size\n        self.EMPTY = 0 \n        self.UNKNOWN = 1\n        self.LAST_ACTION = 1\n        self.ships_board = torch.zeros(self.grid_size, self.grid_size)\n        self.nA = self.grid_size * self.grid_size # Size of action space n*n\n     \n        self.num_small_ships =  num_small_ships\n        self.num_medium_ships = num_medium_ships\n        self.num_large_ships = num_large_ships\n#         self.observation_space = spaces.Discrete(self.grid_size * self.grid_size) # Obeservation space\n#         self.action_space = spaces.Tuple((spaces.Discrete(self.grid_size),spaces.Discrete(self.grid_size))) # Action space\n#         self.nS = 2*self.grid_size * self.grid_size\n        \n        self.max_steps = max_steps\n        self.num_steps = 0\n            \n    def calculate_reward(self, action):\n        '''\n        Return reward for an action\n        :param action: action taken\n        :return reward \n        ''' \n        \n        x = action[0] # x coordinate of the action\n        y = action[1] # y coordinate of the action\n        \n        if (self.s[0,x,y] != self.UNKNOWN): #Already revealed\n            if (self.s[1,x,y] == self.EMPTY): #Punish for repeating hitting empty place\n                return torch.tensor(-1.0) \n            else: #Keep hitting the ship\n                self.num_hits = self.num_hits + 1\n                return torch.tensor(1.0) \n        else: #Not revealed\n            if (self.ships_board[x,y] == self.EMPTY): #Empty place\n                return torch.tensor(-1.0)\n            else: #Found a ship\n                self.num_hits = self.num_hits + 1\n                return torch.tensor(1.0)\n            \n    def update_board(self, action):\n        \"\"\"\n        Update the playing board\n        :param action: action taken\n        :return the current state after taking the action\n        \"\"\"\n        x = action[0] # x coordinate of the action\n        y = action[1] # y coordinate of the action\n        \n        if self.s[0,x,y] == self.UNKNOWN:\n            if self.ships_board[x,y] != self.EMPTY:\n                self.s[1,x,y] = self.ships_board[x,y] - 1\n            else:\n                self.s[1,x,y] = self.EMPTY\n        else:\n            self.s[1,x,y] = max(self.s[1,x,y]-1,0)\n            \n        self.s[0,x,y] = 1-self.UNKNOWN\n        \n        self.s[2] = (-1) * torch.ones(self.grid_size, self.grid_size)  \n        self.s[2, x, y] = 1\n        return self.s\n\n    def step(self, action):\n        \"\"\" \n          Take a step by executing an action\n          :param actionL action taken\n          :return next_state: the state after taking the action\n          :return reward: reward for taking action from the current state\n          :return done: Whether all the ships have been destroyed \n          :return info: none\n        \"\"\"\n        reward = self.calculate_reward(action)\n        next_state = self.update_board(action)\n        if self.num_hits >= self.total_hits:\n            done = torch.tensor(1)\n        else:\n            done = torch.tensor(0)\n        info = {}\n        return next_state, reward, done, info\n\n    def observe(self):\n        \"\"\"\n          Return the current state\n        \"\"\"\n        return self.s\n\n    def reset(self):\n        \"\"\"\n          Reset the environment\n          :return the initial playing board\n        \"\"\"\n        self.ships_board = self.EMPTY * torch.ones(self.grid_size, self.grid_size)\n        self.num_steps = 0\n        \n        self.s = torch.ones(3, self.grid_size, self.grid_size)  \n        self.s[0] = self.UNKNOWN * self.s[0]    # Revealed position\n        self.s[1] = self.EMPTY * self.s[1]      # Ship's health\n        self.s[2] = (-1) * self.s[2]            # Last action\n        \n        \n        num_small_ships = self.num_small_ships\n        num_medium_ships = self.num_medium_ships\n        num_large_ships = self.num_large_ships\n\n        while num_small_ships>0:\n            i = random.randint(0, self.grid_size-1)\n            j = random.randint(0, self.grid_size-1)\n            if self.ships_board[i, j] == 0:\n                self.ships_board[i, j] = 1\n                num_small_ships -= 1\n\n        while num_medium_ships>0:\n            i = random.randint(0, self.grid_size-1)\n            j = random.randint(0, self.grid_size-1)\n            if self.ships_board[i, j] == 0:\n                self.ships_board[i, j] = 2\n                num_medium_ships -= 1\n\n        while num_large_ships>0:\n            i = random.randint(0, self.grid_size-1)\n            j = random.randint(0, self.grid_size-1)\n            if self.ships_board[i, j] == 0:\n                self.ships_board[i, j] = 3\n                num_large_ships -= 1\n        \n#         self.ships_board[0, 0] = 3\n#         self.ships_board[2, 2] = 2\n        \n        self.total_hits = torch.sum(self.ships_board)\n        self.num_hits = 0\n        return self.s\n\n    def render(self, mode='human'):\n        \"\"\"\n          Render the current playing board\n        \"\"\"\n        for i in range(self.grid_size):\n          print(\"-------------------------------------------\")\n          line = \"\"\n          for j in range(self.grid_size):     \n            line += \" | \"\n            if self.s[i,j] == self.EMPTY:\n              line += \"O\"\n            elif self.s[i,j] == self.UNKNOWN:\n              line += \" \"\n            else:\n              line += str(int(self.s[i,j]))\n          line += \" | \"\n          print(line)","execution_count":54,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# REPLAY BUFFER"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Replay():\n    \"\"\"\n    Memory for storing experience \n    \"\"\"\n    def __init__(self):\n#         self.buffer = []\n        self.buffer = deque(maxlen=100000)\n\n    def __len__(self):\n        return len(self.buffer)\n\n    def add(self, state, action, reward, next_state, done):\n        \"\"\"\n        Add a new experience to the buffer\n        :param state: The state before taking the action\n        :param action: action taken\n        :param reward: Reward for taking that action\n        :param next_state: The state that the agent enters after taking the action\n        :param done: Whether the agent finishes the game\n        \"\"\"\n        self.buffer.append((state, action, reward, next_state, done))\n\n    def sample(self, batch_size):\n        \"\"\"\n          Return a batch of samples from the experience buffer\n          :param batch_size: The number of sample that you want to take\n          :return the batch of samples but decomposed into lists of states, actions, rewards, next_states \n        \"\"\"\n        states, actions, rewards, next_states, dones = [], [], [], [], []\n\n        # Random samples\n        samples = random.sample(self.buffer, batch_size)\n\n        for s in samples:\n            state = s[0]\n            action = s[1]\n            reward = s[2]\n            next_state = s[3]\n            done = s[4]\n            \n            states.append(state)\n            actions.append(action)\n            rewards.append(reward)\n            next_states.append(next_state)\n            dones.append(done)\n        \n        return states, actions, rewards, next_states, dones","execution_count":55,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BASE AGENT"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Base_Agent():\n    def __init__(self, agent_name, env):\n        self.agent_name = agent_name\n        self.env = env\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.list_rewards = pd.DataFrame (columns = [agent_name])\n    \n    def select_action(self, state):\n        pass\n    \n    def train():\n        pass\n    \n    def soft_update_of_target_network(self, local_model, target_model, tau):\n        \"\"\"Updates the target network in the direction of the local network but by taking a step size\n        less than one so the target network's parameter values trail the local networks. This helps stabilise training\"\"\"\n        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n    \n    def runner(self, number_of_episodes, training=False):\n        print(\"########## \" + self.agent_name + \" is running ##########\")\n        \n        for episode in range(number_of_episodes+1):\n            ep_reward = 0.0         # Reward for this episode\n            done = False          # Whether the game is finished\n            loss = 0.0\n            self.env.reset()\n\n            # Get the current state\n            state = self.env.observe().to(self.device, dtype=torch.float)\n            \n            while not done and self.env.num_steps<self.env.max_steps:\n                with torch.no_grad():\n                    self.env.num_steps = self.env.num_steps + 1\n                    # Get and execute the next action for the current state\n                    action = self.select_action(state)\n                    next_state, reward, done, info = env.step(action)\n                    ep_reward = ep_reward + reward.item()\n                    \n                # Start training once the size of the buffer greater than the batch size\n                if training:\n                    # Save what the agent just learnt to the experience buffer.\n                    self.buffer.add(state, action, reward, next_state, done)\n                    loss = loss + self.train()\n                    \n                state = next_state.to(self.device, dtype=torch.float)\n                \n            # Print log\n            print(f'Agent: {self.agent_name} . Episode {episode}/{number_of_episodes}. Number of steps to finish: {self.env.num_steps}. Loss: {loss} '\n                f'Reward: {ep_reward}')\n\n            self.list_rewards = self.list_rewards.append({self.agent_name:ep_reward}, ignore_index=True)\n\n        env.close()","execution_count":56,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RANDOM AGENT"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Random_Agent(Base_Agent):\n    \"\"\"\n    Model a random agent\n    \"\"\"\n    def __init__(self, agent_name, env):\n        super().__init__(agent_name, env)        \n\n    def select_action(self, state):\n        \"\"\"\n          Return an random action\n          :param state: the current state\n          :return action: next action to take\n        \"\"\"\n        random_action = torch.tensor([random.randint(0,self.env.grid_size-1) for i in range(2)])\n        return random_action","execution_count":57,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Battleship Baseline"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Battleship_Baseline(Base_Agent):\n    \"\"\"\n    Baseline for Battleship\n    \"\"\"\n    def __init__(self, agent_name, env):\n        super().__init__(agent_name, env)\n        self.last_action = None\n\n    def select_action(self, state):\n        \"\"\"\n          Return an random action\n          :param state: the current state\n          :return action: next action to take\n        \"\"\"\n      \n        x = random.randint(0,self.env.grid_size-1)\n        y = random.randint(0,self.env.grid_size-1)\n            \n        if self.last_action == None:\n            x = random.randint(0,self.env.grid_size-1)\n            y = random.randint(0,self.env.grid_size-1)\n            random_action = torch.tensor([x, y])\n            self.last_action = random_action\n            return random_action\n        else:\n            if state[1, self.last_action[0], self.last_action[1]] == self.env.EMPTY:\n                while state[0, x, y] != self.env.UNKNOWN and state[1, x, y] == self.env.EMPTY:\n                    x = random.randint(0,self.env.grid_size-1)\n                    y = random.randint(0,self.env.grid_size-1)\n                random_action = torch.tensor([x, y])\n                self.last_action = random_action\n                return random_action\n            else:\n                repeat_action = self.last_action\n                return repeat_action","execution_count":58,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DDQN AGENT"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DDQN(Base_Agent):\n    \"\"\"\n    Model a DQN agent\n    \"\"\"\n    def __init__(self, agent_name, env, epsilon, epsilon_decay_rate, gamma, batch_size):\n        super().__init__(agent_name, env)        \n        \n        self.epsilon = epsilon # For deciding exploitation or exploration\n        self.epsilon_decay_rate = epsilon_decay_rate # Epsilon is decayed after each episode with a fixed rate\n        self.gamma = gamma # The weight for future rewards\n        self.batch_size = batch_size\n        \n        self.buffer = Replay()             # Experience Buffer\n        \n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        \n        self.main_dqn = CNN(input_dim=(3, self.env.grid_size, self.env.grid_size), \n                            layers_info=[[\"conv\", 3, 2, 1, 1], \n                            [\"linear\", 64],\n                            [\"linear\", 8],\n                            [\"linear\", self.env.nA]],\n                            hidden_activations=\"relu\", \n#                             output_activation=\"softmax\", \n#                             dropout=0.5,\n                            initialiser=\"xavier\", \n                            batch_norm=False)\n#         self.main_dqn = RNN(input_dim=2, layers_info=[[\"lstm\", 16], [\"linear\", self.env.nA]],\n#             hidden_activations=\"relu\",\n#             batch_norm=False, dropout=0.0, initialiser=\"xavier\")\n\n\n        self.target_dqn = CNN(input_dim=(3, self.env.grid_size, self.env.grid_size), \n                            layers_info=[[\"conv\", 3, 2, 1, 1], \n                            [\"linear\", 64],\n                            [\"linear\", 8],\n                            [\"linear\", self.env.nA]],\n                            hidden_activations=\"relu\", \n#                             output_activation=\"softmax\", \n#                             dropout=0.5,\n                            initialiser=\"xavier\", \n                            batch_norm=False)\n    \n        # Send models to GPU\n        self.main_dqn.to(self.device)\n        self.target_dqn.to(self.device)\n        \n         # Optimizer and Loss function\n        self.optimizer = torch.optim.Adam(self.main_dqn.parameters(), lr=1e-4)\n        self.mse = torch.nn.MSELoss()\n        self.L1 = torch.nn.SmoothL1Loss()\n        \n#         self.list_number_of_actions = pd.DataFrame (columns = ['episode','length'])\n#         self.list_cumulative_loss = pd.DataFrame (columns = ['episode','loss'])\n#         self.list_number_of_actions.length = self.list_number_of_actions['length'].astype(float)\n#         self.list_cumulative_loss.length = self.list_cumulative_loss['loss'].astype(float)\n\n    def select_action(self, state):\n        \"\"\"\n          Return an action to take based on epsilon (greedy or random action)\n          :param state: the current state\n          :return action: next action to take\n        \"\"\"\n        random_number = np.random.uniform()\n        if random_number < self.epsilon:\n            # Random action\n            return torch.tensor([random.randint(0,self.env.grid_size-1) for i in range(2)])\n        else:\n            # Greedy action\n            state = [state]\n            state = torch.stack(state)\n            state = state.to(self.device, dtype=torch.float)\n            argmax = torch.argmax(self.main_dqn(state)).item()\n            return torch.tensor(np.unravel_index(argmax, (self.env.grid_size, self.env.grid_size)))\n       \n    def train(self):\n        \"\"\"\n        Train the network with a batch of samples\n        :param states: The state before taking the action\n        :param actions: action taken\n        :param rewards: Reward for taking that action\n        :param next_states: The state that the agent enters after taking the action\n        :return loss: the loss value after training the batch of samples\n        \"\"\"\n        if len(self.buffer) >= self.batch_size:\n            with torch.no_grad():\n                states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n                        \n            # Send data to GPU\n            states = torch.stack(states).to(self.device, dtype=torch.float) \n            actions = torch.stack(actions).to(self.device, dtype=torch.float)\n            rewards = torch.stack(rewards).to(self.device, dtype=torch.float) \n            next_states = torch.stack(next_states).to(self.device, dtype=torch.float)\n            dones = torch.stack(dones).to(self.device, dtype=torch.float)\n            ravel = torch.tensor([[self.env.grid_size*1.0], [1*1.0]], dtype=torch.float64).to(self.device, dtype=torch.float)\n            actions = torch.matmul(actions,ravel) # size([128, 1])    \n\n            # Calculate target Q values using the Target Network\n            selection = torch.argmax(self.main_dqn(next_states), dim = 1).unsqueeze(1)\n            evaluation = self.target_dqn(next_states).gather(1, selection.long()) #size [256,1]\n            evaluation = torch.reshape(evaluation, (-1,))\n\n            #Create Done mask\n            nonzero_indices = torch.nonzero(dones).reshape(-1).tolist()  \n            dones_mask = torch.eye(self.batch_size)\n            for index in nonzero_indices:\n                dones_mask[index,index] = 0\n            dones_mask = dones_mask.to(self.device, dtype=torch.float)\n\n            # Calculte target\n            target = rewards + torch.matmul(dones_mask, evaluation*self.gamma)\n            target = target.detach()\n\n            # Calculate Q values using the Main Network        \n            n_classes = self.env.grid_size*self.env.grid_size\n            n_samples = self.batch_size\n            labels = torch.flatten(actions.type(torch.LongTensor), start_dim=0)\n            labels_tensor = torch.as_tensor(labels)\n            action_masks = torch.nn.functional.one_hot(labels_tensor, num_classes=n_classes).to(self.device, dtype=torch.float)\n            q_value = torch.sum(action_masks*self.main_dqn(states), dim=1)\n\n\n            # Calculate loss\n            loss = self.mse(target, q_value)\n#             loss = self.L1(target, q_value)\n\n            # Optimize the model\n            self.optimizer.zero_grad()\n            loss.backward()\n#             for param in self.main_dqn.parameters():\n#                 # Clip the target to avoid exploding gradients\n#                 param.grad.data.clamp_(-1e-6,1e-6)\n            self.optimizer.step()\n            \n            # Soft Copy the Main Network's weights to the Target Network \n            self.soft_update_of_target_network(self.main_dqn, self.target_dqn,tau=1e-3)\n            \n            # Epsilon is decayed since the agent is getting more and more knowledge\n            self.epsilon = self.epsilon * self.epsilon_decay_rate\n            print(\"Epsilon =\", self.epsilon)\n\n            return loss\n        return 0","execution_count":59,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SACD AGENT"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n            \nclass SAC_Discrete(Base_Agent):\n    '''\n    Soft Actor Critic for discrete action space\n    '''\n    def __init__(self, agent_name, env, gamma, batch_size, automatic_entropy_tuning):\n        super().__init__(agent_name, env)        \n       \n        self.gamma = gamma # The weight for future rewards\n        self.batch_size = batch_size\n        self.automatic_entropy_tuning = automatic_entropy_tuning\n        self.buffer = Replay()\n        self.L1loss = torch.nn.SmoothL1Loss()\n        self.mse = torch.nn.MSELoss()\n        \n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        \n        if self.automatic_entropy_tuning:\n            # we set the max possible entropy as the target entropy\n            self.target_entropy = -np.log((1.0 / self.env.nA)) * 0.98\n            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n            self.alpha = self.log_alpha.detach().clone().exp()\n            self.alpha_optim = torch.optim.Adam([self.log_alpha], lr=1e-4, eps=1e-4)\n        else:\n            self.alpha = 0.2\n            self.add_extra_noise = False\n            self.do_evaluation_iterations = False\n        \n        \n        self.critic_local = CNN(input_dim=(3, self.env.grid_size, self.env.grid_size), layers_info=[[\"conv\", 3, 2, 1, 1], \n                            [\"linear\", 64],\n                            [\"linear\", 8],\n                            [\"linear\", self.env.nA]],\n                            hidden_activations=\"relu\", \n#                             output_activation=\"softmax\", dropout=0.0,\n                            initialiser=\"xavier\", \n                            batch_norm=False)\n        self.critic_target = CNN(input_dim=(3, self.env.grid_size, self.env.grid_size), layers_info=[[\"conv\", 3, 2, 1, 1], \n                            [\"linear\", 64],\n                            [\"linear\", 8],\n                            [\"linear\", self.env.nA]],\n                            hidden_activations=\"relu\", \n#                             output_activation=\"softmax\", dropout=0.0,\n                            initialiser=\"xavier\", \n                            batch_norm=False)\n        self.critic_local_2 = CNN(input_dim=(3, self.env.grid_size, self.env.grid_size), layers_info=[[\"conv\", 3, 2, 1, 1], \n                            [\"linear\", 64],\n                            [\"linear\", 8],\n                            [\"linear\", self.env.nA]],\n                            hidden_activations=\"relu\", \n#                             output_activation=\"softmax\", dropout=0.0,\n                            initialiser=\"xavier\", \n                            batch_norm=False)\n        self.critic_target_2 = CNN(input_dim=(3, self.env.grid_size, self.env.grid_size), layers_info=[[\"conv\", 3, 2, 1, 1], \n                            [\"linear\", 64],\n                            [\"linear\", 8],\n                            [\"linear\", self.env.nA]],\n                            hidden_activations=\"relu\", \n#                             output_activation=\"softmax\", dropout=0.0,\n                            initialiser=\"xavier\", \n                            batch_norm=False)\n        self.actor_local = CNN(input_dim=(3, self.env.grid_size, self.env.grid_size), layers_info=[[\"conv\", 3, 2, 1, 1], \n                            [\"linear\", 64],\n                            [\"linear\", 8],\n                            [\"linear\", self.env.nA]],\n                            hidden_activations=\"relu\", \n                            output_activation=\"softmax\", \n#                             dropout=0.0,\n                            initialiser=\"xavier\", \n                            batch_norm=False)\n        \n        self.critic_local.to(self.device)\n        self.critic_target.to(self.device)\n        self.critic_local_2.to(self.device)\n        self.critic_target_2.to(self.device)\n        self.actor_local.to(self.device)\n        \n        self.actor_optimizer = torch.optim.Adam(self.actor_local.parameters(), lr=1e-4)\n        self.critic_optimizer = torch.optim.Adam(self.critic_local.parameters(), lr=1e-4)\n        self.critic_optimizer_2 = torch.optim.Adam(self.critic_local_2.parameters(), lr=1e-4)\n        \n        copy_model_over(self.critic_local, self.critic_target)\n        copy_model_over(self.critic_local_2, self.critic_target_2)\n\n    def create_actor_distribution(self, action_probabilities):\n        \"\"\"Creates a distribution that the actor can then use to randomly draw actions\"\"\"\n        \n        action_distribution = Categorical(action_probabilities)  # this creates a distribution to sample from\n        return action_distribution\n\n\n    def produce_action_and_action_info(self, state):\n        '''\n        Given the state, produces an action, the probability of the action, the log probability of the action, and\n        the argmax action\n        '''\n        action_probabilities = self.actor_local(state)\n        max_probability_action = torch.argmax(action_probabilities, dim=-1)\n        action_distribution = self.create_actor_distribution(action_probabilities)\n        \n        action = action_distribution.sample().cpu()\n        # Have to deal with situation of 0.0 probabilities because we can't do log 0\n        z = action_probabilities == 0.0\n        z = z.float() * 1e-8\n        log_action_probabilities = torch.log(action_probabilities + z)\n        return action, (action_probabilities, log_action_probabilities), max_probability_action\n\n    def calculate_critic_losses(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch):\n        '''\n        Calculates the losses for the two critics. This is the ordinary Q-learning loss except the additional entropy\n        term is taken into account\n        '''\n        with torch.no_grad():\n            ravel = torch.tensor([[self.env.grid_size*1.0], [1*1.0]], dtype=torch.float64).to(self.device, dtype=torch.float)\n            action_batch = torch.matmul(action_batch,ravel)\n            \n            next_state_action, (action_probabilities, log_action_probabilities), _ = self.produce_action_and_action_info(next_state_batch)\n            \n            qf1_next_target = self.critic_target(next_state_batch)\n            qf2_next_target = self.critic_target_2(next_state_batch)\n           \n            min_qf_next_target = action_probabilities * (torch.min(qf1_next_target, qf2_next_target) - self.alpha * log_action_probabilities)\n            min_qf_next_target = min_qf_next_target.sum(dim=1).unsqueeze(-1)\n            \n            next_q_value = reward_batch + torch.matmul(done_batch, self.gamma * min_qf_next_target)\n\n        \n        qf1 = self.critic_local(state_batch).gather(1, action_batch.long())\n        qf2 = self.critic_local_2(state_batch).gather(1, action_batch.long())\n\n        qf1_loss = self.mse(qf1, next_q_value)\n        qf2_loss = self.mse(qf2, next_q_value)\n\n        return qf1_loss, qf2_loss\n\n    def calculate_actor_loss(self, state_batch):\n        '''\n        Calculates the loss for the actor. This loss includes the additional entropy term\n        '''\n        action, (action_probabilities, log_action_probabilities), _ = self.produce_action_and_action_info(state_batch)\n        qf1_pi = self.critic_local(state_batch)\n        qf2_pi = self.critic_local_2(state_batch)\n        min_qf_pi = torch.min(qf1_pi, qf2_pi)\n        \n        inside_term = self.alpha * log_action_probabilities - min_qf_pi\n        log_action_probabilities = torch.sum(log_action_probabilities * action_probabilities, dim=1)\n        policy_loss = (action_probabilities * inside_term).sum(dim=1).mean()\n        return policy_loss, log_action_probabilities\n\n    def select_action(self, state):\n        state = [state]\n        state = torch.stack(state)\n        state = state.to(self.device, dtype=torch.float)\n        action, _, _ = self.produce_action_and_action_info(state)\n        action = action.detach().clone()\n        return torch.tensor(np.unravel_index(action.item(), (self.env.grid_size, self.env.grid_size)))\n\n    def take_optimisation_step(self, optimizer, network, loss, clipping_norm=None, retain_graph=False):\n        \"\"\"Takes an optimisation step by calculating gradients given the loss and then updating the parameters\"\"\"\n        if not isinstance(network, list): network = [network]\n        with torch.autograd.set_detect_anomaly(True):\n            optimizer.zero_grad()\n            loss.backward(retain_graph=retain_graph)\n            if clipping_norm is not None:\n                for net in network:\n                    torch.nn.utils.clip_grad_norm_(net.parameters(), clipping_norm) #clip gradients to help stabilise training\n#             optimizer.step()\n\n    def calculate_entropy_tuning_loss(self, log_pi):\n        \"\"\"Calculates the loss for the entropy temperature parameter. This is only relevant if self.automatic_entropy_tuning\n        is True.\"\"\"\n        alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n        return alpha_loss\n\n    def train(self):\n        \"\"\"\n        Train the network with a batch of samples\n        :param states: The state before taking the action\n        :param actions: action taken\n        :param rewards: Reward for taking that action\n        :param next_states: The state that the agent enters after taking the action\n        :return loss: the loss value after training the batch of samples\n        \"\"\"\n        if len(self.buffer) >= self.batch_size:\n            with torch.no_grad():\n                states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n                states = torch.stack(states).to(self.device, dtype=torch.float)\n                actions = torch.stack(actions).to(self.device, dtype=torch.float)\n                rewards = torch.stack(rewards).to(self.device, dtype=torch.float)\n                next_states = torch.stack(next_states).to(self.device, dtype=torch.float)\n                rewards = torch.reshape(rewards, (self.batch_size, 1))\n\n                dones = torch.stack(dones).to(self.device, dtype=torch.long)\n                nonzero_indices = torch.nonzero(dones).reshape(-1).tolist()  \n                dones_mask = torch.eye(self.batch_size)\n                for index in nonzero_indices:\n                    dones_mask[index,index] = 0\n                dones_mask = dones_mask.to(self.device, dtype=torch.float)\n        \n            #Updates the parameters for both critics\n            qf1_loss, qf2_loss = self.calculate_critic_losses(states, actions, rewards, next_states, dones_mask)\n            self.take_optimisation_step(self.critic_optimizer, self.critic_local, qf1_loss,None)\n            self.take_optimisation_step(self.critic_optimizer_2, self.critic_local_2, qf2_loss,None)\n            self.critic_optimizer.step()\n            self.critic_optimizer_2.step()\n            self.soft_update_of_target_network(self.critic_local, self.critic_target,tau=1e-3)\n            self.soft_update_of_target_network(self.critic_local_2, self.critic_target_2,tau=1e-3)\n\n            # update_actor_parameters\n            policy_loss, log_pi = self.calculate_actor_loss(states)\n            self.take_optimisation_step(self.actor_optimizer, self.actor_local, policy_loss,None)\n            self.actor_optimizer.step()\n\n            if self.automatic_entropy_tuning: \n                alpha_loss = self.calculate_entropy_tuning_loss(log_pi)\n            else: \n                alpha_loss = None\n\n            if alpha_loss is not None:\n                self.take_optimisation_step(self.alpha_optim, None, alpha_loss, None)\n                self.alpha = self.log_alpha.detach().clone().exp()\n\n#         return policy_loss, qf1_loss, qf2_loss\n            return qf1_loss\n        return 0","execution_count":60,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RUN EXPERIMENTS"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = BatteshipEnv(grid_size=3, num_small_ships=1, num_medium_ships=1, num_large_ships=1, max_steps=20)\n\nagent_random = Random_Agent(agent_name=\"random_agent\", env=env)\nagent_ddqn = DDQN(agent_name=\"ddqn_agent\", env=env, epsilon=0.99, epsilon_decay_rate=0.9999, gamma=0.99, batch_size=256)\nagent_sacd = SAC_Discrete(agent_name=\"sacd_agent\", env=env, gamma=0.99, batch_size=256, automatic_entropy_tuning=True)\nagent_battleship_baseline = Battleship_Baseline(agent_name=\"battleship_baseline\", env=env)\n\nagent_ddqn.runner(2000, training=True)\nagent_sacd.runner(2000, training=True)\nagent_battleship_baseline.runner(2000)\nagent_random.runner(2000)\n\n","execution_count":61,"outputs":[{"output_type":"stream","text":"########## ddqn_agent is running ##########\nAgent: ddqn_agent . Episode 0/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: ddqn_agent . Episode 1/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: ddqn_agent . Episode 2/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: ddqn_agent . Episode 3/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: ddqn_agent . Episode 4/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: ddqn_agent . Episode 5/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: ddqn_agent . Episode 6/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: ddqn_agent . Episode 7/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: ddqn_agent . Episode 8/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: ddqn_agent . Episode 9/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: ddqn_agent . Episode 10/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: ddqn_agent . Episode 11/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: ddqn_agent . Episode 12/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nEpsilon = 0.989901\nEpsilon = 0.9898020099\nEpsilon = 0.98970302969901\nEpsilon = 0.9896040593960401\nEpsilon = 0.9895050989901005\nEpsilon = 0.9894061484802015\nEpsilon = 0.9893072078653534\nEpsilon = 0.9892082771445669\nEpsilon = 0.9891093563168525\nEpsilon = 0.9890104453812208\nEpsilon = 0.9889115443366827\nEpsilon = 0.9888126531822491\nEpsilon = 0.9887137719169309\nEpsilon = 0.9886149005397392\nEpsilon = 0.9885160390496852\nAgent: ddqn_agent . Episode 13/2000. Number of steps to finish: 20. Loss: 12.182026863098145 Reward: -12.0\nEpsilon = 0.9884171874457802\nEpsilon = 0.9883183457270357\nEpsilon = 0.988219513892463\nEpsilon = 0.9881206919410738\nEpsilon = 0.9880218798718797\nEpsilon = 0.9879230776838925\nEpsilon = 0.9878242853761242\nEpsilon = 0.9877255029475867\nEpsilon = 0.9876267303972919\nEpsilon = 0.9875279677242521\nEpsilon = 0.9874292149274797\nEpsilon = 0.987330472005987\nEpsilon = 0.9872317389587865\nEpsilon = 0.9871330157848905\nEpsilon = 0.987034302483312\nEpsilon = 0.9869355990530637\nEpsilon = 0.9868369054931584\nEpsilon = 0.9867382218026091\nEpsilon = 0.9866395479804289\nEpsilon = 0.9865408840256308\nAgent: ddqn_agent . Episode 14/2000. Number of steps to finish: 20. Loss: 15.884140968322754 Reward: -12.0\nEpsilon = 0.9864422299372282\nEpsilon = 0.9863435857142345\nEpsilon = 0.9862449513556631\nEpsilon = 0.9861463268605276\nEpsilon = 0.9860477122278416\nEpsilon = 0.9859491074566188\nEpsilon = 0.9858505125458732\nEpsilon = 0.9857519274946186\nEpsilon = 0.9856533523018691\nEpsilon = 0.9855547869666389\nEpsilon = 0.9854562314879423\nEpsilon = 0.9853576858647934\nEpsilon = 0.985259150096207\nEpsilon = 0.9851606241811974\nEpsilon = 0.9850621081187794\nEpsilon = 0.9849636019079675\nEpsilon = 0.9848651055477767\nEpsilon = 0.984766619037222\nEpsilon = 0.9846681423753183\nEpsilon = 0.9845696755610808\nAgent: ddqn_agent . Episode 15/2000. Number of steps to finish: 20. Loss: 15.975727081298828 Reward: -10.0\nEpsilon = 0.9844712185935247\nEpsilon = 0.9843727714716654\nEpsilon = 0.9842743341945182\nEpsilon = 0.9841759067610987\nEpsilon = 0.9840774891704226\nEpsilon = 0.9839790814215056\nEpsilon = 0.9838806835133634\nEpsilon = 0.9837822954450122\nEpsilon = 0.9836839172154677\nEpsilon = 0.9835855488237462\nEpsilon = 0.9834871902688638\nEpsilon = 0.9833888415498369\nEpsilon = 0.9832905026656819\nEpsilon = 0.9831921736154153\nEpsilon = 0.9830938543980539\nEpsilon = 0.9829955450126141\nEpsilon = 0.9828972454581129\nEpsilon = 0.982798955733567\nEpsilon = 0.9827006758379937\nEpsilon = 0.9826024057704099\nAgent: ddqn_agent . Episode 16/2000. Number of steps to finish: 20. Loss: 15.735519409179688 Reward: -10.0\nEpsilon = 0.9825041455298329\nEpsilon = 0.9824058951152799\nEpsilon = 0.9823076545257684\nEpsilon = 0.9822094237603158\nEpsilon = 0.9821112028179398\nEpsilon = 0.9820129916976581\nEpsilon = 0.9819147903984884\nEpsilon = 0.9818165989194485\nEpsilon = 0.9817184172595566\nEpsilon = 0.9816202454178307\nEpsilon = 0.9815220833932888\nEpsilon = 0.9814239311849495\nEpsilon = 0.9813257887918311\nEpsilon = 0.9812276562129519\nEpsilon = 0.9811295334473307\nEpsilon = 0.981031420493986\nEpsilon = 0.9809333173519366\nEpsilon = 0.9808352240202014\nEpsilon = 0.9807371404977994\nEpsilon = 0.9806390667837496\nAgent: ddqn_agent . Episode 17/2000. Number of steps to finish: 20. Loss: 15.44227409362793 Reward: -14.0\nEpsilon = 0.9805410028770712\nEpsilon = 0.9804429487767835\nEpsilon = 0.9803449044819058\nEpsilon = 0.9802468699914576\nEpsilon = 0.9801488453044584\nEpsilon = 0.980050830419928\nEpsilon = 0.979952825336886\nEpsilon = 0.9798548300543524\nEpsilon = 0.9797568445713469\nEpsilon = 0.9796588688868898\nEpsilon = 0.9795609030000011\nEpsilon = 0.9794629469097011\nEpsilon = 0.9793650006150102\nEpsilon = 0.9792670641149487\nEpsilon = 0.9791691374085372\nEpsilon = 0.9790712204947963\nEpsilon = 0.9789733133727468\nEpsilon = 0.9788754160414096\nEpsilon = 0.9787775284998055\nEpsilon = 0.9786796507469555\nAgent: ddqn_agent . Episode 18/2000. Number of steps to finish: 20. Loss: 14.920522689819336 Reward: -14.0\nEpsilon = 0.9785817827818808\nEpsilon = 0.9784839246036026\nEpsilon = 0.9783860762111423\nEpsilon = 0.9782882376035211\nEpsilon = 0.9781904087797608\nEpsilon = 0.9780925897388829\nEpsilon = 0.9779947804799091\nEpsilon = 0.9778969810018611\nEpsilon = 0.9777991913037609\nEpsilon = 0.9777014113846305\nEpsilon = 0.9776036412434921\nEpsilon = 0.9775058808793677\nEpsilon = 0.9774081302912798\nEpsilon = 0.9773103894782507\nEpsilon = 0.9772126584393028\nAgent: ddqn_agent . Episode 19/2000. Number of steps to finish: 15. Loss: 11.111217498779297 Reward: -3.0\nEpsilon = 0.9771149371734589\nEpsilon = 0.9770172256797416\nEpsilon = 0.9769195239571736\nEpsilon = 0.9768218320047779\nEpsilon = 0.9767241498215774\nEpsilon = 0.9766264774065953\nEpsilon = 0.9765288147588546\nEpsilon = 0.9764311618773787\nEpsilon = 0.9763335187611909\nEpsilon = 0.9762358854093148\nEpsilon = 0.9761382618207739\nEpsilon = 0.9760406479945918\nEpsilon = 0.9759430439297924\nEpsilon = 0.9758454496253994\nEpsilon = 0.9757478650804369\nEpsilon = 0.9756502902939288\nEpsilon = 0.9755527252648994\nEpsilon = 0.9754551699923729\nEpsilon = 0.9753576244753737\nEpsilon = 0.9752600887129261\nAgent: ddqn_agent . Episode 20/2000. Number of steps to finish: 20. Loss: 14.507153511047363 Reward: -14.0\nEpsilon = 0.9751625627040549\nEpsilon = 0.9750650464477845\nEpsilon = 0.9749675399431397\nEpsilon = 0.9748700431891454\nEpsilon = 0.9747725561848265\nEpsilon = 0.974675078929208\nEpsilon = 0.9745776114213152\nEpsilon = 0.974480153660173\nEpsilon = 0.974382705644807\nEpsilon = 0.9742852673742425\nEpsilon = 0.974187838847505\nAgent: ddqn_agent . Episode 21/2000. Number of steps to finish: 11. Loss: 7.921102523803711 Reward: 1.0\nEpsilon = 0.9740904200636203\nEpsilon = 0.973993011021614\nEpsilon = 0.9738956117205118\nEpsilon = 0.9737982221593398\nEpsilon = 0.9737008423371238\nEpsilon = 0.9736034722528901\nEpsilon = 0.9735061119056648\nEpsilon = 0.9734087612944743\nEpsilon = 0.9733114204183448\nEpsilon = 0.973214089276303\nEpsilon = 0.9731167678673754\nEpsilon = 0.9730194561905887\nEpsilon = 0.9729221542449696\nEpsilon = 0.9728248620295451\nEpsilon = 0.9727275795433421\nEpsilon = 0.9726303067853878\nEpsilon = 0.9725330437547093\nEpsilon = 0.9724357904503338\nEpsilon = 0.9723385468712887\nEpsilon = 0.9722413130166017\nAgent: ddqn_agent . Episode 22/2000. Number of steps to finish: 20. Loss: 14.638446807861328 Reward: -10.0\nEpsilon = 0.9721440888853\nEpsilon = 0.9720468744764115\nEpsilon = 0.9719496697889638\nEpsilon = 0.9718524748219849\nEpsilon = 0.9717552895745027\nEpsilon = 0.9716581140455453\nEpsilon = 0.9715609482341407\nEpsilon = 0.9714637921393173\nEpsilon = 0.9713666457601035\nEpsilon = 0.9712695090955275\nEpsilon = 0.9711723821446179\nEpsilon = 0.9710752649064034\nEpsilon = 0.9709781573799128\nEpsilon = 0.9708810595641748\nEpsilon = 0.9707839714582184\nEpsilon = 0.9706868930610726\nEpsilon = 0.9705898243717664\nEpsilon = 0.9704927653893293\nEpsilon = 0.9703957161127903\nEpsilon = 0.9702986765411791\nAgent: ddqn_agent . Episode 23/2000. Number of steps to finish: 20. Loss: 14.469564437866211 Reward: -10.0\nEpsilon = 0.9702016466735249\nEpsilon = 0.9701046265088575\nEpsilon = 0.9700076160462067\nEpsilon = 0.9699106152846021\nEpsilon = 0.9698136242230736\nEpsilon = 0.9697166428606513\nEpsilon = 0.9696196711963653\nEpsilon = 0.9695227092292457\nEpsilon = 0.9694257569583228\nEpsilon = 0.969328814382627\nEpsilon = 0.9692318815011887\nEpsilon = 0.9691349583130385\nEpsilon = 0.9690380448172072\nEpsilon = 0.9689411410127255\nEpsilon = 0.9688442468986243\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.9687473624739344\nEpsilon = 0.968650487737687\nEpsilon = 0.9685536226889133\nEpsilon = 0.9684567673266444\nEpsilon = 0.9683599216499118\nAgent: ddqn_agent . Episode 24/2000. Number of steps to finish: 20. Loss: 14.11107063293457 Reward: -10.0\nEpsilon = 0.9682630856577468\nEpsilon = 0.968166259349181\nEpsilon = 0.9680694427232461\nEpsilon = 0.9679726357789739\nEpsilon = 0.9678758385153959\nEpsilon = 0.9677790509315444\nEpsilon = 0.9676822730264513\nEpsilon = 0.9675855047991486\nEpsilon = 0.9674887462486688\nEpsilon = 0.9673919973740439\nEpsilon = 0.9672952581743065\nEpsilon = 0.9671985286484891\nEpsilon = 0.9671018087956242\nEpsilon = 0.9670050986147447\nEpsilon = 0.9669083981048833\nEpsilon = 0.9668117072650728\nEpsilon = 0.9667150260943463\nEpsilon = 0.966618354591737\nEpsilon = 0.9665216927562778\nEpsilon = 0.9664250405870022\nAgent: ddqn_agent . Episode 25/2000. Number of steps to finish: 20. Loss: 14.177722930908203 Reward: -10.0\nEpsilon = 0.9663283980829435\nEpsilon = 0.9662317652431351\nEpsilon = 0.9661351420666109\nEpsilon = 0.9660385285524042\nEpsilon = 0.965941924699549\nEpsilon = 0.965845330507079\nEpsilon = 0.9657487459740283\nEpsilon = 0.9656521710994309\nEpsilon = 0.965555605882321\nEpsilon = 0.9654590503217328\nEpsilon = 0.9653625044167007\nEpsilon = 0.965265968166259\nEpsilon = 0.9651694415694424\nEpsilon = 0.9650729246252855\nEpsilon = 0.964976417332823\nEpsilon = 0.9648799196910898\nEpsilon = 0.9647834316991206\nEpsilon = 0.9646869533559507\nEpsilon = 0.964590484660615\nEpsilon = 0.964494025612149\nAgent: ddqn_agent . Episode 26/2000. Number of steps to finish: 20. Loss: 14.159937858581543 Reward: -12.0\nEpsilon = 0.9643975762095879\nEpsilon = 0.9643011364519669\nEpsilon = 0.9642047063383217\nEpsilon = 0.9641082858676878\nEpsilon = 0.964011875039101\nEpsilon = 0.9639154738515971\nEpsilon = 0.963819082304212\nEpsilon = 0.9637227003959816\nEpsilon = 0.9636263281259421\nEpsilon = 0.9635299654931295\nEpsilon = 0.9634336124965802\nEpsilon = 0.9633372691353306\nEpsilon = 0.963240935408417\nEpsilon = 0.9631446113148762\nEpsilon = 0.9630482968537447\nAgent: ddqn_agent . Episode 27/2000. Number of steps to finish: 15. Loss: 10.56729793548584 Reward: -3.0\nEpsilon = 0.9629519920240593\nEpsilon = 0.9628556968248569\nEpsilon = 0.9627594112551745\nEpsilon = 0.962663135314049\nEpsilon = 0.9625668690005176\nEpsilon = 0.9624706123136175\nEpsilon = 0.9623743652523862\nEpsilon = 0.962278127815861\nEpsilon = 0.9621819000030793\nEpsilon = 0.9620856818130791\nEpsilon = 0.9619894732448978\nEpsilon = 0.9618932742975733\nEpsilon = 0.9617970849701436\nEpsilon = 0.9617009052616466\nEpsilon = 0.9616047351711204\nEpsilon = 0.9615085746976033\nEpsilon = 0.9614124238401336\nEpsilon = 0.9613162825977496\nEpsilon = 0.9612201509694899\nEpsilon = 0.961124028954393\nAgent: ddqn_agent . Episode 28/2000. Number of steps to finish: 20. Loss: 14.185012817382812 Reward: -14.0\nEpsilon = 0.9610279165514976\nEpsilon = 0.9609318137598424\nEpsilon = 0.9608357205784664\nEpsilon = 0.9607396370064086\nEpsilon = 0.960643563042708\nEpsilon = 0.9605474986864038\nEpsilon = 0.9604514439365351\nEpsilon = 0.9603553987921415\nEpsilon = 0.9602593632522622\nEpsilon = 0.9601633373159371\nEpsilon = 0.9600673209822055\nEpsilon = 0.9599713142501073\nEpsilon = 0.9598753171186822\nEpsilon = 0.9597793295869704\nEpsilon = 0.9596833516540118\nEpsilon = 0.9595873833188464\nEpsilon = 0.9594914245805145\nEpsilon = 0.9593954754380565\nEpsilon = 0.9592995358905126\nEpsilon = 0.9592036059369236\nAgent: ddqn_agent . Episode 29/2000. Number of steps to finish: 20. Loss: 14.28493881225586 Reward: -10.0\nEpsilon = 0.9591076855763299\nEpsilon = 0.9590117748077722\nEpsilon = 0.9589158736302914\nEpsilon = 0.9588199820429284\nEpsilon = 0.9587241000447241\nEpsilon = 0.9586282276347197\nEpsilon = 0.9585323648119563\nEpsilon = 0.958436511575475\nEpsilon = 0.9583406679243175\nEpsilon = 0.9582448338575251\nEpsilon = 0.9581490093741394\nEpsilon = 0.958053194473202\nAgent: ddqn_agent . Episode 30/2000. Number of steps to finish: 12. Loss: 8.532219886779785 Reward: 0.0\nEpsilon = 0.9579573891537547\nEpsilon = 0.9578615934148393\nEpsilon = 0.9577658072554979\nEpsilon = 0.9576700306747723\nEpsilon = 0.9575742636717048\nEpsilon = 0.9574785062453377\nEpsilon = 0.9573827583947132\nEpsilon = 0.9572870201188738\nEpsilon = 0.9571912914168619\nEpsilon = 0.9570955722877202\nEpsilon = 0.9569998627304915\nEpsilon = 0.9569041627442185\nEpsilon = 0.9568084723279441\nEpsilon = 0.9567127914807113\nEpsilon = 0.9566171202015632\nEpsilon = 0.9565214584895431\nEpsilon = 0.9564258063436941\nEpsilon = 0.9563301637630598\nEpsilon = 0.9562345307466835\nEpsilon = 0.9561389072936088\nAgent: ddqn_agent . Episode 31/2000. Number of steps to finish: 20. Loss: 14.541019439697266 Reward: -12.0\nEpsilon = 0.9560432934028795\nEpsilon = 0.9559476890735392\nEpsilon = 0.9558520943046319\nEpsilon = 0.9557565090952015\nEpsilon = 0.9556609334442919\nEpsilon = 0.9555653673509475\nEpsilon = 0.9554698108142125\nEpsilon = 0.9553742638331311\nEpsilon = 0.9552787264067477\nEpsilon = 0.955183198534107\nEpsilon = 0.9550876802142536\nEpsilon = 0.9549921714462322\nEpsilon = 0.9548966722290876\nEpsilon = 0.9548011825618646\nEpsilon = 0.9547057024436084\nEpsilon = 0.954610231873364\nEpsilon = 0.9545147708501767\nEpsilon = 0.9544193193730917\nEpsilon = 0.9543238774411543\nEpsilon = 0.9542284450534102\nAgent: ddqn_agent . Episode 32/2000. Number of steps to finish: 20. Loss: 14.457463264465332 Reward: -10.0\nEpsilon = 0.9541330222089048\nEpsilon = 0.954037608906684\nEpsilon = 0.9539422051457933\nEpsilon = 0.9538468109252788\nEpsilon = 0.9537514262441863\nEpsilon = 0.9536560511015619\nEpsilon = 0.9535606854964518\nEpsilon = 0.9534653294279022\nEpsilon = 0.9533699828949594\nEpsilon = 0.95327464589667\nEpsilon = 0.9531793184320804\nEpsilon = 0.9530840005002371\nEpsilon = 0.9529886921001871\nEpsilon = 0.9528933932309771\nEpsilon = 0.952798103891654\nEpsilon = 0.9527028240812648\nEpsilon = 0.9526075537988568\nEpsilon = 0.9525122930434768\nEpsilon = 0.9524170418141725\nEpsilon = 0.9523218001099911\nAgent: ddqn_agent . Episode 33/2000. Number of steps to finish: 20. Loss: 15.2019681930542 Reward: -12.0\nEpsilon = 0.9522265679299802\nEpsilon = 0.9521313452731872\nEpsilon = 0.9520361321386599\nEpsilon = 0.9519409285254461\nEpsilon = 0.9518457344325936\nEpsilon = 0.9517505498591503\nEpsilon = 0.9516553748041644\nEpsilon = 0.9515602092666839\nEpsilon = 0.9514650532457573\nEpsilon = 0.9513699067404326\nEpsilon = 0.9512747697497587\nEpsilon = 0.9511796422727837\nEpsilon = 0.9510845243085565\nEpsilon = 0.9509894158561256\nEpsilon = 0.9508943169145401\nEpsilon = 0.9507992274828486\nEpsilon = 0.9507041475601004\nEpsilon = 0.9506090771453444\nEpsilon = 0.9505140162376299\nEpsilon = 0.9504189648360061\nAgent: ddqn_agent . Episode 34/2000. Number of steps to finish: 20. Loss: 15.904247283935547 Reward: -10.0\nEpsilon = 0.9503239229395225\nEpsilon = 0.9502288905472286\nEpsilon = 0.9501338676581739\nEpsilon = 0.9500388542714081\nEpsilon = 0.949943850385981\nEpsilon = 0.9498488560009424\nEpsilon = 0.9497538711153423\nEpsilon = 0.9496588957282307\nEpsilon = 0.9495639298386579\nEpsilon = 0.949468973445674\nEpsilon = 0.9493740265483295\nEpsilon = 0.9492790891456746\nEpsilon = 0.9491841612367601\nEpsilon = 0.9490892428206364\nEpsilon = 0.9489943338963543\nEpsilon = 0.9488994344629648\nEpsilon = 0.9488045445195185\nEpsilon = 0.9487096640650665\nEpsilon = 0.94861479309866\nEpsilon = 0.9485199316193502\nAgent: ddqn_agent . Episode 35/2000. Number of steps to finish: 20. Loss: 15.682250022888184 Reward: -10.0\nEpsilon = 0.9484250796261883\nEpsilon = 0.9483302371182256\nEpsilon = 0.9482354040945138\nEpsilon = 0.9481405805541043\nEpsilon = 0.9480457664960489\nEpsilon = 0.9479509619193993\nEpsilon = 0.9478561668232074\nEpsilon = 0.9477613812065251\nEpsilon = 0.9476666050684045\nEpsilon = 0.9475718384078977\nEpsilon = 0.9474770812240569\nEpsilon = 0.9473823335159346\nEpsilon = 0.947287595282583\nEpsilon = 0.9471928665230548\nEpsilon = 0.9470981472364025\nEpsilon = 0.9470034374216789\nEpsilon = 0.9469087370779368\nEpsilon = 0.9468140462042289\nEpsilon = 0.9467193647996085\nEpsilon = 0.9466246928631286\nAgent: ddqn_agent . Episode 36/2000. Number of steps to finish: 20. Loss: 15.616568565368652 Reward: -12.0\nEpsilon = 0.9465300303938422\nEpsilon = 0.9464353773908029\nEpsilon = 0.9463407338530638\nEpsilon = 0.9462460997796786\nEpsilon = 0.9461514751697006\nEpsilon = 0.9460568600221836\nEpsilon = 0.9459622543361814\nEpsilon = 0.9458676581107478\nEpsilon = 0.9457730713449367\nEpsilon = 0.9456784940378022\nEpsilon = 0.9455839261883985\nEpsilon = 0.9454893677957796\nEpsilon = 0.9453948188590001\nEpsilon = 0.9453002793771142\nEpsilon = 0.9452057493491766\nEpsilon = 0.9451112287742417\nEpsilon = 0.9450167176513643\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.9449222159795991\nEpsilon = 0.9448277237580012\nEpsilon = 0.9447332409856254\nAgent: ddqn_agent . Episode 37/2000. Number of steps to finish: 20. Loss: 15.680402755737305 Reward: -10.0\nEpsilon = 0.9446387676615269\nEpsilon = 0.9445443037847607\nEpsilon = 0.9444498493543823\nEpsilon = 0.9443554043694469\nEpsilon = 0.9442609688290099\nEpsilon = 0.944166542732127\nEpsilon = 0.9440721260778537\nEpsilon = 0.943977718865246\nAgent: ddqn_agent . Episode 38/2000. Number of steps to finish: 8. Loss: 6.389926910400391 Reward: 4.0\nEpsilon = 0.9438833210933595\nEpsilon = 0.9437889327612502\nEpsilon = 0.9436945538679741\nEpsilon = 0.9436001844125873\nEpsilon = 0.9435058243941461\nEpsilon = 0.9434114738117066\nEpsilon = 0.9433171326643255\nEpsilon = 0.943222800951059\nEpsilon = 0.9431284786709639\nEpsilon = 0.9430341658230968\nEpsilon = 0.9429398624065145\nEpsilon = 0.9428455684202739\nEpsilon = 0.9427512838634319\nEpsilon = 0.9426570087350455\nEpsilon = 0.9425627430341721\nEpsilon = 0.9424684867598687\nEpsilon = 0.9423742399111927\nEpsilon = 0.9422800024872017\nEpsilon = 0.942185774486953\nEpsilon = 0.9420915559095042\nAgent: ddqn_agent . Episode 39/2000. Number of steps to finish: 20. Loss: 15.632894515991211 Reward: -12.0\nEpsilon = 0.9419973467539133\nEpsilon = 0.9419031470192379\nEpsilon = 0.941808956704536\nEpsilon = 0.9417147758088655\nEpsilon = 0.9416206043312847\nEpsilon = 0.9415264422708516\nEpsilon = 0.9414322896266245\nEpsilon = 0.9413381463976619\nEpsilon = 0.9412440125830221\nEpsilon = 0.9411498881817638\nEpsilon = 0.9410557731929456\nEpsilon = 0.9409616676156264\nEpsilon = 0.9408675714488648\nEpsilon = 0.94077348469172\nAgent: ddqn_agent . Episode 40/2000. Number of steps to finish: 14. Loss: 10.95859146118164 Reward: -2.0\nEpsilon = 0.9406794073432508\nEpsilon = 0.9405853394025165\nEpsilon = 0.9404912808685763\nEpsilon = 0.9403972317404894\nEpsilon = 0.9403031920173154\nEpsilon = 0.9402091616981136\nEpsilon = 0.9401151407819438\nEpsilon = 0.9400211292678656\nEpsilon = 0.9399271271549389\nEpsilon = 0.9398331344422234\nEpsilon = 0.9397391511287791\nEpsilon = 0.9396451772136662\nEpsilon = 0.9395512126959449\nEpsilon = 0.9394572575746754\nEpsilon = 0.9393633118489179\nEpsilon = 0.9392693755177329\nEpsilon = 0.9391754485801812\nEpsilon = 0.9390815310353232\nEpsilon = 0.9389876228822197\nEpsilon = 0.9388937241199315\nAgent: ddqn_agent . Episode 41/2000. Number of steps to finish: 20. Loss: 16.184127807617188 Reward: -12.0\nEpsilon = 0.9387998347475196\nEpsilon = 0.9387059547640448\nEpsilon = 0.9386120841685685\nEpsilon = 0.9385182229601516\nEpsilon = 0.9384243711378556\nEpsilon = 0.9383305287007419\nEpsilon = 0.9382366956478718\nEpsilon = 0.938142871978307\nEpsilon = 0.9380490576911092\nEpsilon = 0.9379552527853402\nEpsilon = 0.9378614572600616\nEpsilon = 0.9377676711143357\nEpsilon = 0.9376738943472243\nEpsilon = 0.9375801269577896\nEpsilon = 0.9374863689450939\nEpsilon = 0.9373926203081994\nEpsilon = 0.9372988810461685\nEpsilon = 0.9372051511580639\nEpsilon = 0.9371114306429481\nEpsilon = 0.9370177194998838\nAgent: ddqn_agent . Episode 42/2000. Number of steps to finish: 20. Loss: 15.555513381958008 Reward: -12.0\nEpsilon = 0.9369240177279338\nEpsilon = 0.936830325326161\nEpsilon = 0.9367366422936283\nEpsilon = 0.936642968629399\nEpsilon = 0.936549304332536\nEpsilon = 0.9364556494021028\nEpsilon = 0.9363620038371626\nEpsilon = 0.9362683676367789\nEpsilon = 0.9361747408000153\nEpsilon = 0.9360811233259353\nEpsilon = 0.9359875152136027\nEpsilon = 0.9358939164620813\nEpsilon = 0.9358003270704351\nEpsilon = 0.9357067470377282\nEpsilon = 0.9356131763630244\nEpsilon = 0.9355196150453882\nEpsilon = 0.9354260630838837\nEpsilon = 0.9353325204775753\nEpsilon = 0.9352389872255276\nEpsilon = 0.935145463326805\nAgent: ddqn_agent . Episode 43/2000. Number of steps to finish: 20. Loss: 15.26114273071289 Reward: -10.0\nEpsilon = 0.9350519487804724\nEpsilon = 0.9349584435855943\nEpsilon = 0.9348649477412357\nEpsilon = 0.9347714612464616\nEpsilon = 0.934677984100337\nEpsilon = 0.934584516301927\nEpsilon = 0.9344910578502967\nEpsilon = 0.9343976087445117\nEpsilon = 0.9343041689836373\nEpsilon = 0.934210738566739\nEpsilon = 0.9341173174928823\nEpsilon = 0.934023905761133\nEpsilon = 0.9339305033705569\nEpsilon = 0.9338371103202199\nEpsilon = 0.9337437266091879\nEpsilon = 0.933650352236527\nEpsilon = 0.9335569872013034\nEpsilon = 0.9334636315025833\nEpsilon = 0.933370285139433\nEpsilon = 0.9332769481109191\nAgent: ddqn_agent . Episode 44/2000. Number of steps to finish: 20. Loss: 15.295613288879395 Reward: -12.0\nEpsilon = 0.933183620416108\nEpsilon = 0.9330903020540664\nEpsilon = 0.932996993023861\nEpsilon = 0.9329036933245586\nEpsilon = 0.9328104029552261\nEpsilon = 0.9327171219149306\nEpsilon = 0.9326238502027391\nEpsilon = 0.9325305878177188\nEpsilon = 0.9324373347589371\nEpsilon = 0.9323440910254612\nEpsilon = 0.9322508566163586\nEpsilon = 0.932157631530697\nEpsilon = 0.932064415767544\nEpsilon = 0.9319712093259672\nEpsilon = 0.9318780122050346\nEpsilon = 0.9317848244038142\nEpsilon = 0.9316916459213738\nEpsilon = 0.9315984767567816\nEpsilon = 0.931505316909106\nEpsilon = 0.9314121663774151\nAgent: ddqn_agent . Episode 45/2000. Number of steps to finish: 20. Loss: 14.89254093170166 Reward: -12.0\nEpsilon = 0.9313190251607774\nEpsilon = 0.9312258932582613\nEpsilon = 0.9311327706689355\nEpsilon = 0.9310396573918687\nEpsilon = 0.9309465534261295\nEpsilon = 0.930853458770787\nEpsilon = 0.9307603734249099\nEpsilon = 0.9306672973875675\nEpsilon = 0.9305742306578287\nEpsilon = 0.930481173234763\nEpsilon = 0.9303881251174395\nEpsilon = 0.9302950863049277\nEpsilon = 0.9302020567962973\nEpsilon = 0.9301090365906177\nEpsilon = 0.9300160256869586\nEpsilon = 0.92992302408439\nEpsilon = 0.9298300317819815\nEpsilon = 0.9297370487788034\nEpsilon = 0.9296440750739255\nEpsilon = 0.9295511106664182\nAgent: ddqn_agent . Episode 46/2000. Number of steps to finish: 20. Loss: 14.749595642089844 Reward: -10.0\nEpsilon = 0.9294581555553515\nEpsilon = 0.929365209739796\nEpsilon = 0.929272273218822\nEpsilon = 0.9291793459915001\nEpsilon = 0.929086428056901\nEpsilon = 0.9289935194140954\nEpsilon = 0.928900620062154\nEpsilon = 0.9288077300001478\nEpsilon = 0.9287148492271479\nEpsilon = 0.9286219777422252\nEpsilon = 0.9285291155444511\nEpsilon = 0.9284362626328966\nEpsilon = 0.9283434190066333\nEpsilon = 0.9282505846647326\nEpsilon = 0.9281577596062662\nEpsilon = 0.9280649438303056\nEpsilon = 0.9279721373359225\nEpsilon = 0.927879340122189\nEpsilon = 0.9277865521881767\nEpsilon = 0.9276937735329579\nAgent: ddqn_agent . Episode 47/2000. Number of steps to finish: 20. Loss: 14.473986625671387 Reward: -10.0\nEpsilon = 0.9276010041556046\nEpsilon = 0.9275082440551891\nEpsilon = 0.9274154932307835\nEpsilon = 0.9273227516814605\nEpsilon = 0.9272300194062923\nEpsilon = 0.9271372964043517\nEpsilon = 0.9270445826747113\nEpsilon = 0.9269518782164439\nEpsilon = 0.9268591830286222\nEpsilon = 0.9267664971103194\nEpsilon = 0.9266738204606084\nEpsilon = 0.9265811530785624\nEpsilon = 0.9264884949632545\nEpsilon = 0.9263958461137581\nEpsilon = 0.9263032065291468\nEpsilon = 0.9262105762084939\nEpsilon = 0.926117955150873\nEpsilon = 0.9260253433553579\nEpsilon = 0.9259327408210224\nEpsilon = 0.9258401475469403\nAgent: ddqn_agent . Episode 48/2000. Number of steps to finish: 20. Loss: 14.04911994934082 Reward: -12.0\nEpsilon = 0.9257475635321857\nEpsilon = 0.9256549887758325\nEpsilon = 0.9255624232769549\nEpsilon = 0.9254698670346272\nEpsilon = 0.9253773200479237\nEpsilon = 0.9252847823159189\nEpsilon = 0.9251922538376873\nEpsilon = 0.9250997346123035\nEpsilon = 0.9250072246388423\nEpsilon = 0.9249147239163784\nEpsilon = 0.9248222324439868\nEpsilon = 0.9247297502207423\nEpsilon = 0.9246372772457203\nEpsilon = 0.9245448135179958\nEpsilon = 0.924452359036644\nEpsilon = 0.9243599138007403\nEpsilon = 0.9242674778093602\nEpsilon = 0.9241750510615794\nEpsilon = 0.9240826335564732\nEpsilon = 0.9239902252931176\nAgent: ddqn_agent . Episode 49/2000. Number of steps to finish: 20. Loss: 14.311573028564453 Reward: -10.0\nEpsilon = 0.9238978262705883\nEpsilon = 0.9238054364879612\nEpsilon = 0.9237130559443124\nEpsilon = 0.9236206846387179\nEpsilon = 0.9235283225702541\nEpsilon = 0.923435969737997\nEpsilon = 0.9233436261410233\nEpsilon = 0.9232512917784091\nEpsilon = 0.9231589666492314\nEpsilon = 0.9230666507525664\nEpsilon = 0.9229743440874912\nEpsilon = 0.9228820466530825\nEpsilon = 0.9227897584484172\nAgent: ddqn_agent . Episode 50/2000. Number of steps to finish: 13. Loss: 9.583527565002441 Reward: -1.0\nEpsilon = 0.9226974794725723\nEpsilon = 0.922605209724625\nEpsilon = 0.9225129492036526\nEpsilon = 0.9224206979087322\nEpsilon = 0.9223284558389413\nEpsilon = 0.9222362229933574\nEpsilon = 0.9221439993710581\nEpsilon = 0.922051784971121\nEpsilon = 0.9219595797926239\nEpsilon = 0.9218673838346446\nEpsilon = 0.9217751970962612\nEpsilon = 0.9216830195765516\nEpsilon = 0.921590851274594\nEpsilon = 0.9214986921894665\nEpsilon = 0.9214065423202475\nEpsilon = 0.9213144016660155\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.9212222702258489\nEpsilon = 0.9211301479988263\nEpsilon = 0.9210380349840265\nEpsilon = 0.920945931180528\nAgent: ddqn_agent . Episode 51/2000. Number of steps to finish: 20. Loss: 14.546595573425293 Reward: -12.0\nEpsilon = 0.9208538365874099\nEpsilon = 0.9207617512037513\nEpsilon = 0.9206696750286308\nEpsilon = 0.920577608061128\nEpsilon = 0.9204855503003219\nEpsilon = 0.9203935017452919\nEpsilon = 0.9203014623951173\nEpsilon = 0.9202094322488779\nEpsilon = 0.9201174113056529\nEpsilon = 0.9200253995645223\nEpsilon = 0.9199333970245659\nEpsilon = 0.9198414036848634\nEpsilon = 0.919749419544495\nEpsilon = 0.9196574446025405\nEpsilon = 0.9195654788580803\nEpsilon = 0.9194735223101944\nEpsilon = 0.9193815749579634\nEpsilon = 0.9192896368004676\nEpsilon = 0.9191977078367876\nEpsilon = 0.9191057880660038\nAgent: ddqn_agent . Episode 52/2000. Number of steps to finish: 20. Loss: 14.629061698913574 Reward: -10.0\nEpsilon = 0.9190138774871972\nEpsilon = 0.9189219760994485\nEpsilon = 0.9188300839018385\nEpsilon = 0.9187382008934484\nEpsilon = 0.918646327073359\nEpsilon = 0.9185544624406518\nEpsilon = 0.9184626069944077\nEpsilon = 0.9183707607337083\nEpsilon = 0.9182789236576349\nEpsilon = 0.9181870957652691\nEpsilon = 0.9180952770556926\nEpsilon = 0.918003467527987\nEpsilon = 0.9179116671812342\nEpsilon = 0.9178198760145161\nEpsilon = 0.9177280940269147\nEpsilon = 0.917636321217512\nEpsilon = 0.9175445575853903\nEpsilon = 0.9174528031296318\nEpsilon = 0.9173610578493189\nEpsilon = 0.9172693217435339\nAgent: ddqn_agent . Episode 53/2000. Number of steps to finish: 20. Loss: 14.298834800720215 Reward: -10.0\nEpsilon = 0.9171775948113596\nEpsilon = 0.9170858770518785\nEpsilon = 0.9169941684641734\nEpsilon = 0.916902469047327\nEpsilon = 0.9168107788004223\nEpsilon = 0.9167190977225422\nEpsilon = 0.91662742581277\nEpsilon = 0.9165357630701887\nEpsilon = 0.9164441094938817\nEpsilon = 0.9163524650829323\nEpsilon = 0.9162608298364241\nEpsilon = 0.9161692037534405\nEpsilon = 0.9160775868330652\nEpsilon = 0.9159859790743818\nEpsilon = 0.9158943804764744\nEpsilon = 0.9158027910384268\nEpsilon = 0.9157112107593229\nEpsilon = 0.915619639638247\nEpsilon = 0.9155280776742832\nEpsilon = 0.9154365248665158\nAgent: ddqn_agent . Episode 54/2000. Number of steps to finish: 20. Loss: 13.926287651062012 Reward: -14.0\nEpsilon = 0.9153449812140292\nEpsilon = 0.9152534467159078\nEpsilon = 0.9151619213712362\nEpsilon = 0.9150704051790991\nEpsilon = 0.9149788981385812\nEpsilon = 0.9148874002487674\nEpsilon = 0.9147959115087425\nEpsilon = 0.9147044319175917\nEpsilon = 0.9146129614743999\nEpsilon = 0.9145215001782524\nEpsilon = 0.9144300480282346\nEpsilon = 0.9143386050234318\nEpsilon = 0.9142471711629294\nEpsilon = 0.9141557464458131\nEpsilon = 0.9140643308711686\nEpsilon = 0.9139729244380814\nEpsilon = 0.9138815271456376\nEpsilon = 0.913790138992923\nEpsilon = 0.9136987599790237\nEpsilon = 0.9136073901030258\nAgent: ddqn_agent . Episode 55/2000. Number of steps to finish: 20. Loss: 14.483583450317383 Reward: -10.0\nEpsilon = 0.9135160293640155\nEpsilon = 0.9134246777610792\nEpsilon = 0.9133333352933031\nEpsilon = 0.9132420019597738\nEpsilon = 0.9131506777595778\nEpsilon = 0.9130593626918019\nEpsilon = 0.9129680567555327\nEpsilon = 0.9128767599498572\nEpsilon = 0.9127854722738622\nEpsilon = 0.9126941937266349\nEpsilon = 0.9126029243072622\nEpsilon = 0.9125116640148315\nEpsilon = 0.91242041284843\nEpsilon = 0.9123291708071452\nEpsilon = 0.9122379378900645\nEpsilon = 0.9121467140962755\nEpsilon = 0.9120554994248659\nEpsilon = 0.9119642938749234\nEpsilon = 0.9118730974455359\nEpsilon = 0.9117819101357914\nAgent: ddqn_agent . Episode 56/2000. Number of steps to finish: 20. Loss: 14.347569465637207 Reward: -10.0\nEpsilon = 0.9116907319447778\nEpsilon = 0.9115995628715833\nEpsilon = 0.9115084029152961\nEpsilon = 0.9114172520750046\nEpsilon = 0.911326110349797\nEpsilon = 0.911234977738762\nEpsilon = 0.9111438542409882\nEpsilon = 0.9110527398555641\nEpsilon = 0.9109616345815785\nEpsilon = 0.9108705384181204\nEpsilon = 0.9107794513642786\nEpsilon = 0.9106883734191422\nEpsilon = 0.9105973045818003\nEpsilon = 0.9105062448513421\nEpsilon = 0.910415194226857\nEpsilon = 0.9103241527074344\nEpsilon = 0.9102331202921636\nEpsilon = 0.9101420969801344\nEpsilon = 0.9100510827704363\nEpsilon = 0.9099600776621593\nAgent: ddqn_agent . Episode 57/2000. Number of steps to finish: 20. Loss: 14.516691207885742 Reward: -10.0\nEpsilon = 0.9098690816543931\nEpsilon = 0.9097780947462276\nEpsilon = 0.909687116936753\nEpsilon = 0.9095961482250593\nEpsilon = 0.9095051886102368\nEpsilon = 0.9094142380913758\nEpsilon = 0.9093232966675666\nEpsilon = 0.9092323643378999\nEpsilon = 0.9091414411014661\nEpsilon = 0.909050526957356\nEpsilon = 0.9089596219046603\nEpsilon = 0.9088687259424698\nEpsilon = 0.9087778390698755\nEpsilon = 0.9086869612859686\nEpsilon = 0.90859609258984\nEpsilon = 0.9085052329805811\nEpsilon = 0.908414382457283\nEpsilon = 0.9083235410190373\nEpsilon = 0.9082327086649354\nEpsilon = 0.9081418853940689\nAgent: ddqn_agent . Episode 58/2000. Number of steps to finish: 20. Loss: 14.462297439575195 Reward: -10.0\nEpsilon = 0.9080510712055295\nEpsilon = 0.907960266098409\nEpsilon = 0.9078694700717991\nEpsilon = 0.9077786831247919\nEpsilon = 0.9076879052564795\nEpsilon = 0.9075971364659539\nEpsilon = 0.9075063767523073\nEpsilon = 0.907415626114632\nEpsilon = 0.9073248845520205\nEpsilon = 0.9072341520635654\nEpsilon = 0.9071434286483591\nEpsilon = 0.9070527143054943\nEpsilon = 0.9069620090340638\nEpsilon = 0.9068713128331604\nEpsilon = 0.906780625701877\nEpsilon = 0.9066899476393069\nEpsilon = 0.9065992786445429\nEpsilon = 0.9065086187166785\nEpsilon = 0.9064179678548069\nEpsilon = 0.9063273260580215\nAgent: ddqn_agent . Episode 59/2000. Number of steps to finish: 20. Loss: 14.126453399658203 Reward: -12.0\nEpsilon = 0.9062366933254157\nEpsilon = 0.9061460696560831\nEpsilon = 0.9060554550491176\nEpsilon = 0.9059648495036127\nEpsilon = 0.9058742530186623\nEpsilon = 0.9057836655933604\nEpsilon = 0.9056930872268011\nEpsilon = 0.9056025179180784\nEpsilon = 0.9055119576662867\nEpsilon = 0.90542140647052\nEpsilon = 0.905330864329873\nEpsilon = 0.9052403312434399\nEpsilon = 0.9051498072103156\nEpsilon = 0.9050592922295946\nEpsilon = 0.9049687863003716\nEpsilon = 0.9048782894217415\nEpsilon = 0.9047878015927994\nEpsilon = 0.9046973228126401\nEpsilon = 0.9046068530803589\nEpsilon = 0.9045163923950509\nAgent: ddqn_agent . Episode 60/2000. Number of steps to finish: 20. Loss: 14.035728454589844 Reward: -12.0\nEpsilon = 0.9044259407558114\nEpsilon = 0.9043354981617359\nEpsilon = 0.9042450646119197\nEpsilon = 0.9041546401054585\nEpsilon = 0.904064224641448\nEpsilon = 0.9039738182189838\nEpsilon = 0.9038834208371619\nEpsilon = 0.9037930324950781\nEpsilon = 0.9037026531918286\nEpsilon = 0.9036122829265094\nEpsilon = 0.9035219216982168\nEpsilon = 0.903431569506047\nEpsilon = 0.9033412263490964\nEpsilon = 0.9032508922264615\nEpsilon = 0.9031605671372388\nEpsilon = 0.9030702510805251\nEpsilon = 0.902979944055417\nEpsilon = 0.9028896460610115\nAgent: ddqn_agent . Episode 61/2000. Number of steps to finish: 18. Loss: 12.706062316894531 Reward: -6.0\nEpsilon = 0.9027993570964055\nEpsilon = 0.9027090771606958\nEpsilon = 0.9026188062529797\nEpsilon = 0.9025285443723544\nEpsilon = 0.9024382915179172\nEpsilon = 0.9023480476887654\nEpsilon = 0.9022578128839965\nEpsilon = 0.9021675871027082\nEpsilon = 0.9020773703439979\nEpsilon = 0.9019871626069635\nEpsilon = 0.9018969638907028\nEpsilon = 0.9018067741943138\nEpsilon = 0.9017165935168944\nEpsilon = 0.9016264218575427\nEpsilon = 0.901536259215357\nEpsilon = 0.9014461055894354\nEpsilon = 0.9013559609788765\nEpsilon = 0.9012658253827787\nEpsilon = 0.9011756988002404\nEpsilon = 0.9010855812303604\nAgent: ddqn_agent . Episode 62/2000. Number of steps to finish: 20. Loss: 14.005094528198242 Reward: -12.0\nEpsilon = 0.9009954726722373\nEpsilon = 0.9009053731249701\nEpsilon = 0.9008152825876576\nEpsilon = 0.9007252010593989\nEpsilon = 0.900635128539293\nEpsilon = 0.900545065026439\nEpsilon = 0.9004550105199364\nEpsilon = 0.9003649650188844\nEpsilon = 0.9002749285223826\nEpsilon = 0.9001849010295303\nEpsilon = 0.9000948825394274\nEpsilon = 0.9000048730511735\nEpsilon = 0.8999148725638684\nEpsilon = 0.899824881076612\nEpsilon = 0.8997348985885043\nEpsilon = 0.8996449250986455\nEpsilon = 0.8995549606061356\nEpsilon = 0.899465005110075\nEpsilon = 0.899375058609564\nEpsilon = 0.8992851211037031\nAgent: ddqn_agent . Episode 63/2000. Number of steps to finish: 20. Loss: 13.97527027130127 Reward: -10.0\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.8991951925915928\nEpsilon = 0.8991052730723336\nEpsilon = 0.8990153625450263\nEpsilon = 0.8989254610087718\nEpsilon = 0.8988355684626709\nEpsilon = 0.8987456849058246\nEpsilon = 0.8986558103373341\nEpsilon = 0.8985659447563004\nEpsilon = 0.8984760881618248\nEpsilon = 0.8983862405530086\nEpsilon = 0.8982964019289533\nEpsilon = 0.8982065722887604\nEpsilon = 0.8981167516315316\nEpsilon = 0.8980269399563684\nEpsilon = 0.8979371372623728\nEpsilon = 0.8978473435486466\nEpsilon = 0.8977575588142918\nEpsilon = 0.8976677830584103\nEpsilon = 0.8975780162801045\nEpsilon = 0.8974882584784765\nAgent: ddqn_agent . Episode 64/2000. Number of steps to finish: 20. Loss: 14.049817085266113 Reward: -10.0\nEpsilon = 0.8973985096526286\nEpsilon = 0.8973087698016633\nEpsilon = 0.8972190389246831\nEpsilon = 0.8971293170207907\nEpsilon = 0.8970396040890886\nEpsilon = 0.8969499001286797\nEpsilon = 0.8968602051386668\nEpsilon = 0.896770519118153\nEpsilon = 0.8966808420662412\nEpsilon = 0.8965911739820346\nEpsilon = 0.8965015148646364\nEpsilon = 0.8964118647131499\nEpsilon = 0.8963222235266787\nEpsilon = 0.896232591304326\nEpsilon = 0.8961429680451956\nEpsilon = 0.896053353748391\nEpsilon = 0.8959637484130162\nEpsilon = 0.8958741520381749\nEpsilon = 0.8957845646229711\nEpsilon = 0.8956949861665088\nAgent: ddqn_agent . Episode 65/2000. Number of steps to finish: 20. Loss: 14.031919479370117 Reward: -10.0\nEpsilon = 0.8956054166678922\nEpsilon = 0.8955158561262254\nEpsilon = 0.8954263045406128\nEpsilon = 0.8953367619101588\nEpsilon = 0.8952472282339677\nEpsilon = 0.8951577035111443\nEpsilon = 0.8950681877407932\nEpsilon = 0.8949786809220192\nEpsilon = 0.894889183053927\nEpsilon = 0.8947996941356215\nEpsilon = 0.894710214166208\nEpsilon = 0.8946207431447913\nEpsilon = 0.8945312810704769\nAgent: ddqn_agent . Episode 66/2000. Number of steps to finish: 13. Loss: 9.269758224487305 Reward: -1.0\nEpsilon = 0.8944418279423698\nEpsilon = 0.8943523837595756\nEpsilon = 0.8942629485211997\nEpsilon = 0.8941735222263476\nEpsilon = 0.8940841048741249\nEpsilon = 0.8939946964636375\nEpsilon = 0.8939052969939911\nEpsilon = 0.8938159064642918\nEpsilon = 0.8937265248736453\nEpsilon = 0.893637152221158\nEpsilon = 0.893547788505936\nEpsilon = 0.8934584337270853\nEpsilon = 0.8933690878837126\nEpsilon = 0.8932797509749243\nEpsilon = 0.8931904229998268\nEpsilon = 0.8931011039575268\nEpsilon = 0.8930117938471311\nEpsilon = 0.8929224926677465\nAgent: ddqn_agent . Episode 67/2000. Number of steps to finish: 18. Loss: 12.799527168273926 Reward: -6.0\nEpsilon = 0.8928332004184797\nEpsilon = 0.8927439170984378\nEpsilon = 0.892654642706728\nEpsilon = 0.8925653772424573\nEpsilon = 0.8924761207047331\nEpsilon = 0.8923868730926627\nEpsilon = 0.8922976344053535\nEpsilon = 0.892208404641913\nEpsilon = 0.8921191838014488\nEpsilon = 0.8920299718830687\nEpsilon = 0.8919407688858804\nEpsilon = 0.8918515748089918\nEpsilon = 0.8917623896515109\nEpsilon = 0.8916732134125458\nEpsilon = 0.8915840460912046\nEpsilon = 0.8914948876865955\nEpsilon = 0.8914057381978268\nEpsilon = 0.891316597624007\nEpsilon = 0.8912274659642445\nEpsilon = 0.8911383432176481\nAgent: ddqn_agent . Episode 68/2000. Number of steps to finish: 20. Loss: 14.232619285583496 Reward: -16.0\nEpsilon = 0.8910492293833263\nEpsilon = 0.890960124460388\nEpsilon = 0.890871028447942\nEpsilon = 0.8907819413450971\nEpsilon = 0.8906928631509626\nEpsilon = 0.8906037938646475\nEpsilon = 0.890514733485261\nEpsilon = 0.8904256820119125\nEpsilon = 0.8903366394437113\nEpsilon = 0.8902476057797669\nEpsilon = 0.890158581019189\nEpsilon = 0.890069565161087\nEpsilon = 0.8899805582045709\nEpsilon = 0.8898915601487505\nEpsilon = 0.8898025709927356\nEpsilon = 0.8897135907356364\nEpsilon = 0.8896246193765628\nEpsilon = 0.8895356569146251\nEpsilon = 0.8894467033489336\nEpsilon = 0.8893577586785987\nAgent: ddqn_agent . Episode 69/2000. Number of steps to finish: 20. Loss: 13.513800621032715 Reward: -16.0\nEpsilon = 0.8892688229027308\nEpsilon = 0.8891798960204406\nEpsilon = 0.8890909780308385\nEpsilon = 0.8890020689330353\nEpsilon = 0.888913168726142\nEpsilon = 0.8888242774092694\nEpsilon = 0.8887353949815284\nEpsilon = 0.8886465214420303\nEpsilon = 0.8885576567898861\nEpsilon = 0.8884688010242071\nEpsilon = 0.8883799541441048\nEpsilon = 0.8882911161486904\nEpsilon = 0.8882022870370755\nEpsilon = 0.8881134668083718\nEpsilon = 0.888024655461691\nEpsilon = 0.8879358529961447\nEpsilon = 0.8878470594108452\nAgent: ddqn_agent . Episode 70/2000. Number of steps to finish: 17. Loss: 11.978857040405273 Reward: -5.0\nEpsilon = 0.8877582747049041\nEpsilon = 0.8876694988774336\nEpsilon = 0.8875807319275459\nEpsilon = 0.8874919738543532\nEpsilon = 0.8874032246569677\nEpsilon = 0.8873144843345021\nEpsilon = 0.8872257528860686\nEpsilon = 0.8871370303107801\nEpsilon = 0.887048316607749\nEpsilon = 0.8869596117760882\nEpsilon = 0.8868709158149106\nEpsilon = 0.8867822287233291\nEpsilon = 0.8866935505004568\nEpsilon = 0.8866048811454068\nEpsilon = 0.8865162206572923\nEpsilon = 0.8864275690352266\nEpsilon = 0.886338926278323\nEpsilon = 0.8862502923856952\nEpsilon = 0.8861616673564566\nEpsilon = 0.886073051189721\nAgent: ddqn_agent . Episode 71/2000. Number of steps to finish: 20. Loss: 13.777359962463379 Reward: -10.0\nEpsilon = 0.8859844438846021\nEpsilon = 0.8858958454402136\nEpsilon = 0.8858072558556696\nEpsilon = 0.885718675130084\nEpsilon = 0.885630103262571\nEpsilon = 0.8855415402522447\nEpsilon = 0.8854529860982195\nEpsilon = 0.8853644407996097\nEpsilon = 0.8852759043555298\nEpsilon = 0.8851873767650943\nEpsilon = 0.8850988580274178\nEpsilon = 0.885010348141615\nEpsilon = 0.8849218471068009\nEpsilon = 0.8848333549220903\nEpsilon = 0.8847448715865981\nEpsilon = 0.8846563970994394\nEpsilon = 0.8845679314597295\nEpsilon = 0.8844794746665835\nEpsilon = 0.8843910267191168\nEpsilon = 0.884302587616445\nAgent: ddqn_agent . Episode 72/2000. Number of steps to finish: 20. Loss: 13.82990837097168 Reward: -10.0\nEpsilon = 0.8842141573576834\nEpsilon = 0.8841257359419477\nEpsilon = 0.8840373233683535\nEpsilon = 0.8839489196360166\nEpsilon = 0.8838605247440531\nEpsilon = 0.8837721386915787\nEpsilon = 0.8836837614777096\nEpsilon = 0.8835953931015618\nEpsilon = 0.8835070335622516\nEpsilon = 0.8834186828588954\nEpsilon = 0.8833303409906095\nEpsilon = 0.8832420079565104\nEpsilon = 0.8831536837557148\nEpsilon = 0.8830653683873392\nEpsilon = 0.8829770618505004\nEpsilon = 0.8828887641443154\nEpsilon = 0.882800475267901\nEpsilon = 0.8827121952203741\nEpsilon = 0.8826239240008521\nEpsilon = 0.882535661608452\nAgent: ddqn_agent . Episode 73/2000. Number of steps to finish: 20. Loss: 14.026986122131348 Reward: -12.0\nEpsilon = 0.8824474080422912\nEpsilon = 0.8823591633014869\nEpsilon = 0.8822709273851568\nEpsilon = 0.8821827002924183\nEpsilon = 0.882094482022389\nEpsilon = 0.8820062725741868\nEpsilon = 0.8819180719469294\nEpsilon = 0.8818298801397347\nEpsilon = 0.8817416971517208\nEpsilon = 0.8816535229820056\nEpsilon = 0.8815653576297074\nEpsilon = 0.8814772010939445\nEpsilon = 0.8813890533738351\nEpsilon = 0.8813009144684978\nEpsilon = 0.881212784377051\nEpsilon = 0.8811246630986133\nEpsilon = 0.8810365506323035\nEpsilon = 0.8809484469772403\nEpsilon = 0.8808603521325425\nEpsilon = 0.8807722660973293\nAgent: ddqn_agent . Episode 74/2000. Number of steps to finish: 20. Loss: 13.319005012512207 Reward: -8.0\nEpsilon = 0.8806841888707195\nEpsilon = 0.8805961204518324\nEpsilon = 0.8805080608397873\nEpsilon = 0.8804200100337033\nEpsilon = 0.8803319680327\nEpsilon = 0.8802439348358967\nEpsilon = 0.8801559104424131\nEpsilon = 0.8800678948513689\nEpsilon = 0.8799798880618838\nEpsilon = 0.8798918900730776\nEpsilon = 0.8798039008840702\nEpsilon = 0.8797159204939818\nEpsilon = 0.8796279489019324\nEpsilon = 0.8795399861070422\nEpsilon = 0.8794520321084315\nEpsilon = 0.8793640869052206\nEpsilon = 0.8792761504965301\nEpsilon = 0.8791882228814805\nEpsilon = 0.8791003040591924\nEpsilon = 0.8790123940287865\nAgent: ddqn_agent . Episode 75/2000. Number of steps to finish: 20. Loss: 13.50312614440918 Reward: -12.0\nEpsilon = 0.8789244927893837\nEpsilon = 0.8788366003401047\nEpsilon = 0.8787487166800707\nEpsilon = 0.8786608418084028\nEpsilon = 0.8785729757242219\nEpsilon = 0.8784851184266494\nEpsilon = 0.8783972699148068\nEpsilon = 0.8783094301878154\nEpsilon = 0.8782215992447966\nEpsilon = 0.8781337770848722\nEpsilon = 0.8780459637071637\nEpsilon = 0.877958159110793\nEpsilon = 0.8778703632948819\nEpsilon = 0.8777825762585524\nEpsilon = 0.8776947980009265\nEpsilon = 0.8776070285211265\nEpsilon = 0.8775192678182744\nEpsilon = 0.8774315158914926\nEpsilon = 0.8773437727399035\nEpsilon = 0.8772560383626296\nAgent: ddqn_agent . Episode 76/2000. Number of steps to finish: 20. Loss: 13.683274269104004 Reward: -12.0\nEpsilon = 0.8771683127587933\nEpsilon = 0.8770805959275174\nEpsilon = 0.8769928878679246\nEpsilon = 0.8769051885791378\nEpsilon = 0.8768174980602799\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.8767298163104739\nEpsilon = 0.8766421433288428\nEpsilon = 0.8765544791145099\nEpsilon = 0.8764668236665984\nEpsilon = 0.8763791769842318\nAgent: ddqn_agent . Episode 77/2000. Number of steps to finish: 10. Loss: 6.761260986328125 Reward: 2.0\nEpsilon = 0.8762915390665335\nEpsilon = 0.8762039099126268\nEpsilon = 0.8761162895216354\nEpsilon = 0.8760286778926833\nEpsilon = 0.875941075024894\nEpsilon = 0.8758534809173916\nEpsilon = 0.8757658955692998\nEpsilon = 0.8756783189797429\nEpsilon = 0.8755907511478449\nEpsilon = 0.8755031920727301\nEpsilon = 0.8754156417535228\nEpsilon = 0.8753281001893475\nEpsilon = 0.8752405673793285\nEpsilon = 0.8751530433225906\nEpsilon = 0.8750655280182583\nEpsilon = 0.8749780214654566\nAgent: ddqn_agent . Episode 78/2000. Number of steps to finish: 16. Loss: 11.010323524475098 Reward: -4.0\nEpsilon = 0.87489052366331\nEpsilon = 0.8748030346109437\nEpsilon = 0.8747155543074826\nEpsilon = 0.8746280827520518\nEpsilon = 0.8745406199437766\nEpsilon = 0.8744531658817822\nEpsilon = 0.874365720565194\nEpsilon = 0.8742782839931375\nEpsilon = 0.8741908561647381\nEpsilon = 0.8741034370791216\nEpsilon = 0.8740160267354138\nEpsilon = 0.8739286251327403\nEpsilon = 0.873841232270227\nEpsilon = 0.873753848147\nEpsilon = 0.8736664727621852\nEpsilon = 0.8735791061149091\nEpsilon = 0.8734917482042976\nAgent: ddqn_agent . Episode 79/2000. Number of steps to finish: 17. Loss: 11.697771072387695 Reward: -5.0\nEpsilon = 0.8734043990294772\nEpsilon = 0.8733170585895743\nEpsilon = 0.8732297268837154\nEpsilon = 0.873142403911027\nEpsilon = 0.8730550896706359\nEpsilon = 0.8729677841616689\nEpsilon = 0.8728804873832527\nEpsilon = 0.8727931993345144\nEpsilon = 0.8727059200145809\nEpsilon = 0.8726186494225795\nEpsilon = 0.8725313875576373\nEpsilon = 0.8724441344188815\nEpsilon = 0.8723568900054396\nEpsilon = 0.8722696543164391\nEpsilon = 0.8721824273510075\nEpsilon = 0.8720952091082724\nEpsilon = 0.8720079995873616\nEpsilon = 0.8719207987874029\nEpsilon = 0.8718336067075242\nEpsilon = 0.8717464233468535\nAgent: ddqn_agent . Episode 80/2000. Number of steps to finish: 20. Loss: 13.784454345703125 Reward: -10.0\nEpsilon = 0.8716592487045188\nEpsilon = 0.8715720827796484\nEpsilon = 0.8714849255713704\nEpsilon = 0.8713977770788133\nEpsilon = 0.8713106373011054\nEpsilon = 0.8712235062373752\nEpsilon = 0.8711363838867515\nEpsilon = 0.8710492702483629\nEpsilon = 0.8709621653213381\nEpsilon = 0.8708750691048059\nEpsilon = 0.8707879815978955\nEpsilon = 0.8707009027997357\nEpsilon = 0.8706138327094557\nEpsilon = 0.8705267713261847\nEpsilon = 0.8704397186490521\nEpsilon = 0.8703526746771872\nEpsilon = 0.8702656394097196\nEpsilon = 0.8701786128457786\nEpsilon = 0.870091594984494\nEpsilon = 0.8700045858249955\nAgent: ddqn_agent . Episode 81/2000. Number of steps to finish: 20. Loss: 13.60875415802002 Reward: -12.0\nEpsilon = 0.8699175853664131\nEpsilon = 0.8698305936078764\nEpsilon = 0.8697436105485157\nEpsilon = 0.8696566361874608\nEpsilon = 0.869569670523842\nEpsilon = 0.8694827135567896\nEpsilon = 0.8693957652854339\nEpsilon = 0.8693088257089054\nEpsilon = 0.8692218948263346\nEpsilon = 0.869134972636852\nEpsilon = 0.8690480591395884\nEpsilon = 0.8689611543336744\nEpsilon = 0.868874258218241\nEpsilon = 0.8687873707924192\nEpsilon = 0.86870049205534\nEpsilon = 0.8686136220061345\nEpsilon = 0.8685267606439339\nEpsilon = 0.8684399079678695\nEpsilon = 0.8683530639770728\nEpsilon = 0.868266228670675\nAgent: ddqn_agent . Episode 82/2000. Number of steps to finish: 20. Loss: 13.490803718566895 Reward: -12.0\nEpsilon = 0.868179402047808\nEpsilon = 0.8680925841076031\nEpsilon = 0.8680057748491924\nEpsilon = 0.8679189742717075\nEpsilon = 0.8678321823742804\nEpsilon = 0.867745399156043\nEpsilon = 0.8676586246161274\nEpsilon = 0.8675718587536658\nEpsilon = 0.8674851015677905\nEpsilon = 0.8673983530576337\nEpsilon = 0.867311613222328\nEpsilon = 0.8672248820610058\nEpsilon = 0.8671381595727997\nEpsilon = 0.8670514457568425\nEpsilon = 0.8669647406122668\nEpsilon = 0.8668780441382056\nEpsilon = 0.8667913563337918\nEpsilon = 0.8667046771981585\nEpsilon = 0.8666180067304387\nEpsilon = 0.8665313449297656\nAgent: ddqn_agent . Episode 83/2000. Number of steps to finish: 20. Loss: 13.552774429321289 Reward: -10.0\nEpsilon = 0.8664446917952726\nEpsilon = 0.8663580473260931\nEpsilon = 0.8662714115213606\nEpsilon = 0.8661847843802084\nEpsilon = 0.8660981659017704\nEpsilon = 0.8660115560851802\nEpsilon = 0.8659249549295717\nEpsilon = 0.8658383624340787\nEpsilon = 0.8657517785978354\nEpsilon = 0.8656652034199755\nEpsilon = 0.8655786368996335\nEpsilon = 0.8654920790359436\nEpsilon = 0.86540552982804\nEpsilon = 0.8653189892750572\nEpsilon = 0.8652324573761297\nEpsilon = 0.8651459341303921\nEpsilon = 0.8650594195369791\nEpsilon = 0.8649729135950254\nEpsilon = 0.8648864163036659\nEpsilon = 0.8647999276620355\nAgent: ddqn_agent . Episode 84/2000. Number of steps to finish: 20. Loss: 13.270060539245605 Reward: -10.0\nEpsilon = 0.8647134476692693\nEpsilon = 0.8646269763245024\nEpsilon = 0.8645405136268699\nEpsilon = 0.8644540595755072\nEpsilon = 0.8643676141695497\nEpsilon = 0.8642811774081327\nEpsilon = 0.8641947492903919\nEpsilon = 0.8641083298154628\nEpsilon = 0.8640219189824813\nEpsilon = 0.863935516790583\nEpsilon = 0.863849123238904\nEpsilon = 0.86376273832658\nEpsilon = 0.8636763620527473\nEpsilon = 0.863589994416542\nEpsilon = 0.8635036354171004\nEpsilon = 0.8634172850535587\nEpsilon = 0.8633309433250534\nEpsilon = 0.863244610230721\nEpsilon = 0.8631582857696979\nEpsilon = 0.8630719699411209\nAgent: ddqn_agent . Episode 85/2000. Number of steps to finish: 20. Loss: 13.371021270751953 Reward: -14.0\nEpsilon = 0.8629856627441268\nEpsilon = 0.8628993641778524\nEpsilon = 0.8628130742414346\nEpsilon = 0.8627267929340104\nEpsilon = 0.862640520254717\nEpsilon = 0.8625542562026915\nEpsilon = 0.8624680007770713\nEpsilon = 0.8623817539769936\nEpsilon = 0.8622955158015959\nEpsilon = 0.8622092862500158\nEpsilon = 0.8621230653213908\nEpsilon = 0.8620368530148587\nEpsilon = 0.8619506493295572\nEpsilon = 0.8618644542646243\nEpsilon = 0.8617782678191979\nEpsilon = 0.861692089992416\nEpsilon = 0.8616059207834167\nEpsilon = 0.8615197601913384\nEpsilon = 0.8614336082153192\nEpsilon = 0.8613474648544976\nAgent: ddqn_agent . Episode 86/2000. Number of steps to finish: 20. Loss: 13.42529296875 Reward: -10.0\nEpsilon = 0.8612613301080122\nEpsilon = 0.8611752039750014\nEpsilon = 0.8610890864546039\nEpsilon = 0.8610029775459584\nEpsilon = 0.8609168772482039\nEpsilon = 0.8608307855604791\nEpsilon = 0.860744702481923\nEpsilon = 0.8606586280116748\nEpsilon = 0.8605725621488737\nEpsilon = 0.8604865048926588\nEpsilon = 0.8604004562421695\nEpsilon = 0.8603144161965454\nEpsilon = 0.8602283847549257\nEpsilon = 0.8601423619164502\nEpsilon = 0.8600563476802586\nEpsilon = 0.8599703420454906\nEpsilon = 0.8598843450112861\nEpsilon = 0.8597983565767849\nEpsilon = 0.8597123767411272\nEpsilon = 0.8596264055034532\nAgent: ddqn_agent . Episode 87/2000. Number of steps to finish: 20. Loss: 13.784873962402344 Reward: -14.0\nEpsilon = 0.8595404428629028\nEpsilon = 0.8594544888186165\nEpsilon = 0.8593685433697347\nEpsilon = 0.8592826065153977\nEpsilon = 0.8591966782547461\nEpsilon = 0.8591107585869207\nEpsilon = 0.859024847511062\nEpsilon = 0.858938945026311\nEpsilon = 0.8588530511318083\nEpsilon = 0.8587671658266951\nEpsilon = 0.8586812891101124\nEpsilon = 0.8585954209812015\nEpsilon = 0.8585095614391033\nEpsilon = 0.8584237104829594\nEpsilon = 0.8583378681119112\nEpsilon = 0.8582520343251\nEpsilon = 0.8581662091216675\nEpsilon = 0.8580803925007554\nEpsilon = 0.8579945844615053\nEpsilon = 0.8579087850030592\nAgent: ddqn_agent . Episode 88/2000. Number of steps to finish: 20. Loss: 13.74660873413086 Reward: -14.0\nEpsilon = 0.857822994124559\nEpsilon = 0.8577372118251465\nEpsilon = 0.857651438103964\nEpsilon = 0.8575656729601536\nEpsilon = 0.8574799163928576\nEpsilon = 0.8573941684012183\nEpsilon = 0.8573084289843782\nEpsilon = 0.8572226981414798\nEpsilon = 0.8571369758716656\nEpsilon = 0.8570512621740785\nEpsilon = 0.856965557047861\nEpsilon = 0.8568798604921563\nEpsilon = 0.856794172506107\nEpsilon = 0.8567084930888564\nEpsilon = 0.8566228222395476\nEpsilon = 0.8565371599573236\nEpsilon = 0.8564515062413279\nEpsilon = 0.8563658610907038\nEpsilon = 0.8562802245045947\nEpsilon = 0.8561945964821442\nAgent: ddqn_agent . Episode 89/2000. Number of steps to finish: 20. Loss: 12.86876106262207 Reward: -14.0\nEpsilon = 0.856108977022496\nEpsilon = 0.8560233661247938\nEpsilon = 0.8559377637881813\nEpsilon = 0.8558521700118025\nEpsilon = 0.8557665847948013\nEpsilon = 0.8556810081363218\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.8555954400355081\nEpsilon = 0.8555098804915046\nEpsilon = 0.8554243295034554\nEpsilon = 0.855338787070505\nEpsilon = 0.855253253191798\nEpsilon = 0.8551677278664789\nEpsilon = 0.8550822110936922\nEpsilon = 0.8549967028725829\nEpsilon = 0.8549112032022956\nAgent: ddqn_agent . Episode 90/2000. Number of steps to finish: 15. Loss: 9.897772789001465 Reward: -3.0\nEpsilon = 0.8548257120819754\nEpsilon = 0.8547402295107672\nEpsilon = 0.8546547554878161\nEpsilon = 0.8545692900122673\nEpsilon = 0.8544838330832661\nEpsilon = 0.8543983846999579\nEpsilon = 0.8543129448614879\nEpsilon = 0.8542275135670018\nEpsilon = 0.8541420908156451\nEpsilon = 0.8540566766065635\nEpsilon = 0.8539712709389028\nEpsilon = 0.8538858738118089\nEpsilon = 0.8538004852244278\nEpsilon = 0.8537151051759053\nEpsilon = 0.8536297336653877\nEpsilon = 0.8535443706920213\nEpsilon = 0.8534590162549521\nEpsilon = 0.8533736703533267\nEpsilon = 0.8532883329862914\nEpsilon = 0.8532030041529928\nAgent: ddqn_agent . Episode 91/2000. Number of steps to finish: 20. Loss: 13.19228744506836 Reward: -12.0\nEpsilon = 0.8531176838525775\nEpsilon = 0.8530323720841922\nEpsilon = 0.8529470688469838\nEpsilon = 0.8528617741400991\nEpsilon = 0.8527764879626851\nEpsilon = 0.8526912103138888\nEpsilon = 0.8526059411928575\nEpsilon = 0.8525206805987382\nEpsilon = 0.8524354285306783\nEpsilon = 0.8523501849878252\nEpsilon = 0.8522649499693264\nEpsilon = 0.8521797234743296\nEpsilon = 0.8520945055019822\nEpsilon = 0.8520092960514319\nEpsilon = 0.8519240951218268\nAgent: ddqn_agent . Episode 92/2000. Number of steps to finish: 15. Loss: 9.794179916381836 Reward: -3.0\nEpsilon = 0.8518389027123145\nEpsilon = 0.8517537188220433\nEpsilon = 0.8516685434501611\nEpsilon = 0.8515833765958161\nEpsilon = 0.8514982182581565\nEpsilon = 0.8514130684363307\nEpsilon = 0.851327927129487\nEpsilon = 0.8512427943367741\nEpsilon = 0.8511576700573404\nEpsilon = 0.8510725542903347\nEpsilon = 0.8509874470349057\nEpsilon = 0.8509023482902022\nEpsilon = 0.8508172580553731\nEpsilon = 0.8507321763295675\nEpsilon = 0.8506471031119346\nEpsilon = 0.8505620384016234\nEpsilon = 0.8504769821977832\nEpsilon = 0.8503919344995634\nEpsilon = 0.8503068953061135\nEpsilon = 0.8502218646165829\nAgent: ddqn_agent . Episode 93/2000. Number of steps to finish: 20. Loss: 13.469229698181152 Reward: -12.0\nEpsilon = 0.8501368424301212\nEpsilon = 0.8500518287458783\nEpsilon = 0.8499668235630037\nEpsilon = 0.8498818268806474\nEpsilon = 0.8497968386979593\nEpsilon = 0.8497118590140895\nEpsilon = 0.8496268878281881\nEpsilon = 0.8495419251394053\nEpsilon = 0.8494569709468913\nEpsilon = 0.8493720252497966\nEpsilon = 0.8492870880472716\nEpsilon = 0.8492021593384669\nEpsilon = 0.849117239122533\nEpsilon = 0.8490323273986208\nEpsilon = 0.8489474241658809\nEpsilon = 0.8488625294234643\nEpsilon = 0.848777643170522\nEpsilon = 0.8486927654062049\nEpsilon = 0.8486078961296643\nAgent: ddqn_agent . Episode 94/2000. Number of steps to finish: 19. Loss: 12.692131996154785 Reward: -7.0\nEpsilon = 0.8485230353400512\nEpsilon = 0.8484381830365173\nEpsilon = 0.8483533392182137\nEpsilon = 0.8482685038842919\nEpsilon = 0.8481836770339034\nEpsilon = 0.8480988586662\nEpsilon = 0.8480140487803335\nEpsilon = 0.8479292473754555\nEpsilon = 0.847844454450718\nEpsilon = 0.847759670005273\nEpsilon = 0.8476748940382725\nEpsilon = 0.8475901265488687\nEpsilon = 0.8475053675362137\nEpsilon = 0.8474206169994601\nEpsilon = 0.8473358749377602\nEpsilon = 0.8472511413502665\nEpsilon = 0.8471664162361314\nEpsilon = 0.8470816995945079\nEpsilon = 0.8469969914245484\nEpsilon = 0.8469122917254059\nAgent: ddqn_agent . Episode 95/2000. Number of steps to finish: 20. Loss: 13.103839874267578 Reward: -14.0\nEpsilon = 0.8468276004962334\nEpsilon = 0.8467429177361838\nEpsilon = 0.8466582434444102\nEpsilon = 0.8465735776200658\nEpsilon = 0.8464889202623038\nEpsilon = 0.8464042713702776\nEpsilon = 0.8463196309431406\nEpsilon = 0.8462349989800463\nEpsilon = 0.8461503754801483\nEpsilon = 0.8460657604426003\nEpsilon = 0.845981153866556\nAgent: ddqn_agent . Episode 96/2000. Number of steps to finish: 11. Loss: 7.4042134284973145 Reward: 1.0\nEpsilon = 0.8458965557511694\nEpsilon = 0.8458119660955943\nEpsilon = 0.8457273848989847\nEpsilon = 0.8456428121604948\nEpsilon = 0.8455582478792787\nEpsilon = 0.8454736920544907\nEpsilon = 0.8453891446852853\nEpsilon = 0.8453046057708168\nEpsilon = 0.8452200753102397\nEpsilon = 0.8451355533027086\nEpsilon = 0.8450510397473784\nEpsilon = 0.8449665346434037\nEpsilon = 0.8448820379899393\nEpsilon = 0.8447975497861403\nEpsilon = 0.8447130700311617\nEpsilon = 0.8446285987241586\nEpsilon = 0.8445441358642862\nEpsilon = 0.8444596814506998\nEpsilon = 0.8443752354825548\nEpsilon = 0.8442907979590065\nAgent: ddqn_agent . Episode 97/2000. Number of steps to finish: 20. Loss: 13.26513957977295 Reward: -12.0\nEpsilon = 0.8442063688792106\nEpsilon = 0.8441219482423227\nEpsilon = 0.8440375360474984\nEpsilon = 0.8439531322938937\nEpsilon = 0.8438687369806643\nEpsilon = 0.8437843501069663\nEpsilon = 0.8436999716719557\nEpsilon = 0.8436156016747884\nEpsilon = 0.843531240114621\nEpsilon = 0.8434468869906095\nEpsilon = 0.8433625423019105\nEpsilon = 0.8432782060476803\nEpsilon = 0.8431938782270756\nEpsilon = 0.8431095588392529\nAgent: ddqn_agent . Episode 98/2000. Number of steps to finish: 14. Loss: 9.353291511535645 Reward: -2.0\nEpsilon = 0.843025247883369\nEpsilon = 0.8429409453585807\nEpsilon = 0.8428566512640449\nEpsilon = 0.8427723655989184\nEpsilon = 0.8426880883623585\nEpsilon = 0.8426038195535223\nEpsilon = 0.842519559171567\nEpsilon = 0.8424353072156499\nEpsilon = 0.8423510636849283\nEpsilon = 0.8422668285785598\nEpsilon = 0.8421826018957019\nEpsilon = 0.8420983836355124\nEpsilon = 0.8420141737971488\nEpsilon = 0.8419299723797691\nEpsilon = 0.8418457793825311\nEpsilon = 0.8417615948045929\nEpsilon = 0.8416774186451125\nEpsilon = 0.841593250903248\nEpsilon = 0.8415090915781577\nEpsilon = 0.8414249406689999\nAgent: ddqn_agent . Episode 99/2000. Number of steps to finish: 20. Loss: 13.078234672546387 Reward: -14.0\nEpsilon = 0.841340798174933\nEpsilon = 0.8412566640951156\nEpsilon = 0.8411725384287061\nEpsilon = 0.8410884211748633\nEpsilon = 0.8410043123327458\nEpsilon = 0.8409202119015124\nEpsilon = 0.8408361198803223\nEpsilon = 0.8407520362683343\nEpsilon = 0.8406679610647074\nEpsilon = 0.8405838942686009\nEpsilon = 0.8404998358791741\nEpsilon = 0.8404157858955862\nEpsilon = 0.8403317443169966\nEpsilon = 0.8402477111425649\nEpsilon = 0.8401636863714507\nEpsilon = 0.8400796700028136\nEpsilon = 0.8399956620358133\nEpsilon = 0.8399116624696098\nEpsilon = 0.8398276713033629\nEpsilon = 0.8397436885362326\nAgent: ddqn_agent . Episode 100/2000. Number of steps to finish: 20. Loss: 13.195117950439453 Reward: -10.0\nEpsilon = 0.839659714167379\nEpsilon = 0.8395757481959623\nEpsilon = 0.8394917906211427\nEpsilon = 0.8394078414420806\nEpsilon = 0.8393239006579364\nEpsilon = 0.8392399682678706\nEpsilon = 0.8391560442710438\nEpsilon = 0.8390721286666167\nEpsilon = 0.8389882214537501\nEpsilon = 0.8389043226316047\nEpsilon = 0.8388204321993415\nEpsilon = 0.8387365501561216\nEpsilon = 0.838652676501106\nEpsilon = 0.8385688112334558\nEpsilon = 0.8384849543523325\nEpsilon = 0.8384011058568973\nEpsilon = 0.8383172657463116\nEpsilon = 0.8382334340197369\nEpsilon = 0.838149610676335\nEpsilon = 0.8380657957152674\nAgent: ddqn_agent . Episode 101/2000. Number of steps to finish: 20. Loss: 13.215673446655273 Reward: -12.0\nEpsilon = 0.8379819891356959\nEpsilon = 0.8378981909367823\nEpsilon = 0.8378144011176887\nEpsilon = 0.8377306196775769\nEpsilon = 0.8376468466156092\nEpsilon = 0.8375630819309476\nEpsilon = 0.8374793256227545\nEpsilon = 0.8373955776901922\nEpsilon = 0.8373118381324233\nEpsilon = 0.83722810694861\nEpsilon = 0.8371443841379153\nEpsilon = 0.8370606696995014\nEpsilon = 0.8369769636325315\nEpsilon = 0.8368932659361682\nEpsilon = 0.8368095766095746\nEpsilon = 0.8367258956519136\nEpsilon = 0.8366422230623484\nEpsilon = 0.8365585588400422\nEpsilon = 0.8364749029841582\nEpsilon = 0.8363912554938597\nAgent: ddqn_agent . Episode 102/2000. Number of steps to finish: 20. Loss: 12.858491897583008 Reward: -12.0\nEpsilon = 0.8363076163683103\nEpsilon = 0.8362239856066734\nEpsilon = 0.8361403632081128\nEpsilon = 0.836056749171792\nEpsilon = 0.8359731434968748\nEpsilon = 0.8358895461825251\nEpsilon = 0.8358059572279068\nEpsilon = 0.8357223766321841\nEpsilon = 0.8356388043945209\nEpsilon = 0.8355552405140815\nEpsilon = 0.8354716849900301\nEpsilon = 0.8353881378215311\nEpsilon = 0.835304599007749\nEpsilon = 0.8352210685478483\nEpsilon = 0.8351375464409935\nEpsilon = 0.8350540326863494\nEpsilon = 0.8349705272830807\nAgent: ddqn_agent . Episode 103/2000. Number of steps to finish: 17. Loss: 11.363950729370117 Reward: -5.0\nEpsilon = 0.8348870302303525\nEpsilon = 0.8348035415273295\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.8347200611731768\nEpsilon = 0.8346365891670594\nEpsilon = 0.8345531255081428\nEpsilon = 0.8344696701955919\nEpsilon = 0.8343862232285724\nEpsilon = 0.8343027846062495\nEpsilon = 0.8342193543277889\nEpsilon = 0.8341359323923562\nEpsilon = 0.8340525187991169\nEpsilon = 0.833969113547237\nEpsilon = 0.8338857166358823\nEpsilon = 0.8338023280642187\nEpsilon = 0.8337189478314123\nEpsilon = 0.8336355759366292\nEpsilon = 0.8335522123790355\nEpsilon = 0.8334688571577976\nEpsilon = 0.8333855102720819\nEpsilon = 0.8333021717210547\nAgent: ddqn_agent . Episode 104/2000. Number of steps to finish: 20. Loss: 13.390775680541992 Reward: -16.0\nEpsilon = 0.8332188415038826\nEpsilon = 0.8331355196197322\nEpsilon = 0.8330522060677702\nEpsilon = 0.8329689008471635\nEpsilon = 0.8328856039570788\nEpsilon = 0.832802315396683\nEpsilon = 0.8327190351651433\nEpsilon = 0.8326357632616268\nEpsilon = 0.8325524996853007\nEpsilon = 0.8324692444353321\nEpsilon = 0.8323859975108886\nEpsilon = 0.8323027589111375\nEpsilon = 0.8322195286352464\nEpsilon = 0.8321363066823829\nEpsilon = 0.8320530930517147\nEpsilon = 0.8319698877424095\nEpsilon = 0.8318866907536353\nEpsilon = 0.83180350208456\nEpsilon = 0.8317203217343515\nAgent: ddqn_agent . Episode 105/2000. Number of steps to finish: 19. Loss: 12.673730850219727 Reward: -7.0\nEpsilon = 0.8316371497021781\nEpsilon = 0.8315539859872079\nEpsilon = 0.8314708305886092\nEpsilon = 0.8313876835055503\nEpsilon = 0.8313045447371997\nEpsilon = 0.831221414282726\nEpsilon = 0.8311382921412978\nEpsilon = 0.8310551783120836\nEpsilon = 0.8309720727942524\nEpsilon = 0.830888975586973\nEpsilon = 0.8308058866894144\nEpsilon = 0.8307228061007454\nEpsilon = 0.8306397338201353\nEpsilon = 0.8305566698467534\nEpsilon = 0.8304736141797687\nEpsilon = 0.8303905668183508\nEpsilon = 0.8303075277616689\nEpsilon = 0.8302244970088928\nEpsilon = 0.8301414745591918\nEpsilon = 0.8300584604117359\nAgent: ddqn_agent . Episode 106/2000. Number of steps to finish: 20. Loss: 13.225057601928711 Reward: -10.0\nEpsilon = 0.8299754545656948\nEpsilon = 0.8298924570202382\nEpsilon = 0.8298094677745361\nEpsilon = 0.8297264868277587\nEpsilon = 0.829643514179076\nEpsilon = 0.8295605498276581\nEpsilon = 0.8294775937726754\nEpsilon = 0.8293946460132982\nEpsilon = 0.8293117065486968\nEpsilon = 0.829228775378042\nEpsilon = 0.8291458525005042\nEpsilon = 0.8290629379152542\nEpsilon = 0.8289800316214627\nEpsilon = 0.8288971336183005\nEpsilon = 0.8288142439049387\nAgent: ddqn_agent . Episode 107/2000. Number of steps to finish: 15. Loss: 10.028887748718262 Reward: -3.0\nEpsilon = 0.8287313624805482\nEpsilon = 0.8286484893443002\nEpsilon = 0.8285656244953659\nEpsilon = 0.8284827679329163\nEpsilon = 0.828399919656123\nEpsilon = 0.8283170796641575\nEpsilon = 0.828234247956191\nEpsilon = 0.8281514245313955\nEpsilon = 0.8280686093889423\nEpsilon = 0.8279858025280035\nEpsilon = 0.8279030039477506\nEpsilon = 0.8278202136473558\nEpsilon = 0.8277374316259911\nEpsilon = 0.8276546578828285\nEpsilon = 0.8275718924170403\nEpsilon = 0.8274891352277987\nEpsilon = 0.8274063863142759\nEpsilon = 0.8273236456756444\nEpsilon = 0.8272409133110769\nEpsilon = 0.8271581892197457\nAgent: ddqn_agent . Episode 108/2000. Number of steps to finish: 20. Loss: 13.165754318237305 Reward: -10.0\nEpsilon = 0.8270754734008238\nEpsilon = 0.8269927658534837\nEpsilon = 0.8269100665768984\nEpsilon = 0.8268273755702407\nEpsilon = 0.8267446928326837\nEpsilon = 0.8266620183634005\nEpsilon = 0.8265793521615642\nEpsilon = 0.8264966942263481\nEpsilon = 0.8264140445569255\nEpsilon = 0.8263314031524698\nEpsilon = 0.8262487700121546\nEpsilon = 0.8261661451351534\nEpsilon = 0.8260835285206399\nEpsilon = 0.8260009201677879\nEpsilon = 0.8259183200757712\nEpsilon = 0.8258357282437636\nEpsilon = 0.8257531446709393\nEpsilon = 0.8256705693564722\nEpsilon = 0.8255880022995365\nEpsilon = 0.8255054434993067\nAgent: ddqn_agent . Episode 109/2000. Number of steps to finish: 20. Loss: 13.44412899017334 Reward: -12.0\nEpsilon = 0.8254228929549567\nEpsilon = 0.8253403506656612\nEpsilon = 0.8252578166305947\nEpsilon = 0.8251752908489317\nEpsilon = 0.8250927733198469\nEpsilon = 0.8250102640425149\nEpsilon = 0.8249277630161106\nEpsilon = 0.824845270239809\nEpsilon = 0.824762785712785\nEpsilon = 0.8246803094342138\nEpsilon = 0.8245978414032704\nEpsilon = 0.8245153816191301\nEpsilon = 0.8244329300809682\nEpsilon = 0.8243504867879601\nEpsilon = 0.8242680517392813\nEpsilon = 0.8241856249341074\nEpsilon = 0.824103206371614\nEpsilon = 0.8240207960509769\nEpsilon = 0.8239383939713718\nEpsilon = 0.8238560001319747\nAgent: ddqn_agent . Episode 110/2000. Number of steps to finish: 20. Loss: 13.085030555725098 Reward: -10.0\nEpsilon = 0.8237736145319615\nEpsilon = 0.8236912371705083\nEpsilon = 0.8236088680467912\nEpsilon = 0.8235265071599865\nEpsilon = 0.8234441545092706\nEpsilon = 0.8233618100938196\nEpsilon = 0.8232794739128102\nEpsilon = 0.8231971459654189\nEpsilon = 0.8231148262508224\nEpsilon = 0.8230325147681974\nEpsilon = 0.8229502115167205\nEpsilon = 0.8228679164955689\nEpsilon = 0.8227856297039193\nEpsilon = 0.822703351140949\nEpsilon = 0.8226210808058348\nAgent: ddqn_agent . Episode 111/2000. Number of steps to finish: 15. Loss: 10.03719711303711 Reward: -3.0\nEpsilon = 0.8225388186977542\nEpsilon = 0.8224565648158845\nEpsilon = 0.8223743191594028\nEpsilon = 0.8222920817274869\nEpsilon = 0.8222098525193142\nEpsilon = 0.8221276315340623\nEpsilon = 0.822045418770909\nEpsilon = 0.8219632142290318\nEpsilon = 0.8218810179076089\nEpsilon = 0.8217988298058182\nEpsilon = 0.8217166499228377\nEpsilon = 0.8216344782578454\nEpsilon = 0.8215523148100197\nEpsilon = 0.8214701595785386\nEpsilon = 0.8213880125625808\nEpsilon = 0.8213058737613245\nEpsilon = 0.8212237431739483\nEpsilon = 0.8211416207996309\nEpsilon = 0.821059506637551\nEpsilon = 0.8209774006868873\nAgent: ddqn_agent . Episode 112/2000. Number of steps to finish: 20. Loss: 13.407029151916504 Reward: -10.0\nEpsilon = 0.8208953029468186\nEpsilon = 0.8208132134165239\nEpsilon = 0.8207311320951822\nEpsilon = 0.8206490589819727\nEpsilon = 0.8205669940760745\nEpsilon = 0.8204849373766668\nEpsilon = 0.8204028888829292\nEpsilon = 0.820320848594041\nEpsilon = 0.8202388165091816\nEpsilon = 0.8201567926275307\nEpsilon = 0.820074776948268\nEpsilon = 0.8199927694705732\nEpsilon = 0.8199107701936261\nEpsilon = 0.8198287791166068\nEpsilon = 0.8197467962386952\nEpsilon = 0.8196648215590713\nEpsilon = 0.8195828550769154\nEpsilon = 0.8195008967914077\nEpsilon = 0.8194189467017285\nEpsilon = 0.8193370048070584\nAgent: ddqn_agent . Episode 113/2000. Number of steps to finish: 20. Loss: 13.13477611541748 Reward: -12.0\nEpsilon = 0.8192550711065777\nEpsilon = 0.8191731455994671\nEpsilon = 0.8190912282849071\nEpsilon = 0.8190093191620786\nEpsilon = 0.8189274182301625\nEpsilon = 0.8188455254883394\nEpsilon = 0.8187636409357906\nEpsilon = 0.818681764571697\nEpsilon = 0.8185998963952398\nEpsilon = 0.8185180364056003\nEpsilon = 0.8184361846019598\nEpsilon = 0.8183543409834996\nEpsilon = 0.8182725055494012\nEpsilon = 0.8181906782988463\nEpsilon = 0.8181088592310164\nEpsilon = 0.8180270483450933\nEpsilon = 0.8179452456402588\nEpsilon = 0.8178634511156948\nEpsilon = 0.8177816647705832\nEpsilon = 0.8176998866041062\nAgent: ddqn_agent . Episode 114/2000. Number of steps to finish: 20. Loss: 12.897980690002441 Reward: -14.0\nEpsilon = 0.8176181166154458\nEpsilon = 0.8175363548037843\nEpsilon = 0.8174546011683039\nEpsilon = 0.8173728557081871\nEpsilon = 0.8172911184226163\nEpsilon = 0.817209389310774\nEpsilon = 0.8171276683718429\nEpsilon = 0.8170459556050057\nEpsilon = 0.8169642510094453\nEpsilon = 0.8168825545843443\nEpsilon = 0.8168008663288859\nEpsilon = 0.816719186242253\nEpsilon = 0.8166375143236289\nEpsilon = 0.8165558505721965\nEpsilon = 0.8164741949871392\nEpsilon = 0.8163925475676406\nEpsilon = 0.8163109083128838\nEpsilon = 0.8162292772220525\nEpsilon = 0.8161476542943303\nEpsilon = 0.8160660395289009\nAgent: ddqn_agent . Episode 115/2000. Number of steps to finish: 20. Loss: 13.331986427307129 Reward: -12.0\nEpsilon = 0.815984432924948\nEpsilon = 0.8159028344816556\nEpsilon = 0.8158212441982075\nEpsilon = 0.8157396620737877\nEpsilon = 0.8156580881075803\nEpsilon = 0.8155765222987695\nEpsilon = 0.8154949646465397\nEpsilon = 0.815413415150075\nEpsilon = 0.8153318738085601\nEpsilon = 0.8152503406211793\nEpsilon = 0.8151688155871172\nEpsilon = 0.8150872987055585\nEpsilon = 0.815005789975688\nEpsilon = 0.8149242893966904\nEpsilon = 0.8148427969677507\nEpsilon = 0.814761312688054\nEpsilon = 0.8146798365567852\nEpsilon = 0.8145983685731295\nEpsilon = 0.8145169087362722\nEpsilon = 0.8144354570453987\nAgent: ddqn_agent . Episode 116/2000. Number of steps to finish: 20. Loss: 13.200478553771973 Reward: -10.0\nEpsilon = 0.8143540134996942\nEpsilon = 0.8142725780983442\nEpsilon = 0.8141911508405344\nEpsilon = 0.8141097317254503\nEpsilon = 0.8140283207522778\nEpsilon = 0.8139469179202026\nEpsilon = 0.8138655232284105\nEpsilon = 0.8137841366760877\nEpsilon = 0.8137027582624201\nEpsilon = 0.8136213879865939\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.8135400258477953\nEpsilon = 0.8134586718452105\nEpsilon = 0.8133773259780259\nEpsilon = 0.8132959882454281\nEpsilon = 0.8132146586466036\nEpsilon = 0.8131333371807389\nEpsilon = 0.8130520238470208\nAgent: ddqn_agent . Episode 117/2000. Number of steps to finish: 17. Loss: 10.903022766113281 Reward: -5.0\nEpsilon = 0.8129707186446361\nEpsilon = 0.8128894215727717\nEpsilon = 0.8128081326306144\nEpsilon = 0.8127268518173513\nEpsilon = 0.8126455791321695\nEpsilon = 0.8125643145742563\nEpsilon = 0.8124830581427989\nEpsilon = 0.8124018098369846\nEpsilon = 0.812320569656001\nEpsilon = 0.8122393375990353\nEpsilon = 0.8121581136652755\nEpsilon = 0.8120768978539089\nEpsilon = 0.8119956901641235\nEpsilon = 0.8119144905951071\nEpsilon = 0.8118332991460476\nEpsilon = 0.811752115816133\nEpsilon = 0.8116709406045515\nEpsilon = 0.811589773510491\nEpsilon = 0.8115086145331399\nEpsilon = 0.8114274636716866\nAgent: ddqn_agent . Episode 118/2000. Number of steps to finish: 20. Loss: 12.779616355895996 Reward: -10.0\nEpsilon = 0.8113463209253194\nEpsilon = 0.811265186293227\nEpsilon = 0.8111840597745976\nEpsilon = 0.8111029413686202\nEpsilon = 0.8110218310744833\nEpsilon = 0.8109407288913759\nEpsilon = 0.8108596348184868\nEpsilon = 0.8107785488550049\nEpsilon = 0.8106974710001194\nEpsilon = 0.8106164012530194\nEpsilon = 0.810535339612894\nEpsilon = 0.8104542860789328\nEpsilon = 0.8103732406503249\nEpsilon = 0.8102922033262598\nEpsilon = 0.8102111741059272\nEpsilon = 0.8101301529885166\nEpsilon = 0.8100491399732178\nEpsilon = 0.8099681350592205\nEpsilon = 0.8098871382457146\nEpsilon = 0.80980614953189\nAgent: ddqn_agent . Episode 119/2000. Number of steps to finish: 20. Loss: 13.175749778747559 Reward: -10.0\nEpsilon = 0.8097251689169368\nEpsilon = 0.8096441964000451\nEpsilon = 0.8095632319804051\nEpsilon = 0.8094822756572071\nEpsilon = 0.8094013274296414\nEpsilon = 0.8093203872968985\nEpsilon = 0.8092394552581688\nEpsilon = 0.8091585313126429\nEpsilon = 0.8090776154595116\nEpsilon = 0.8089967076979657\nEpsilon = 0.808915808027196\nEpsilon = 0.8088349164463933\nEpsilon = 0.8087540329547487\nEpsilon = 0.8086731575514532\nEpsilon = 0.808592290235698\nEpsilon = 0.8085114310066744\nEpsilon = 0.8084305798635738\nEpsilon = 0.8083497368055874\nEpsilon = 0.8082689018319069\nEpsilon = 0.8081880749417237\nAgent: ddqn_agent . Episode 120/2000. Number of steps to finish: 20. Loss: 13.175923347473145 Reward: -10.0\nEpsilon = 0.8081072561342296\nEpsilon = 0.8080264454086162\nEpsilon = 0.8079456427640754\nEpsilon = 0.807864848199799\nEpsilon = 0.807784061714979\nEpsilon = 0.8077032833088075\nEpsilon = 0.8076225129804766\nEpsilon = 0.8075417507291786\nEpsilon = 0.8074609965541056\nEpsilon = 0.8073802504544503\nEpsilon = 0.8072995124294048\nEpsilon = 0.8072187824781618\nEpsilon = 0.807138060599914\nEpsilon = 0.807057346793854\nEpsilon = 0.8069766410591747\nEpsilon = 0.8068959433950688\nEpsilon = 0.8068152538007293\nEpsilon = 0.8067345722753492\nEpsilon = 0.8066538988181217\nEpsilon = 0.8065732334282398\nAgent: ddqn_agent . Episode 121/2000. Number of steps to finish: 20. Loss: 13.300162315368652 Reward: -14.0\nEpsilon = 0.806492576104897\nEpsilon = 0.8064119268472865\nEpsilon = 0.8063312856546018\nEpsilon = 0.8062506525260363\nEpsilon = 0.8061700274607837\nEpsilon = 0.8060894104580376\nEpsilon = 0.8060088015169918\nEpsilon = 0.8059282006368401\nEpsilon = 0.8058476078167764\nEpsilon = 0.8057670230559948\nEpsilon = 0.8056864463536892\nEpsilon = 0.8056058777090539\nEpsilon = 0.805525317121283\nEpsilon = 0.8054447645895708\nEpsilon = 0.8053642201131118\nEpsilon = 0.8052836836911005\nEpsilon = 0.8052031553227313\nEpsilon = 0.805122635007199\nEpsilon = 0.8050421227436984\nEpsilon = 0.804961618531424\nAgent: ddqn_agent . Episode 122/2000. Number of steps to finish: 20. Loss: 13.094292640686035 Reward: -10.0\nEpsilon = 0.8048811223695709\nEpsilon = 0.804800634257334\nEpsilon = 0.8047201541939082\nEpsilon = 0.8046396821784888\nEpsilon = 0.804559218210271\nEpsilon = 0.80447876228845\nEpsilon = 0.8043983144122211\nEpsilon = 0.8043178745807799\nEpsilon = 0.8042374427933218\nEpsilon = 0.8041570190490425\nEpsilon = 0.8040766033471376\nEpsilon = 0.8039961956868029\nEpsilon = 0.8039157960672343\nEpsilon = 0.8038354044876276\nEpsilon = 0.8037550209471789\nEpsilon = 0.8036746454450842\nEpsilon = 0.8035942779805396\nEpsilon = 0.8035139185527416\nEpsilon = 0.8034335671608863\nEpsilon = 0.8033532238041703\nAgent: ddqn_agent . Episode 123/2000. Number of steps to finish: 20. Loss: 13.147441864013672 Reward: -10.0\nEpsilon = 0.8032728884817899\nEpsilon = 0.8031925611929417\nEpsilon = 0.8031122419368224\nEpsilon = 0.8030319307126287\nEpsilon = 0.8029516275195574\nEpsilon = 0.8028713323568054\nEpsilon = 0.8027910452235697\nEpsilon = 0.8027107661190473\nEpsilon = 0.8026304950424354\nEpsilon = 0.8025502319929312\nEpsilon = 0.8024699769697319\nEpsilon = 0.802389729972035\nEpsilon = 0.8023094909990377\nEpsilon = 0.8022292600499378\nEpsilon = 0.8021490371239328\nEpsilon = 0.8020688222202205\nEpsilon = 0.8019886153379985\nEpsilon = 0.8019084164764647\nEpsilon = 0.801828225634817\nEpsilon = 0.8017480428122535\nAgent: ddqn_agent . Episode 124/2000. Number of steps to finish: 20. Loss: 13.265716552734375 Reward: -12.0\nEpsilon = 0.8016678680079723\nEpsilon = 0.8015877012211715\nEpsilon = 0.8015075424510494\nEpsilon = 0.8014273916968043\nEpsilon = 0.8013472489576347\nEpsilon = 0.8012671142327389\nEpsilon = 0.8011869875213157\nEpsilon = 0.8011068688225635\nEpsilon = 0.8010267581356812\nEpsilon = 0.8009466554598677\nEpsilon = 0.8008665607943217\nEpsilon = 0.8007864741382423\nEpsilon = 0.8007063954908286\nEpsilon = 0.8006263248512795\nEpsilon = 0.8005462622187943\nEpsilon = 0.8004662075925725\nEpsilon = 0.8003861609718133\nEpsilon = 0.8003061223557161\nEpsilon = 0.8002260917434805\nEpsilon = 0.8001460691343061\nAgent: ddqn_agent . Episode 125/2000. Number of steps to finish: 20. Loss: 12.814315795898438 Reward: -12.0\nEpsilon = 0.8000660545273927\nEpsilon = 0.79998604792194\nEpsilon = 0.7999060493171478\nEpsilon = 0.7998260587122161\nEpsilon = 0.799746076106345\nEpsilon = 0.7996661014987343\nEpsilon = 0.7995861348885844\nEpsilon = 0.7995061762750956\nEpsilon = 0.799426225657468\nEpsilon = 0.7993462830349023\nEpsilon = 0.7992663484065988\nEpsilon = 0.7991864217717581\nEpsilon = 0.799106503129581\nEpsilon = 0.799026592479268\nEpsilon = 0.79894668982002\nEpsilon = 0.7988667951510381\nEpsilon = 0.798786908471523\nEpsilon = 0.7987070297806759\nEpsilon = 0.7986271590776978\nEpsilon = 0.79854729636179\nAgent: ddqn_agent . Episode 126/2000. Number of steps to finish: 20. Loss: 13.05958366394043 Reward: -8.0\nEpsilon = 0.7984674416321539\nEpsilon = 0.7983875948879907\nEpsilon = 0.7983077561285019\nEpsilon = 0.7982279253528891\nEpsilon = 0.7981481025603538\nEpsilon = 0.7980682877500979\nEpsilon = 0.7979884809213229\nEpsilon = 0.7979086820732307\nEpsilon = 0.7978288912050234\nEpsilon = 0.797749108315903\nEpsilon = 0.7976693334050714\nEpsilon = 0.7975895664717308\nEpsilon = 0.7975098075150837\nEpsilon = 0.7974300565343322\nEpsilon = 0.7973503135286788\nEpsilon = 0.797270578497326\nEpsilon = 0.7971908514394762\nEpsilon = 0.7971111323543323\nEpsilon = 0.7970314212410968\nEpsilon = 0.7969517180989727\nAgent: ddqn_agent . Episode 127/2000. Number of steps to finish: 20. Loss: 13.338471412658691 Reward: -10.0\nEpsilon = 0.7968720229271629\nEpsilon = 0.7967923357248702\nEpsilon = 0.7967126564912977\nEpsilon = 0.7966329852256486\nEpsilon = 0.796553321927126\nEpsilon = 0.7964736665949333\nEpsilon = 0.7963940192282739\nEpsilon = 0.7963143798263511\nEpsilon = 0.7962347483883685\nEpsilon = 0.7961551249135297\nEpsilon = 0.7960755094010383\nEpsilon = 0.7959959018500983\nEpsilon = 0.7959163022599133\nEpsilon = 0.7958367106296873\nEpsilon = 0.7957571269586243\nEpsilon = 0.7956775512459284\nEpsilon = 0.7955979834908038\nEpsilon = 0.7955184236924547\nEpsilon = 0.7954388718500855\nEpsilon = 0.7953593279629005\nAgent: ddqn_agent . Episode 128/2000. Number of steps to finish: 20. Loss: 13.234159469604492 Reward: -8.0\nEpsilon = 0.7952797920301042\nEpsilon = 0.7952002640509013\nEpsilon = 0.7951207440244962\nEpsilon = 0.7950412319500937\nEpsilon = 0.7949617278268987\nEpsilon = 0.794882231654116\nEpsilon = 0.7948027434309506\nEpsilon = 0.7947232631566075\nEpsilon = 0.7946437908302918\nEpsilon = 0.7945643264512088\nEpsilon = 0.7944848700185637\nEpsilon = 0.7944054215315619\nEpsilon = 0.7943259809894088\nEpsilon = 0.7942465483913098\nEpsilon = 0.7941671237364707\nAgent: ddqn_agent . Episode 129/2000. Number of steps to finish: 15. Loss: 9.886653900146484 Reward: -3.0\nEpsilon = 0.794087707024097\nEpsilon = 0.7940082982533946\nEpsilon = 0.7939288974235692\nEpsilon = 0.7938495045338269\nEpsilon = 0.7937701195833735\nEpsilon = 0.7936907425714151\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.793611373497158\nEpsilon = 0.7935320123598083\nEpsilon = 0.7934526591585723\nEpsilon = 0.7933733138926565\nEpsilon = 0.7932939765612673\nEpsilon = 0.7932146471636112\nEpsilon = 0.7931353256988949\nEpsilon = 0.793056012166325\nEpsilon = 0.7929767065651084\nEpsilon = 0.7928974088944519\nEpsilon = 0.7928181191535625\nEpsilon = 0.7927388373416472\nEpsilon = 0.792659563457913\nEpsilon = 0.7925802975015672\nAgent: ddqn_agent . Episode 130/2000. Number of steps to finish: 20. Loss: 12.934958457946777 Reward: -14.0\nEpsilon = 0.792501039471817\nEpsilon = 0.7924217893678699\nEpsilon = 0.7923425471889332\nEpsilon = 0.7922633129342143\nEpsilon = 0.7921840866029208\nEpsilon = 0.7921048681942605\nEpsilon = 0.792025657707441\nEpsilon = 0.7919464551416703\nEpsilon = 0.7918672604961561\nEpsilon = 0.7917880737701065\nEpsilon = 0.7917088949627296\nEpsilon = 0.7916297240732333\nEpsilon = 0.791550561100826\nEpsilon = 0.7914714060447159\nEpsilon = 0.7913922589041115\nEpsilon = 0.791313119678221\nEpsilon = 0.7912339883662532\nEpsilon = 0.7911548649674166\nEpsilon = 0.7910757494809199\nEpsilon = 0.7909966419059717\nAgent: ddqn_agent . Episode 131/2000. Number of steps to finish: 20. Loss: 13.025598526000977 Reward: -18.0\nEpsilon = 0.7909175422417811\nEpsilon = 0.790838450487557\nEpsilon = 0.7907593666425082\nEpsilon = 0.790680290705844\nEpsilon = 0.7906012226767735\nEpsilon = 0.7905221625545058\nEpsilon = 0.7904431103382503\nEpsilon = 0.7903640660272165\nEpsilon = 0.7902850296206139\nEpsilon = 0.7902060011176518\nEpsilon = 0.79012698051754\nEpsilon = 0.7900479678194883\nEpsilon = 0.7899689630227064\nEpsilon = 0.789889966126404\nEpsilon = 0.7898109771297914\nEpsilon = 0.7897319960320784\nEpsilon = 0.7896530228324753\nEpsilon = 0.789574057530192\nEpsilon = 0.789495100124439\nEpsilon = 0.7894161506144266\nAgent: ddqn_agent . Episode 132/2000. Number of steps to finish: 20. Loss: 12.635719299316406 Reward: -12.0\nEpsilon = 0.7893372089993652\nEpsilon = 0.7892582752784653\nEpsilon = 0.7891793494509374\nEpsilon = 0.7891004315159923\nEpsilon = 0.7890215214728407\nEpsilon = 0.7889426193206934\nEpsilon = 0.7888637250587613\nEpsilon = 0.7887848386862555\nEpsilon = 0.7887059602023868\nEpsilon = 0.7886270896063666\nEpsilon = 0.788548226897406\nEpsilon = 0.7884693720747162\nEpsilon = 0.7883905251375087\nEpsilon = 0.788311686084995\nEpsilon = 0.7882328549163865\nEpsilon = 0.7881540316308949\nEpsilon = 0.7880752162277318\nEpsilon = 0.787996408706109\nEpsilon = 0.7879176090652383\nEpsilon = 0.7878388173043318\nAgent: ddqn_agent . Episode 133/2000. Number of steps to finish: 20. Loss: 13.200793266296387 Reward: -12.0\nEpsilon = 0.7877600334226014\nEpsilon = 0.7876812574192591\nEpsilon = 0.7876024892935172\nEpsilon = 0.7875237290445878\nEpsilon = 0.7874449766716833\nEpsilon = 0.7873662321740161\nEpsilon = 0.7872874955507987\nEpsilon = 0.7872087668012436\nEpsilon = 0.7871300459245635\nEpsilon = 0.787051332919971\nEpsilon = 0.786972627786679\nAgent: ddqn_agent . Episode 134/2000. Number of steps to finish: 11. Loss: 6.933778762817383 Reward: 1.0\nEpsilon = 0.7868939305239003\nEpsilon = 0.786815241130848\nEpsilon = 0.7867365596067349\nEpsilon = 0.7866578859507742\nEpsilon = 0.7865792201621792\nEpsilon = 0.786500562240163\nEpsilon = 0.786421912183939\nEpsilon = 0.7863432699927206\nEpsilon = 0.7862646356657214\nEpsilon = 0.7861860092021548\nEpsilon = 0.7861073906012346\nEpsilon = 0.7860287798621745\nEpsilon = 0.7859501769841882\nEpsilon = 0.7858715819664899\nEpsilon = 0.7857929948082932\nEpsilon = 0.7857144155088124\nEpsilon = 0.7856358440672615\nEpsilon = 0.7855572804828548\nEpsilon = 0.7854787247548065\nEpsilon = 0.785400176882331\nAgent: ddqn_agent . Episode 135/2000. Number of steps to finish: 20. Loss: 12.703237533569336 Reward: -14.0\nEpsilon = 0.7853216368646427\nEpsilon = 0.7852431047009563\nEpsilon = 0.7851645803904862\nEpsilon = 0.7850860639324472\nEpsilon = 0.7850075553260539\nEpsilon = 0.7849290545705213\nEpsilon = 0.7848505616650643\nEpsilon = 0.7847720766088978\nEpsilon = 0.784693599401237\nEpsilon = 0.7846151300412968\nEpsilon = 0.7845366685282926\nEpsilon = 0.7844582148614399\nEpsilon = 0.7843797690399538\nEpsilon = 0.7843013310630498\nEpsilon = 0.7842229009299435\nEpsilon = 0.7841444786398505\nEpsilon = 0.7840660641919865\nEpsilon = 0.7839876575855673\nEpsilon = 0.7839092588198088\nEpsilon = 0.7838308678939268\nAgent: ddqn_agent . Episode 136/2000. Number of steps to finish: 20. Loss: 12.684995651245117 Reward: -10.0\nEpsilon = 0.7837524848071373\nEpsilon = 0.7836741095586566\nEpsilon = 0.7835957421477007\nEpsilon = 0.7835173825734859\nEpsilon = 0.7834390308352286\nEpsilon = 0.7833606869321451\nEpsilon = 0.7832823508634519\nEpsilon = 0.7832040226283655\nEpsilon = 0.7831257022261027\nEpsilon = 0.78304738965588\nEpsilon = 0.7829690849169145\nEpsilon = 0.7828907880084228\nEpsilon = 0.782812498929622\nEpsilon = 0.7827342176797291\nEpsilon = 0.7826559442579611\nEpsilon = 0.7825776786635353\nEpsilon = 0.782499420895669\nEpsilon = 0.7824211709535794\nEpsilon = 0.782342928836484\nEpsilon = 0.7822646945436004\nAgent: ddqn_agent . Episode 137/2000. Number of steps to finish: 20. Loss: 12.79298210144043 Reward: -12.0\nEpsilon = 0.782186468074146\nEpsilon = 0.7821082494273386\nEpsilon = 0.7820300386023958\nEpsilon = 0.7819518355985356\nEpsilon = 0.7818736404149758\nEpsilon = 0.7817954530509342\nEpsilon = 0.7817172735056291\nEpsilon = 0.7816391017782786\nEpsilon = 0.7815609378681008\nEpsilon = 0.781482781774314\nEpsilon = 0.7814046334961365\nEpsilon = 0.7813264930327869\nEpsilon = 0.7812483603834837\nEpsilon = 0.7811702355474454\nEpsilon = 0.7810921185238906\nEpsilon = 0.7810140093120382\nEpsilon = 0.780935907911107\nEpsilon = 0.7808578143203159\nEpsilon = 0.7807797285388839\nEpsilon = 0.78070165056603\nAgent: ddqn_agent . Episode 138/2000. Number of steps to finish: 20. Loss: 12.906275749206543 Reward: -10.0\nEpsilon = 0.7806235804009735\nEpsilon = 0.7805455180429334\nEpsilon = 0.780467463491129\nEpsilon = 0.7803894167447799\nEpsilon = 0.7803113778031054\nEpsilon = 0.7802333466653252\nEpsilon = 0.7801553233306586\nEpsilon = 0.7800773077983255\nEpsilon = 0.7799993000675457\nEpsilon = 0.7799213001375389\nEpsilon = 0.7798433080075252\nEpsilon = 0.7797653236767245\nEpsilon = 0.7796873471443568\nEpsilon = 0.7796093784096424\nEpsilon = 0.7795314174718014\nEpsilon = 0.7794534643300542\nEpsilon = 0.7793755189836212\nEpsilon = 0.7792975814317229\nEpsilon = 0.7792196516735798\nEpsilon = 0.7791417297084124\nAgent: ddqn_agent . Episode 139/2000. Number of steps to finish: 20. Loss: 12.742189407348633 Reward: -10.0\nEpsilon = 0.7790638155354416\nEpsilon = 0.778985909153888\nEpsilon = 0.7789080105629727\nEpsilon = 0.7788301197619164\nEpsilon = 0.7787522367499402\nEpsilon = 0.7786743615262652\nEpsilon = 0.7785964940901127\nEpsilon = 0.7785186344407037\nEpsilon = 0.7784407825772596\nEpsilon = 0.7783629384990018\nEpsilon = 0.7782851022051519\nEpsilon = 0.7782072736949314\nEpsilon = 0.7781294529675619\nEpsilon = 0.7780516400222651\nEpsilon = 0.7779738348582629\nEpsilon = 0.7778960374747771\nEpsilon = 0.7778182478710296\nEpsilon = 0.7777404660462425\nEpsilon = 0.7776626919996379\nAgent: ddqn_agent . Episode 140/2000. Number of steps to finish: 19. Loss: 11.970366477966309 Reward: -7.0\nEpsilon = 0.7775849257304379\nEpsilon = 0.7775071672378648\nEpsilon = 0.7774294165211411\nEpsilon = 0.777351673579489\nEpsilon = 0.777273938412131\nEpsilon = 0.7771962110182898\nEpsilon = 0.777118491397188\nEpsilon = 0.7770407795480483\nEpsilon = 0.7769630754700935\nEpsilon = 0.7768853791625465\nEpsilon = 0.7768076906246303\nEpsilon = 0.7767300098555678\nEpsilon = 0.7766523368545822\nEpsilon = 0.7765746716208968\nEpsilon = 0.7764970141537347\nEpsilon = 0.7764193644523194\nEpsilon = 0.7763417225158742\nEpsilon = 0.7762640883436226\nEpsilon = 0.7761864619347882\nEpsilon = 0.7761088432885948\nAgent: ddqn_agent . Episode 141/2000. Number of steps to finish: 20. Loss: 12.678550720214844 Reward: -10.0\nEpsilon = 0.7760312324042659\nEpsilon = 0.7759536292810254\nEpsilon = 0.7758760339180973\nEpsilon = 0.7757984463147055\nEpsilon = 0.7757208664700741\nEpsilon = 0.775643294383427\nEpsilon = 0.7755657300539888\nEpsilon = 0.7754881734809834\nEpsilon = 0.7754106246636353\nEpsilon = 0.7753330836011689\nEpsilon = 0.7752555502928088\nEpsilon = 0.7751780247377795\nEpsilon = 0.7751005069353057\nEpsilon = 0.7750229968846122\nEpsilon = 0.7749454945849237\nEpsilon = 0.7748680000354652\nEpsilon = 0.7747905132354617\nEpsilon = 0.7747130341841382\nEpsilon = 0.7746355628807198\nEpsilon = 0.7745580993244318\nAgent: ddqn_agent . Episode 142/2000. Number of steps to finish: 20. Loss: 12.501715660095215 Reward: -10.0\nEpsilon = 0.7744806435144993\nEpsilon = 0.7744031954501479\nEpsilon = 0.7743257551306029\nEpsilon = 0.7742483225550898\nEpsilon = 0.7741708977228343\nEpsilon = 0.774093480633062\nEpsilon = 0.7740160712849987\nEpsilon = 0.7739386696778702\nEpsilon = 0.7738612758109024\nEpsilon = 0.7737838896833212\nEpsilon = 0.7737065112943529\nEpsilon = 0.7736291406432234\nEpsilon = 0.7735517777291591\nEpsilon = 0.7734744225513862\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.773397075109131\nEpsilon = 0.7733197354016201\nEpsilon = 0.7732424034280799\nEpsilon = 0.7731650791877371\nEpsilon = 0.7730877626798184\nAgent: ddqn_agent . Episode 143/2000. Number of steps to finish: 19. Loss: 12.237895011901855 Reward: -7.0\nEpsilon = 0.7730104539035504\nEpsilon = 0.77293315285816\nEpsilon = 0.7728558595428742\nEpsilon = 0.7727785739569198\nEpsilon = 0.7727012960995242\nEpsilon = 0.7726240259699143\nEpsilon = 0.7725467635673173\nEpsilon = 0.7724695088909606\nEpsilon = 0.7723922619400715\nEpsilon = 0.7723150227138775\nEpsilon = 0.7722377912116062\nEpsilon = 0.7721605674324851\nEpsilon = 0.7720833513757418\nEpsilon = 0.7720061430406042\nEpsilon = 0.7719289424263002\nEpsilon = 0.7718517495320576\nEpsilon = 0.7717745643571045\nEpsilon = 0.7716973869006688\nEpsilon = 0.7716202171619787\nEpsilon = 0.7715430551402626\nAgent: ddqn_agent . Episode 144/2000. Number of steps to finish: 20. Loss: 12.668037414550781 Reward: -10.0\nEpsilon = 0.7714659008347485\nEpsilon = 0.771388754244665\nEpsilon = 0.7713116153692405\nEpsilon = 0.7712344842077036\nEpsilon = 0.7711573607592828\nEpsilon = 0.7710802450232068\nEpsilon = 0.7710031369987045\nEpsilon = 0.7709260366850046\nEpsilon = 0.7708489440813361\nEpsilon = 0.7707718591869279\nEpsilon = 0.7706947820010093\nEpsilon = 0.7706177125228092\nEpsilon = 0.770540650751557\nEpsilon = 0.7704635966864818\nEpsilon = 0.7703865503268131\nEpsilon = 0.7703095116717804\nEpsilon = 0.7702324807206132\nEpsilon = 0.7701554574725412\nEpsilon = 0.770078441926794\nEpsilon = 0.7700014340826014\nAgent: ddqn_agent . Episode 145/2000. Number of steps to finish: 20. Loss: 12.797819137573242 Reward: -12.0\nEpsilon = 0.7699244339391931\nEpsilon = 0.7698474414957992\nEpsilon = 0.7697704567516497\nEpsilon = 0.7696934797059746\nEpsilon = 0.7696165103580039\nEpsilon = 0.7695395487069682\nEpsilon = 0.7694625947520974\nEpsilon = 0.7693856484926223\nEpsilon = 0.769308709927773\nEpsilon = 0.7692317790567802\nEpsilon = 0.7691548558788746\nEpsilon = 0.7690779403932867\nEpsilon = 0.7690010325992473\nAgent: ddqn_agent . Episode 146/2000. Number of steps to finish: 13. Loss: 8.155080795288086 Reward: -1.0\nEpsilon = 0.7689241324959875\nEpsilon = 0.7688472400827379\nEpsilon = 0.7687703553587296\nEpsilon = 0.7686934783231938\nEpsilon = 0.7686166089753614\nEpsilon = 0.768539747314464\nEpsilon = 0.7684628933397325\nEpsilon = 0.7683860470503986\nEpsilon = 0.7683092084456936\nEpsilon = 0.7682323775248491\nEpsilon = 0.7681555542870966\nEpsilon = 0.7680787387316679\nEpsilon = 0.7680019308577947\nEpsilon = 0.7679251306647089\nEpsilon = 0.7678483381516424\nEpsilon = 0.7677715533178273\nEpsilon = 0.7676947761624956\nEpsilon = 0.7676180066848793\nEpsilon = 0.7675412448842108\nEpsilon = 0.7674644907597225\nAgent: ddqn_agent . Episode 147/2000. Number of steps to finish: 20. Loss: 12.653040885925293 Reward: -10.0\nEpsilon = 0.7673877443106465\nEpsilon = 0.7673110055362155\nEpsilon = 0.7672342744356618\nEpsilon = 0.7671575510082183\nEpsilon = 0.7670808352531174\nEpsilon = 0.7670041271695921\nEpsilon = 0.7669274267568752\nEpsilon = 0.7668507340141996\nEpsilon = 0.7667740489407981\nEpsilon = 0.7666973715359041\nEpsilon = 0.7666207017987505\nEpsilon = 0.7665440397285707\nEpsilon = 0.7664673853245978\nEpsilon = 0.7663907385860653\nEpsilon = 0.7663140995122067\nEpsilon = 0.7662374681022556\nEpsilon = 0.7661608443554454\nAgent: ddqn_agent . Episode 148/2000. Number of steps to finish: 17. Loss: 10.837918281555176 Reward: -5.0\nEpsilon = 0.7660842282710099\nEpsilon = 0.7660076198481828\nEpsilon = 0.765931019086198\nEpsilon = 0.7658544259842893\nEpsilon = 0.7657778405416908\nEpsilon = 0.7657012627576367\nEpsilon = 0.765624692631361\nEpsilon = 0.7655481301620979\nEpsilon = 0.7654715753490817\nEpsilon = 0.7653950281915468\nEpsilon = 0.7653184886887276\nEpsilon = 0.7652419568398587\nEpsilon = 0.7651654326441748\nEpsilon = 0.7650889161009103\nEpsilon = 0.7650124072093003\nEpsilon = 0.7649359059685794\nEpsilon = 0.7648594123779826\nEpsilon = 0.7647829264367448\nEpsilon = 0.7647064481441012\nEpsilon = 0.7646299774992867\nAgent: ddqn_agent . Episode 149/2000. Number of steps to finish: 20. Loss: 12.750468254089355 Reward: -10.0\nEpsilon = 0.7645535145015369\nEpsilon = 0.7644770591500867\nEpsilon = 0.7644006114441717\nEpsilon = 0.7643241713830273\nEpsilon = 0.764247738965889\nEpsilon = 0.7641713141919925\nEpsilon = 0.7640948970605733\nEpsilon = 0.7640184875708673\nEpsilon = 0.7639420857221102\nEpsilon = 0.763865691513538\nEpsilon = 0.7637893049443867\nEpsilon = 0.7637129260138923\nEpsilon = 0.7636365547212909\nEpsilon = 0.7635601910658187\nEpsilon = 0.7634838350467121\nEpsilon = 0.7634074866632075\nEpsilon = 0.7633311459145411\nEpsilon = 0.7632548127999497\nEpsilon = 0.7631784873186697\nEpsilon = 0.7631021694699378\nAgent: ddqn_agent . Episode 150/2000. Number of steps to finish: 20. Loss: 12.895925521850586 Reward: -12.0\nEpsilon = 0.7630258592529908\nEpsilon = 0.7629495566670655\nEpsilon = 0.7628732617113988\nEpsilon = 0.7627969743852276\nEpsilon = 0.7627206946877891\nEpsilon = 0.7626444226183203\nEpsilon = 0.7625681581760585\nEpsilon = 0.7624919013602409\nEpsilon = 0.7624156521701049\nEpsilon = 0.7623394106048879\nEpsilon = 0.7622631766638274\nEpsilon = 0.762186950346161\nEpsilon = 0.7621107316511264\nEpsilon = 0.7620345205779613\nEpsilon = 0.7619583171259035\nEpsilon = 0.7618821212941909\nEpsilon = 0.7618059330820615\nEpsilon = 0.7617297524887533\nEpsilon = 0.7616535795135044\nEpsilon = 0.7615774141555531\nAgent: ddqn_agent . Episode 151/2000. Number of steps to finish: 20. Loss: 12.672342300415039 Reward: -10.0\nEpsilon = 0.7615012564141376\nEpsilon = 0.7614251062884961\nEpsilon = 0.7613489637778673\nEpsilon = 0.7612728288814895\nEpsilon = 0.7611967015986013\nEpsilon = 0.7611205819284415\nEpsilon = 0.7610444698702487\nEpsilon = 0.7609683654232616\nEpsilon = 0.7608922685867193\nEpsilon = 0.7608161793598607\nEpsilon = 0.7607400977419247\nEpsilon = 0.7606640237321506\nEpsilon = 0.7605879573297774\nEpsilon = 0.7605118985340444\nEpsilon = 0.760435847344191\nEpsilon = 0.7603598037594566\nEpsilon = 0.7602837677790806\nEpsilon = 0.7602077394023027\nEpsilon = 0.7601317186283625\nEpsilon = 0.7600557054564997\nAgent: ddqn_agent . Episode 152/2000. Number of steps to finish: 20. Loss: 12.756156921386719 Reward: -12.0\nEpsilon = 0.759979699885954\nEpsilon = 0.7599037019159655\nEpsilon = 0.7598277115457739\nEpsilon = 0.7597517287746193\nEpsilon = 0.7596757536017418\nEpsilon = 0.7595997860263816\nEpsilon = 0.7595238260477789\nEpsilon = 0.7594478736651742\nEpsilon = 0.7593719288778077\nEpsilon = 0.7592959916849199\nEpsilon = 0.7592200620857514\nEpsilon = 0.7591441400795429\nEpsilon = 0.759068225665535\nEpsilon = 0.7589923188429685\nEpsilon = 0.7589164196110841\nEpsilon = 0.758840527969123\nEpsilon = 0.7587646439163261\nEpsilon = 0.7586887674519345\nEpsilon = 0.7586128985751893\nAgent: ddqn_agent . Episode 153/2000. Number of steps to finish: 19. Loss: 11.995737075805664 Reward: -7.0\nEpsilon = 0.7585370372853318\nEpsilon = 0.7584611835816033\nEpsilon = 0.7583853374632451\nEpsilon = 0.7583094989294988\nEpsilon = 0.7582336679796059\nEpsilon = 0.758157844612808\nEpsilon = 0.7580820288283467\nEpsilon = 0.758006220625464\nEpsilon = 0.7579304200034014\nEpsilon = 0.757854626961401\nEpsilon = 0.7577788414987049\nEpsilon = 0.7577030636145551\nEpsilon = 0.7576272933081937\nEpsilon = 0.7575515305788628\nEpsilon = 0.7574757754258049\nEpsilon = 0.7574000278482623\nEpsilon = 0.7573242878454776\nEpsilon = 0.757248555416693\nEpsilon = 0.7571728305611514\nEpsilon = 0.7570971132780953\nAgent: ddqn_agent . Episode 154/2000. Number of steps to finish: 20. Loss: 12.989992141723633 Reward: -12.0\nEpsilon = 0.7570214035667675\nEpsilon = 0.7569457014264108\nEpsilon = 0.7568700068562682\nEpsilon = 0.7567943198555825\nEpsilon = 0.756718640423597\nEpsilon = 0.7566429685595547\nEpsilon = 0.7565673042626988\nEpsilon = 0.7564916475322725\nEpsilon = 0.7564159983675193\nEpsilon = 0.7563403567676825\nEpsilon = 0.7562647227320057\nEpsilon = 0.7561890962597325\nEpsilon = 0.7561134773501066\nEpsilon = 0.7560378660023716\nEpsilon = 0.7559622622157713\nEpsilon = 0.7558866659895498\nEpsilon = 0.7558110773229508\nEpsilon = 0.7557354962152185\nEpsilon = 0.755659922665597\nEpsilon = 0.7555843566733305\nAgent: ddqn_agent . Episode 155/2000. Number of steps to finish: 20. Loss: 13.27092456817627 Reward: -10.0\nEpsilon = 0.7555087982376631\nEpsilon = 0.7554332473578393\nEpsilon = 0.7553577040331035\nEpsilon = 0.7552821682627002\nEpsilon = 0.755206640045874\nEpsilon = 0.7551311193818694\nEpsilon = 0.7550556062699312\nEpsilon = 0.7549801007093042\nEpsilon = 0.7549046026992333\nEpsilon = 0.7548291122389634\nEpsilon = 0.7547536293277396\nEpsilon = 0.7546781539648069\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.7546026861494104\nEpsilon = 0.7545272258807955\nEpsilon = 0.7544517731582074\nEpsilon = 0.7543763279808916\nEpsilon = 0.7543008903480936\nEpsilon = 0.7542254602590588\nEpsilon = 0.7541500377130329\nEpsilon = 0.7540746227092616\nAgent: ddqn_agent . Episode 156/2000. Number of steps to finish: 20. Loss: 12.771806716918945 Reward: -10.0\nEpsilon = 0.7539992152469907\nEpsilon = 0.7539238153254659\nEpsilon = 0.7538484229439334\nEpsilon = 0.753773038101639\nEpsilon = 0.7536976607978289\nEpsilon = 0.753622291031749\nEpsilon = 0.7535469288026458\nEpsilon = 0.7534715741097656\nEpsilon = 0.7533962269523546\nEpsilon = 0.7533208873296594\nEpsilon = 0.7532455552409264\nEpsilon = 0.7531702306854023\nEpsilon = 0.7530949136623338\nEpsilon = 0.7530196041709676\nEpsilon = 0.7529443022105505\nEpsilon = 0.7528690077803295\nEpsilon = 0.7527937208795514\nEpsilon = 0.7527184415074635\nEpsilon = 0.7526431696633127\nEpsilon = 0.7525679053463463\nAgent: ddqn_agent . Episode 157/2000. Number of steps to finish: 20. Loss: 13.083918571472168 Reward: -10.0\nEpsilon = 0.7524926485558117\nEpsilon = 0.7524173992909561\nEpsilon = 0.752342157551027\nEpsilon = 0.7522669233352719\nEpsilon = 0.7521916966429384\nEpsilon = 0.7521164774732741\nEpsilon = 0.7520412658255269\nEpsilon = 0.7519660616989443\nEpsilon = 0.7518908650927745\nAgent: ddqn_agent . Episode 158/2000. Number of steps to finish: 9. Loss: 6.076841354370117 Reward: 3.0\nEpsilon = 0.7518156760062652\nEpsilon = 0.7517404944386645\nEpsilon = 0.7516653203892206\nEpsilon = 0.7515901538571818\nEpsilon = 0.7515149948417961\nEpsilon = 0.751439843342312\nEpsilon = 0.7513646993579778\nEpsilon = 0.7512895628880419\nEpsilon = 0.7512144339317531\nEpsilon = 0.75113931248836\nEpsilon = 0.7510641985571112\nEpsilon = 0.7509890921372554\nEpsilon = 0.7509139932280416\nEpsilon = 0.7508389018287188\nEpsilon = 0.750763817938536\nEpsilon = 0.7506887415567421\nEpsilon = 0.7506136726825865\nEpsilon = 0.7505386113153183\nEpsilon = 0.7504635574541868\nAgent: ddqn_agent . Episode 159/2000. Number of steps to finish: 19. Loss: 12.219721794128418 Reward: -7.0\nEpsilon = 0.7503885110984414\nEpsilon = 0.7503134722473316\nEpsilon = 0.7502384409001068\nEpsilon = 0.7501634170560169\nEpsilon = 0.7500884007143113\nEpsilon = 0.7500133918742399\nEpsilon = 0.7499383905350525\nEpsilon = 0.749863396695999\nEpsilon = 0.7497884103563294\nEpsilon = 0.7497134315152938\nEpsilon = 0.7496384601721423\nEpsilon = 0.7495634963261251\nEpsilon = 0.7494885399764925\nEpsilon = 0.7494135911224948\nEpsilon = 0.7493386497633826\nEpsilon = 0.7492637158984062\nEpsilon = 0.7491887895268164\nEpsilon = 0.7491138706478637\nEpsilon = 0.749038959260799\nEpsilon = 0.7489640553648729\nAgent: ddqn_agent . Episode 160/2000. Number of steps to finish: 20. Loss: 13.27658748626709 Reward: -10.0\nEpsilon = 0.7488891589593364\nEpsilon = 0.7488142700434405\nEpsilon = 0.7487393886164362\nEpsilon = 0.7486645146775746\nEpsilon = 0.7485896482261069\nEpsilon = 0.7485147892612842\nEpsilon = 0.7484399377823581\nEpsilon = 0.7483650937885798\nEpsilon = 0.748290257279201\nEpsilon = 0.7482154282534731\nEpsilon = 0.7481406067106477\nEpsilon = 0.7480657926499767\nEpsilon = 0.7479909860707118\nEpsilon = 0.7479161869721047\nEpsilon = 0.7478413953534075\nEpsilon = 0.7477666112138722\nEpsilon = 0.7476918345527508\nEpsilon = 0.7476170653692955\nEpsilon = 0.7475423036627585\nEpsilon = 0.7474675494323922\nAgent: ddqn_agent . Episode 161/2000. Number of steps to finish: 20. Loss: 12.938217163085938 Reward: -10.0\nEpsilon = 0.747392802677449\nEpsilon = 0.7473180633971813\nEpsilon = 0.7472433315908417\nEpsilon = 0.7471686072576826\nEpsilon = 0.7470938903969568\nEpsilon = 0.7470191810079172\nEpsilon = 0.7469444790898164\nEpsilon = 0.7468697846419075\nEpsilon = 0.7467950976634433\nEpsilon = 0.746720418153677\nEpsilon = 0.7466457461118616\nEpsilon = 0.7465710815372505\nEpsilon = 0.7464964244290968\nEpsilon = 0.746421774786654\nEpsilon = 0.7463471326091753\nAgent: ddqn_agent . Episode 162/2000. Number of steps to finish: 15. Loss: 9.672384262084961 Reward: -3.0\nEpsilon = 0.7462724978959144\nEpsilon = 0.7461978706461249\nEpsilon = 0.7461232508590603\nEpsilon = 0.7460486385339744\nEpsilon = 0.745974033670121\nEpsilon = 0.745899436266754\nEpsilon = 0.7458248463231274\nEpsilon = 0.7457502638384951\nEpsilon = 0.7456756888121112\nEpsilon = 0.74560112124323\nEpsilon = 0.7455265611311056\nEpsilon = 0.7454520084749925\nEpsilon = 0.745377463274145\nEpsilon = 0.7453029255278176\nEpsilon = 0.7452283952352649\nEpsilon = 0.7451538723957414\nEpsilon = 0.7450793570085018\nEpsilon = 0.745004849072801\nEpsilon = 0.7449303485878936\nEpsilon = 0.7448558555530349\nAgent: ddqn_agent . Episode 163/2000. Number of steps to finish: 20. Loss: 13.025659561157227 Reward: -16.0\nEpsilon = 0.7447813699674796\nEpsilon = 0.7447068918304828\nEpsilon = 0.7446324211412997\nEpsilon = 0.7445579578991856\nEpsilon = 0.7444835021033956\nEpsilon = 0.7444090537531853\nEpsilon = 0.74433461284781\nEpsilon = 0.7442601793865252\nEpsilon = 0.7441857533685866\nEpsilon = 0.7441113347932498\nEpsilon = 0.7440369236597705\nEpsilon = 0.7439625199674045\nEpsilon = 0.7438881237154078\nEpsilon = 0.7438137349030363\nEpsilon = 0.743739353529546\nEpsilon = 0.7436649795941931\nEpsilon = 0.7435906130962336\nEpsilon = 0.743516254034924\nEpsilon = 0.7434419024095206\nAgent: ddqn_agent . Episode 164/2000. Number of steps to finish: 19. Loss: 12.272591590881348 Reward: -7.0\nEpsilon = 0.7433675582192796\nEpsilon = 0.7432932214634577\nEpsilon = 0.7432188921413113\nEpsilon = 0.7431445702520972\nEpsilon = 0.743070255795072\nEpsilon = 0.7429959487694925\nEpsilon = 0.7429216491746156\nEpsilon = 0.7428473570096981\nEpsilon = 0.7427730722739971\nEpsilon = 0.7426987949667697\nEpsilon = 0.7426245250872731\nEpsilon = 0.7425502626347643\nEpsilon = 0.7424760076085009\nEpsilon = 0.74240176000774\nEpsilon = 0.7423275198317393\nEpsilon = 0.7422532870797561\nEpsilon = 0.7421790617510482\nEpsilon = 0.7421048438448731\nEpsilon = 0.7420306333604886\nEpsilon = 0.7419564302971525\nAgent: ddqn_agent . Episode 165/2000. Number of steps to finish: 20. Loss: 12.86762809753418 Reward: -14.0\nEpsilon = 0.7418822346541228\nEpsilon = 0.7418080464306575\nEpsilon = 0.7417338656260144\nEpsilon = 0.7416596922394518\nEpsilon = 0.7415855262702279\nEpsilon = 0.7415113677176008\nEpsilon = 0.7414372165808291\nEpsilon = 0.741363072859171\nEpsilon = 0.7412889365518851\nEpsilon = 0.7412148076582299\nEpsilon = 0.7411406861774641\nEpsilon = 0.7410665721088463\nEpsilon = 0.7409924654516354\nAgent: ddqn_agent . Episode 166/2000. Number of steps to finish: 13. Loss: 8.599074363708496 Reward: -1.0\nEpsilon = 0.7409183662050903\nEpsilon = 0.7408442743684698\nEpsilon = 0.7407701899410329\nEpsilon = 0.7406961129220389\nEpsilon = 0.7406220433107467\nEpsilon = 0.7405479811064156\nEpsilon = 0.740473926308305\nEpsilon = 0.7403998789156742\nEpsilon = 0.7403258389277826\nEpsilon = 0.7402518063438898\nEpsilon = 0.7401777811632554\nEpsilon = 0.7401037633851391\nEpsilon = 0.7400297530088006\nEpsilon = 0.7399557500334997\nEpsilon = 0.7398817544584964\nEpsilon = 0.7398077662830506\nAgent: ddqn_agent . Episode 167/2000. Number of steps to finish: 16. Loss: 10.590590476989746 Reward: -4.0\nEpsilon = 0.7397337855064222\nEpsilon = 0.7396598121278716\nEpsilon = 0.7395858461466588\nEpsilon = 0.7395118875620441\nEpsilon = 0.7394379363732879\nEpsilon = 0.7393639925796506\nEpsilon = 0.7392900561803926\nEpsilon = 0.7392161271747746\nEpsilon = 0.7391422055620571\nEpsilon = 0.7390682913415009\nEpsilon = 0.7389943845123668\nEpsilon = 0.7389204850739155\nEpsilon = 0.7388465930254081\nEpsilon = 0.7387727083661055\nEpsilon = 0.7386988310952689\nEpsilon = 0.7386249612121594\nEpsilon = 0.7385510987160382\nEpsilon = 0.7384772436061666\nEpsilon = 0.738403395881806\nEpsilon = 0.7383295555422179\nAgent: ddqn_agent . Episode 168/2000. Number of steps to finish: 20. Loss: 12.950776100158691 Reward: -10.0\nEpsilon = 0.7382557225866637\nEpsilon = 0.7381818970144051\nEpsilon = 0.7381080788247036\nEpsilon = 0.7380342680168211\nEpsilon = 0.7379604645900195\nEpsilon = 0.7378866685435606\nEpsilon = 0.7378128798767062\nEpsilon = 0.7377390985887186\nEpsilon = 0.7376653246788597\nEpsilon = 0.7375915581463919\nEpsilon = 0.7375177989905772\nEpsilon = 0.7374440472106781\nEpsilon = 0.7373703028059571\nEpsilon = 0.7372965657756765\nEpsilon = 0.7372228361190989\nEpsilon = 0.737149113835487\nEpsilon = 0.7370753989241035\nEpsilon = 0.737001691384211\nEpsilon = 0.7369279912150726\nEpsilon = 0.7368542984159511\nAgent: ddqn_agent . Episode 169/2000. Number of steps to finish: 20. Loss: 12.880849838256836 Reward: -12.0\nEpsilon = 0.7367806129861095\nEpsilon = 0.7367069349248109\nEpsilon = 0.7366332642313184\nEpsilon = 0.7365596009048953\nEpsilon = 0.7364859449448048\nEpsilon = 0.7364122963503104\nEpsilon = 0.7363386551206753\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.7362650212551632\nEpsilon = 0.7361913947530377\nEpsilon = 0.7361177756135624\nEpsilon = 0.736044163836001\nEpsilon = 0.7359705594196174\nEpsilon = 0.7358969623636755\nEpsilon = 0.7358233726674391\nEpsilon = 0.7357497903301724\nEpsilon = 0.7356762153511394\nEpsilon = 0.7356026477296043\nEpsilon = 0.7355290874648314\nEpsilon = 0.7354555345560849\nEpsilon = 0.7353819890026293\nAgent: ddqn_agent . Episode 170/2000. Number of steps to finish: 20. Loss: 13.041703224182129 Reward: -14.0\nEpsilon = 0.7353084508037291\nEpsilon = 0.7352349199586488\nEpsilon = 0.7351613964666529\nEpsilon = 0.7350878803270062\nEpsilon = 0.7350143715389735\nEpsilon = 0.7349408701018196\nEpsilon = 0.7348673760148094\nEpsilon = 0.734793889277208\nEpsilon = 0.7347204098882802\nEpsilon = 0.7346469378472914\nEpsilon = 0.7345734731535066\nEpsilon = 0.7345000158061913\nEpsilon = 0.7344265658046106\nEpsilon = 0.7343531231480301\nAgent: ddqn_agent . Episode 171/2000. Number of steps to finish: 14. Loss: 8.822071075439453 Reward: -2.0\nEpsilon = 0.7342796878357154\nEpsilon = 0.7342062598669319\nEpsilon = 0.7341328392409452\nEpsilon = 0.7340594259570211\nEpsilon = 0.7339860200144255\nEpsilon = 0.7339126214124241\nEpsilon = 0.7338392301502829\nEpsilon = 0.7337658462272678\nEpsilon = 0.7336924696426451\nEpsilon = 0.7336191003956809\nEpsilon = 0.7335457384856413\nAgent: ddqn_agent . Episode 172/2000. Number of steps to finish: 11. Loss: 7.443056106567383 Reward: 1.0\nEpsilon = 0.7334723839117927\nEpsilon = 0.7333990366734016\nEpsilon = 0.7333256967697342\nEpsilon = 0.7332523642000572\nEpsilon = 0.7331790389636372\nEpsilon = 0.7331057210597409\nEpsilon = 0.733032410487635\nEpsilon = 0.7329591072465863\nEpsilon = 0.7328858113358616\nEpsilon = 0.7328125227547281\nEpsilon = 0.7327392415024526\nEpsilon = 0.7326659675783024\nEpsilon = 0.7325927009815446\nEpsilon = 0.7325194417114465\nEpsilon = 0.7324461897672753\nEpsilon = 0.7323729451482985\nEpsilon = 0.7322997078537837\nEpsilon = 0.7322264778829983\nEpsilon = 0.73215325523521\nEpsilon = 0.7320800399096865\nAgent: ddqn_agent . Episode 173/2000. Number of steps to finish: 20. Loss: 13.31179141998291 Reward: -12.0\nEpsilon = 0.7320068319056956\nEpsilon = 0.731933631222505\nEpsilon = 0.7318604378593827\nEpsilon = 0.7317872518155968\nEpsilon = 0.7317140730904153\nEpsilon = 0.7316409016831062\nEpsilon = 0.7315677375929379\nEpsilon = 0.7314945808191786\nEpsilon = 0.7314214313610967\nEpsilon = 0.7313482892179606\nEpsilon = 0.7312751543890388\nEpsilon = 0.7312020268735999\nEpsilon = 0.7311289066709126\nEpsilon = 0.7310557937802454\nEpsilon = 0.7309826882008674\nEpsilon = 0.7309095899320474\nEpsilon = 0.7308364989730541\nAgent: ddqn_agent . Episode 174/2000. Number of steps to finish: 17. Loss: 11.557053565979004 Reward: -5.0\nEpsilon = 0.7307634153231568\nEpsilon = 0.7306903389816245\nEpsilon = 0.7306172699477264\nEpsilon = 0.7305442082207316\nEpsilon = 0.7304711537999096\nEpsilon = 0.7303981066845295\nEpsilon = 0.7303250668738611\nEpsilon = 0.7302520343671738\nEpsilon = 0.7301790091637371\nEpsilon = 0.7301059912628207\nEpsilon = 0.7300329806636945\nEpsilon = 0.7299599773656281\nEpsilon = 0.7298869813678915\nEpsilon = 0.7298139926697548\nEpsilon = 0.7297410112704878\nEpsilon = 0.7296680371693608\nEpsilon = 0.7295950703656439\nEpsilon = 0.7295221108586073\nEpsilon = 0.7294491586475215\nEpsilon = 0.7293762137316567\nAgent: ddqn_agent . Episode 175/2000. Number of steps to finish: 20. Loss: 13.636446952819824 Reward: -10.0\nEpsilon = 0.7293032761102836\nEpsilon = 0.7292303457826725\nEpsilon = 0.7291574227480943\nEpsilon = 0.7290845070058195\nEpsilon = 0.7290115985551189\nEpsilon = 0.7289386973952634\nEpsilon = 0.7288658035255239\nEpsilon = 0.7287929169451713\nEpsilon = 0.7287200376534768\nEpsilon = 0.7286471656497114\nEpsilon = 0.7285743009331465\nEpsilon = 0.7285014435030531\nEpsilon = 0.7284285933587028\nEpsilon = 0.728355750499367\nEpsilon = 0.7282829149243171\nEpsilon = 0.7282100866328246\nEpsilon = 0.7281372656241614\nEpsilon = 0.7280644518975989\nEpsilon = 0.7279916454524092\nEpsilon = 0.727918846287864\nAgent: ddqn_agent . Episode 176/2000. Number of steps to finish: 20. Loss: 13.000744819641113 Reward: -14.0\nEpsilon = 0.7278460544032351\nEpsilon = 0.7277732697977948\nEpsilon = 0.727700492470815\nEpsilon = 0.7276277224215679\nEpsilon = 0.7275549596493258\nEpsilon = 0.7274822041533608\nEpsilon = 0.7274094559329455\nEpsilon = 0.7273367149873522\nEpsilon = 0.7272639813158535\nEpsilon = 0.7271912549177219\nEpsilon = 0.7271185357922301\nEpsilon = 0.7270458239386508\nEpsilon = 0.726973119356257\nEpsilon = 0.7269004220443214\nEpsilon = 0.7268277320021169\nEpsilon = 0.7267550492289168\nAgent: ddqn_agent . Episode 177/2000. Number of steps to finish: 16. Loss: 10.739734649658203 Reward: -4.0\nEpsilon = 0.7266823737239939\nEpsilon = 0.7266097054866215\nEpsilon = 0.7265370445160728\nEpsilon = 0.7264643908116212\nEpsilon = 0.7263917443725401\nEpsilon = 0.7263191051981028\nEpsilon = 0.7262464732875831\nEpsilon = 0.7261738486402544\nEpsilon = 0.7261012312553904\nEpsilon = 0.7260286211322649\nEpsilon = 0.7259560182701517\nEpsilon = 0.7258834226683246\nEpsilon = 0.7258108343260578\nEpsilon = 0.7257382532426252\nEpsilon = 0.725665679417301\nAgent: ddqn_agent . Episode 178/2000. Number of steps to finish: 15. Loss: 10.223740577697754 Reward: -3.0\nEpsilon = 0.7255931128493592\nEpsilon = 0.7255205535380743\nEpsilon = 0.7254480014827205\nEpsilon = 0.7253754566825722\nEpsilon = 0.7253029191369039\nEpsilon = 0.7252303888449902\nEpsilon = 0.7251578658061058\nEpsilon = 0.7250853500195251\nEpsilon = 0.7250128414845232\nEpsilon = 0.7249403402003747\nEpsilon = 0.7248678461663547\nAgent: ddqn_agent . Episode 179/2000. Number of steps to finish: 11. Loss: 7.151400089263916 Reward: 1.0\nEpsilon = 0.7247953593817381\nEpsilon = 0.7247228798458\nEpsilon = 0.7246504075578154\nEpsilon = 0.7245779425170596\nEpsilon = 0.724505484722808\nEpsilon = 0.7244330341743357\nEpsilon = 0.7243605908709183\nEpsilon = 0.7242881548118312\nEpsilon = 0.72421572599635\nEpsilon = 0.7241433044237503\nEpsilon = 0.7240708900933079\nEpsilon = 0.7239984830042986\nEpsilon = 0.7239260831559982\nEpsilon = 0.7238536905476826\nEpsilon = 0.7237813051786278\nEpsilon = 0.72370892704811\nEpsilon = 0.7236365561554051\nEpsilon = 0.7235641924997895\nEpsilon = 0.7234918360805396\nEpsilon = 0.7234194868969315\nAgent: ddqn_agent . Episode 180/2000. Number of steps to finish: 20. Loss: 13.227372169494629 Reward: -12.0\nEpsilon = 0.7233471449482418\nEpsilon = 0.723274810233747\nEpsilon = 0.7232024827527236\nEpsilon = 0.7231301625044483\nEpsilon = 0.7230578494881978\nEpsilon = 0.7229855437032491\nEpsilon = 0.7229132451488788\nEpsilon = 0.7228409538243639\nEpsilon = 0.7227686697289815\nEpsilon = 0.7226963928620086\nEpsilon = 0.7226241232227224\nEpsilon = 0.7225518608104001\nEpsilon = 0.7224796056243191\nEpsilon = 0.7224073576637567\nEpsilon = 0.7223351169279904\nEpsilon = 0.7222628834162975\nEpsilon = 0.7221906571279559\nEpsilon = 0.7221184380622432\nEpsilon = 0.722046226218437\nEpsilon = 0.7219740215958151\nAgent: ddqn_agent . Episode 181/2000. Number of steps to finish: 20. Loss: 13.249107360839844 Reward: -10.0\nEpsilon = 0.7219018241936556\nEpsilon = 0.7218296340112362\nEpsilon = 0.7217574510478351\nEpsilon = 0.7216852753027303\nEpsilon = 0.7216131067752001\nEpsilon = 0.7215409454645225\nEpsilon = 0.7214687913699761\nEpsilon = 0.7213966444908391\nEpsilon = 0.72132450482639\nEpsilon = 0.7212523723759073\nAgent: ddqn_agent . Episode 182/2000. Number of steps to finish: 10. Loss: 6.723641395568848 Reward: 2.0\nEpsilon = 0.7211802471386698\nEpsilon = 0.7211081291139559\nEpsilon = 0.7210360183010446\nEpsilon = 0.7209639146992145\nEpsilon = 0.7208918183077446\nEpsilon = 0.7208197291259139\nEpsilon = 0.7207476471530013\nEpsilon = 0.7206755723882861\nEpsilon = 0.7206035048310472\nEpsilon = 0.720531444480564\nEpsilon = 0.720459391336116\nEpsilon = 0.7203873453969825\nEpsilon = 0.7203153066624428\nEpsilon = 0.7202432751317766\nEpsilon = 0.7201712508042634\nEpsilon = 0.7200992336791829\nEpsilon = 0.7200272237558151\nEpsilon = 0.7199552210334395\nEpsilon = 0.7198832255113361\nEpsilon = 0.7198112371887849\nAgent: ddqn_agent . Episode 183/2000. Number of steps to finish: 20. Loss: 13.75942611694336 Reward: -10.0\nEpsilon = 0.7197392560650661\nEpsilon = 0.7196672821394595\nEpsilon = 0.7195953154112456\nEpsilon = 0.7195233558797044\nEpsilon = 0.7194514035441164\nEpsilon = 0.719379458403762\nEpsilon = 0.7193075204579217\nEpsilon = 0.7192355897058759\nEpsilon = 0.7191636661469053\nEpsilon = 0.7190917497802907\nEpsilon = 0.7190198406053127\nEpsilon = 0.7189479386212522\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.71887604382739\nEpsilon = 0.7188041562230073\nEpsilon = 0.718732275807385\nEpsilon = 0.7186604025798042\nEpsilon = 0.7185885365395462\nEpsilon = 0.7185166776858923\nEpsilon = 0.7184448260181238\nEpsilon = 0.718372981535522\nAgent: ddqn_agent . Episode 184/2000. Number of steps to finish: 20. Loss: 13.431105613708496 Reward: -18.0\nEpsilon = 0.7183011442373685\nEpsilon = 0.7182293141229448\nEpsilon = 0.7181574911915325\nEpsilon = 0.7180856754424133\nEpsilon = 0.7180138668748691\nEpsilon = 0.7179420654881816\nEpsilon = 0.7178702712816328\nEpsilon = 0.7177984842545047\nEpsilon = 0.7177267044060792\nEpsilon = 0.7176549317356387\nEpsilon = 0.7175831662424651\nEpsilon = 0.7175114079258409\nEpsilon = 0.7174396567850483\nAgent: ddqn_agent . Episode 185/2000. Number of steps to finish: 13. Loss: 8.617666244506836 Reward: -1.0\nEpsilon = 0.7173679128193697\nEpsilon = 0.7172961760280878\nEpsilon = 0.717224446410485\nEpsilon = 0.717152723965844\nEpsilon = 0.7170810086934474\nEpsilon = 0.7170093005925781\nEpsilon = 0.7169375996625188\nEpsilon = 0.7168659059025525\nEpsilon = 0.7167942193119623\nEpsilon = 0.7167225398900311\nEpsilon = 0.7166508676360421\nEpsilon = 0.7165792025492785\nEpsilon = 0.7165075446290237\nEpsilon = 0.7164358938745607\nEpsilon = 0.7163642502851733\nEpsilon = 0.7162926138601448\nAgent: ddqn_agent . Episode 186/2000. Number of steps to finish: 16. Loss: 10.60811710357666 Reward: -4.0\nEpsilon = 0.7162209845987587\nEpsilon = 0.7161493625002988\nEpsilon = 0.7160777475640487\nEpsilon = 0.7160061397892924\nEpsilon = 0.7159345391753135\nEpsilon = 0.715862945721396\nEpsilon = 0.7157913594268238\nEpsilon = 0.7157197802908811\nEpsilon = 0.715648208312852\nEpsilon = 0.7155766434920208\nEpsilon = 0.7155050858276716\nEpsilon = 0.7154335353190888\nEpsilon = 0.7153619919655569\nEpsilon = 0.7152904557663604\nEpsilon = 0.7152189267207838\nEpsilon = 0.7151474048281117\nEpsilon = 0.7150758900876288\nEpsilon = 0.7150043824986201\nAgent: ddqn_agent . Episode 187/2000. Number of steps to finish: 18. Loss: 12.16293716430664 Reward: -6.0\nEpsilon = 0.7149328820603702\nEpsilon = 0.7148613887721642\nEpsilon = 0.714789902633287\nEpsilon = 0.7147184236430237\nEpsilon = 0.7146469518006594\nEpsilon = 0.7145754871054794\nEpsilon = 0.7145040295567688\nEpsilon = 0.7144325791538132\nEpsilon = 0.7143611358958978\nEpsilon = 0.7142896997823082\nEpsilon = 0.7142182708123299\nEpsilon = 0.7141468489852487\nEpsilon = 0.7140754343003503\nEpsilon = 0.7140040267569202\nEpsilon = 0.7139326263542445\nEpsilon = 0.7138612330916091\nEpsilon = 0.7137898469683\nEpsilon = 0.7137184679836032\nEpsilon = 0.7136470961368048\nEpsilon = 0.7135757314271911\nAgent: ddqn_agent . Episode 188/2000. Number of steps to finish: 20. Loss: 13.726028442382812 Reward: -10.0\nEpsilon = 0.7135043738540484\nEpsilon = 0.713433023416663\nEpsilon = 0.7133616801143213\nEpsilon = 0.7132903439463099\nEpsilon = 0.7132190149119152\nEpsilon = 0.713147693010424\nEpsilon = 0.713076378241123\nEpsilon = 0.7130050706032989\nEpsilon = 0.7129337700962386\nEpsilon = 0.712862476719229\nEpsilon = 0.712791190471557\nAgent: ddqn_agent . Episode 189/2000. Number of steps to finish: 11. Loss: 7.414309024810791 Reward: 1.0\nEpsilon = 0.7127199113525099\nEpsilon = 0.7126486393613747\nEpsilon = 0.7125773744974385\nEpsilon = 0.7125061167599888\nEpsilon = 0.7124348661483128\nEpsilon = 0.712363622661698\nEpsilon = 0.7122923862994318\nEpsilon = 0.7122211570608019\nEpsilon = 0.7121499349450958\nEpsilon = 0.7120787199516013\nEpsilon = 0.7120075120796062\nEpsilon = 0.7119363113283983\nEpsilon = 0.7118651176972655\nEpsilon = 0.7117939311854957\nEpsilon = 0.7117227517923772\nEpsilon = 0.7116515795171979\nEpsilon = 0.7115804143592462\nEpsilon = 0.7115092563178104\nEpsilon = 0.7114381053921786\nEpsilon = 0.7113669615816394\nAgent: ddqn_agent . Episode 190/2000. Number of steps to finish: 20. Loss: 13.368592262268066 Reward: -10.0\nEpsilon = 0.7112958248854813\nEpsilon = 0.7112246953029927\nEpsilon = 0.7111535728334624\nEpsilon = 0.711082457476179\nEpsilon = 0.7110113492304314\nEpsilon = 0.7109402480955084\nEpsilon = 0.7108691540706988\nEpsilon = 0.7107980671552918\nEpsilon = 0.7107269873485763\nEpsilon = 0.7106559146498415\nEpsilon = 0.7105848490583765\nEpsilon = 0.7105137905734706\nEpsilon = 0.7104427391944133\nEpsilon = 0.7103716949204938\nEpsilon = 0.7103006577510017\nEpsilon = 0.7102296276852266\nEpsilon = 0.7101586047224581\nEpsilon = 0.7100875888619859\nEpsilon = 0.7100165801030996\nEpsilon = 0.7099455784450893\nAgent: ddqn_agent . Episode 191/2000. Number of steps to finish: 20. Loss: 13.361771583557129 Reward: -14.0\nEpsilon = 0.7098745838872448\nEpsilon = 0.709803596428856\nEpsilon = 0.7097326160692131\nEpsilon = 0.7096616428076062\nEpsilon = 0.7095906766433255\nEpsilon = 0.7095197175756612\nEpsilon = 0.7094487656039037\nEpsilon = 0.7093778207273433\nEpsilon = 0.7093068829452707\nEpsilon = 0.7092359522569761\nEpsilon = 0.7091650286617505\nEpsilon = 0.7090941121588843\nEpsilon = 0.7090232027476684\nEpsilon = 0.7089523004273937\nEpsilon = 0.7088814051973509\nEpsilon = 0.7088105170568312\nEpsilon = 0.7087396360051255\nEpsilon = 0.708668762041525\nEpsilon = 0.7085978951653208\nAgent: ddqn_agent . Episode 192/2000. Number of steps to finish: 19. Loss: 12.775006294250488 Reward: -7.0\nEpsilon = 0.7085270353758043\nEpsilon = 0.7084561826722667\nEpsilon = 0.7083853370539995\nEpsilon = 0.7083144985202942\nEpsilon = 0.7082436670704422\nEpsilon = 0.7081728427037352\nEpsilon = 0.7081020254194648\nEpsilon = 0.7080312152169228\nEpsilon = 0.7079604120954012\nEpsilon = 0.7078896160541917\nEpsilon = 0.7078188270925863\nEpsilon = 0.707748045209877\nEpsilon = 0.707677270405356\nEpsilon = 0.7076065026783155\nEpsilon = 0.7075357420280477\nEpsilon = 0.7074649884538449\nEpsilon = 0.7073942419549994\nEpsilon = 0.707323502530804\nEpsilon = 0.7072527701805509\nEpsilon = 0.7071820449035329\nAgent: ddqn_agent . Episode 193/2000. Number of steps to finish: 20. Loss: 13.622476577758789 Reward: -12.0\nEpsilon = 0.7071113266990425\nEpsilon = 0.7070406155663727\nEpsilon = 0.706969911504816\nEpsilon = 0.7068992145136656\nEpsilon = 0.7068285245922142\nEpsilon = 0.706757841739755\nEpsilon = 0.706687165955581\nEpsilon = 0.7066164972389855\nEpsilon = 0.7065458355892615\nEpsilon = 0.7064751810057026\nEpsilon = 0.706404533487602\nEpsilon = 0.7063338930342533\nEpsilon = 0.7062632596449498\nEpsilon = 0.7061926333189854\nEpsilon = 0.7061220140556536\nEpsilon = 0.706051401854248\nEpsilon = 0.7059807967140626\nEpsilon = 0.7059101986343912\nEpsilon = 0.7058396076145278\nAgent: ddqn_agent . Episode 194/2000. Number of steps to finish: 19. Loss: 12.660394668579102 Reward: -7.0\nEpsilon = 0.7057690236537664\nEpsilon = 0.705698446751401\nEpsilon = 0.7056278769067259\nEpsilon = 0.7055573141190352\nEpsilon = 0.7054867583876233\nEpsilon = 0.7054162097117845\nEpsilon = 0.7053456680908133\nEpsilon = 0.7052751335240043\nEpsilon = 0.7052046060106519\nEpsilon = 0.7051340855500509\nEpsilon = 0.7050635721414958\nEpsilon = 0.7049930657842817\nEpsilon = 0.7049225664777032\nEpsilon = 0.7048520742210554\nAgent: ddqn_agent . Episode 195/2000. Number of steps to finish: 14. Loss: 9.761503219604492 Reward: -2.0\nEpsilon = 0.7047815890136333\nEpsilon = 0.7047111108547319\nEpsilon = 0.7046406397436464\nEpsilon = 0.7045701756796721\nEpsilon = 0.7044997186621041\nEpsilon = 0.7044292686902379\nEpsilon = 0.704358825763369\nEpsilon = 0.7042883898807926\nEpsilon = 0.7042179610418045\nEpsilon = 0.7041475392457003\nEpsilon = 0.7040771244917757\nEpsilon = 0.7040067167793266\nEpsilon = 0.7039363161076486\nEpsilon = 0.7038659224760379\nAgent: ddqn_agent . Episode 196/2000. Number of steps to finish: 14. Loss: 9.502074241638184 Reward: -2.0\nEpsilon = 0.7037955358837903\nEpsilon = 0.7037251563302019\nEpsilon = 0.7036547838145689\nEpsilon = 0.7035844183361875\nEpsilon = 0.7035140598943539\nEpsilon = 0.7034437084883645\nEpsilon = 0.7033733641175156\nEpsilon = 0.7033030267811039\nEpsilon = 0.7032326964784258\nEpsilon = 0.703162373208778\nEpsilon = 0.703092056971457\nEpsilon = 0.7030217477657599\nEpsilon = 0.7029514455909833\nAgent: ddqn_agent . Episode 197/2000. Number of steps to finish: 13. Loss: 8.920814514160156 Reward: -1.0\nEpsilon = 0.7028811504464242\nEpsilon = 0.7028108623313796\nEpsilon = 0.7027405812451465\nEpsilon = 0.702670307187022\nEpsilon = 0.7026000401563033\nEpsilon = 0.7025297801522876\nEpsilon = 0.7024595271742724\nEpsilon = 0.702389281221555\nEpsilon = 0.7023190422934328\nEpsilon = 0.7022488103892034\nEpsilon = 0.7021785855081645\nEpsilon = 0.7021083676496137\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.7020381568128488\nEpsilon = 0.7019679529971675\nEpsilon = 0.7018977562018678\nAgent: ddqn_agent . Episode 198/2000. Number of steps to finish: 15. Loss: 10.101083755493164 Reward: -3.0\nEpsilon = 0.7018275664262476\nEpsilon = 0.7017573836696049\nEpsilon = 0.701687207931238\nEpsilon = 0.7016170392104448\nEpsilon = 0.7015468775065238\nEpsilon = 0.7014767228187732\nEpsilon = 0.7014065751464913\nEpsilon = 0.7013364344889766\nEpsilon = 0.7012663008455278\nEpsilon = 0.7011961742154432\nEpsilon = 0.7011260545980217\nEpsilon = 0.7010559419925619\nEpsilon = 0.7009858363983626\nEpsilon = 0.7009157378147228\nEpsilon = 0.7008456462409414\nEpsilon = 0.7007755616763173\nEpsilon = 0.7007054841201497\nEpsilon = 0.7006354135717376\nEpsilon = 0.7005653500303805\nEpsilon = 0.7004952934953774\nAgent: ddqn_agent . Episode 199/2000. Number of steps to finish: 20. Loss: 13.784454345703125 Reward: -10.0\nEpsilon = 0.7004252439660279\nEpsilon = 0.7003552014416313\nEpsilon = 0.7002851659214872\nEpsilon = 0.700215137404895\nEpsilon = 0.7001451158911546\nEpsilon = 0.7000751013795655\nEpsilon = 0.7000050938694276\nEpsilon = 0.6999350933600407\nEpsilon = 0.6998650998507047\nEpsilon = 0.6997951133407196\nEpsilon = 0.6997251338293856\nEpsilon = 0.6996551613160027\nEpsilon = 0.6995851957998711\nEpsilon = 0.6995152372802911\nEpsilon = 0.6994452857565631\nEpsilon = 0.6993753412279874\nEpsilon = 0.6993054036938646\nEpsilon = 0.6992354731534952\nEpsilon = 0.6991655496061798\nEpsilon = 0.6990956330512192\nAgent: ddqn_agent . Episode 200/2000. Number of steps to finish: 20. Loss: 13.427247047424316 Reward: -12.0\nEpsilon = 0.6990257234879141\nEpsilon = 0.6989558209155654\nEpsilon = 0.6988859253334738\nEpsilon = 0.6988160367409405\nEpsilon = 0.6987461551372665\nEpsilon = 0.6986762805217528\nEpsilon = 0.6986064128937006\nEpsilon = 0.6985365522524113\nEpsilon = 0.698466698597186\nEpsilon = 0.6983968519273263\nEpsilon = 0.6983270122421336\nEpsilon = 0.6982571795409094\nEpsilon = 0.6981873538229553\nAgent: ddqn_agent . Episode 201/2000. Number of steps to finish: 13. Loss: 8.733996391296387 Reward: -1.0\nEpsilon = 0.698117535087573\nEpsilon = 0.6980477233340643\nEpsilon = 0.6979779185617309\nEpsilon = 0.6979081207698747\nEpsilon = 0.6978383299577977\nEpsilon = 0.697768546124802\nEpsilon = 0.6976987692701895\nEpsilon = 0.6976289993932625\nEpsilon = 0.6975592364933232\nEpsilon = 0.6974894805696739\nEpsilon = 0.6974197316216169\nEpsilon = 0.6973499896484547\nEpsilon = 0.69728025464949\nEpsilon = 0.697210526624025\nEpsilon = 0.6971408055713627\nEpsilon = 0.6970710914908056\nEpsilon = 0.6970013843816565\nEpsilon = 0.6969316842432183\nEpsilon = 0.696861991074794\nEpsilon = 0.6967923048756864\nAgent: ddqn_agent . Episode 202/2000. Number of steps to finish: 20. Loss: 14.01475715637207 Reward: -14.0\nEpsilon = 0.6967226256451989\nEpsilon = 0.6966529533826344\nEpsilon = 0.6965832880872962\nEpsilon = 0.6965136297584875\nEpsilon = 0.6964439783955116\nEpsilon = 0.6963743339976721\nEpsilon = 0.6963046965642724\nEpsilon = 0.6962350660946159\nEpsilon = 0.6961654425880065\nEpsilon = 0.6960958260437478\nEpsilon = 0.6960262164611434\nEpsilon = 0.6959566138394973\nEpsilon = 0.6958870181781133\nEpsilon = 0.6958174294762954\nAgent: ddqn_agent . Episode 203/2000. Number of steps to finish: 14. Loss: 9.644285202026367 Reward: -2.0\nEpsilon = 0.6957478477333479\nEpsilon = 0.6956782729485745\nEpsilon = 0.6956087051212797\nEpsilon = 0.6955391442507676\nEpsilon = 0.6954695903363425\nEpsilon = 0.6954000433773089\nEpsilon = 0.6953305033729712\nEpsilon = 0.6952609703226339\nEpsilon = 0.6951914442256016\nEpsilon = 0.6951219250811791\nEpsilon = 0.695052412888671\nEpsilon = 0.6949829076473821\nEpsilon = 0.6949134093566174\nEpsilon = 0.6948439180156817\nEpsilon = 0.6947744336238801\nEpsilon = 0.6947049561805178\nEpsilon = 0.6946354856848997\nEpsilon = 0.6945660221363312\nEpsilon = 0.6944965655341176\nEpsilon = 0.6944271158775642\nAgent: ddqn_agent . Episode 204/2000. Number of steps to finish: 20. Loss: 13.465638160705566 Reward: -18.0\nEpsilon = 0.6943576731659765\nEpsilon = 0.69428823739866\nEpsilon = 0.6942188085749201\nEpsilon = 0.6941493866940627\nEpsilon = 0.6940799717553933\nEpsilon = 0.6940105637582177\nEpsilon = 0.6939411627018419\nEpsilon = 0.6938717685855718\nEpsilon = 0.6938023814087132\nEpsilon = 0.6937330011705723\nEpsilon = 0.6936636278704553\nEpsilon = 0.6935942615076682\nEpsilon = 0.6935249020815175\nEpsilon = 0.6934555495913094\nEpsilon = 0.6933862040363503\nEpsilon = 0.6933168654159467\nEpsilon = 0.6932475337294052\nEpsilon = 0.6931782089760322\nEpsilon = 0.6931088911551346\nAgent: ddqn_agent . Episode 205/2000. Number of steps to finish: 19. Loss: 12.821171760559082 Reward: -7.0\nEpsilon = 0.6930395802660191\nEpsilon = 0.6929702763079925\nEpsilon = 0.6929009792803617\nEpsilon = 0.6928316891824337\nEpsilon = 0.6927624060135155\nEpsilon = 0.6926931297729141\nEpsilon = 0.6926238604599368\nEpsilon = 0.6925545980738909\nEpsilon = 0.6924853426140835\nEpsilon = 0.6924160940798221\nEpsilon = 0.692346852470414\nEpsilon = 0.692277617785167\nEpsilon = 0.6922083900233885\nEpsilon = 0.6921391691843862\nEpsilon = 0.6920699552674678\nEpsilon = 0.6920007482719411\nEpsilon = 0.691931548197114\nEpsilon = 0.6918623550422943\nEpsilon = 0.6917931688067901\nEpsilon = 0.6917239894899094\nAgent: ddqn_agent . Episode 206/2000. Number of steps to finish: 20. Loss: 13.541939735412598 Reward: -12.0\nEpsilon = 0.6916548170909604\nEpsilon = 0.6915856516092513\nEpsilon = 0.6915164930440905\nEpsilon = 0.691447341394786\nEpsilon = 0.6913781966606466\nEpsilon = 0.6913090588409805\nEpsilon = 0.6912399279350964\nEpsilon = 0.6911708039423029\nEpsilon = 0.6911016868619086\nEpsilon = 0.6910325766932225\nEpsilon = 0.6909634734355532\nEpsilon = 0.6908943770882097\nEpsilon = 0.6908252876505009\nEpsilon = 0.6907562051217359\nEpsilon = 0.6906871295012237\nEpsilon = 0.6906180607882736\nEpsilon = 0.6905489989821948\nEpsilon = 0.6904799440822966\nEpsilon = 0.6904108960878884\nEpsilon = 0.6903418549982796\nAgent: ddqn_agent . Episode 207/2000. Number of steps to finish: 20. Loss: 13.323960304260254 Reward: -14.0\nEpsilon = 0.6902728208127797\nEpsilon = 0.6902037935306985\nEpsilon = 0.6901347731513454\nEpsilon = 0.6900657596740303\nEpsilon = 0.6899967530980629\nEpsilon = 0.689927753422753\nEpsilon = 0.6898587606474108\nEpsilon = 0.6897897747713461\nEpsilon = 0.6897207957938689\nEpsilon = 0.6896518237142896\nEpsilon = 0.6895828585319181\nEpsilon = 0.689513900246065\nEpsilon = 0.6894449488560404\nEpsilon = 0.6893760043611548\nEpsilon = 0.6893070667607186\nEpsilon = 0.6892381360540426\nEpsilon = 0.6891692122404373\nEpsilon = 0.6891002953192132\nEpsilon = 0.6890313852896813\nEpsilon = 0.6889624821511524\nAgent: ddqn_agent . Episode 208/2000. Number of steps to finish: 20. Loss: 13.489197731018066 Reward: -10.0\nEpsilon = 0.6888935859029373\nEpsilon = 0.688824696544347\nEpsilon = 0.6887558140746926\nEpsilon = 0.6886869384932851\nEpsilon = 0.6886180697994357\nEpsilon = 0.6885492079924558\nEpsilon = 0.6884803530716566\nEpsilon = 0.6884115050363494\nEpsilon = 0.6883426638858458\nEpsilon = 0.6882738296194572\nEpsilon = 0.6882050022364952\nEpsilon = 0.6881361817362716\nEpsilon = 0.688067368118098\nEpsilon = 0.6879985613812862\nEpsilon = 0.6879297615251481\nEpsilon = 0.6878609685489956\nEpsilon = 0.6877921824521407\nEpsilon = 0.6877234032338955\nEpsilon = 0.687654630893572\nEpsilon = 0.6875858654304827\nAgent: ddqn_agent . Episode 209/2000. Number of steps to finish: 20. Loss: 13.487924575805664 Reward: -10.0\nEpsilon = 0.6875171068439396\nEpsilon = 0.6874483551332552\nEpsilon = 0.6873796102977419\nEpsilon = 0.6873108723367121\nEpsilon = 0.6872421412494784\nEpsilon = 0.6871734170353534\nEpsilon = 0.6871046996936498\nEpsilon = 0.6870359892236805\nEpsilon = 0.6869672856247581\nEpsilon = 0.6868985888961956\nEpsilon = 0.686829899037306\nEpsilon = 0.6867612160474023\nEpsilon = 0.6866925399257975\nEpsilon = 0.6866238706718049\nEpsilon = 0.6865552082847377\nEpsilon = 0.6864865527639092\nEpsilon = 0.6864179041086329\nEpsilon = 0.686349262318222\nEpsilon = 0.6862806273919902\nEpsilon = 0.686211999329251\nAgent: ddqn_agent . Episode 210/2000. Number of steps to finish: 20. Loss: 13.436370849609375 Reward: -12.0\nEpsilon = 0.6861433781293181\nEpsilon = 0.6860747637915052\nEpsilon = 0.6860061563151261\nEpsilon = 0.6859375556994945\nEpsilon = 0.6858689619439247\nEpsilon = 0.6858003750477303\nEpsilon = 0.6857317950102255\nEpsilon = 0.6856632218307245\nEpsilon = 0.6855946555085414\nEpsilon = 0.6855260960429905\nEpsilon = 0.6854575434333863\nEpsilon = 0.6853889976790429\nEpsilon = 0.685320458779275\nEpsilon = 0.6852519267333972\nEpsilon = 0.6851834015407239\nEpsilon = 0.6851148832005698\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.6850463717122497\nEpsilon = 0.6849778670750785\nEpsilon = 0.684909369288371\nEpsilon = 0.6848408783514421\nAgent: ddqn_agent . Episode 211/2000. Number of steps to finish: 20. Loss: 13.897856712341309 Reward: -10.0\nEpsilon = 0.684772394263607\nEpsilon = 0.6847039170241807\nEpsilon = 0.6846354466324782\nEpsilon = 0.6845669830878149\nEpsilon = 0.6844985263895061\nEpsilon = 0.6844300765368672\nEpsilon = 0.6843616335292135\nEpsilon = 0.6842931973658606\nEpsilon = 0.684224768046124\nEpsilon = 0.6841563455693194\nEpsilon = 0.6840879299347624\nEpsilon = 0.6840195211417689\nEpsilon = 0.6839511191896547\nEpsilon = 0.6838827240777358\nEpsilon = 0.683814335805328\nEpsilon = 0.6837459543717475\nEpsilon = 0.6836775797763103\nEpsilon = 0.6836092120183327\nEpsilon = 0.6835408510971309\nEpsilon = 0.6834724970120212\nAgent: ddqn_agent . Episode 212/2000. Number of steps to finish: 20. Loss: 13.528013229370117 Reward: -8.0\nEpsilon = 0.68340414976232\nEpsilon = 0.6833358093473437\nEpsilon = 0.683267475766409\nEpsilon = 0.6831991490188324\nEpsilon = 0.6831308291039305\nEpsilon = 0.6830625160210202\nEpsilon = 0.682994209769418\nEpsilon = 0.6829259103484411\nEpsilon = 0.6828576177574063\nEpsilon = 0.6827893319956305\nEpsilon = 0.6827210530624309\nEpsilon = 0.6826527809571247\nEpsilon = 0.682584515679029\nEpsilon = 0.6825162572274611\nEpsilon = 0.6824480056017384\nEpsilon = 0.6823797608011782\nEpsilon = 0.6823115228250981\nEpsilon = 0.6822432916728156\nEpsilon = 0.6821750673436483\nEpsilon = 0.6821068498369139\nAgent: ddqn_agent . Episode 213/2000. Number of steps to finish: 20. Loss: 13.848533630371094 Reward: -12.0\nEpsilon = 0.6820386391519303\nEpsilon = 0.6819704352880152\nEpsilon = 0.6819022382444864\nEpsilon = 0.6818340480206619\nEpsilon = 0.6817658646158599\nEpsilon = 0.6816976880293982\nEpsilon = 0.6816295182605953\nEpsilon = 0.6815613553087692\nEpsilon = 0.6814931991732383\nEpsilon = 0.681425049853321\nEpsilon = 0.6813569073483356\nEpsilon = 0.6812887716576008\nEpsilon = 0.681220642780435\nEpsilon = 0.681152520716157\nEpsilon = 0.6810844054640853\nEpsilon = 0.6810162970235389\nEpsilon = 0.6809481953938366\nEpsilon = 0.6808801005742972\nEpsilon = 0.6808120125642397\nEpsilon = 0.6807439313629833\nAgent: ddqn_agent . Episode 214/2000. Number of steps to finish: 20. Loss: 13.679524421691895 Reward: -12.0\nEpsilon = 0.680675856969847\nEpsilon = 0.68060778938415\nEpsilon = 0.6805397286052116\nEpsilon = 0.6804716746323511\nEpsilon = 0.6804036274648879\nEpsilon = 0.6803355871021415\nEpsilon = 0.6802675535434313\nEpsilon = 0.6801995267880769\nEpsilon = 0.6801315068353981\nEpsilon = 0.6800634936847146\nEpsilon = 0.6799954873353462\nEpsilon = 0.6799274877866127\nEpsilon = 0.679859495037834\nEpsilon = 0.6797915090883303\nEpsilon = 0.6797235299374215\nEpsilon = 0.6796555575844277\nEpsilon = 0.6795875920286694\nEpsilon = 0.6795196332694665\nEpsilon = 0.6794516813061395\nEpsilon = 0.679383736138009\nAgent: ddqn_agent . Episode 215/2000. Number of steps to finish: 20. Loss: 13.563639640808105 Reward: -12.0\nEpsilon = 0.6793157977643952\nEpsilon = 0.6792478661846187\nEpsilon = 0.6791799413980003\nEpsilon = 0.6791120234038606\nEpsilon = 0.6790441122015202\nEpsilon = 0.6789762077903\nEpsilon = 0.678908310169521\nEpsilon = 0.678840419338504\nEpsilon = 0.6787725352965702\nEpsilon = 0.6787046580430406\nAgent: ddqn_agent . Episode 216/2000. Number of steps to finish: 10. Loss: 6.779049873352051 Reward: 2.0\nEpsilon = 0.6786367875772363\nEpsilon = 0.6785689238984787\nEpsilon = 0.6785010670060888\nEpsilon = 0.6784332168993882\nEpsilon = 0.6783653735776982\nEpsilon = 0.6782975370403405\nEpsilon = 0.6782297072866365\nEpsilon = 0.6781618843159078\nEpsilon = 0.6780940681274763\nEpsilon = 0.6780262587206636\nEpsilon = 0.6779584560947916\nEpsilon = 0.6778906602491821\nEpsilon = 0.6778228711831572\nEpsilon = 0.6777550888960389\nEpsilon = 0.6776873133871493\nEpsilon = 0.6776195446558105\nEpsilon = 0.677551782701345\nEpsilon = 0.6774840275230748\nEpsilon = 0.6774162791203225\nEpsilon = 0.6773485374924105\nAgent: ddqn_agent . Episode 217/2000. Number of steps to finish: 20. Loss: 13.721207618713379 Reward: -12.0\nEpsilon = 0.6772808026386613\nEpsilon = 0.6772130745583974\nEpsilon = 0.6771453532509416\nEpsilon = 0.6770776387156165\nEpsilon = 0.677009930951745\nEpsilon = 0.6769422299586498\nEpsilon = 0.6768745357356539\nEpsilon = 0.6768068482820804\nEpsilon = 0.6767391675972522\nEpsilon = 0.6766714936804924\nEpsilon = 0.6766038265311244\nEpsilon = 0.6765361661484712\nEpsilon = 0.6764685125318564\nEpsilon = 0.6764008656806032\nEpsilon = 0.6763332255940352\nEpsilon = 0.6762655922714758\nEpsilon = 0.6761979657122487\nEpsilon = 0.6761303459156774\nEpsilon = 0.6760627328810859\nAgent: ddqn_agent . Episode 218/2000. Number of steps to finish: 19. Loss: 12.954514503479004 Reward: -7.0\nEpsilon = 0.6759951266077978\nEpsilon = 0.675927527095137\nEpsilon = 0.6758599343424275\nEpsilon = 0.6757923483489933\nEpsilon = 0.6757247691141584\nEpsilon = 0.675657196637247\nEpsilon = 0.6755896309175833\nEpsilon = 0.6755220719544915\nEpsilon = 0.6754545197472961\nEpsilon = 0.6753869742953214\nEpsilon = 0.6753194355978919\nEpsilon = 0.6752519036543321\nEpsilon = 0.6751843784639666\nEpsilon = 0.6751168600261203\nEpsilon = 0.6750493483401176\nEpsilon = 0.6749818434052837\nEpsilon = 0.6749143452209432\nEpsilon = 0.6748468537864211\nEpsilon = 0.6747793691010424\nEpsilon = 0.6747118911641323\nAgent: ddqn_agent . Episode 219/2000. Number of steps to finish: 20. Loss: 13.763917922973633 Reward: -10.0\nEpsilon = 0.6746444199750159\nEpsilon = 0.6745769555330184\nEpsilon = 0.6745094978374652\nEpsilon = 0.6744420468876814\nEpsilon = 0.6743746026829927\nEpsilon = 0.6743071652227244\nEpsilon = 0.6742397345062021\nEpsilon = 0.6741723105327515\nEpsilon = 0.6741048933016982\nEpsilon = 0.6740374828123681\nEpsilon = 0.6739700790640869\nEpsilon = 0.6739026820561805\nEpsilon = 0.673835291787975\nEpsilon = 0.6737679082587962\nEpsilon = 0.6737005314679703\nEpsilon = 0.6736331614148234\nEpsilon = 0.673565798098682\nEpsilon = 0.6734984415188722\nAgent: ddqn_agent . Episode 220/2000. Number of steps to finish: 18. Loss: 12.343239784240723 Reward: -6.0\nEpsilon = 0.6734310916747203\nEpsilon = 0.6733637485655528\nEpsilon = 0.6732964121906962\nEpsilon = 0.6732290825494772\nEpsilon = 0.6731617596412223\nEpsilon = 0.6730944434652582\nEpsilon = 0.6730271340209116\nEpsilon = 0.6729598313075096\nEpsilon = 0.6728925353243789\nEpsilon = 0.6728252460708465\nEpsilon = 0.6727579635462394\nEpsilon = 0.6726906877498848\nEpsilon = 0.6726234186811099\nEpsilon = 0.6725561563392417\nEpsilon = 0.6724889007236078\nEpsilon = 0.6724216518335354\nEpsilon = 0.672354409668352\nEpsilon = 0.6722871742273852\nEpsilon = 0.6722199455099624\nEpsilon = 0.6721527235154114\nAgent: ddqn_agent . Episode 221/2000. Number of steps to finish: 20. Loss: 13.480133056640625 Reward: -12.0\nEpsilon = 0.6720855082430599\nEpsilon = 0.6720182996922356\nEpsilon = 0.6719510978622664\nEpsilon = 0.6718839027524801\nEpsilon = 0.6718167143622049\nEpsilon = 0.6717495326907686\nEpsilon = 0.6716823577374996\nEpsilon = 0.6716151895017258\nEpsilon = 0.6715480279827757\nEpsilon = 0.6714808731799774\nEpsilon = 0.6714137250926594\nEpsilon = 0.6713465837201501\nEpsilon = 0.671279449061778\nEpsilon = 0.6712123211168719\nEpsilon = 0.6711451998847602\nEpsilon = 0.6710780853647718\nEpsilon = 0.6710109775562353\nEpsilon = 0.6709438764584796\nEpsilon = 0.6708767820708338\nEpsilon = 0.6708096943926267\nAgent: ddqn_agent . Episode 222/2000. Number of steps to finish: 20. Loss: 13.654691696166992 Reward: -10.0\nEpsilon = 0.6707426134231874\nEpsilon = 0.6706755391618451\nEpsilon = 0.6706084716079289\nEpsilon = 0.6705414107607681\nEpsilon = 0.670474356619692\nEpsilon = 0.67040730918403\nEpsilon = 0.6703402684531117\nEpsilon = 0.6702732344262664\nEpsilon = 0.6702062071028237\nEpsilon = 0.6701391864821135\nEpsilon = 0.6700721725634653\nEpsilon = 0.6700051653462089\nEpsilon = 0.6699381648296743\nEpsilon = 0.6698711710131913\nEpsilon = 0.66980418389609\nEpsilon = 0.6697372034777004\nEpsilon = 0.6696702297573526\nEpsilon = 0.6696032627343769\nEpsilon = 0.6695363024081035\nEpsilon = 0.6694693487778627\nAgent: ddqn_agent . Episode 223/2000. Number of steps to finish: 20. Loss: 13.76630973815918 Reward: -8.0\nEpsilon = 0.6694024018429849\nEpsilon = 0.6693354616028007\nEpsilon = 0.6692685280566404\nEpsilon = 0.6692016012038348\nEpsilon = 0.6691346810437144\nEpsilon = 0.66906776757561\nEpsilon = 0.6690008607988525\nEpsilon = 0.6689339607127726\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.6688670673167013\nEpsilon = 0.6688001806099696\nAgent: ddqn_agent . Episode 224/2000. Number of steps to finish: 10. Loss: 6.8348307609558105 Reward: 2.0\nEpsilon = 0.6687333005919086\nEpsilon = 0.6686664272618494\nEpsilon = 0.6685995606191233\nEpsilon = 0.6685327006630614\nEpsilon = 0.6684658473929951\nEpsilon = 0.6683990008082558\nEpsilon = 0.6683321609081749\nEpsilon = 0.6682653276920841\nEpsilon = 0.6681985011593149\nEpsilon = 0.668131681309199\nEpsilon = 0.668064868141068\nEpsilon = 0.6679980616542539\nEpsilon = 0.6679312618480885\nEpsilon = 0.6678644687219036\nEpsilon = 0.6677976822750314\nEpsilon = 0.667730902506804\nEpsilon = 0.6676641294165533\nEpsilon = 0.6675973630036116\nEpsilon = 0.6675306032673113\nEpsilon = 0.6674638502069845\nAgent: ddqn_agent . Episode 225/2000. Number of steps to finish: 20. Loss: 13.701301574707031 Reward: -10.0\nEpsilon = 0.6673971038219638\nEpsilon = 0.6673303641115816\nEpsilon = 0.6672636310751705\nEpsilon = 0.6671969047120629\nEpsilon = 0.6671301850215917\nEpsilon = 0.6670634720030896\nEpsilon = 0.6669967656558893\nEpsilon = 0.6669300659793237\nEpsilon = 0.6668633729727258\nEpsilon = 0.6667966866354286\nEpsilon = 0.666730006966765\nEpsilon = 0.6666633339660684\nEpsilon = 0.6665966676326718\nEpsilon = 0.6665300079659086\nEpsilon = 0.666463354965112\nEpsilon = 0.6663967086296155\nEpsilon = 0.6663300689587526\nEpsilon = 0.6662634359518567\nEpsilon = 0.6661968096082614\nAgent: ddqn_agent . Episode 226/2000. Number of steps to finish: 19. Loss: 12.721985816955566 Reward: -7.0\nEpsilon = 0.6661301899273007\nEpsilon = 0.666063576908308\nEpsilon = 0.6659969705506171\nEpsilon = 0.6659303708535621\nEpsilon = 0.6658637778164768\nEpsilon = 0.6657971914386951\nEpsilon = 0.6657306117195513\nEpsilon = 0.6656640386583793\nEpsilon = 0.6655974722545135\nEpsilon = 0.665530912507288\nEpsilon = 0.6654643594160373\nEpsilon = 0.6653978129800957\nEpsilon = 0.6653312731987977\nEpsilon = 0.6652647400714778\nEpsilon = 0.6651982135974707\nEpsilon = 0.6651316937761109\nEpsilon = 0.6650651806067334\nEpsilon = 0.6649986740886727\nEpsilon = 0.6649321742212639\nEpsilon = 0.6648656810038418\nAgent: ddqn_agent . Episode 227/2000. Number of steps to finish: 20. Loss: 13.552770614624023 Reward: -12.0\nEpsilon = 0.6647991944357414\nEpsilon = 0.6647327145162979\nEpsilon = 0.6646662412448463\nEpsilon = 0.6645997746207218\nEpsilon = 0.6645333146432598\nEpsilon = 0.6644668613117956\nEpsilon = 0.6644004146256643\nEpsilon = 0.6643339745842017\nEpsilon = 0.6642675411867434\nEpsilon = 0.6642011144326246\nEpsilon = 0.6641346943211813\nEpsilon = 0.6640682808517492\nEpsilon = 0.6640018740236641\nEpsilon = 0.6639354738362617\nEpsilon = 0.6638690802888781\nEpsilon = 0.6638026933808493\nEpsilon = 0.6637363131115113\nEpsilon = 0.6636699394802001\nEpsilon = 0.6636035724862521\nEpsilon = 0.6635372121290035\nAgent: ddqn_agent . Episode 228/2000. Number of steps to finish: 20. Loss: 14.093643188476562 Reward: -12.0\nEpsilon = 0.6634708584077906\nEpsilon = 0.6634045113219499\nEpsilon = 0.6633381708708177\nEpsilon = 0.6632718370537306\nEpsilon = 0.6632055098700252\nEpsilon = 0.6631391893190383\nEpsilon = 0.6630728754001064\nEpsilon = 0.6630065681125664\nEpsilon = 0.6629402674557552\nEpsilon = 0.6628739734290097\nEpsilon = 0.6628076860316667\nEpsilon = 0.6627414052630636\nEpsilon = 0.6626751311225373\nEpsilon = 0.6626088636094251\nEpsilon = 0.6625426027230641\nEpsilon = 0.6624763484627918\nEpsilon = 0.6624101008279455\nAgent: ddqn_agent . Episode 229/2000. Number of steps to finish: 17. Loss: 11.550625801086426 Reward: -5.0\nEpsilon = 0.6623438598178627\nEpsilon = 0.662277625431881\nEpsilon = 0.6622113976693378\nEpsilon = 0.6621451765295708\nEpsilon = 0.6620789620119178\nEpsilon = 0.6620127541157167\nEpsilon = 0.6619465528403051\nEpsilon = 0.6618803581850211\nEpsilon = 0.6618141701492026\nEpsilon = 0.6617479887321878\nEpsilon = 0.6616818139333146\nEpsilon = 0.6616156457519212\nEpsilon = 0.661549484187346\nAgent: ddqn_agent . Episode 230/2000. Number of steps to finish: 13. Loss: 8.763727188110352 Reward: -1.0\nEpsilon = 0.6614833292389273\nEpsilon = 0.6614171809060034\nEpsilon = 0.6613510391879128\nEpsilon = 0.6612849040839941\nEpsilon = 0.6612187755935857\nEpsilon = 0.6611526537160263\nEpsilon = 0.6610865384506547\nEpsilon = 0.6610204297968096\nEpsilon = 0.66095432775383\nEpsilon = 0.6608882323210546\nEpsilon = 0.6608221434978225\nEpsilon = 0.6607560612834728\nEpsilon = 0.6606899856773444\nEpsilon = 0.6606239166787766\nEpsilon = 0.6605578542871088\nEpsilon = 0.66049179850168\nEpsilon = 0.6604257493218298\nEpsilon = 0.6603597067468977\nEpsilon = 0.660293670776223\nEpsilon = 0.6602276414091454\nAgent: ddqn_agent . Episode 231/2000. Number of steps to finish: 20. Loss: 13.432940483093262 Reward: -12.0\nEpsilon = 0.6601616186450044\nEpsilon = 0.6600956024831399\nEpsilon = 0.6600295929228917\nEpsilon = 0.6599635899635994\nEpsilon = 0.659897593604603\nEpsilon = 0.6598316038452425\nEpsilon = 0.659765620684858\nEpsilon = 0.6596996441227895\nEpsilon = 0.6596336741583771\nEpsilon = 0.6595677107909613\nEpsilon = 0.6595017540198822\nEpsilon = 0.6594358038444802\nEpsilon = 0.6593698602640957\nEpsilon = 0.6593039232780693\nEpsilon = 0.6592379928857415\nEpsilon = 0.6591720690864529\nEpsilon = 0.6591061518795442\nEpsilon = 0.6590402412643563\nEpsilon = 0.6589743372402299\nEpsilon = 0.6589084398065059\nAgent: ddqn_agent . Episode 232/2000. Number of steps to finish: 20. Loss: 13.959527969360352 Reward: -10.0\nEpsilon = 0.6588425489625253\nEpsilon = 0.658776664707629\nEpsilon = 0.6587107870411583\nEpsilon = 0.6586449159624541\nEpsilon = 0.6585790514708578\nEpsilon = 0.6585131935657107\nEpsilon = 0.6584473422463542\nEpsilon = 0.6583814975121295\nEpsilon = 0.6583156593623782\nEpsilon = 0.658249827796442\nEpsilon = 0.6581840028136624\nEpsilon = 0.658118184413381\nEpsilon = 0.6580523725949396\nEpsilon = 0.6579865673576801\nAgent: ddqn_agent . Episode 233/2000. Number of steps to finish: 14. Loss: 9.764772415161133 Reward: -2.0\nEpsilon = 0.6579207687009444\nEpsilon = 0.6578549766240743\nEpsilon = 0.6577891911264119\nEpsilon = 0.6577234122072992\nEpsilon = 0.6576576398660785\nEpsilon = 0.6575918741020919\nEpsilon = 0.6575261149146816\nEpsilon = 0.6574603623031902\nEpsilon = 0.6573946162669598\nEpsilon = 0.6573288768053331\nEpsilon = 0.6572631439176526\nEpsilon = 0.6571974176032609\nEpsilon = 0.6571316978615006\nEpsilon = 0.6570659846917145\nEpsilon = 0.6570002780932453\nEpsilon = 0.656934578065436\nAgent: ddqn_agent . Episode 234/2000. Number of steps to finish: 16. Loss: 11.242777824401855 Reward: -4.0\nEpsilon = 0.6568688846076295\nEpsilon = 0.6568031977191687\nEpsilon = 0.6567375173993968\nEpsilon = 0.6566718436476568\nEpsilon = 0.6566061764632921\nEpsilon = 0.6565405158456458\nEpsilon = 0.6564748617940612\nEpsilon = 0.6564092143078818\nEpsilon = 0.656343573386451\nEpsilon = 0.6562779390291124\nEpsilon = 0.6562123112352094\nEpsilon = 0.6561466900040859\nEpsilon = 0.6560810753350855\nEpsilon = 0.656015467227552\nEpsilon = 0.6559498656808292\nEpsilon = 0.6558842706942611\nEpsilon = 0.6558186822671918\nEpsilon = 0.6557531003989651\nEpsilon = 0.6556875250889251\nEpsilon = 0.6556219563364163\nAgent: ddqn_agent . Episode 235/2000. Number of steps to finish: 20. Loss: 13.724578857421875 Reward: -12.0\nEpsilon = 0.6555563941407827\nEpsilon = 0.6554908385013687\nEpsilon = 0.6554252894175185\nEpsilon = 0.6553597468885768\nEpsilon = 0.655294210913888\nEpsilon = 0.6552286814927966\nEpsilon = 0.6551631586246472\nEpsilon = 0.6550976423087848\nEpsilon = 0.6550321325445538\nEpsilon = 0.6549666293312993\nEpsilon = 0.6549011326683662\nEpsilon = 0.6548356425550994\nEpsilon = 0.6547701589908439\nEpsilon = 0.6547046819749448\nEpsilon = 0.6546392115067473\nEpsilon = 0.6545737475855966\nEpsilon = 0.654508290210838\nEpsilon = 0.6544428393818169\nEpsilon = 0.6543773950978787\nEpsilon = 0.654311957358369\nAgent: ddqn_agent . Episode 236/2000. Number of steps to finish: 20. Loss: 13.820874214172363 Reward: -12.0\nEpsilon = 0.6542465261626331\nEpsilon = 0.6541811015100168\nEpsilon = 0.6541156833998658\nEpsilon = 0.6540502718315259\nEpsilon = 0.6539848668043428\nEpsilon = 0.6539194683176623\nEpsilon = 0.6538540763708306\nEpsilon = 0.6537886909631935\nEpsilon = 0.6537233120940972\nEpsilon = 0.6536579397628878\nEpsilon = 0.6535925739689115\nEpsilon = 0.6535272147115145\nEpsilon = 0.6534618619900434\nEpsilon = 0.6533965158038444\nEpsilon = 0.653331176152264\nEpsilon = 0.6532658430346487\nEpsilon = 0.6532005164503453\nEpsilon = 0.6531351963987002\nEpsilon = 0.6530698828790603\nEpsilon = 0.6530045758907724\nAgent: ddqn_agent . Episode 237/2000. Number of steps to finish: 20. Loss: 13.304765701293945 Reward: -10.0\nEpsilon = 0.6529392754331833\nEpsilon = 0.65287398150564\nEpsilon = 0.6528086941074894\nEpsilon = 0.6527434132380787\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.652678138896755\nEpsilon = 0.6526128710828653\nEpsilon = 0.652547609795757\nEpsilon = 0.6524823550347775\nEpsilon = 0.652417106799274\nEpsilon = 0.6523518650885941\nEpsilon = 0.6522866299020853\nEpsilon = 0.6522214012390951\nEpsilon = 0.6521561790989712\nEpsilon = 0.6520909634810613\nEpsilon = 0.6520257543847132\nEpsilon = 0.6519605518092747\nEpsilon = 0.6518953557540937\nEpsilon = 0.6518301662185183\nEpsilon = 0.6517649832018965\nAgent: ddqn_agent . Episode 238/2000. Number of steps to finish: 19. Loss: 13.228342056274414 Reward: -7.0\nEpsilon = 0.6516998067035763\nEpsilon = 0.651634636722906\nEpsilon = 0.6515694732592338\nEpsilon = 0.6515043163119079\nEpsilon = 0.6514391658802767\nEpsilon = 0.6513740219636887\nEpsilon = 0.6513088845614923\nEpsilon = 0.6512437536730361\nEpsilon = 0.6511786292976688\nEpsilon = 0.6511135114347391\nEpsilon = 0.6510484000835955\nEpsilon = 0.6509832952435872\nEpsilon = 0.6509181969140628\nEpsilon = 0.6508531050943714\nEpsilon = 0.650788019783862\nEpsilon = 0.6507229409818837\nEpsilon = 0.6506578686877855\nEpsilon = 0.6505928029009167\nEpsilon = 0.6505277436206266\nEpsilon = 0.6504626908462645\nAgent: ddqn_agent . Episode 239/2000. Number of steps to finish: 20. Loss: 13.803045272827148 Reward: -10.0\nEpsilon = 0.6503976445771799\nEpsilon = 0.6503326048127221\nEpsilon = 0.6502675715522409\nEpsilon = 0.6502025447950857\nEpsilon = 0.6501375245406061\nEpsilon = 0.6500725107881521\nEpsilon = 0.6500075035370733\nEpsilon = 0.6499425027867196\nEpsilon = 0.649877508536441\nEpsilon = 0.6498125207855874\nEpsilon = 0.6497475395335088\nEpsilon = 0.6496825647795554\nEpsilon = 0.6496175965230775\nEpsilon = 0.6495526347634252\nEpsilon = 0.6494876794999488\nEpsilon = 0.6494227307319989\nAgent: ddqn_agent . Episode 240/2000. Number of steps to finish: 16. Loss: 10.915447235107422 Reward: -4.0\nEpsilon = 0.6493577884589257\nEpsilon = 0.6492928526800797\nEpsilon = 0.6492279233948117\nEpsilon = 0.6491630006024722\nEpsilon = 0.649098084302412\nEpsilon = 0.6490331744939818\nEpsilon = 0.6489682711765324\nEpsilon = 0.6489033743494148\nEpsilon = 0.6488384840119799\nEpsilon = 0.6487736001635787\nEpsilon = 0.6487087228035623\nEpsilon = 0.6486438519312819\nEpsilon = 0.6485789875460888\nEpsilon = 0.6485141296473341\nEpsilon = 0.6484492782343694\nEpsilon = 0.6483844333065459\nEpsilon = 0.6483195948632153\nEpsilon = 0.648254762903729\nAgent: ddqn_agent . Episode 241/2000. Number of steps to finish: 18. Loss: 12.833459854125977 Reward: -6.0\nEpsilon = 0.6481899374274386\nEpsilon = 0.6481251184336958\nEpsilon = 0.6480603059218525\nEpsilon = 0.6479954998912603\nEpsilon = 0.6479307003412711\nEpsilon = 0.6478659072712369\nEpsilon = 0.6478011206805099\nEpsilon = 0.6477363405684419\nEpsilon = 0.647671566934385\nEpsilon = 0.6476067997776915\nEpsilon = 0.6475420390977138\nEpsilon = 0.6474772848938041\nEpsilon = 0.6474125371653147\nEpsilon = 0.6473477959115982\nEpsilon = 0.6472830611320071\nEpsilon = 0.6472183328258939\nAgent: ddqn_agent . Episode 242/2000. Number of steps to finish: 16. Loss: 10.754805564880371 Reward: -4.0\nEpsilon = 0.6471536109926114\nEpsilon = 0.6470888956315122\nEpsilon = 0.6470241867419491\nEpsilon = 0.6469594843232749\nEpsilon = 0.6468947883748426\nEpsilon = 0.6468300988960051\nEpsilon = 0.6467654158861155\nEpsilon = 0.6467007393445269\nEpsilon = 0.6466360692705925\nEpsilon = 0.6465714056636654\nEpsilon = 0.6465067485230991\nEpsilon = 0.6464420978482468\nEpsilon = 0.646377453638462\nEpsilon = 0.6463128158930982\nEpsilon = 0.6462481846115089\nEpsilon = 0.6461835597930478\nEpsilon = 0.6461189414370685\nEpsilon = 0.6460543295429247\nEpsilon = 0.6459897241099705\nEpsilon = 0.6459251251375595\nAgent: ddqn_agent . Episode 243/2000. Number of steps to finish: 20. Loss: 13.668009757995605 Reward: -12.0\nEpsilon = 0.6458605326250457\nEpsilon = 0.6457959465717832\nEpsilon = 0.645731366977126\nEpsilon = 0.6456667938404282\nEpsilon = 0.6456022271610442\nEpsilon = 0.6455376669383281\nEpsilon = 0.6454731131716344\nEpsilon = 0.6454085658603173\nEpsilon = 0.6453440250037312\nEpsilon = 0.6452794906012309\nEpsilon = 0.6452149626521708\nEpsilon = 0.6451504411559056\nEpsilon = 0.64508592611179\nEpsilon = 0.6450214175191789\nEpsilon = 0.6449569153774269\nEpsilon = 0.6448924196858892\nEpsilon = 0.6448279304439206\nEpsilon = 0.6447634476508762\nEpsilon = 0.6446989713061112\nEpsilon = 0.6446345014089806\nAgent: ddqn_agent . Episode 244/2000. Number of steps to finish: 20. Loss: 13.880695343017578 Reward: -8.0\nEpsilon = 0.6445700379588397\nEpsilon = 0.6445055809550438\nEpsilon = 0.6444411303969484\nEpsilon = 0.6443766862839087\nEpsilon = 0.6443122486152802\nEpsilon = 0.6442478173904187\nEpsilon = 0.6441833926086796\nEpsilon = 0.6441189742694188\nEpsilon = 0.6440545623719919\nEpsilon = 0.6439901569157547\nEpsilon = 0.6439257579000631\nEpsilon = 0.6438613653242731\nEpsilon = 0.6437969791877407\nEpsilon = 0.643732599489822\nEpsilon = 0.643668226229873\nEpsilon = 0.64360385940725\nEpsilon = 0.6435394990213092\nEpsilon = 0.6434751450714071\nEpsilon = 0.6434107975569\nEpsilon = 0.6433464564771443\nAgent: ddqn_agent . Episode 245/2000. Number of steps to finish: 20. Loss: 13.869169235229492 Reward: -18.0\nEpsilon = 0.6432821218314966\nEpsilon = 0.6432177936193134\nEpsilon = 0.6431534718399515\nEpsilon = 0.6430891564927675\nEpsilon = 0.6430248475771183\nEpsilon = 0.6429605450923606\nEpsilon = 0.6428962490378514\nEpsilon = 0.6428319594129476\nEpsilon = 0.6427676762170063\nEpsilon = 0.6427033994493846\nEpsilon = 0.6426391291094397\nAgent: ddqn_agent . Episode 246/2000. Number of steps to finish: 11. Loss: 7.620378494262695 Reward: 1.0\nEpsilon = 0.6425748651965287\nEpsilon = 0.6425106077100091\nEpsilon = 0.6424463566492381\nEpsilon = 0.6423821120135732\nEpsilon = 0.6423178738023718\nEpsilon = 0.6422536420149916\nEpsilon = 0.6421894166507901\nEpsilon = 0.642125197709125\nEpsilon = 0.6420609851893541\nEpsilon = 0.6419967790908351\nEpsilon = 0.641932579412926\nEpsilon = 0.6418683861549848\nEpsilon = 0.6418041993163692\nEpsilon = 0.6417400188964376\nEpsilon = 0.641675844894548\nEpsilon = 0.6416116773100585\nEpsilon = 0.6415475161423275\nEpsilon = 0.6414833613907133\nEpsilon = 0.6414192130545742\nEpsilon = 0.6413550711332687\nAgent: ddqn_agent . Episode 247/2000. Number of steps to finish: 20. Loss: 13.608084678649902 Reward: -8.0\nEpsilon = 0.6412909356261554\nEpsilon = 0.6412268065325928\nEpsilon = 0.6411626838519395\nEpsilon = 0.6410985675835543\nEpsilon = 0.641034457726796\nEpsilon = 0.6409703542810233\nEpsilon = 0.6409062572455952\nEpsilon = 0.6408421666198707\nEpsilon = 0.6407780824032088\nEpsilon = 0.6407140045949684\nEpsilon = 0.6406499331945089\nEpsilon = 0.6405858682011895\nEpsilon = 0.6405218096143693\nEpsilon = 0.6404577574334079\nEpsilon = 0.6403937116576646\nEpsilon = 0.6403296722864988\nEpsilon = 0.6402656393192702\nEpsilon = 0.6402016127553383\nEpsilon = 0.6401375925940628\nEpsilon = 0.6400735788348034\nAgent: ddqn_agent . Episode 248/2000. Number of steps to finish: 20. Loss: 14.204540252685547 Reward: -10.0\nEpsilon = 0.64000957147692\nEpsilon = 0.6399455705197723\nEpsilon = 0.6398815759627203\nEpsilon = 0.6398175878051241\nEpsilon = 0.6397536060463436\nEpsilon = 0.6396896306857389\nEpsilon = 0.6396256617226703\nEpsilon = 0.6395616991564981\nEpsilon = 0.6394977429865825\nEpsilon = 0.6394337932122838\nEpsilon = 0.6393698498329625\nEpsilon = 0.6393059128479792\nEpsilon = 0.6392419822566944\nEpsilon = 0.6391780580584687\nEpsilon = 0.6391141402526629\nEpsilon = 0.6390502288386376\nEpsilon = 0.6389863238157537\nEpsilon = 0.6389224251833722\nEpsilon = 0.6388585329408538\nEpsilon = 0.6387946470875597\nAgent: ddqn_agent . Episode 249/2000. Number of steps to finish: 20. Loss: 13.534778594970703 Reward: -12.0\nEpsilon = 0.638730767622851\nEpsilon = 0.6386668945460887\nEpsilon = 0.6386030278566341\nEpsilon = 0.6385391675538485\nEpsilon = 0.6384753136370932\nEpsilon = 0.6384114661057295\nEpsilon = 0.638347624959119\nEpsilon = 0.6382837901966231\nEpsilon = 0.6382199618176034\nEpsilon = 0.6381561398214217\nEpsilon = 0.6380923242074396\nEpsilon = 0.6380285149750188\nEpsilon = 0.6379647121235212\nEpsilon = 0.6379009156523089\nEpsilon = 0.6378371255607437\nEpsilon = 0.6377733418481876\nEpsilon = 0.6377095645140027\nEpsilon = 0.6376457935575514\nEpsilon = 0.6375820289781956\nAgent: ddqn_agent . Episode 250/2000. Number of steps to finish: 19. Loss: 13.281970024108887 Reward: -7.0\nEpsilon = 0.6375182707752978\nEpsilon = 0.6374545189482203\nEpsilon = 0.6373907734963254\nEpsilon = 0.6373270344189759\nEpsilon = 0.637263301715534\nEpsilon = 0.6371995753853624\nEpsilon = 0.6371358554278239\nEpsilon = 0.6370721418422811\nEpsilon = 0.6370084346280969\nEpsilon = 0.636944733784634\nEpsilon = 0.6368810393112555\nEpsilon = 0.6368173512073244\nEpsilon = 0.6367536694722037\nEpsilon = 0.6366899941052565\nEpsilon = 0.6366263251058459\nEpsilon = 0.6365626624733354\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.636499006207088\nEpsilon = 0.6364353563064673\nEpsilon = 0.6363717127708367\nEpsilon = 0.6363080755995596\nAgent: ddqn_agent . Episode 251/2000. Number of steps to finish: 20. Loss: 13.813931465148926 Reward: -12.0\nEpsilon = 0.6362444447919996\nEpsilon = 0.6361808203475204\nEpsilon = 0.6361172022654857\nEpsilon = 0.6360535905452592\nEpsilon = 0.6359899851862048\nEpsilon = 0.6359263861876862\nEpsilon = 0.6358627935490674\nEpsilon = 0.6357992072697125\nEpsilon = 0.6357356273489856\nEpsilon = 0.6356720537862507\nEpsilon = 0.6356084865808721\nEpsilon = 0.635544925732214\nEpsilon = 0.6354813712396408\nEpsilon = 0.6354178231025168\nEpsilon = 0.6353542813202065\nEpsilon = 0.6352907458920746\nEpsilon = 0.6352272168174854\nEpsilon = 0.6351636940958036\nEpsilon = 0.635100177726394\nEpsilon = 0.6350366677086214\nAgent: ddqn_agent . Episode 252/2000. Number of steps to finish: 20. Loss: 13.727027893066406 Reward: -10.0\nEpsilon = 0.6349731640418506\nEpsilon = 0.6349096667254464\nEpsilon = 0.6348461757587738\nEpsilon = 0.6347826911411979\nEpsilon = 0.6347192128720838\nEpsilon = 0.6346557409507966\nEpsilon = 0.6345922753767015\nEpsilon = 0.6345288161491638\nEpsilon = 0.6344653632675489\nEpsilon = 0.6344019167312221\nEpsilon = 0.634338476539549\nEpsilon = 0.634275042691895\nEpsilon = 0.6342116151876259\nEpsilon = 0.6341481940261071\nEpsilon = 0.6340847792067045\nEpsilon = 0.6340213707287838\nEpsilon = 0.6339579685917109\nEpsilon = 0.6338945727948517\nEpsilon = 0.6338311833375723\nEpsilon = 0.6337678002192385\nAgent: ddqn_agent . Episode 253/2000. Number of steps to finish: 20. Loss: 14.094175338745117 Reward: -10.0\nEpsilon = 0.6337044234392166\nEpsilon = 0.6336410529968727\nEpsilon = 0.633577688891573\nEpsilon = 0.6335143311226838\nEpsilon = 0.6334509796895715\nEpsilon = 0.6333876345916026\nEpsilon = 0.6333242958281434\nEpsilon = 0.6332609633985606\nEpsilon = 0.6331976373022208\nEpsilon = 0.6331343175384906\nEpsilon = 0.6330710041067368\nEpsilon = 0.6330076970063261\nEpsilon = 0.6329443962366255\nEpsilon = 0.6328811017970019\nEpsilon = 0.6328178136868222\nEpsilon = 0.6327545319054535\nEpsilon = 0.632691256452263\nEpsilon = 0.6326279873266177\nEpsilon = 0.632564724527885\nEpsilon = 0.6325014680554323\nAgent: ddqn_agent . Episode 254/2000. Number of steps to finish: 20. Loss: 13.909154891967773 Reward: -8.0\nEpsilon = 0.6324382179086268\nEpsilon = 0.6323749740868359\nEpsilon = 0.6323117365894272\nEpsilon = 0.6322485054157683\nEpsilon = 0.6321852805652267\nEpsilon = 0.6321220620371703\nEpsilon = 0.6320588498309665\nEpsilon = 0.6319956439459834\nEpsilon = 0.6319324443815888\nEpsilon = 0.6318692511371506\nEpsilon = 0.6318060642120369\nEpsilon = 0.6317428836056157\nEpsilon = 0.6316797093172551\nEpsilon = 0.6316165413463234\nEpsilon = 0.6315533796921887\nEpsilon = 0.6314902243542195\nEpsilon = 0.6314270753317841\nAgent: ddqn_agent . Episode 255/2000. Number of steps to finish: 17. Loss: 11.797148704528809 Reward: -5.0\nEpsilon = 0.631363932624251\nEpsilon = 0.6313007962309886\nEpsilon = 0.6312376661513655\nEpsilon = 0.6311745423847503\nEpsilon = 0.6311114249305119\nEpsilon = 0.6310483137880188\nEpsilon = 0.63098520895664\nEpsilon = 0.6309221104357443\nEpsilon = 0.6308590182247007\nEpsilon = 0.6307959323228782\nEpsilon = 0.630732852729646\nEpsilon = 0.630669779444373\nEpsilon = 0.6306067124664285\nEpsilon = 0.6305436517951819\nEpsilon = 0.6304805974300024\nEpsilon = 0.6304175493702594\nEpsilon = 0.6303545076153224\nEpsilon = 0.6302914721645609\nEpsilon = 0.6302284430173445\nEpsilon = 0.6301654201730428\nAgent: ddqn_agent . Episode 256/2000. Number of steps to finish: 20. Loss: 14.112642288208008 Reward: -12.0\nEpsilon = 0.6301024036310254\nEpsilon = 0.6300393933906623\nEpsilon = 0.6299763894513232\nEpsilon = 0.629913391812378\nEpsilon = 0.6298504004731968\nEpsilon = 0.6297874154331495\nEpsilon = 0.6297244366916063\nEpsilon = 0.6296614642479371\nEpsilon = 0.6295984981015124\nEpsilon = 0.6295355382517022\nEpsilon = 0.629472584697877\nEpsilon = 0.6294096374394073\nEpsilon = 0.6293466964756633\nEpsilon = 0.6292837618060158\nEpsilon = 0.6292208334298351\nEpsilon = 0.6291579113464921\nEpsilon = 0.6290949955553575\nEpsilon = 0.629032086055802\nEpsilon = 0.6289691828471964\nEpsilon = 0.6289062859289116\nAgent: ddqn_agent . Episode 257/2000. Number of steps to finish: 20. Loss: 14.008097648620605 Reward: -10.0\nEpsilon = 0.6288433953003187\nEpsilon = 0.6287805109607887\nEpsilon = 0.6287176329096926\nEpsilon = 0.6286547611464016\nEpsilon = 0.628591895670287\nEpsilon = 0.6285290364807199\nEpsilon = 0.6284661835770718\nEpsilon = 0.6284033369587142\nEpsilon = 0.6283404966250183\nEpsilon = 0.6282776625753559\nEpsilon = 0.6282148348090983\nEpsilon = 0.6281520133256174\nEpsilon = 0.6280891981242849\nEpsilon = 0.6280263892044724\nEpsilon = 0.627963586565552\nEpsilon = 0.6279007902068954\nEpsilon = 0.6278380001278747\nEpsilon = 0.6277752163278619\nEpsilon = 0.6277124388062292\nEpsilon = 0.6276496675623485\nAgent: ddqn_agent . Episode 258/2000. Number of steps to finish: 20. Loss: 13.805899620056152 Reward: -14.0\nEpsilon = 0.6275869025955922\nEpsilon = 0.6275241439053327\nEpsilon = 0.6274613914909422\nEpsilon = 0.627398645351793\nEpsilon = 0.6273359054872578\nEpsilon = 0.6272731718967091\nEpsilon = 0.6272104445795194\nEpsilon = 0.6271477235350614\nEpsilon = 0.6270850087627079\nEpsilon = 0.6270223002618316\nEpsilon = 0.6269595980318055\nEpsilon = 0.6268969020720023\nAgent: ddqn_agent . Episode 259/2000. Number of steps to finish: 12. Loss: 8.224675178527832 Reward: 0.0\nEpsilon = 0.6268342123817952\nEpsilon = 0.626771528960557\nEpsilon = 0.6267088518076609\nEpsilon = 0.6266461809224801\nEpsilon = 0.6265835163043879\nEpsilon = 0.6265208579527575\nEpsilon = 0.6264582058669622\nEpsilon = 0.6263955600463755\nEpsilon = 0.6263329204903709\nEpsilon = 0.6262702871983219\nEpsilon = 0.626207660169602\nEpsilon = 0.6261450394035851\nEpsilon = 0.6260824248996447\nEpsilon = 0.6260198166571548\nEpsilon = 0.625957214675489\nEpsilon = 0.6258946189540215\nEpsilon = 0.6258320294921261\nEpsilon = 0.625769446289177\nEpsilon = 0.6257068693445481\nEpsilon = 0.6256442986576136\nAgent: ddqn_agent . Episode 260/2000. Number of steps to finish: 20. Loss: 13.775430679321289 Reward: -14.0\nEpsilon = 0.6255817342277479\nEpsilon = 0.625519176054325\nEpsilon = 0.6254566241367197\nEpsilon = 0.625394078474306\nEpsilon = 0.6253315390664586\nEpsilon = 0.625269005912552\nEpsilon = 0.6252064790119607\nEpsilon = 0.6251439583640594\nEpsilon = 0.6250814439682231\nEpsilon = 0.6250189358238263\nEpsilon = 0.6249564339302439\nEpsilon = 0.6248939382868509\nEpsilon = 0.6248314488930222\nEpsilon = 0.6247689657481329\nEpsilon = 0.6247064888515581\nEpsilon = 0.624644018202673\nEpsilon = 0.6245815538008528\nEpsilon = 0.6245190956454727\nEpsilon = 0.6244566437359081\nEpsilon = 0.6243941980715345\nAgent: ddqn_agent . Episode 261/2000. Number of steps to finish: 20. Loss: 13.8043212890625 Reward: -12.0\nEpsilon = 0.6243317586517274\nEpsilon = 0.6242693254758622\nEpsilon = 0.6242068985433147\nEpsilon = 0.6241444778534604\nEpsilon = 0.6240820634056751\nEpsilon = 0.6240196551993346\nEpsilon = 0.6239572532338147\nEpsilon = 0.6238948575084913\nEpsilon = 0.6238324680227404\nEpsilon = 0.6237700847759381\nEpsilon = 0.6237077077674605\nEpsilon = 0.6236453369966838\nEpsilon = 0.6235829724629841\nEpsilon = 0.6235206141657378\nEpsilon = 0.6234582621043212\nEpsilon = 0.6233959162781108\nEpsilon = 0.6233335766864829\nEpsilon = 0.6232712433288143\nEpsilon = 0.6232089162044815\nEpsilon = 0.6231465953128611\nAgent: ddqn_agent . Episode 262/2000. Number of steps to finish: 20. Loss: 14.01269245147705 Reward: -14.0\nEpsilon = 0.6230842806533298\nEpsilon = 0.6230219722252645\nEpsilon = 0.622959670028042\nEpsilon = 0.6228973740610392\nEpsilon = 0.6228350843236331\nEpsilon = 0.6227728008152007\nEpsilon = 0.6227105235351191\nEpsilon = 0.6226482524827657\nEpsilon = 0.6225859876575174\nEpsilon = 0.6225237290587516\nEpsilon = 0.6224614766858457\nEpsilon = 0.6223992305381771\nAgent: ddqn_agent . Episode 263/2000. Number of steps to finish: 12. Loss: 8.141437530517578 Reward: 0.0\nEpsilon = 0.6223369906151234\nEpsilon = 0.6222747569160618\nEpsilon = 0.6222125294403702\nEpsilon = 0.6221503081874261\nEpsilon = 0.6220880931566074\nEpsilon = 0.6220258843472918\nEpsilon = 0.6219636817588571\nEpsilon = 0.6219014853906812\nEpsilon = 0.6218392952421422\nEpsilon = 0.621777111312618\nEpsilon = 0.6217149336014868\nEpsilon = 0.6216527621081266\nEpsilon = 0.6215905968319158\nEpsilon = 0.6215284377722325\nAgent: ddqn_agent . Episode 264/2000. Number of steps to finish: 14. Loss: 9.637083053588867 Reward: -2.0\nEpsilon = 0.6214662849284553\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.6214041382999625\nEpsilon = 0.6213419978861325\nEpsilon = 0.6212798636863439\nEpsilon = 0.6212177356999753\nEpsilon = 0.6211556139264053\nEpsilon = 0.6210934983650126\nEpsilon = 0.6210313890151761\nEpsilon = 0.6209692858762745\nAgent: ddqn_agent . Episode 265/2000. Number of steps to finish: 9. Loss: 6.442300796508789 Reward: 3.0\nEpsilon = 0.6209071889476869\nEpsilon = 0.6208450982287922\nEpsilon = 0.6207830137189693\nEpsilon = 0.6207209354175974\nEpsilon = 0.6206588633240557\nEpsilon = 0.6205967974377233\nEpsilon = 0.6205347377579795\nEpsilon = 0.6204726842842038\nEpsilon = 0.6204106370157754\nEpsilon = 0.6203485959520738\nEpsilon = 0.6202865610924786\nEpsilon = 0.6202245324363693\nEpsilon = 0.6201625099831257\nAgent: ddqn_agent . Episode 266/2000. Number of steps to finish: 13. Loss: 9.034773826599121 Reward: -1.0\nEpsilon = 0.6201004937321274\nEpsilon = 0.6200384836827542\nEpsilon = 0.6199764798343859\nEpsilon = 0.6199144821864024\nEpsilon = 0.6198524907381838\nEpsilon = 0.61979050548911\nEpsilon = 0.619728526438561\nEpsilon = 0.6196665535859172\nEpsilon = 0.6196045869305585\nEpsilon = 0.6195426264718655\nEpsilon = 0.6194806722092183\nEpsilon = 0.6194187241419974\nEpsilon = 0.6193567822695832\nEpsilon = 0.6192948465913563\nEpsilon = 0.6192329171066971\nEpsilon = 0.6191709938149865\nEpsilon = 0.619109076715605\nEpsilon = 0.6190471658079334\nEpsilon = 0.6189852610913527\nEpsilon = 0.6189233625652435\nAgent: ddqn_agent . Episode 267/2000. Number of steps to finish: 20. Loss: 14.260543823242188 Reward: -8.0\nEpsilon = 0.618861470228987\nEpsilon = 0.6187995840819641\nEpsilon = 0.6187377041235559\nEpsilon = 0.6186758303531436\nEpsilon = 0.6186139627701083\nEpsilon = 0.6185521013738313\nEpsilon = 0.618490246163694\nEpsilon = 0.6184283971390776\nEpsilon = 0.6183665542993637\nEpsilon = 0.6183047176439338\nEpsilon = 0.6182428871721695\nEpsilon = 0.6181810628834522\nEpsilon = 0.6181192447771638\nEpsilon = 0.6180574328526861\nEpsilon = 0.6179956271094008\nEpsilon = 0.6179338275466899\nEpsilon = 0.6178720341639352\nEpsilon = 0.6178102469605188\nEpsilon = 0.6177484659358228\nEpsilon = 0.6176866910892292\nAgent: ddqn_agent . Episode 268/2000. Number of steps to finish: 20. Loss: 14.011903762817383 Reward: -10.0\nEpsilon = 0.6176249224201203\nEpsilon = 0.6175631599278784\nEpsilon = 0.6175014036118855\nEpsilon = 0.6174396534715244\nEpsilon = 0.6173779095061772\nEpsilon = 0.6173161717152266\nEpsilon = 0.6172544400980551\nEpsilon = 0.6171927146540452\nEpsilon = 0.6171309953825799\nEpsilon = 0.6170692822830416\nEpsilon = 0.6170075753548133\nEpsilon = 0.6169458745972779\nEpsilon = 0.6168841800098182\nEpsilon = 0.6168224915918171\nEpsilon = 0.616760809342658\nEpsilon = 0.6166991332617238\nAgent: ddqn_agent . Episode 269/2000. Number of steps to finish: 16. Loss: 11.126376152038574 Reward: -4.0\nEpsilon = 0.6166374633483976\nEpsilon = 0.6165757996020628\nEpsilon = 0.6165141420221025\nEpsilon = 0.6164524906079003\nEpsilon = 0.6163908453588395\nEpsilon = 0.6163292062743037\nEpsilon = 0.6162675733536762\nEpsilon = 0.6162059465963409\nEpsilon = 0.6161443260016812\nEpsilon = 0.6160827115690811\nEpsilon = 0.6160211032979243\nEpsilon = 0.6159595011875945\nEpsilon = 0.6158979052374757\nAgent: ddqn_agent . Episode 270/2000. Number of steps to finish: 13. Loss: 8.956099510192871 Reward: -1.0\nEpsilon = 0.6158363154469519\nEpsilon = 0.6157747318154072\nEpsilon = 0.6157131543422256\nEpsilon = 0.6156515830267915\nEpsilon = 0.6155900178684888\nEpsilon = 0.6155284588667019\nEpsilon = 0.6154669060208152\nEpsilon = 0.6154053593302131\nEpsilon = 0.6153438187942801\nEpsilon = 0.6152822844124007\nEpsilon = 0.6152207561839595\nEpsilon = 0.6151592341083412\nEpsilon = 0.6150977181849303\nEpsilon = 0.6150362084131118\nEpsilon = 0.6149747047922705\nEpsilon = 0.6149132073217912\nAgent: ddqn_agent . Episode 271/2000. Number of steps to finish: 16. Loss: 11.376469612121582 Reward: -4.0\nEpsilon = 0.614851716001059\nEpsilon = 0.6147902308294589\nEpsilon = 0.614728751806376\nEpsilon = 0.6146672789311953\nEpsilon = 0.6146058122033022\nEpsilon = 0.6145443516220819\nEpsilon = 0.6144828971869196\nEpsilon = 0.614421448897201\nAgent: ddqn_agent . Episode 272/2000. Number of steps to finish: 8. Loss: 5.625062465667725 Reward: 4.0\nEpsilon = 0.6143600067523113\nEpsilon = 0.6142985707516361\nEpsilon = 0.6142371408945609\nEpsilon = 0.6141757171804715\nEpsilon = 0.6141142996087534\nEpsilon = 0.6140528881787926\nEpsilon = 0.6139914828899747\nEpsilon = 0.6139300837416858\nEpsilon = 0.6138686907333116\nEpsilon = 0.6138073038642383\nEpsilon = 0.6137459231338519\nEpsilon = 0.6136845485415385\nEpsilon = 0.6136231800866844\nEpsilon = 0.6135618177686757\nEpsilon = 0.6135004615868989\nEpsilon = 0.6134391115407402\nEpsilon = 0.6133777676295861\nEpsilon = 0.6133164298528231\nEpsilon = 0.6132550982098378\nEpsilon = 0.6131937727000168\nAgent: ddqn_agent . Episode 273/2000. Number of steps to finish: 20. Loss: 14.276595115661621 Reward: -10.0\nEpsilon = 0.6131324533227469\nEpsilon = 0.6130711400774146\nEpsilon = 0.6130098329634068\nEpsilon = 0.6129485319801105\nEpsilon = 0.6128872371269124\nEpsilon = 0.6128259484031997\nEpsilon = 0.6127646658083594\nEpsilon = 0.6127033893417786\nEpsilon = 0.6126421190028444\nEpsilon = 0.6125808547909442\nEpsilon = 0.6125195967054651\nEpsilon = 0.6124583447457945\nEpsilon = 0.61239709891132\nAgent: ddqn_agent . Episode 274/2000. Number of steps to finish: 13. Loss: 9.322898864746094 Reward: -1.0\nEpsilon = 0.6123358592014289\nEpsilon = 0.6122746256155087\nEpsilon = 0.6122133981529472\nEpsilon = 0.6121521768131318\nEpsilon = 0.6120909615954505\nEpsilon = 0.6120297524992909\nEpsilon = 0.611968549524041\nEpsilon = 0.6119073526690886\nEpsilon = 0.6118461619338217\nEpsilon = 0.6117849773176283\nEpsilon = 0.6117237988198965\nEpsilon = 0.6116626264400145\nEpsilon = 0.6116014601773705\nEpsilon = 0.6115403000313527\nEpsilon = 0.6114791460013496\nEpsilon = 0.6114179980867495\nAgent: ddqn_agent . Episode 275/2000. Number of steps to finish: 16. Loss: 11.490660667419434 Reward: -4.0\nEpsilon = 0.6113568562869408\nEpsilon = 0.6112957206013121\nEpsilon = 0.6112345910292519\nEpsilon = 0.611173467570149\nEpsilon = 0.611112350223392\nEpsilon = 0.6110512389883698\nEpsilon = 0.610990133864471\nEpsilon = 0.6109290348510845\nEpsilon = 0.6108679419475994\nEpsilon = 0.6108068551534047\nEpsilon = 0.6107457744678894\nEpsilon = 0.6106846998904426\nEpsilon = 0.6106236314204536\nEpsilon = 0.6105625690573115\nEpsilon = 0.6105015128004058\nEpsilon = 0.6104404626491258\nEpsilon = 0.6103794186028608\nEpsilon = 0.6103183806610005\nEpsilon = 0.6102573488229345\nAgent: ddqn_agent . Episode 276/2000. Number of steps to finish: 19. Loss: 12.983875274658203 Reward: -7.0\nEpsilon = 0.6101963230880522\nEpsilon = 0.6101353034557434\nEpsilon = 0.6100742899253978\nEpsilon = 0.6100132824964053\nEpsilon = 0.6099522811681557\nEpsilon = 0.6098912859400389\nEpsilon = 0.609830296811445\nEpsilon = 0.6097693137817638\nEpsilon = 0.6097083368503856\nEpsilon = 0.6096473660167006\nEpsilon = 0.6095864012800989\nEpsilon = 0.6095254426399709\nEpsilon = 0.6094644900957069\nEpsilon = 0.6094035436466974\nEpsilon = 0.6093426032923327\nEpsilon = 0.6092816690320034\nAgent: ddqn_agent . Episode 277/2000. Number of steps to finish: 16. Loss: 11.050004959106445 Reward: -4.0\nEpsilon = 0.6092207408651003\nEpsilon = 0.6091598187910138\nEpsilon = 0.6090989028091347\nEpsilon = 0.6090379929188537\nEpsilon = 0.6089770891195618\nEpsilon = 0.6089161914106499\nEpsilon = 0.6088552997915089\nEpsilon = 0.6087944142615297\nEpsilon = 0.6087335348201035\nEpsilon = 0.6086726614666215\nEpsilon = 0.6086117942004748\nEpsilon = 0.6085509330210548\nEpsilon = 0.6084900779277527\nEpsilon = 0.6084292289199599\nEpsilon = 0.6083683859970679\nEpsilon = 0.6083075491584682\nEpsilon = 0.6082467184035524\nEpsilon = 0.6081858937317121\nEpsilon = 0.6081250751423389\nEpsilon = 0.6080642626348247\nAgent: ddqn_agent . Episode 278/2000. Number of steps to finish: 20. Loss: 14.526386260986328 Reward: -12.0\nEpsilon = 0.6080034562085612\nEpsilon = 0.6079426558629404\nEpsilon = 0.6078818615973541\nEpsilon = 0.6078210734111944\nEpsilon = 0.6077602913038533\nEpsilon = 0.607699515274723\nEpsilon = 0.6076387453231955\nEpsilon = 0.6075779814486632\nEpsilon = 0.6075172236505183\nEpsilon = 0.6074564719281532\nEpsilon = 0.6073957262809604\nEpsilon = 0.6073349867083323\nEpsilon = 0.6072742532096614\nEpsilon = 0.6072135257843405\nEpsilon = 0.6071528044317621\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.607092089151319\nEpsilon = 0.6070313799424039\nEpsilon = 0.6069706768044096\nEpsilon = 0.6069099797367292\nEpsilon = 0.6068492887387555\nAgent: ddqn_agent . Episode 279/2000. Number of steps to finish: 20. Loss: 14.020939826965332 Reward: -14.0\nEpsilon = 0.6067886038098816\nEpsilon = 0.6067279249495007\nEpsilon = 0.6066672521570057\nEpsilon = 0.60660658543179\nEpsilon = 0.6065459247732469\nEpsilon = 0.6064852701807696\nEpsilon = 0.6064246216537515\nEpsilon = 0.6063639791915861\nEpsilon = 0.6063033427936669\nEpsilon = 0.6062427124593875\nEpsilon = 0.6061820881881416\nEpsilon = 0.6061214699793228\nEpsilon = 0.6060608578323249\nEpsilon = 0.6060002517465417\nEpsilon = 0.6059396517213671\nEpsilon = 0.605879057756195\nEpsilon = 0.6058184698504194\nEpsilon = 0.6057578880034344\nEpsilon = 0.6056973122146341\nAgent: ddqn_agent . Episode 280/2000. Number of steps to finish: 19. Loss: 13.114630699157715 Reward: -7.0\nEpsilon = 0.6056367424834126\nEpsilon = 0.6055761788091643\nEpsilon = 0.6055156211912833\nEpsilon = 0.6054550696291642\nEpsilon = 0.6053945241222013\nEpsilon = 0.605333984669789\nEpsilon = 0.605273451271322\nEpsilon = 0.605212923926195\nEpsilon = 0.6051524026338023\nEpsilon = 0.6050918873935389\nEpsilon = 0.6050313782047996\nEpsilon = 0.6049708750669791\nEpsilon = 0.6049103779794724\nEpsilon = 0.6048498869416745\nEpsilon = 0.6047894019529804\nEpsilon = 0.604728923012785\nEpsilon = 0.6046684501204838\nEpsilon = 0.6046079832754717\nEpsilon = 0.6045475224771442\nEpsilon = 0.6044870677248965\nAgent: ddqn_agent . Episode 281/2000. Number of steps to finish: 20. Loss: 14.897863388061523 Reward: -14.0\nEpsilon = 0.604426619018124\nEpsilon = 0.6043661763562221\nEpsilon = 0.6043057397385865\nEpsilon = 0.6042453091646126\nEpsilon = 0.6041848846336961\nEpsilon = 0.6041244661452327\nEpsilon = 0.6040640536986183\nEpsilon = 0.6040036472932484\nEpsilon = 0.6039432469285191\nEpsilon = 0.6038828526038262\nEpsilon = 0.6038224643185659\nEpsilon = 0.603762082072134\nEpsilon = 0.6037017058639268\nEpsilon = 0.6036413356933404\nEpsilon = 0.6035809715597711\nEpsilon = 0.6035206134626151\nEpsilon = 0.6034602614012688\nEpsilon = 0.6033999153751287\nEpsilon = 0.6033395753835912\nAgent: ddqn_agent . Episode 282/2000. Number of steps to finish: 19. Loss: 13.200348854064941 Reward: -7.0\nEpsilon = 0.6032792414260528\nEpsilon = 0.6032189135019103\nEpsilon = 0.6031585916105601\nEpsilon = 0.6030982757513991\nEpsilon = 0.603037965923824\nEpsilon = 0.6029776621272316\nEpsilon = 0.6029173643610188\nEpsilon = 0.6028570726245828\nEpsilon = 0.6027967869173203\nEpsilon = 0.6027365072386286\nEpsilon = 0.6026762335879048\nEpsilon = 0.602615965964546\nEpsilon = 0.6025557043679496\nAgent: ddqn_agent . Episode 283/2000. Number of steps to finish: 13. Loss: 9.102302551269531 Reward: -1.0\nEpsilon = 0.6024954487975128\nEpsilon = 0.6024351992526331\nEpsilon = 0.6023749557327078\nEpsilon = 0.6023147182371346\nEpsilon = 0.6022544867653109\nEpsilon = 0.6021942613166343\nEpsilon = 0.6021340418905027\nEpsilon = 0.6020738284863136\nEpsilon = 0.602013621103465\nEpsilon = 0.6019534197413546\nEpsilon = 0.6018932243993805\nEpsilon = 0.6018330350769406\nEpsilon = 0.601772851773433\nEpsilon = 0.6017126744882556\nEpsilon = 0.6016525032208068\nEpsilon = 0.6015923379704847\nEpsilon = 0.6015321787366876\nEpsilon = 0.601472025518814\nAgent: ddqn_agent . Episode 284/2000. Number of steps to finish: 18. Loss: 13.138448715209961 Reward: -6.0\nEpsilon = 0.6014118783162621\nEpsilon = 0.6013517371284305\nEpsilon = 0.6012916019547176\nEpsilon = 0.6012314727945222\nEpsilon = 0.6011713496472427\nEpsilon = 0.601111232512278\nEpsilon = 0.6010511213890268\nEpsilon = 0.6009910162768879\nEpsilon = 0.6009309171752603\nEpsilon = 0.6008708240835428\nEpsilon = 0.6008107370011344\nEpsilon = 0.6007506559274343\nEpsilon = 0.6006905808618416\nEpsilon = 0.6006305118037554\nEpsilon = 0.600570448752575\nEpsilon = 0.6005103917076998\nEpsilon = 0.6004503406685291\nEpsilon = 0.6003902956344622\nEpsilon = 0.6003302566048988\nEpsilon = 0.6002702235792383\nAgent: ddqn_agent . Episode 285/2000. Number of steps to finish: 20. Loss: 14.16252613067627 Reward: -14.0\nEpsilon = 0.6002101965568803\nEpsilon = 0.6001501755372247\nEpsilon = 0.600090160519671\nEpsilon = 0.600030151503619\nEpsilon = 0.5999701484884687\nEpsilon = 0.5999101514736199\nEpsilon = 0.5998501604584726\nEpsilon = 0.5997901754424267\nEpsilon = 0.5997301964248825\nEpsilon = 0.59967022340524\nEpsilon = 0.5996102563828994\nEpsilon = 0.5995502953572611\nEpsilon = 0.5994903403277254\nEpsilon = 0.5994303912936927\nEpsilon = 0.5993704482545633\nEpsilon = 0.5993105112097379\nEpsilon = 0.5992505801586169\nEpsilon = 0.599190655100601\nEpsilon = 0.5991307360350909\nEpsilon = 0.5990708229614874\nAgent: ddqn_agent . Episode 286/2000. Number of steps to finish: 20. Loss: 14.075550079345703 Reward: -12.0\nEpsilon = 0.5990109158791912\nEpsilon = 0.5989510147876033\nEpsilon = 0.5988911196861245\nEpsilon = 0.5988312305741559\nEpsilon = 0.5987713474510985\nEpsilon = 0.5987114703163534\nEpsilon = 0.5986515991693218\nEpsilon = 0.5985917340094048\nEpsilon = 0.5985318748360039\nEpsilon = 0.5984720216485203\nEpsilon = 0.5984121744463554\nEpsilon = 0.5983523332289108\nEpsilon = 0.5982924979955879\nEpsilon = 0.5982326687457884\nAgent: ddqn_agent . Episode 287/2000. Number of steps to finish: 14. Loss: 9.98275089263916 Reward: -2.0\nEpsilon = 0.5981728454789138\nEpsilon = 0.598113028194366\nEpsilon = 0.5980532168915466\nEpsilon = 0.5979934115698574\nEpsilon = 0.5979336122287005\nEpsilon = 0.5978738188674776\nEpsilon = 0.5978140314855909\nEpsilon = 0.5977542500824423\nEpsilon = 0.5976944746574341\nEpsilon = 0.5976347052099683\nEpsilon = 0.5975749417394474\nEpsilon = 0.5975151842452734\nEpsilon = 0.5974554327268489\nEpsilon = 0.5973956871835762\nEpsilon = 0.5973359476148579\nEpsilon = 0.5972762140200963\nAgent: ddqn_agent . Episode 288/2000. Number of steps to finish: 16. Loss: 10.98448371887207 Reward: -4.0\nEpsilon = 0.5972164863986943\nEpsilon = 0.5971567647500544\nEpsilon = 0.5970970490735794\nEpsilon = 0.5970373393686721\nEpsilon = 0.5969776356347353\nEpsilon = 0.5969179378711718\nEpsilon = 0.5968582460773847\nEpsilon = 0.596798560252777\nEpsilon = 0.5967388803967517\nEpsilon = 0.5966792065087121\nEpsilon = 0.5966195385880613\nEpsilon = 0.5965598766342025\nEpsilon = 0.5965002206465391\nEpsilon = 0.5964405706244744\nEpsilon = 0.5963809265674119\nEpsilon = 0.5963212884747552\nEpsilon = 0.5962616563459077\nEpsilon = 0.5962020301802731\nEpsilon = 0.5961424099772551\nEpsilon = 0.5960827957362573\nAgent: ddqn_agent . Episode 289/2000. Number of steps to finish: 20. Loss: 14.277605056762695 Reward: -14.0\nEpsilon = 0.5960231874566837\nEpsilon = 0.595963585137938\nEpsilon = 0.5959039887794242\nEpsilon = 0.5958443983805463\nEpsilon = 0.5957848139407083\nEpsilon = 0.5957252354593142\nEpsilon = 0.5956656629357683\nEpsilon = 0.5956060963694747\nEpsilon = 0.5955465357598377\nEpsilon = 0.5954869811062617\nEpsilon = 0.5954274324081511\nEpsilon = 0.5953678896649103\nEpsilon = 0.5953083528759437\nEpsilon = 0.5952488220406562\nAgent: ddqn_agent . Episode 290/2000. Number of steps to finish: 14. Loss: 10.02182674407959 Reward: -2.0\nEpsilon = 0.5951892971584521\nEpsilon = 0.5951297782287364\nEpsilon = 0.5950702652509134\nEpsilon = 0.5950107582243883\nEpsilon = 0.5949512571485659\nEpsilon = 0.594891762022851\nEpsilon = 0.5948322728466487\nEpsilon = 0.5947727896193641\nEpsilon = 0.5947133123404021\nEpsilon = 0.5946538410091681\nEpsilon = 0.5945943756250671\nEpsilon = 0.5945349161875046\nEpsilon = 0.5944754626958859\nEpsilon = 0.5944160151496163\nEpsilon = 0.5943565735481013\nEpsilon = 0.5942971378907466\nAgent: ddqn_agent . Episode 291/2000. Number of steps to finish: 16. Loss: 11.547191619873047 Reward: -4.0\nEpsilon = 0.5942377081769575\nEpsilon = 0.5941782844061398\nEpsilon = 0.5941188665776992\nEpsilon = 0.5940594546910414\nEpsilon = 0.5940000487455723\nEpsilon = 0.5939406487406977\nEpsilon = 0.5938812546758236\nEpsilon = 0.5938218665503561\nEpsilon = 0.593762484363701\nEpsilon = 0.5937031081152647\nEpsilon = 0.5936437378044532\nEpsilon = 0.5935843734306728\nEpsilon = 0.5935250149933298\nEpsilon = 0.5934656624918304\nEpsilon = 0.5934063159255812\nAgent: ddqn_agent . Episode 292/2000. Number of steps to finish: 15. Loss: 10.394621849060059 Reward: -3.0\nEpsilon = 0.5933469752939887\nEpsilon = 0.5932876405964593\nEpsilon = 0.5932283118323997\nEpsilon = 0.5931689890012164\nEpsilon = 0.5931096721023162\nEpsilon = 0.593050361135106\nEpsilon = 0.5929910560989925\nEpsilon = 0.5929317569933826\nEpsilon = 0.5928724638176833\nEpsilon = 0.5928131765713016\nEpsilon = 0.5927538952536445\nEpsilon = 0.5926946198641192\nEpsilon = 0.5926353504021328\nEpsilon = 0.5925760868670926\nAgent: ddqn_agent . Episode 293/2000. Number of steps to finish: 14. Loss: 9.987885475158691 Reward: -2.0\nEpsilon = 0.5925168292584059\nEpsilon = 0.59245757757548\nEpsilon = 0.5923983318177225\nEpsilon = 0.5923390919845407\nEpsilon = 0.5922798580753422\nEpsilon = 0.5922206300895347\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.5921614080265258\nEpsilon = 0.5921021918857232\nEpsilon = 0.5920429816665347\nEpsilon = 0.591983777368368\nEpsilon = 0.5919245789906312\nEpsilon = 0.5918653865327321\nEpsilon = 0.5918061999940789\nEpsilon = 0.5917470193740795\nEpsilon = 0.5916878446721421\nEpsilon = 0.5916286758876749\nEpsilon = 0.5915695130200861\nEpsilon = 0.5915103560687841\nEpsilon = 0.5914512050331773\nEpsilon = 0.591392059912674\nAgent: ddqn_agent . Episode 294/2000. Number of steps to finish: 20. Loss: 14.28596019744873 Reward: -12.0\nEpsilon = 0.5913329207066828\nEpsilon = 0.5912737874146121\nEpsilon = 0.5912146600358706\nEpsilon = 0.591155538569867\nEpsilon = 0.5910964230160101\nEpsilon = 0.5910373133737085\nEpsilon = 0.5909782096423711\nEpsilon = 0.5909191118214069\nEpsilon = 0.5908600199102247\nEpsilon = 0.5908009339082337\nEpsilon = 0.5907418538148429\nEpsilon = 0.5906827796294615\nEpsilon = 0.5906237113514985\nEpsilon = 0.5905646489803633\nEpsilon = 0.5905055925154653\nEpsilon = 0.5904465419562137\nEpsilon = 0.5903874973020181\nEpsilon = 0.5903284585522879\nEpsilon = 0.5902694257064327\nEpsilon = 0.590210398763862\nAgent: ddqn_agent . Episode 295/2000. Number of steps to finish: 20. Loss: 14.567120552062988 Reward: -10.0\nEpsilon = 0.5901513777239856\nEpsilon = 0.5900923625862132\nEpsilon = 0.5900333533499547\nEpsilon = 0.5899743500146196\nEpsilon = 0.5899153525796181\nEpsilon = 0.5898563610443602\nEpsilon = 0.5897973754082558\nEpsilon = 0.5897383956707151\nEpsilon = 0.589679421831148\nEpsilon = 0.5896204538889649\nEpsilon = 0.589561491843576\nAgent: ddqn_agent . Episode 296/2000. Number of steps to finish: 11. Loss: 7.664634704589844 Reward: 1.0\nEpsilon = 0.5895025356943916\nEpsilon = 0.5894435854408222\nEpsilon = 0.5893846410822782\nEpsilon = 0.58932570261817\nEpsilon = 0.5892667700479082\nEpsilon = 0.5892078433709034\nEpsilon = 0.5891489225865663\nEpsilon = 0.5890900076943076\nEpsilon = 0.5890310986935382\nEpsilon = 0.5889721955836689\nEpsilon = 0.5889132983641105\nEpsilon = 0.5888544070342742\nEpsilon = 0.5887955215935707\nEpsilon = 0.5887366420414114\nAgent: ddqn_agent . Episode 297/2000. Number of steps to finish: 14. Loss: 9.962764739990234 Reward: -2.0\nEpsilon = 0.5886777683772073\nEpsilon = 0.5886189006003696\nEpsilon = 0.5885600387103096\nEpsilon = 0.5885011827064386\nEpsilon = 0.5884423325881679\nEpsilon = 0.5883834883549091\nEpsilon = 0.5883246500060736\nEpsilon = 0.588265817541073\nEpsilon = 0.5882069909593189\nEpsilon = 0.588148170260223\nEpsilon = 0.5880893554431971\nEpsilon = 0.5880305465076527\nEpsilon = 0.587971743453002\nEpsilon = 0.5879129462786566\nEpsilon = 0.5878541549840288\nEpsilon = 0.5877953695685304\nEpsilon = 0.5877365900315735\nEpsilon = 0.5876778163725703\nEpsilon = 0.5876190485909331\nEpsilon = 0.587560286686074\nAgent: ddqn_agent . Episode 298/2000. Number of steps to finish: 20. Loss: 14.270051002502441 Reward: -14.0\nEpsilon = 0.5875015306574054\nEpsilon = 0.5874427805043396\nEpsilon = 0.5873840362262892\nEpsilon = 0.5873252978226666\nEpsilon = 0.5872665652928843\nEpsilon = 0.5872078386363551\nEpsilon = 0.5871491178524915\nEpsilon = 0.5870904029407062\nEpsilon = 0.5870316939004122\nEpsilon = 0.5869729907310222\nEpsilon = 0.5869142934319491\nEpsilon = 0.5868556020026059\nEpsilon = 0.5867969164424056\nEpsilon = 0.5867382367507614\nEpsilon = 0.5866795629270863\nEpsilon = 0.5866208949707936\nEpsilon = 0.5865622328812965\nEpsilon = 0.5865035766580083\nEpsilon = 0.5864449263003425\nEpsilon = 0.5863862818077125\nAgent: ddqn_agent . Episode 299/2000. Number of steps to finish: 20. Loss: 14.390803337097168 Reward: -12.0\nEpsilon = 0.5863276431795317\nEpsilon = 0.5862690104152137\nEpsilon = 0.5862103835141722\nEpsilon = 0.5861517624758208\nEpsilon = 0.5860931472995732\nEpsilon = 0.5860345379848433\nEpsilon = 0.5859759345310448\nEpsilon = 0.5859173369375917\nEpsilon = 0.585858745203898\nEpsilon = 0.5858001593293777\nEpsilon = 0.5857415793134447\nEpsilon = 0.5856830051555134\nEpsilon = 0.5856244368549979\nEpsilon = 0.5855658744113124\nEpsilon = 0.5855073178238712\nEpsilon = 0.5854487670920888\nEpsilon = 0.5853902222153796\nEpsilon = 0.5853316831931581\nEpsilon = 0.5852731500248388\nEpsilon = 0.5852146227098364\nAgent: ddqn_agent . Episode 300/2000. Number of steps to finish: 20. Loss: 14.587814331054688 Reward: -10.0\nEpsilon = 0.5851561012475655\nEpsilon = 0.5850975856374407\nEpsilon = 0.585039075878877\nEpsilon = 0.5849805719712892\nEpsilon = 0.584922073914092\nEpsilon = 0.5848635817067006\nEpsilon = 0.58480509534853\nEpsilon = 0.5847466148389951\nEpsilon = 0.5846881401775113\nEpsilon = 0.5846296713634935\nEpsilon = 0.5845712083963572\nEpsilon = 0.5845127512755176\nEpsilon = 0.5844543000003901\nEpsilon = 0.5843958545703901\nEpsilon = 0.5843374149849331\nEpsilon = 0.5842789812434346\nEpsilon = 0.5842205533453102\nEpsilon = 0.5841621312899756\nEpsilon = 0.5841037150768467\nEpsilon = 0.584045304705339\nAgent: ddqn_agent . Episode 301/2000. Number of steps to finish: 20. Loss: 14.204268455505371 Reward: -14.0\nEpsilon = 0.5839869001748684\nEpsilon = 0.583928501484851\nEpsilon = 0.5838701086347025\nEpsilon = 0.583811721623839\nEpsilon = 0.5837533404516766\nEpsilon = 0.5836949651176314\nEpsilon = 0.5836365956211197\nEpsilon = 0.5835782319615576\nEpsilon = 0.5835198741383614\nEpsilon = 0.5834615221509475\nEpsilon = 0.5834031759987324\nAgent: ddqn_agent . Episode 302/2000. Number of steps to finish: 11. Loss: 7.842898368835449 Reward: 1.0\nEpsilon = 0.5833448356811325\nEpsilon = 0.5832865011975644\nEpsilon = 0.5832281725474446\nEpsilon = 0.5831698497301899\nEpsilon = 0.5831115327452169\nEpsilon = 0.5830532215919424\nEpsilon = 0.5829949162697832\nEpsilon = 0.5829366167781562\nEpsilon = 0.5828783231164785\nEpsilon = 0.5828200352841668\nEpsilon = 0.5827617532806384\nEpsilon = 0.5827034771053103\nEpsilon = 0.5826452067575998\nEpsilon = 0.5825869422369241\nEpsilon = 0.5825286835427004\nAgent: ddqn_agent . Episode 303/2000. Number of steps to finish: 15. Loss: 10.514294624328613 Reward: -3.0\nEpsilon = 0.5824704306743461\nEpsilon = 0.5824121836312787\nEpsilon = 0.5823539424129155\nEpsilon = 0.5822957070186742\nEpsilon = 0.5822374774479723\nEpsilon = 0.5821792537002275\nEpsilon = 0.5821210357748575\nEpsilon = 0.58206282367128\nEpsilon = 0.582004617388913\nEpsilon = 0.5819464169271741\nEpsilon = 0.5818882222854813\nEpsilon = 0.5818300334632528\nEpsilon = 0.5817718504599064\nEpsilon = 0.5817136732748605\nEpsilon = 0.581655501907533\nEpsilon = 0.5815973363573422\nEpsilon = 0.5815391766237065\nEpsilon = 0.5814810227060441\nEpsilon = 0.5814228746037735\nEpsilon = 0.5813647323163131\nAgent: ddqn_agent . Episode 304/2000. Number of steps to finish: 20. Loss: 14.186677932739258 Reward: -12.0\nEpsilon = 0.5813065958430815\nEpsilon = 0.5812484651834973\nEpsilon = 0.5811903403369789\nEpsilon = 0.5811322213029452\nEpsilon = 0.5810741080808148\nEpsilon = 0.5810160006700068\nEpsilon = 0.5809578990699398\nEpsilon = 0.5808998032800328\nEpsilon = 0.5808417132997048\nEpsilon = 0.5807836291283748\nEpsilon = 0.580725550765462\nEpsilon = 0.5806674782103854\nEpsilon = 0.5806094114625644\nEpsilon = 0.5805513505214182\nEpsilon = 0.5804932953863661\nAgent: ddqn_agent . Episode 305/2000. Number of steps to finish: 15. Loss: 10.6629638671875 Reward: -3.0\nEpsilon = 0.5804352460568274\nEpsilon = 0.5803772025322217\nEpsilon = 0.5803191648119684\nEpsilon = 0.5802611328954872\nEpsilon = 0.5802031067821977\nEpsilon = 0.5801450864715194\nEpsilon = 0.5800870719628722\nEpsilon = 0.5800290632556759\nEpsilon = 0.5799710603493504\nEpsilon = 0.5799130632433155\nEpsilon = 0.5798550719369912\nEpsilon = 0.5797970864297974\nEpsilon = 0.5797391067211545\nEpsilon = 0.5796811328104824\nEpsilon = 0.5796231646972013\nEpsilon = 0.5795652023807316\nEpsilon = 0.5795072458604935\nEpsilon = 0.5794492951359075\nEpsilon = 0.5793913502063939\nEpsilon = 0.5793334110713733\nAgent: ddqn_agent . Episode 306/2000. Number of steps to finish: 20. Loss: 14.298273086547852 Reward: -18.0\nEpsilon = 0.5792754777302661\nEpsilon = 0.5792175501824931\nEpsilon = 0.5791596284274748\nEpsilon = 0.579101712464632\nEpsilon = 0.5790438022933856\nEpsilon = 0.5789858979131562\nEpsilon = 0.5789279993233649\nEpsilon = 0.5788701065234326\nEpsilon = 0.5788122195127803\nEpsilon = 0.578754338290829\nEpsilon = 0.5786964628569999\nEpsilon = 0.5786385932107142\nEpsilon = 0.5785807293513932\nEpsilon = 0.578522871278458\nEpsilon = 0.5784650189913302\nEpsilon = 0.5784071724894311\nAgent: ddqn_agent . Episode 307/2000. Number of steps to finish: 16. Loss: 11.465645790100098 Reward: -4.0\nEpsilon = 0.5783493317721822\nEpsilon = 0.5782914968390049\nEpsilon = 0.578233667689321\nEpsilon = 0.5781758443225521\nEpsilon = 0.5781180267381199\nEpsilon = 0.578060214935446\nEpsilon = 0.5780024089139525\nEpsilon = 0.5779446086730611\nEpsilon = 0.5778868142121938\nEpsilon = 0.5778290255307725\nEpsilon = 0.5777712426282194\nEpsilon = 0.5777134655039566\nEpsilon = 0.5776556941574063\nEpsilon = 0.5775979285879905\nEpsilon = 0.5775401687951317\nEpsilon = 0.5774824147782522\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.5774246665367744\nEpsilon = 0.5773669240701207\nEpsilon = 0.5773091873777136\nEpsilon = 0.5772514564589759\nAgent: ddqn_agent . Episode 308/2000. Number of steps to finish: 20. Loss: 14.334220886230469 Reward: -12.0\nEpsilon = 0.5771937313133301\nEpsilon = 0.5771360119401987\nEpsilon = 0.5770782983390047\nEpsilon = 0.5770205905091708\nEpsilon = 0.5769628884501199\nEpsilon = 0.5769051921612749\nEpsilon = 0.5768475016420588\nEpsilon = 0.5767898168918946\nEpsilon = 0.5767321379102054\nEpsilon = 0.5766744646964144\nEpsilon = 0.5766167972499447\nEpsilon = 0.5765591355702198\nEpsilon = 0.5765014796566628\nEpsilon = 0.5764438295086971\nEpsilon = 0.5763861851257462\nEpsilon = 0.5763285465072336\nEpsilon = 0.5762709136525829\nEpsilon = 0.5762132865612176\nEpsilon = 0.5761556652325615\nEpsilon = 0.5760980496660382\nAgent: ddqn_agent . Episode 309/2000. Number of steps to finish: 20. Loss: 14.300811767578125 Reward: -10.0\nEpsilon = 0.5760404398610716\nEpsilon = 0.5759828358170854\nEpsilon = 0.5759252375335038\nEpsilon = 0.5758676450097504\nEpsilon = 0.5758100582452494\nEpsilon = 0.5757524772394249\nEpsilon = 0.5756949019917009\nEpsilon = 0.5756373325015017\nEpsilon = 0.5755797687682516\nEpsilon = 0.5755222107913748\nEpsilon = 0.5754646585702956\nEpsilon = 0.5754071121044386\nEpsilon = 0.5753495713932282\nEpsilon = 0.5752920364360888\nEpsilon = 0.5752345072324453\nEpsilon = 0.5751769837817221\nEpsilon = 0.575119466083344\nEpsilon = 0.5750619541367357\nEpsilon = 0.575004447941322\nEpsilon = 0.5749469474965279\nAgent: ddqn_agent . Episode 310/2000. Number of steps to finish: 20. Loss: 13.959507942199707 Reward: -12.0\nEpsilon = 0.5748894528017782\nEpsilon = 0.574831963856498\nEpsilon = 0.5747744806601124\nEpsilon = 0.5747170032120463\nEpsilon = 0.5746595315117251\nEpsilon = 0.574602065558574\nEpsilon = 0.5745446053520181\nEpsilon = 0.574487150891483\nEpsilon = 0.5744297021763938\nEpsilon = 0.5743722592061762\nEpsilon = 0.5743148219802555\nEpsilon = 0.5742573904980575\nEpsilon = 0.5741999647590077\nEpsilon = 0.5741425447625318\nEpsilon = 0.5740851305080555\nEpsilon = 0.5740277219950047\nEpsilon = 0.5739703192228053\nEpsilon = 0.573912922190883\nEpsilon = 0.5738555308986639\nEpsilon = 0.573798145345574\nAgent: ddqn_agent . Episode 311/2000. Number of steps to finish: 20. Loss: 14.61925983428955 Reward: -10.0\nEpsilon = 0.5737407655310395\nEpsilon = 0.5736833914544863\nEpsilon = 0.5736260231153408\nEpsilon = 0.5735686605130293\nEpsilon = 0.573511303646978\nEpsilon = 0.5734539525166134\nEpsilon = 0.5733966071213616\nEpsilon = 0.5733392674606496\nEpsilon = 0.5732819335339036\nEpsilon = 0.5732246053405502\nEpsilon = 0.5731672828800162\nEpsilon = 0.5731099661517282\nEpsilon = 0.5730526551551129\nEpsilon = 0.5729953498895974\nEpsilon = 0.5729380503546084\nEpsilon = 0.5728807565495729\nEpsilon = 0.572823468473918\nEpsilon = 0.5727661861270705\nAgent: ddqn_agent . Episode 312/2000. Number of steps to finish: 18. Loss: 13.076737403869629 Reward: -6.0\nEpsilon = 0.5727089095084579\nEpsilon = 0.572651638617507\nEpsilon = 0.5725943734536453\nEpsilon = 0.5725371140162999\nEpsilon = 0.5724798603048983\nEpsilon = 0.5724226123188678\nEpsilon = 0.572365370057636\nEpsilon = 0.5723081335206303\nEpsilon = 0.5722509027072782\nEpsilon = 0.5721936776170075\nEpsilon = 0.5721364582492457\nEpsilon = 0.5720792446034209\nEpsilon = 0.5720220366789606\nEpsilon = 0.5719648344752927\nEpsilon = 0.5719076379918452\nEpsilon = 0.571850447228046\nEpsilon = 0.5717932621833232\nEpsilon = 0.5717360828571049\nEpsilon = 0.5716789092488191\nEpsilon = 0.5716217413578942\nAgent: ddqn_agent . Episode 313/2000. Number of steps to finish: 20. Loss: 14.374833106994629 Reward: -10.0\nEpsilon = 0.5715645791837585\nEpsilon = 0.5715074227258401\nEpsilon = 0.5714502719835676\nEpsilon = 0.5713931269563692\nEpsilon = 0.5713359876436737\nEpsilon = 0.5712788540449093\nEpsilon = 0.5712217261595048\nEpsilon = 0.5711646039868888\nEpsilon = 0.5711074875264902\nEpsilon = 0.5710503767777375\nEpsilon = 0.5709932717400598\nEpsilon = 0.5709361724128857\nEpsilon = 0.5708790787956445\nEpsilon = 0.570821990887765\nEpsilon = 0.5707649086886762\nEpsilon = 0.5707078321978074\nEpsilon = 0.5706507614145876\nEpsilon = 0.5705936963384461\nEpsilon = 0.5705366369688123\nEpsilon = 0.5704795833051154\nAgent: ddqn_agent . Episode 314/2000. Number of steps to finish: 20. Loss: 14.84013843536377 Reward: -18.0\nEpsilon = 0.5704225353467849\nEpsilon = 0.5703654930932502\nEpsilon = 0.5703084565439409\nEpsilon = 0.5702514256982866\nEpsilon = 0.5701944005557168\nEpsilon = 0.5701373811156613\nEpsilon = 0.5700803673775497\nEpsilon = 0.570023359340812\nEpsilon = 0.569966357004878\nEpsilon = 0.5699093603691775\nEpsilon = 0.5698523694331405\nEpsilon = 0.5697953841961972\nEpsilon = 0.5697384046577776\nEpsilon = 0.5696814308173118\nEpsilon = 0.5696244626742301\nEpsilon = 0.5695675002279627\nEpsilon = 0.5695105434779398\nEpsilon = 0.5694535924235921\nEpsilon = 0.5693966470643497\nEpsilon = 0.5693397073996432\nAgent: ddqn_agent . Episode 315/2000. Number of steps to finish: 20. Loss: 14.649078369140625 Reward: -14.0\nEpsilon = 0.5692827734289033\nEpsilon = 0.5692258451515604\nEpsilon = 0.5691689225670452\nEpsilon = 0.5691120056747885\nEpsilon = 0.569055094474221\nEpsilon = 0.5689981889647736\nEpsilon = 0.5689412891458772\nEpsilon = 0.5688843950169625\nEpsilon = 0.5688275065774608\nEpsilon = 0.5687706238268031\nEpsilon = 0.5687137467644204\nEpsilon = 0.568656875389744\nEpsilon = 0.568600009702205\nEpsilon = 0.5685431497012349\nEpsilon = 0.5684862953862647\nEpsilon = 0.5684294467567261\nEpsilon = 0.5683726038120505\nEpsilon = 0.5683157665516693\nEpsilon = 0.5682589349750141\nEpsilon = 0.5682021090815166\nAgent: ddqn_agent . Episode 316/2000. Number of steps to finish: 20. Loss: 14.133622169494629 Reward: -12.0\nEpsilon = 0.5681452888706084\nEpsilon = 0.5680884743417214\nEpsilon = 0.5680316654942872\nEpsilon = 0.5679748623277378\nEpsilon = 0.567918064841505\nEpsilon = 0.5678612730350209\nEpsilon = 0.5678044869077173\nEpsilon = 0.5677477064590266\nEpsilon = 0.5676909316883807\nEpsilon = 0.5676341625952118\nEpsilon = 0.5675773991789523\nEpsilon = 0.5675206414390345\nEpsilon = 0.5674638893748906\nEpsilon = 0.5674071429859531\nEpsilon = 0.5673504022716545\nEpsilon = 0.5672936672314273\nEpsilon = 0.5672369378647041\nEpsilon = 0.5671802141709176\nEpsilon = 0.5671234961495005\nEpsilon = 0.5670667837998856\nAgent: ddqn_agent . Episode 317/2000. Number of steps to finish: 20. Loss: 14.03158950805664 Reward: -12.0\nEpsilon = 0.5670100771215056\nEpsilon = 0.5669533761137935\nEpsilon = 0.5668966807761822\nEpsilon = 0.5668399911081046\nEpsilon = 0.5667833071089937\nEpsilon = 0.5667266287782828\nEpsilon = 0.566669956115405\nEpsilon = 0.5666132891197935\nEpsilon = 0.5665566277908815\nEpsilon = 0.5664999721281023\nEpsilon = 0.5664433221308895\nEpsilon = 0.5663866777986765\nEpsilon = 0.5663300391308966\nEpsilon = 0.5662734061269835\nEpsilon = 0.5662167787863708\nEpsilon = 0.5661601571084921\nEpsilon = 0.5661035410927813\nEpsilon = 0.566046930738672\nEpsilon = 0.5659903260455981\nEpsilon = 0.5659337270129935\nAgent: ddqn_agent . Episode 318/2000. Number of steps to finish: 20. Loss: 13.997686386108398 Reward: -12.0\nEpsilon = 0.5658771336402922\nEpsilon = 0.5658205459269282\nEpsilon = 0.5657639638723355\nEpsilon = 0.5657073874759483\nEpsilon = 0.5656508167372007\nEpsilon = 0.565594251655527\nEpsilon = 0.5655376922303614\nEpsilon = 0.5654811384611385\nEpsilon = 0.5654245903472923\nEpsilon = 0.5653680478882576\nEpsilon = 0.5653115110834688\nEpsilon = 0.5652549799323604\nEpsilon = 0.5651984544343672\nEpsilon = 0.5651419345889238\nEpsilon = 0.5650854203954649\nAgent: ddqn_agent . Episode 319/2000. Number of steps to finish: 15. Loss: 10.529685020446777 Reward: -3.0\nEpsilon = 0.5650289118534254\nEpsilon = 0.5649724089622401\nEpsilon = 0.5649159117213439\nEpsilon = 0.5648594201301718\nEpsilon = 0.5648029341881587\nEpsilon = 0.56474645389474\nEpsilon = 0.5646899792493505\nEpsilon = 0.5646335102514256\nEpsilon = 0.5645770469004004\nEpsilon = 0.5645205891957105\nEpsilon = 0.5644641371367909\nEpsilon = 0.5644076907230772\nEpsilon = 0.5643512499540049\nEpsilon = 0.5642948148290095\nEpsilon = 0.5642383853475266\nEpsilon = 0.5641819615089918\nEpsilon = 0.5641255433128409\nEpsilon = 0.5640691307585096\nEpsilon = 0.5640127238454338\nEpsilon = 0.5639563225730493\nAgent: ddqn_agent . Episode 320/2000. Number of steps to finish: 20. Loss: 14.536613464355469 Reward: -12.0\nEpsilon = 0.5638999269407919\nEpsilon = 0.5638435369480979\nEpsilon = 0.563787152594403\nEpsilon = 0.5637307738791436\nEpsilon = 0.5636744008017557\nEpsilon = 0.5636180333616755\nEpsilon = 0.5635616715583394\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.5635053153911835\nEpsilon = 0.5634489648596445\nEpsilon = 0.5633926199631585\nEpsilon = 0.5633362807011622\nEpsilon = 0.563279947073092\nEpsilon = 0.5632236190783847\nEpsilon = 0.5631672967164769\nEpsilon = 0.5631109799868053\nEpsilon = 0.5630546688888066\nEpsilon = 0.5629983634219178\nEpsilon = 0.5629420635855756\nEpsilon = 0.5628857693792171\nEpsilon = 0.5628294808022791\nAgent: ddqn_agent . Episode 321/2000. Number of steps to finish: 20. Loss: 14.125651359558105 Reward: -10.0\nEpsilon = 0.5627731978541989\nEpsilon = 0.5627169205344135\nEpsilon = 0.56266064884236\nEpsilon = 0.5626043827774758\nEpsilon = 0.562548122339198\nEpsilon = 0.5624918675269641\nEpsilon = 0.5624356183402114\nEpsilon = 0.5623793747783774\nEpsilon = 0.5623231368408996\nEpsilon = 0.5622669045272155\nEpsilon = 0.5622106778367628\nEpsilon = 0.5621544567689791\nEpsilon = 0.5620982413233022\nEpsilon = 0.5620420314991699\nEpsilon = 0.56198582729602\nEpsilon = 0.5619296287132903\nEpsilon = 0.561873435750419\nEpsilon = 0.561817248406844\nEpsilon = 0.5617610666820033\nEpsilon = 0.5617048905753351\nAgent: ddqn_agent . Episode 322/2000. Number of steps to finish: 20. Loss: 14.709712982177734 Reward: -10.0\nEpsilon = 0.5616487200862776\nEpsilon = 0.561592555214269\nEpsilon = 0.5615363959587476\nEpsilon = 0.5614802423191517\nEpsilon = 0.5614240942949198\nEpsilon = 0.5613679518854903\nEpsilon = 0.5613118150903017\nEpsilon = 0.5612556839087927\nEpsilon = 0.5611995583404018\nEpsilon = 0.5611434383845678\nEpsilon = 0.5610873240407294\nEpsilon = 0.5610312153083253\nEpsilon = 0.5609751121867945\nEpsilon = 0.5609190146755758\nEpsilon = 0.5608629227741082\nEpsilon = 0.5608068364818308\nEpsilon = 0.5607507557981827\nEpsilon = 0.5606946807226029\nEpsilon = 0.5606386112545306\nEpsilon = 0.5605825473934052\nAgent: ddqn_agent . Episode 323/2000. Number of steps to finish: 20. Loss: 14.276382446289062 Reward: -12.0\nEpsilon = 0.5605264891386659\nEpsilon = 0.560470436489752\nEpsilon = 0.5604143894461031\nEpsilon = 0.5603583480071586\nEpsilon = 0.5603023121723578\nEpsilon = 0.5602462819411406\nEpsilon = 0.5601902573129465\nEpsilon = 0.5601342382872152\nEpsilon = 0.5600782248633865\nEpsilon = 0.5600222170409002\nEpsilon = 0.5599662148191961\nEpsilon = 0.5599102181977141\nEpsilon = 0.5598542271758944\nEpsilon = 0.5597982417531768\nAgent: ddqn_agent . Episode 324/2000. Number of steps to finish: 14. Loss: 10.20003890991211 Reward: -2.0\nEpsilon = 0.5597422619290016\nEpsilon = 0.5596862877028087\nEpsilon = 0.5596303190740385\nEpsilon = 0.5595743560421311\nEpsilon = 0.559518398606527\nEpsilon = 0.5594624467666663\nEpsilon = 0.5594065005219897\nEpsilon = 0.5593505598719375\nEpsilon = 0.5592946248159503\nEpsilon = 0.5592386953534687\nEpsilon = 0.5591827714839334\nEpsilon = 0.559126853206785\nEpsilon = 0.5590709405214643\nEpsilon = 0.5590150334274122\nEpsilon = 0.5589591319240694\nEpsilon = 0.558903236010877\nEpsilon = 0.5588473456872759\nEpsilon = 0.5587914609527072\nEpsilon = 0.5587355818066119\nEpsilon = 0.5586797082484312\nAgent: ddqn_agent . Episode 325/2000. Number of steps to finish: 20. Loss: 14.493998527526855 Reward: -14.0\nEpsilon = 0.5586238402776064\nEpsilon = 0.5585679778935786\nEpsilon = 0.5585121210957893\nEpsilon = 0.5584562698836797\nEpsilon = 0.5584004242566913\nEpsilon = 0.5583445842142657\nEpsilon = 0.5582887497558443\nEpsilon = 0.5582329208808687\nAgent: ddqn_agent . Episode 326/2000. Number of steps to finish: 8. Loss: 5.8794121742248535 Reward: 4.0\nEpsilon = 0.5581770975887806\nEpsilon = 0.5581212798790218\nEpsilon = 0.5580654677510339\nEpsilon = 0.5580096612042589\nEpsilon = 0.5579538602381384\nEpsilon = 0.5578980648521146\nEpsilon = 0.5578422750456293\nEpsilon = 0.5577864908181248\nEpsilon = 0.557730712169043\nEpsilon = 0.5576749390978262\nAgent: ddqn_agent . Episode 327/2000. Number of steps to finish: 10. Loss: 7.020766258239746 Reward: 2.0\nEpsilon = 0.5576191716039164\nEpsilon = 0.557563409686756\nEpsilon = 0.5575076533457873\nEpsilon = 0.5574519025804527\nEpsilon = 0.5573961573901947\nEpsilon = 0.5573404177744556\nEpsilon = 0.5572846837326783\nEpsilon = 0.557228955264305\nAgent: ddqn_agent . Episode 328/2000. Number of steps to finish: 8. Loss: 5.687010288238525 Reward: 4.0\nEpsilon = 0.5571732323687786\nEpsilon = 0.5571175150455417\nEpsilon = 0.5570618032940372\nEpsilon = 0.5570060971137079\nEpsilon = 0.5569503965039965\nEpsilon = 0.556894701464346\nEpsilon = 0.5568390119941996\nEpsilon = 0.5567833280930002\nEpsilon = 0.556727649760191\nEpsilon = 0.556671976995215\nEpsilon = 0.5566163097975154\nAgent: ddqn_agent . Episode 329/2000. Number of steps to finish: 11. Loss: 8.192344665527344 Reward: 1.0\nEpsilon = 0.5565606481665357\nEpsilon = 0.5565049921017191\nEpsilon = 0.5564493416025089\nEpsilon = 0.5563936966683487\nEpsilon = 0.5563380572986819\nEpsilon = 0.5562824234929521\nEpsilon = 0.5562267952506028\nEpsilon = 0.5561711725710777\nEpsilon = 0.5561155554538206\nEpsilon = 0.5560599438982753\nEpsilon = 0.5560043379038855\nEpsilon = 0.5559487374700951\nEpsilon = 0.5558931425963481\nEpsilon = 0.5558375532820885\nEpsilon = 0.5557819695267603\nEpsilon = 0.5557263913298076\nEpsilon = 0.5556708186906746\nEpsilon = 0.5556152516088055\nEpsilon = 0.5555596900836447\nEpsilon = 0.5555041341146364\nAgent: ddqn_agent . Episode 330/2000. Number of steps to finish: 20. Loss: 14.563373565673828 Reward: -14.0\nEpsilon = 0.555448583701225\nEpsilon = 0.5553930388428548\nEpsilon = 0.5553374995389706\nEpsilon = 0.5552819657890167\nEpsilon = 0.5552264375924377\nEpsilon = 0.5551709149486785\nEpsilon = 0.5551153978571837\nEpsilon = 0.555059886317398\nEpsilon = 0.5550043803287663\nEpsilon = 0.5549488798907335\nEpsilon = 0.5548933850027444\nEpsilon = 0.5548378956642441\nEpsilon = 0.5547824118746777\nEpsilon = 0.5547269336334902\nEpsilon = 0.5546714609401269\nEpsilon = 0.5546159937940328\nEpsilon = 0.5545605321946534\nEpsilon = 0.554505076141434\nEpsilon = 0.5544496256338198\nEpsilon = 0.5543941806712565\nAgent: ddqn_agent . Episode 331/2000. Number of steps to finish: 20. Loss: 14.46988296508789 Reward: -12.0\nEpsilon = 0.5543387412531894\nEpsilon = 0.5542833073790641\nEpsilon = 0.5542278790483262\nEpsilon = 0.5541724562604213\nEpsilon = 0.5541170390147953\nEpsilon = 0.5540616273108938\nEpsilon = 0.5540062211481628\nEpsilon = 0.5539508205260479\nEpsilon = 0.5538954254439954\nEpsilon = 0.553840035901451\nEpsilon = 0.5537846518978609\nEpsilon = 0.5537292734326711\nEpsilon = 0.5536739005053278\nEpsilon = 0.5536185331152773\nEpsilon = 0.5535631712619657\nEpsilon = 0.5535078149448396\nEpsilon = 0.5534524641633451\nEpsilon = 0.5533971189169288\nEpsilon = 0.5533417792050371\nEpsilon = 0.5532864450271167\nAgent: ddqn_agent . Episode 332/2000. Number of steps to finish: 20. Loss: 14.17795467376709 Reward: -16.0\nEpsilon = 0.5532311163826139\nEpsilon = 0.5531757932709757\nEpsilon = 0.5531204756916486\nEpsilon = 0.5530651636440794\nEpsilon = 0.553009857127715\nEpsilon = 0.5529545561420023\nEpsilon = 0.5528992606863881\nEpsilon = 0.5528439707603194\nEpsilon = 0.5527886863632434\nEpsilon = 0.5527334074946071\nEpsilon = 0.5526781341538576\nEpsilon = 0.5526228663404422\nEpsilon = 0.5525676040538082\nEpsilon = 0.5525123472934028\nEpsilon = 0.5524570960586734\nEpsilon = 0.5524018503490675\nEpsilon = 0.5523466101640326\nEpsilon = 0.5522913755030162\nAgent: ddqn_agent . Episode 333/2000. Number of steps to finish: 18. Loss: 12.928961753845215 Reward: -6.0\nEpsilon = 0.5522361463654659\nEpsilon = 0.5521809227508294\nEpsilon = 0.5521257046585544\nEpsilon = 0.5520704920880886\nEpsilon = 0.5520152850388798\nEpsilon = 0.5519600835103758\nEpsilon = 0.5519048875020248\nEpsilon = 0.5518496970132746\nEpsilon = 0.5517945120435732\nEpsilon = 0.5517393325923688\nEpsilon = 0.5516841586591096\nEpsilon = 0.5516289902432436\nEpsilon = 0.5515738273442193\nAgent: ddqn_agent . Episode 334/2000. Number of steps to finish: 13. Loss: 9.431928634643555 Reward: -1.0\nEpsilon = 0.5515186699614849\nEpsilon = 0.5514635180944888\nEpsilon = 0.5514083717426793\nEpsilon = 0.551353230905505\nEpsilon = 0.5512980955824145\nEpsilon = 0.5512429657728563\nEpsilon = 0.551187841476279\nEpsilon = 0.5511327226921313\nEpsilon = 0.5510776094198621\nEpsilon = 0.5510225016589201\nEpsilon = 0.5509673994087542\nEpsilon = 0.5509123026688133\nEpsilon = 0.5508572114385464\nEpsilon = 0.5508021257174025\nAgent: ddqn_agent . Episode 335/2000. Number of steps to finish: 14. Loss: 10.216431617736816 Reward: -2.0\nEpsilon = 0.5507470455048308\nEpsilon = 0.5506919708002803\nEpsilon = 0.5506369016032003\nEpsilon = 0.55058183791304\nEpsilon = 0.5505267797292488\nEpsilon = 0.5504717270512759\nEpsilon = 0.5504166798785708\nEpsilon = 0.5503616382105829\nEpsilon = 0.5503066020467619\nEpsilon = 0.5502515713865572\nEpsilon = 0.5501965462294186\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.5501415265747956\nEpsilon = 0.5500865124221381\nEpsilon = 0.5500315037708959\nEpsilon = 0.5499765006205188\nEpsilon = 0.5499215029704568\nEpsilon = 0.5498665108201598\nEpsilon = 0.5498115241690777\nEpsilon = 0.5497565430166609\nAgent: ddqn_agent . Episode 336/2000. Number of steps to finish: 19. Loss: 13.257593154907227 Reward: -7.0\nEpsilon = 0.5497015673623592\nEpsilon = 0.5496465972056229\nEpsilon = 0.5495916325459024\nEpsilon = 0.5495366733826478\nEpsilon = 0.5494817197153096\nEpsilon = 0.5494267715433381\nEpsilon = 0.5493718288661837\nEpsilon = 0.5493168916832971\nEpsilon = 0.5492619599941287\nEpsilon = 0.5492070337981293\nEpsilon = 0.5491521130947495\nEpsilon = 0.54909719788344\nEpsilon = 0.5490422881636516\nEpsilon = 0.5489873839348353\nEpsilon = 0.5489324851964418\nEpsilon = 0.5488775919479222\nEpsilon = 0.5488227041887274\nAgent: ddqn_agent . Episode 337/2000. Number of steps to finish: 17. Loss: 11.842260360717773 Reward: -5.0\nEpsilon = 0.5487678219183085\nEpsilon = 0.5487129451361167\nEpsilon = 0.548658073841603\nEpsilon = 0.5486032080342189\nEpsilon = 0.5485483477134155\nEpsilon = 0.5484934928786442\nEpsilon = 0.5484386435293563\nEpsilon = 0.5483837996650034\nEpsilon = 0.5483289612850369\nEpsilon = 0.5482741283889084\nEpsilon = 0.5482193009760695\nEpsilon = 0.5481644790459719\nEpsilon = 0.5481096625980673\nEpsilon = 0.5480548516318074\nEpsilon = 0.5480000461466443\nEpsilon = 0.5479452461420297\nEpsilon = 0.5478904516174155\nEpsilon = 0.5478356625722537\nEpsilon = 0.5477808790059965\nEpsilon = 0.5477261009180959\nAgent: ddqn_agent . Episode 338/2000. Number of steps to finish: 20. Loss: 13.989877700805664 Reward: -12.0\nEpsilon = 0.5476713283080041\nEpsilon = 0.5476165611751733\nEpsilon = 0.5475617995190558\nEpsilon = 0.5475070433391039\nEpsilon = 0.54745229263477\nEpsilon = 0.5473975474055065\nEpsilon = 0.5473428076507659\nEpsilon = 0.5472880733700008\nEpsilon = 0.5472333445626638\nEpsilon = 0.5471786212282076\nEpsilon = 0.5471239033660847\nEpsilon = 0.5470691909757481\nEpsilon = 0.5470144840566505\nAgent: ddqn_agent . Episode 339/2000. Number of steps to finish: 13. Loss: 9.523454666137695 Reward: -1.0\nEpsilon = 0.5469597826082448\nEpsilon = 0.546905086629984\nEpsilon = 0.546850396121321\nEpsilon = 0.5467957110817089\nEpsilon = 0.5467410315106007\nEpsilon = 0.5466863574074496\nEpsilon = 0.5466316887717089\nEpsilon = 0.5465770256028317\nEpsilon = 0.5465223679002714\nEpsilon = 0.5464677156634814\nEpsilon = 0.5464130688919151\nEpsilon = 0.546358427585026\nEpsilon = 0.5463037917422675\nEpsilon = 0.5462491613630933\nEpsilon = 0.546194536446957\nEpsilon = 0.5461399169933123\nEpsilon = 0.546085303001613\nEpsilon = 0.5460306944713128\nEpsilon = 0.5459760914018656\nEpsilon = 0.5459214937927255\nAgent: ddqn_agent . Episode 340/2000. Number of steps to finish: 20. Loss: 14.2962646484375 Reward: -18.0\nEpsilon = 0.5458669016433462\nEpsilon = 0.5458123149531818\nEpsilon = 0.5457577337216866\nEpsilon = 0.5457031579483144\nEpsilon = 0.5456485876325196\nEpsilon = 0.5455940227737563\nEpsilon = 0.545539463371479\nEpsilon = 0.5454849094251418\nEpsilon = 0.5454303609341993\nEpsilon = 0.5453758178981059\nEpsilon = 0.545321280316316\nEpsilon = 0.5452667481882844\nEpsilon = 0.5452122215134655\nEpsilon = 0.5451577002913142\nEpsilon = 0.545103184521285\nEpsilon = 0.5450486742028329\nEpsilon = 0.5449941693354126\nEpsilon = 0.544939669918479\nAgent: ddqn_agent . Episode 341/2000. Number of steps to finish: 18. Loss: 13.108880996704102 Reward: -6.0\nEpsilon = 0.5448851759514871\nEpsilon = 0.544830687433892\nEpsilon = 0.5447762043651486\nEpsilon = 0.5447217267447121\nEpsilon = 0.5446672545720376\nEpsilon = 0.5446127878465804\nEpsilon = 0.5445583265677958\nEpsilon = 0.5445038707351391\nEpsilon = 0.5444494203480655\nEpsilon = 0.5443949754060308\nAgent: ddqn_agent . Episode 342/2000. Number of steps to finish: 10. Loss: 7.350502014160156 Reward: 2.0\nEpsilon = 0.5443405359084902\nEpsilon = 0.5442861018548993\nEpsilon = 0.5442316732447139\nEpsilon = 0.5441772500773894\nEpsilon = 0.5441228323523817\nEpsilon = 0.5440684200691465\nEpsilon = 0.5440140132271396\nEpsilon = 0.5439596118258169\nEpsilon = 0.5439052158646343\nEpsilon = 0.5438508253430477\nEpsilon = 0.5437964402605134\nEpsilon = 0.5437420606164874\nEpsilon = 0.5436876864104258\nEpsilon = 0.5436333176417847\nEpsilon = 0.5435789543100206\nEpsilon = 0.5435245964145896\nEpsilon = 0.5434702439549481\nEpsilon = 0.5434158969305527\nEpsilon = 0.5433615553408596\nEpsilon = 0.5433072191853255\nAgent: ddqn_agent . Episode 343/2000. Number of steps to finish: 20. Loss: 14.396775245666504 Reward: -12.0\nEpsilon = 0.543252888463407\nEpsilon = 0.5431985631745607\nEpsilon = 0.5431442433182433\nEpsilon = 0.5430899288939115\nEpsilon = 0.5430356199010221\nEpsilon = 0.542981316339032\nEpsilon = 0.5429270182073981\nEpsilon = 0.5428727255055773\nEpsilon = 0.5428184382330268\nEpsilon = 0.5427641563892035\nEpsilon = 0.5427098799735646\nEpsilon = 0.5426556089855672\nEpsilon = 0.5426013434246686\nEpsilon = 0.5425470832903261\nEpsilon = 0.5424928285819971\nEpsilon = 0.5424385792991389\nEpsilon = 0.5423843354412089\nEpsilon = 0.5423300970076648\nEpsilon = 0.542275863997964\nEpsilon = 0.5422216364115642\nAgent: ddqn_agent . Episode 344/2000. Number of steps to finish: 20. Loss: 14.240494728088379 Reward: -12.0\nEpsilon = 0.542167414247923\nEpsilon = 0.5421131975064982\nEpsilon = 0.5420589861867475\nEpsilon = 0.5420047802881288\nEpsilon = 0.5419505798101\nEpsilon = 0.541896384752119\nEpsilon = 0.5418421951136437\nEpsilon = 0.5417880108941324\nEpsilon = 0.541733832093043\nEpsilon = 0.5416796587098337\nEpsilon = 0.5416254907439627\nEpsilon = 0.5415713281948883\nEpsilon = 0.5415171710620688\nEpsilon = 0.5414630193449625\nEpsilon = 0.5414088730430281\nEpsilon = 0.5413547321557238\nEpsilon = 0.5413005966825083\nEpsilon = 0.54124646662284\nEpsilon = 0.5411923419761777\nEpsilon = 0.5411382227419801\nAgent: ddqn_agent . Episode 345/2000. Number of steps to finish: 20. Loss: 14.274582862854004 Reward: -18.0\nEpsilon = 0.5410841089197059\nEpsilon = 0.541030000508814\nEpsilon = 0.5409758975087632\nEpsilon = 0.5409217999190123\nEpsilon = 0.5408677077390205\nEpsilon = 0.5408136209682466\nEpsilon = 0.5407595396061498\nEpsilon = 0.5407054636521892\nEpsilon = 0.540651393105824\nEpsilon = 0.5405973279665134\nEpsilon = 0.5405432682337168\nEpsilon = 0.5404892139068934\nEpsilon = 0.5404351649855027\nEpsilon = 0.5403811214690042\nEpsilon = 0.5403270833568573\nEpsilon = 0.5402730506485216\nEpsilon = 0.5402190233434567\nEpsilon = 0.5401650014411223\nEpsilon = 0.5401109849409782\nEpsilon = 0.5400569738424841\nAgent: ddqn_agent . Episode 346/2000. Number of steps to finish: 20. Loss: 13.87873363494873 Reward: -16.0\nEpsilon = 0.5400029681450998\nEpsilon = 0.5399489678482854\nEpsilon = 0.5398949729515006\nEpsilon = 0.5398409834542054\nEpsilon = 0.5397869993558599\nEpsilon = 0.5397330206559243\nEpsilon = 0.5396790473538587\nEpsilon = 0.5396250794491234\nEpsilon = 0.5395711169411785\nEpsilon = 0.5395171598294844\nEpsilon = 0.5394632081135015\nEpsilon = 0.5394092617926901\nEpsilon = 0.5393553208665108\nEpsilon = 0.5393013853344242\nEpsilon = 0.5392474551958908\nEpsilon = 0.5391935304503712\nEpsilon = 0.5391396110973262\nEpsilon = 0.5390856971362165\nEpsilon = 0.5390317885665028\nEpsilon = 0.5389778853876461\nAgent: ddqn_agent . Episode 347/2000. Number of steps to finish: 20. Loss: 14.281352996826172 Reward: -12.0\nEpsilon = 0.5389239875991074\nEpsilon = 0.5388700952003476\nEpsilon = 0.5388162081908275\nEpsilon = 0.5387623265700084\nEpsilon = 0.5387084503373515\nEpsilon = 0.5386545794923178\nEpsilon = 0.5386007140343685\nEpsilon = 0.5385468539629651\nEpsilon = 0.5384929992775688\nEpsilon = 0.538439149977641\nEpsilon = 0.5383853060626432\nEpsilon = 0.538331467532037\nEpsilon = 0.5382776343852838\nEpsilon = 0.5382238066218452\nEpsilon = 0.538169984241183\nAgent: ddqn_agent . Episode 348/2000. Number of steps to finish: 15. Loss: 10.915802001953125 Reward: -3.0\nEpsilon = 0.5381161672427589\nEpsilon = 0.5380623556260347\nEpsilon = 0.538008549390472\nEpsilon = 0.537954748535533\nEpsilon = 0.5379009530606795\nEpsilon = 0.5378471629653735\nEpsilon = 0.5377933782490769\nEpsilon = 0.537739598911252\nEpsilon = 0.5376858249513609\nEpsilon = 0.5376320563688657\nEpsilon = 0.5375782931632288\nAgent: ddqn_agent . Episode 349/2000. Number of steps to finish: 11. Loss: 7.842634677886963 Reward: 1.0\nEpsilon = 0.5375245353339125\nEpsilon = 0.5374707828803792\nEpsilon = 0.5374170358020912\nEpsilon = 0.537363294098511\nEpsilon = 0.5373095577691012\nEpsilon = 0.5372558268133243\nEpsilon = 0.5372021012306429\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.5371483810205199\nEpsilon = 0.5370946661824179\nEpsilon = 0.5370409567157997\nEpsilon = 0.536987252620128\nEpsilon = 0.5369335538948661\nEpsilon = 0.5368798605394766\nEpsilon = 0.5368261725534226\nEpsilon = 0.5367724899361672\nAgent: ddqn_agent . Episode 350/2000. Number of steps to finish: 15. Loss: 10.78797721862793 Reward: -3.0\nEpsilon = 0.5367188126871736\nEpsilon = 0.5366651408059049\nEpsilon = 0.5366114742918243\nEpsilon = 0.5365578131443951\nEpsilon = 0.5365041573630807\nEpsilon = 0.5364505069473443\nEpsilon = 0.5363968618966496\nEpsilon = 0.5363432222104599\nEpsilon = 0.5362895878882389\nEpsilon = 0.5362359589294501\nEpsilon = 0.5361823353335572\nEpsilon = 0.5361287171000239\nEpsilon = 0.5360751042283138\nEpsilon = 0.536021496717891\nEpsilon = 0.5359678945682192\nEpsilon = 0.5359142977787624\nEpsilon = 0.5358607063489845\nEpsilon = 0.5358071202783496\nEpsilon = 0.5357535395663218\nEpsilon = 0.5356999642123651\nAgent: ddqn_agent . Episode 351/2000. Number of steps to finish: 20. Loss: 14.533190727233887 Reward: -10.0\nEpsilon = 0.5356463942159438\nEpsilon = 0.5355928295765222\nEpsilon = 0.5355392702935645\nEpsilon = 0.5354857163665352\nEpsilon = 0.5354321677948986\nEpsilon = 0.535378624578119\nEpsilon = 0.5353250867156613\nEpsilon = 0.5352715542069897\nEpsilon = 0.535218027051569\nEpsilon = 0.5351645052488638\nEpsilon = 0.535110988798339\nEpsilon = 0.5350574776994591\nEpsilon = 0.5350039719516891\nEpsilon = 0.5349504715544939\nEpsilon = 0.5348969765073385\nEpsilon = 0.5348434868096877\nEpsilon = 0.5347900024610068\nEpsilon = 0.5347365234607607\nEpsilon = 0.5346830498084146\nEpsilon = 0.5346295815034338\nAgent: ddqn_agent . Episode 352/2000. Number of steps to finish: 20. Loss: 14.544451713562012 Reward: -10.0\nEpsilon = 0.5345761185452834\nEpsilon = 0.5345226609334289\nEpsilon = 0.5344692086673356\nEpsilon = 0.5344157617464689\nEpsilon = 0.5343623201702943\nEpsilon = 0.5343088839382772\nEpsilon = 0.5342554530498834\nEpsilon = 0.5342020275045785\nEpsilon = 0.534148607301828\nEpsilon = 0.5340951924410978\nEpsilon = 0.5340417829218537\nEpsilon = 0.5339883787435615\nEpsilon = 0.5339349799056872\nEpsilon = 0.5338815864076966\nEpsilon = 0.5338281982490558\nEpsilon = 0.5337748154292309\nEpsilon = 0.533721437947688\nEpsilon = 0.5336680658038933\nEpsilon = 0.5336146989973128\nEpsilon = 0.5335613375274131\nAgent: ddqn_agent . Episode 353/2000. Number of steps to finish: 20. Loss: 14.191617012023926 Reward: -18.0\nEpsilon = 0.5335079813936604\nEpsilon = 0.533454630595521\nEpsilon = 0.5334012851324614\nEpsilon = 0.5333479450039482\nEpsilon = 0.5332946102094478\nEpsilon = 0.5332412807484268\nEpsilon = 0.533187956620352\nEpsilon = 0.53313463782469\nEpsilon = 0.5330813243609075\nEpsilon = 0.5330280162284714\nEpsilon = 0.5329747134268485\nEpsilon = 0.5329214159555059\nEpsilon = 0.5328681238139104\nEpsilon = 0.532814837001529\nEpsilon = 0.5327615555178289\nEpsilon = 0.5327082793622772\nAgent: ddqn_agent . Episode 354/2000. Number of steps to finish: 16. Loss: 11.743106842041016 Reward: -4.0\nEpsilon = 0.5326550085343409\nEpsilon = 0.5326017430334875\nEpsilon = 0.5325484828591842\nEpsilon = 0.5324952280108983\nEpsilon = 0.5324419784880973\nEpsilon = 0.5323887342902485\nEpsilon = 0.5323354954168195\nEpsilon = 0.5322822618672778\nEpsilon = 0.5322290336410911\nEpsilon = 0.532175810737727\nEpsilon = 0.5321225931566532\nEpsilon = 0.5320693808973376\nEpsilon = 0.5320161739592478\nEpsilon = 0.5319629723418519\nEpsilon = 0.5319097760446176\nEpsilon = 0.5318565850670132\nEpsilon = 0.5318033994085065\nEpsilon = 0.5317502190685657\nEpsilon = 0.5316970440466589\nEpsilon = 0.5316438743422542\nAgent: ddqn_agent . Episode 355/2000. Number of steps to finish: 20. Loss: 14.385614395141602 Reward: -14.0\nEpsilon = 0.53159070995482\nEpsilon = 0.5315375508838246\nEpsilon = 0.5314843971287362\nEpsilon = 0.5314312486890234\nEpsilon = 0.5313781055641544\nEpsilon = 0.531324967753598\nEpsilon = 0.5312718352568228\nEpsilon = 0.5312187080732971\nEpsilon = 0.5311655862024898\nEpsilon = 0.5311124696438695\nEpsilon = 0.5310593583969051\nEpsilon = 0.5310062524610654\nEpsilon = 0.5309531518358193\nEpsilon = 0.5309000565206357\nEpsilon = 0.5308469665149836\nEpsilon = 0.5307938818183322\nEpsilon = 0.5307408024301504\nEpsilon = 0.5306877283499074\nEpsilon = 0.5306346595770725\nEpsilon = 0.5305815961111148\nAgent: ddqn_agent . Episode 356/2000. Number of steps to finish: 20. Loss: 14.713650703430176 Reward: -12.0\nEpsilon = 0.5305285379515037\nEpsilon = 0.5304754850977085\nEpsilon = 0.5304224375491987\nEpsilon = 0.5303693953054438\nEpsilon = 0.5303163583659133\nEpsilon = 0.5302633267300767\nEpsilon = 0.5302103003974037\nEpsilon = 0.530157279367364\nEpsilon = 0.5301042636394273\nEpsilon = 0.5300512532130633\nEpsilon = 0.5299982480877421\nEpsilon = 0.5299452482629333\nEpsilon = 0.529892253738107\nEpsilon = 0.5298392645127332\nEpsilon = 0.529786280586282\nEpsilon = 0.5297333019582233\nEpsilon = 0.5296803286280275\nEpsilon = 0.5296273605951647\nEpsilon = 0.5295743978591052\nEpsilon = 0.5295214404193193\nAgent: ddqn_agent . Episode 357/2000. Number of steps to finish: 20. Loss: 13.949134826660156 Reward: -10.0\nEpsilon = 0.5294684882752774\nEpsilon = 0.5294155414264499\nEpsilon = 0.5293625998723073\nEpsilon = 0.52930966361232\nEpsilon = 0.5292567326459587\nEpsilon = 0.5292038069726942\nEpsilon = 0.5291508865919969\nEpsilon = 0.5290979715033377\nEpsilon = 0.5290450617061874\nEpsilon = 0.5289921572000168\nEpsilon = 0.5289392579842968\nEpsilon = 0.5288863640584983\nEpsilon = 0.5288334754220925\nEpsilon = 0.5287805920745503\nEpsilon = 0.5287277140153429\nEpsilon = 0.5286748412439414\nEpsilon = 0.528621973759817\nEpsilon = 0.528569111562441\nEpsilon = 0.5285162546512848\nEpsilon = 0.5284634030258197\nAgent: ddqn_agent . Episode 358/2000. Number of steps to finish: 20. Loss: 14.266091346740723 Reward: -12.0\nEpsilon = 0.5284105566855172\nEpsilon = 0.5283577156298487\nEpsilon = 0.5283048798582857\nEpsilon = 0.5282520493702998\nEpsilon = 0.5281992241653628\nEpsilon = 0.5281464042429462\nEpsilon = 0.528093589602522\nEpsilon = 0.5280407802435617\nEpsilon = 0.5279879761655374\nEpsilon = 0.5279351773679208\nEpsilon = 0.527882383850184\nEpsilon = 0.527829595611799\nEpsilon = 0.5277768126522379\nEpsilon = 0.5277240349709726\nEpsilon = 0.5276712625674755\nEpsilon = 0.5276184954412187\nEpsilon = 0.5275657335916746\nEpsilon = 0.5275129770183155\nEpsilon = 0.5274602257206136\nEpsilon = 0.5274074796980415\nAgent: ddqn_agent . Episode 359/2000. Number of steps to finish: 20. Loss: 14.035746574401855 Reward: -10.0\nEpsilon = 0.5273547389500717\nEpsilon = 0.5273020034761767\nEpsilon = 0.5272492732758292\nEpsilon = 0.5271965483485016\nEpsilon = 0.5271438286936667\nEpsilon = 0.5270911143107974\nEpsilon = 0.5270384051993663\nEpsilon = 0.5269857013588464\nEpsilon = 0.5269330027887105\nEpsilon = 0.5268803094884316\nEpsilon = 0.5268276214574827\nEpsilon = 0.526774938695337\nEpsilon = 0.5267222612014675\nEpsilon = 0.5266695889753473\nAgent: ddqn_agent . Episode 360/2000. Number of steps to finish: 14. Loss: 10.061616897583008 Reward: -2.0\nEpsilon = 0.5266169220164498\nEpsilon = 0.5265642603242482\nEpsilon = 0.5265116038982157\nEpsilon = 0.526458952737826\nEpsilon = 0.5264063068425522\nEpsilon = 0.526353666211868\nEpsilon = 0.5263010308452468\nEpsilon = 0.5262484007421623\nEpsilon = 0.5261957759020881\nEpsilon = 0.5261431563244979\nEpsilon = 0.5260905420088655\nEpsilon = 0.5260379329546646\nEpsilon = 0.5259853291613692\nEpsilon = 0.525932730628453\nEpsilon = 0.5258801373553902\nEpsilon = 0.5258275493416547\nEpsilon = 0.5257749665867205\nEpsilon = 0.5257223890900619\nEpsilon = 0.5256698168511529\nEpsilon = 0.5256172498694678\nAgent: ddqn_agent . Episode 361/2000. Number of steps to finish: 20. Loss: 14.5778169631958 Reward: -10.0\nEpsilon = 0.5255646881444809\nEpsilon = 0.5255121316756665\nEpsilon = 0.5254595804624989\nEpsilon = 0.5254070345044526\nEpsilon = 0.5253544938010022\nEpsilon = 0.5253019583516221\nEpsilon = 0.5252494281557869\nEpsilon = 0.5251969032129713\nEpsilon = 0.52514438352265\nEpsilon = 0.5250918690842977\nEpsilon = 0.5250393598973893\nEpsilon = 0.5249868559613995\nEpsilon = 0.5249343572758034\nEpsilon = 0.5248818638400758\nEpsilon = 0.5248293756536918\nEpsilon = 0.5247768927161265\nEpsilon = 0.5247244150268549\nEpsilon = 0.5246719425853522\nEpsilon = 0.5246194753910937\nEpsilon = 0.5245670134435546\nAgent: ddqn_agent . Episode 362/2000. Number of steps to finish: 20. Loss: 14.424198150634766 Reward: -16.0\nEpsilon = 0.5245145567422103\nEpsilon = 0.524462105286536\nEpsilon = 0.5244096590760073\nEpsilon = 0.5243572181100997\nEpsilon = 0.5243047823882887\nEpsilon = 0.5242523519100499\nEpsilon = 0.5241999266748589\nEpsilon = 0.5241475066821915\nEpsilon = 0.5240950919315233\nEpsilon = 0.5240426824223301\nEpsilon = 0.5239902781540878\nEpsilon = 0.5239378791262724\nEpsilon = 0.5238854853383598\nEpsilon = 0.5238330967898259\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.523780713480147\nEpsilon = 0.5237283354087989\nEpsilon = 0.5236759625752581\nAgent: ddqn_agent . Episode 363/2000. Number of steps to finish: 17. Loss: 12.398677825927734 Reward: -5.0\nEpsilon = 0.5236235949790006\nEpsilon = 0.5235712326195027\nEpsilon = 0.5235188754962408\nEpsilon = 0.5234665236086912\nEpsilon = 0.5234141769563303\nEpsilon = 0.5233618355386347\nEpsilon = 0.5233094993550809\nEpsilon = 0.5232571684051454\nEpsilon = 0.5232048426883048\nEpsilon = 0.523152522204036\nEpsilon = 0.5231002069518156\nEpsilon = 0.5230478969311204\nEpsilon = 0.5229955921414273\nEpsilon = 0.5229432925822132\nEpsilon = 0.5228909982529549\nEpsilon = 0.5228387091531297\nEpsilon = 0.5227864252822144\nEpsilon = 0.5227341466396862\nEpsilon = 0.5226818732250222\nEpsilon = 0.5226296050376997\nAgent: ddqn_agent . Episode 364/2000. Number of steps to finish: 20. Loss: 14.669734954833984 Reward: -10.0\nEpsilon = 0.522577342077196\nEpsilon = 0.5225250843429883\nEpsilon = 0.522472831834554\nEpsilon = 0.5224205845513705\nEpsilon = 0.5223683424929154\nEpsilon = 0.5223161056586662\nEpsilon = 0.5222638740481003\nEpsilon = 0.5222116476606955\nEpsilon = 0.5221594264959294\nEpsilon = 0.5221072105532798\nEpsilon = 0.5220549998322245\nEpsilon = 0.5220027943322413\nEpsilon = 0.5219505940528081\nEpsilon = 0.5218983989934028\nEpsilon = 0.5218462091535035\nEpsilon = 0.5217940245325882\nEpsilon = 0.5217418451301349\nEpsilon = 0.5216896709456219\nEpsilon = 0.5216375019785273\nEpsilon = 0.5215853382283294\nAgent: ddqn_agent . Episode 365/2000. Number of steps to finish: 20. Loss: 14.417557716369629 Reward: -10.0\nEpsilon = 0.5215331796945066\nEpsilon = 0.5214810263765371\nEpsilon = 0.5214288782738995\nEpsilon = 0.5213767353860721\nEpsilon = 0.5213245977125335\nEpsilon = 0.5212724652527623\nEpsilon = 0.521220338006237\nEpsilon = 0.5211682159724363\nEpsilon = 0.5211160991508391\nEpsilon = 0.5210639875409241\nEpsilon = 0.52101188114217\nEpsilon = 0.5209597799540558\nEpsilon = 0.5209076839760605\nEpsilon = 0.5208555932076628\nEpsilon = 0.520803507648342\nEpsilon = 0.5207514272975772\nEpsilon = 0.5206993521548474\nEpsilon = 0.520647282219632\nEpsilon = 0.52059521749141\nEpsilon = 0.5205431579696609\nAgent: ddqn_agent . Episode 366/2000. Number of steps to finish: 20. Loss: 14.247610092163086 Reward: -10.0\nEpsilon = 0.520491103653864\nEpsilon = 0.5204390545434986\nEpsilon = 0.5203870106380443\nEpsilon = 0.5203349719369804\nEpsilon = 0.5202829384397867\nEpsilon = 0.5202309101459427\nEpsilon = 0.5201788870549281\nEpsilon = 0.5201268691662226\nEpsilon = 0.5200748564793061\nEpsilon = 0.5200228489936581\nEpsilon = 0.5199708467087588\nEpsilon = 0.519918849624088\nEpsilon = 0.5198668577391256\nEpsilon = 0.5198148710533517\nEpsilon = 0.5197628895662464\nEpsilon = 0.5197109132772898\nEpsilon = 0.5196589421859621\nEpsilon = 0.5196069762917435\nEpsilon = 0.5195550155941143\nEpsilon = 0.5195030600925549\nAgent: ddqn_agent . Episode 367/2000. Number of steps to finish: 20. Loss: 14.414714813232422 Reward: -14.0\nEpsilon = 0.5194511097865456\nEpsilon = 0.519399164675567\nEpsilon = 0.5193472247590994\nEpsilon = 0.5192952900366236\nEpsilon = 0.5192433605076199\nEpsilon = 0.5191914361715692\nEpsilon = 0.519139517027952\nEpsilon = 0.5190876030762492\nEpsilon = 0.5190356943159417\nEpsilon = 0.51898379074651\nEpsilon = 0.5189318923674354\nEpsilon = 0.5188799991781987\nEpsilon = 0.5188281111782809\nEpsilon = 0.5187762283671631\nEpsilon = 0.5187243507443263\nEpsilon = 0.5186724783092519\nEpsilon = 0.518620611061421\nEpsilon = 0.5185687490003148\nEpsilon = 0.5185168921254147\nEpsilon = 0.5184650404362022\nAgent: ddqn_agent . Episode 368/2000. Number of steps to finish: 20. Loss: 14.27944564819336 Reward: -12.0\nEpsilon = 0.5184131939321586\nEpsilon = 0.5183613526127654\nEpsilon = 0.5183095164775041\nEpsilon = 0.5182576855258564\nEpsilon = 0.5182058597573038\nEpsilon = 0.5181540391713281\nEpsilon = 0.518102223767411\nEpsilon = 0.5180504135450342\nEpsilon = 0.5179986085036797\nEpsilon = 0.5179468086428294\nEpsilon = 0.5178950139619651\nEpsilon = 0.517843224460569\nEpsilon = 0.517791440138123\nEpsilon = 0.5177396609941092\nEpsilon = 0.5176878870280097\nEpsilon = 0.5176361182393069\nEpsilon = 0.517584354627483\nEpsilon = 0.5175325961920202\nEpsilon = 0.517480842932401\nEpsilon = 0.5174290948481078\nAgent: ddqn_agent . Episode 369/2000. Number of steps to finish: 20. Loss: 14.478326797485352 Reward: -10.0\nEpsilon = 0.517377351938623\nEpsilon = 0.5173256142034292\nEpsilon = 0.5172738816420088\nEpsilon = 0.5172221542538447\nEpsilon = 0.5171704320384193\nEpsilon = 0.5171187149952154\nEpsilon = 0.5170670031237159\nEpsilon = 0.5170152964234035\nEpsilon = 0.5169635948937612\nEpsilon = 0.5169118985342719\nEpsilon = 0.5168602073444185\nEpsilon = 0.516808521323684\nEpsilon = 0.5167568404715517\nEpsilon = 0.5167051647875045\nEpsilon = 0.5166534942710258\nEpsilon = 0.5166018289215987\nEpsilon = 0.5165501687387065\nEpsilon = 0.5164985137218326\nEpsilon = 0.5164468638704605\nEpsilon = 0.5163952191840734\nAgent: ddqn_agent . Episode 370/2000. Number of steps to finish: 20. Loss: 14.542956352233887 Reward: -14.0\nEpsilon = 0.5163435796621549\nEpsilon = 0.5162919453041888\nEpsilon = 0.5162403161096584\nEpsilon = 0.5161886920780474\nEpsilon = 0.5161370732088396\nEpsilon = 0.5160854595015187\nEpsilon = 0.5160338509555686\nEpsilon = 0.5159822475704731\nEpsilon = 0.515930649345716\nEpsilon = 0.5158790562807815\nEpsilon = 0.5158274683751534\nEpsilon = 0.5157758856283159\nEpsilon = 0.5157243080397531\nEpsilon = 0.5156727356089491\nEpsilon = 0.5156211683353882\nEpsilon = 0.5155696062185546\nEpsilon = 0.5155180492579328\nEpsilon = 0.515466497453007\nEpsilon = 0.5154149508032617\nEpsilon = 0.5153634093081814\nAgent: ddqn_agent . Episode 371/2000. Number of steps to finish: 20. Loss: 14.337388038635254 Reward: -12.0\nEpsilon = 0.5153118729672506\nEpsilon = 0.5152603417799538\nEpsilon = 0.5152088157457758\nEpsilon = 0.5151572948642013\nEpsilon = 0.5151057791347149\nEpsilon = 0.5150542685568015\nEpsilon = 0.5150027631299458\nEpsilon = 0.5149512628536328\nEpsilon = 0.5148997677273475\nEpsilon = 0.5148482777505747\nEpsilon = 0.5147967929227997\nEpsilon = 0.5147453132435074\nEpsilon = 0.514693838712183\nEpsilon = 0.5146423693283118\nEpsilon = 0.514590905091379\nEpsilon = 0.5145394460008698\nEpsilon = 0.5144879920562697\nEpsilon = 0.5144365432570641\nEpsilon = 0.5143850996027384\nEpsilon = 0.5143336610927781\nAgent: ddqn_agent . Episode 372/2000. Number of steps to finish: 20. Loss: 14.645195007324219 Reward: -12.0\nEpsilon = 0.5142822277266689\nEpsilon = 0.5142307995038962\nEpsilon = 0.5141793764239457\nEpsilon = 0.5141279584863033\nEpsilon = 0.5140765456904547\nEpsilon = 0.5140251380358857\nEpsilon = 0.5139737355220821\nEpsilon = 0.51392233814853\nEpsilon = 0.513870945914715\nEpsilon = 0.5138195588201235\nEpsilon = 0.5137681768642415\nEpsilon = 0.5137168000465551\nEpsilon = 0.5136654283665504\nEpsilon = 0.5136140618237137\nEpsilon = 0.5135627004175314\nEpsilon = 0.5135113441474897\nEpsilon = 0.5134599930130749\nEpsilon = 0.5134086470137735\nEpsilon = 0.5133573061490722\nEpsilon = 0.5133059704184573\nAgent: ddqn_agent . Episode 373/2000. Number of steps to finish: 20. Loss: 14.227527618408203 Reward: -10.0\nEpsilon = 0.5132546398214154\nEpsilon = 0.5132033143574333\nEpsilon = 0.5131519940259975\nEpsilon = 0.5131006788265949\nEpsilon = 0.5130493687587123\nEpsilon = 0.5129980638218364\nEpsilon = 0.5129467640154542\nEpsilon = 0.5128954693390527\nEpsilon = 0.5128441797921188\nEpsilon = 0.5127928953741396\nEpsilon = 0.5127416160846022\nEpsilon = 0.5126903419229938\nEpsilon = 0.5126390728888015\nEpsilon = 0.5125878089815127\nEpsilon = 0.5125365502006145\nAgent: ddqn_agent . Episode 374/2000. Number of steps to finish: 15. Loss: 11.16916275024414 Reward: -3.0\nEpsilon = 0.5124852965455945\nEpsilon = 0.51243404801594\nEpsilon = 0.5123828046111384\nEpsilon = 0.5123315663306772\nEpsilon = 0.5122803331740442\nEpsilon = 0.5122291051407268\nEpsilon = 0.5121778822302128\nEpsilon = 0.5121266644419897\nEpsilon = 0.5120754517755456\nEpsilon = 0.5120242442303681\nEpsilon = 0.511973041805945\nEpsilon = 0.5119218445017645\nEpsilon = 0.5118706523173143\nEpsilon = 0.5118194652520826\nEpsilon = 0.5117682833055573\nEpsilon = 0.5117171064772268\nEpsilon = 0.5116659347665791\nEpsilon = 0.5116147681731024\nEpsilon = 0.5115636066962851\nEpsilon = 0.5115124503356154\nAgent: ddqn_agent . Episode 375/2000. Number of steps to finish: 20. Loss: 14.081948280334473 Reward: -10.0\nEpsilon = 0.5114612990905819\nEpsilon = 0.5114101529606728\nEpsilon = 0.5113590119453768\nEpsilon = 0.5113078760441823\nEpsilon = 0.5112567452565778\nEpsilon = 0.5112056195820521\nEpsilon = 0.5111544990200939\nEpsilon = 0.5111033835701919\nEpsilon = 0.5110522732318349\nEpsilon = 0.5110011680045117\nEpsilon = 0.5109500678877112\nEpsilon = 0.5108989728809225\nEpsilon = 0.5108478829836344\nEpsilon = 0.510796798195336\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.5107457185155164\nEpsilon = 0.5106946439436649\nEpsilon = 0.5106435744792706\nEpsilon = 0.5105925101218226\nEpsilon = 0.5105414508708104\nEpsilon = 0.5104903967257233\nAgent: ddqn_agent . Episode 376/2000. Number of steps to finish: 20. Loss: 14.910632133483887 Reward: -10.0\nEpsilon = 0.5104393476860507\nEpsilon = 0.5103883037512821\nEpsilon = 0.5103372649209069\nEpsilon = 0.5102862311944149\nEpsilon = 0.5102352025712954\nEpsilon = 0.5101841790510383\nEpsilon = 0.5101331606331332\nEpsilon = 0.5100821473170699\nEpsilon = 0.5100311391023382\nEpsilon = 0.509980135988428\nEpsilon = 0.5099291379748291\nAgent: ddqn_agent . Episode 377/2000. Number of steps to finish: 11. Loss: 8.372815132141113 Reward: 1.0\nEpsilon = 0.5098781450610316\nEpsilon = 0.5098271572465255\nEpsilon = 0.5097761745308008\nEpsilon = 0.5097251969133477\nEpsilon = 0.5096742243936564\nEpsilon = 0.509623256971217\nEpsilon = 0.5095722946455199\nEpsilon = 0.5095213374160553\nEpsilon = 0.5094703852823137\nEpsilon = 0.5094194382437854\nEpsilon = 0.5093684962999611\nEpsilon = 0.5093175594503311\nEpsilon = 0.509266627694386\nEpsilon = 0.5092157010316166\nEpsilon = 0.5091647794615135\nEpsilon = 0.5091138629835673\nEpsilon = 0.509062951597269\nEpsilon = 0.5090120453021093\nEpsilon = 0.5089611440975791\nEpsilon = 0.5089102479831693\nAgent: ddqn_agent . Episode 378/2000. Number of steps to finish: 20. Loss: 14.332361221313477 Reward: -12.0\nEpsilon = 0.508859356958371\nEpsilon = 0.5088084710226752\nEpsilon = 0.508757590175573\nEpsilon = 0.5087067144165555\nEpsilon = 0.5086558437451139\nEpsilon = 0.5086049781607394\nEpsilon = 0.5085541176629232\nEpsilon = 0.5085032622511569\nEpsilon = 0.5084524119249318\nEpsilon = 0.5084015666837394\nEpsilon = 0.508350726527071\nEpsilon = 0.5082998914544183\nEpsilon = 0.5082490614652729\nEpsilon = 0.5081982365591264\nEpsilon = 0.5081474167354705\nEpsilon = 0.5080966019937969\nEpsilon = 0.5080457923335976\nEpsilon = 0.5079949877543642\nEpsilon = 0.5079441882555888\nEpsilon = 0.5078933938367632\nAgent: ddqn_agent . Episode 379/2000. Number of steps to finish: 20. Loss: 15.001004219055176 Reward: -10.0\nEpsilon = 0.5078426044973796\nEpsilon = 0.5077918202369299\nEpsilon = 0.5077410410549063\nEpsilon = 0.5076902669508008\nEpsilon = 0.5076394979241057\nEpsilon = 0.5075887339743133\nEpsilon = 0.5075379751009158\nEpsilon = 0.5074872213034057\nEpsilon = 0.5074364725812754\nEpsilon = 0.5073857289340173\nEpsilon = 0.507334990361124\nEpsilon = 0.5072842568620879\nEpsilon = 0.5072335284364017\nEpsilon = 0.5071828050835581\nEpsilon = 0.5071320868030497\nEpsilon = 0.5070813735943693\nEpsilon = 0.5070306654570099\nEpsilon = 0.5069799623904643\nEpsilon = 0.5069292643942253\nEpsilon = 0.5068785714677858\nAgent: ddqn_agent . Episode 380/2000. Number of steps to finish: 20. Loss: 14.538016319274902 Reward: -12.0\nEpsilon = 0.506827883610639\nEpsilon = 0.506777200822278\nEpsilon = 0.5067265231021957\nEpsilon = 0.5066758504498855\nEpsilon = 0.5066251828648405\nEpsilon = 0.506574520346554\nEpsilon = 0.5065238628945193\nEpsilon = 0.5064732105082299\nEpsilon = 0.506422563187179\nEpsilon = 0.5063719209308604\nEpsilon = 0.5063212837387673\nEpsilon = 0.5062706516103934\nEpsilon = 0.5062200245452324\nEpsilon = 0.5061694025427779\nEpsilon = 0.5061187856025235\nEpsilon = 0.5060681737239633\nEpsilon = 0.5060175669065909\nEpsilon = 0.5059669651499002\nEpsilon = 0.5059163684533853\nEpsilon = 0.50586577681654\nAgent: ddqn_agent . Episode 381/2000. Number of steps to finish: 20. Loss: 14.307517051696777 Reward: -12.0\nEpsilon = 0.5058151902388583\nEpsilon = 0.5057646087198344\nEpsilon = 0.5057140322589624\nEpsilon = 0.5056634608557365\nEpsilon = 0.505612894509651\nEpsilon = 0.5055623332202\nEpsilon = 0.505511776986878\nEpsilon = 0.5054612258091793\nEpsilon = 0.5054106796865984\nEpsilon = 0.5053601386186297\nEpsilon = 0.5053096026047679\nEpsilon = 0.5052590716445075\nEpsilon = 0.505208545737343\nEpsilon = 0.5051580248827693\nEpsilon = 0.505107509080281\nEpsilon = 0.505056998329373\nEpsilon = 0.5050064926295401\nEpsilon = 0.5049559919802772\nEpsilon = 0.5049054963810792\nEpsilon = 0.504855005831441\nAgent: ddqn_agent . Episode 382/2000. Number of steps to finish: 20. Loss: 14.776516914367676 Reward: -12.0\nEpsilon = 0.5048045203308579\nEpsilon = 0.5047540398788248\nEpsilon = 0.504703564474837\nEpsilon = 0.5046530941183895\nEpsilon = 0.5046026288089777\nEpsilon = 0.5045521685460967\nEpsilon = 0.5045017133292421\nEpsilon = 0.5044512631579092\nEpsilon = 0.5044008180315934\nEpsilon = 0.5043503779497903\nEpsilon = 0.5042999429119953\nEpsilon = 0.5042495129177041\nEpsilon = 0.5041990879664123\nEpsilon = 0.5041486680576157\nEpsilon = 0.5040982531908099\nEpsilon = 0.5040478433654908\nEpsilon = 0.5039974385811543\nEpsilon = 0.5039470388372962\nEpsilon = 0.5038966441334125\nEpsilon = 0.5038462544689992\nAgent: ddqn_agent . Episode 383/2000. Number of steps to finish: 20. Loss: 14.169829368591309 Reward: -14.0\nEpsilon = 0.5037958698435523\nEpsilon = 0.5037454902565679\nEpsilon = 0.5036951157075422\nEpsilon = 0.5036447461959714\nEpsilon = 0.5035943817213518\nEpsilon = 0.5035440222831797\nEpsilon = 0.5034936678809514\nEpsilon = 0.5034433185141634\nEpsilon = 0.503392974182312\nEpsilon = 0.5033426348848937\nEpsilon = 0.5032923006214053\nEpsilon = 0.5032419713913432\nEpsilon = 0.5031916471942041\nEpsilon = 0.5031413280294846\nEpsilon = 0.5030910138966816\nEpsilon = 0.503040704795292\nEpsilon = 0.5029904007248124\nEpsilon = 0.5029401016847399\nEpsilon = 0.5028898076745715\nAgent: ddqn_agent . Episode 384/2000. Number of steps to finish: 19. Loss: 13.700632095336914 Reward: -7.0\nEpsilon = 0.502839518693804\nEpsilon = 0.5027892347419347\nEpsilon = 0.5027389558184605\nEpsilon = 0.5026886819228786\nEpsilon = 0.5026384130546864\nEpsilon = 0.5025881492133809\nEpsilon = 0.5025378903984595\nEpsilon = 0.5024876366094196\nEpsilon = 0.5024373878457588\nEpsilon = 0.5023871441069742\nEpsilon = 0.5023369053925635\nEpsilon = 0.5022866717020242\nEpsilon = 0.502236443034854\nEpsilon = 0.5021862193905505\nEpsilon = 0.5021360007686114\nEpsilon = 0.5020857871685346\nEpsilon = 0.5020355785898177\nEpsilon = 0.5019853750319587\nEpsilon = 0.5019351764944555\nEpsilon = 0.5018849829768061\nAgent: ddqn_agent . Episode 385/2000. Number of steps to finish: 20. Loss: 14.210525512695312 Reward: -14.0\nEpsilon = 0.5018347944785084\nEpsilon = 0.5017846109990606\nEpsilon = 0.5017344325379607\nEpsilon = 0.5016842590947069\nEpsilon = 0.5016340906687975\nEpsilon = 0.5015839272597307\nEpsilon = 0.5015337688670047\nEpsilon = 0.501483615490118\nEpsilon = 0.501433467128569\nEpsilon = 0.5013833237818561\nEpsilon = 0.5013331854494779\nEpsilon = 0.5012830521309329\nEpsilon = 0.5012329238257198\nEpsilon = 0.5011828005333373\nEpsilon = 0.501132682253284\nEpsilon = 0.5010825689850587\nEpsilon = 0.5010324607281601\nEpsilon = 0.5009823574820873\nEpsilon = 0.5009322592463391\nEpsilon = 0.5008821660204145\nAgent: ddqn_agent . Episode 386/2000. Number of steps to finish: 20. Loss: 14.597756385803223 Reward: -14.0\nEpsilon = 0.5008320778038124\nEpsilon = 0.5007819945960321\nEpsilon = 0.5007319163965726\nEpsilon = 0.5006818432049329\nEpsilon = 0.5006317750206124\nEpsilon = 0.5005817118431104\nEpsilon = 0.5005316536719261\nEpsilon = 0.5004816005065589\nEpsilon = 0.5004315523465083\nEpsilon = 0.5003815091912737\nEpsilon = 0.5003314710403546\nEpsilon = 0.5002814378932505\nEpsilon = 0.5002314097494612\nEpsilon = 0.5001813866084863\nEpsilon = 0.5001313684698255\nEpsilon = 0.5000813553329786\nEpsilon = 0.5000313471974454\nEpsilon = 0.4999813440627256\nEpsilon = 0.49993134592831934\nEpsilon = 0.49988135279372653\nAgent: ddqn_agent . Episode 387/2000. Number of steps to finish: 20. Loss: 14.702960968017578 Reward: -18.0\nEpsilon = 0.4998313646584472\nEpsilon = 0.49978138152198137\nEpsilon = 0.49973140338382915\nEpsilon = 0.49968143024349077\nEpsilon = 0.49963146210046644\nEpsilon = 0.4995814989542564\nEpsilon = 0.49953154080436096\nEpsilon = 0.49948158765028056\nEpsilon = 0.49943163949151553\nEpsilon = 0.4993816963275664\nEpsilon = 0.4993317581579337\nEpsilon = 0.49928182498211787\nEpsilon = 0.49923189679961966\nEpsilon = 0.4991819736099397\nEpsilon = 0.4991320554125787\nEpsilon = 0.49908214220703745\nEpsilon = 0.49903223399281676\nEpsilon = 0.4989823307694175\nEpsilon = 0.49893243253634056\nEpsilon = 0.49888253929308696\nAgent: ddqn_agent . Episode 388/2000. Number of steps to finish: 20. Loss: 14.547429084777832 Reward: -10.0\nEpsilon = 0.49883265103915764\nEpsilon = 0.4987827677740537\nEpsilon = 0.49873288949727634\nEpsilon = 0.4986830162083266\nEpsilon = 0.49863314790670576\nEpsilon = 0.4985832845919151\nEpsilon = 0.4985334262634559\nEpsilon = 0.49848357292082957\nEpsilon = 0.49843372456353746\nEpsilon = 0.4983838811910811\nEpsilon = 0.498334042802962\nEpsilon = 0.49828420939868173\nEpsilon = 0.49823438097774186\nEpsilon = 0.4981845575396441\nEpsilon = 0.4981347390838901\nEpsilon = 0.49808492560998174\nEpsilon = 0.49803511711742077\nEpsilon = 0.497985313605709\nEpsilon = 0.49793551507434847\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.49788572152284105\nAgent: ddqn_agent . Episode 389/2000. Number of steps to finish: 20. Loss: 14.573619842529297 Reward: -14.0\nEpsilon = 0.4978359329506888\nEpsilon = 0.49778614935739374\nEpsilon = 0.497736370742458\nEpsilon = 0.49768659710538377\nEpsilon = 0.49763682844567325\nEpsilon = 0.4975870647628287\nEpsilon = 0.49753730605635244\nEpsilon = 0.4974875523257468\nEpsilon = 0.49743780357051426\nEpsilon = 0.4973880597901572\nEpsilon = 0.4973383209841782\nEpsilon = 0.4972885871520798\nEpsilon = 0.4972388582933646\nEpsilon = 0.49718913440753526\nEpsilon = 0.4971394154940945\nEpsilon = 0.4970897015525451\nEpsilon = 0.49703999258238984\nEpsilon = 0.4969902885831316\nEpsilon = 0.4969405895542733\nEpsilon = 0.49689089549531784\nAgent: ddqn_agent . Episode 390/2000. Number of steps to finish: 20. Loss: 13.959844589233398 Reward: -10.0\nEpsilon = 0.4968412064057683\nEpsilon = 0.49679152228512774\nEpsilon = 0.4967418431328992\nEpsilon = 0.4966921689485859\nEpsilon = 0.49664249973169106\nEpsilon = 0.4965928354817179\nEpsilon = 0.49654317619816973\nEpsilon = 0.4964935218805499\nEpsilon = 0.49644387252836186\nEpsilon = 0.496394228141109\nEpsilon = 0.4963445887182949\nEpsilon = 0.49629495425942305\nEpsilon = 0.49624532476399713\nEpsilon = 0.49619570023152076\nAgent: ddqn_agent . Episode 391/2000. Number of steps to finish: 14. Loss: 10.159232139587402 Reward: -2.0\nEpsilon = 0.4961460806614976\nEpsilon = 0.4960964660534315\nEpsilon = 0.49604685640682616\nEpsilon = 0.4959972517211855\nEpsilon = 0.49594765199601337\nEpsilon = 0.49589805723081376\nEpsilon = 0.4958484674250907\nEpsilon = 0.4957988825783482\nEpsilon = 0.4957493026900904\nEpsilon = 0.49569972775982135\nEpsilon = 0.49565015778704535\nEpsilon = 0.49560059277126667\nEpsilon = 0.4955510327119895\nEpsilon = 0.4955014776087183\nEpsilon = 0.49545192746095745\nEpsilon = 0.49540238226821137\nEpsilon = 0.49535284202998453\nEpsilon = 0.49530330674578155\nEpsilon = 0.495253776415107\nEpsilon = 0.49520425103746546\nAgent: ddqn_agent . Episode 392/2000. Number of steps to finish: 20. Loss: 14.413776397705078 Reward: -16.0\nEpsilon = 0.4951547306123617\nEpsilon = 0.49510521513930045\nEpsilon = 0.49505570461778653\nEpsilon = 0.49500619904732474\nEpsilon = 0.49495669842742\nEpsilon = 0.49490720275757727\nEpsilon = 0.4948577120373015\nEpsilon = 0.4948082262660978\nEpsilon = 0.49475874544347115\nEpsilon = 0.4947092695689268\nEpsilon = 0.4946597986419699\nEpsilon = 0.4946103326621057\nEpsilon = 0.4945608716288395\nEpsilon = 0.4945114155416766\nEpsilon = 0.49446196440012247\nEpsilon = 0.4944125182036825\nEpsilon = 0.4943630769518621\nEpsilon = 0.49431364064416694\nEpsilon = 0.4942642092801025\nEpsilon = 0.49421478285917453\nAgent: ddqn_agent . Episode 393/2000. Number of steps to finish: 20. Loss: 14.676613807678223 Reward: -12.0\nEpsilon = 0.4941653613808886\nEpsilon = 0.4941159448447505\nEpsilon = 0.49406653325026606\nEpsilon = 0.494017126596941\nEpsilon = 0.4939677248842813\nEpsilon = 0.49391832811179287\nEpsilon = 0.4938689362789817\nEpsilon = 0.49381954938535383\nEpsilon = 0.4937701674304153\nEpsilon = 0.4937207904136723\nEpsilon = 0.4936714183346309\nEpsilon = 0.49362205119279745\nEpsilon = 0.4935726889876782\nEpsilon = 0.49352333171877943\nEpsilon = 0.49347397938560755\nAgent: ddqn_agent . Episode 394/2000. Number of steps to finish: 15. Loss: 11.112850189208984 Reward: -3.0\nEpsilon = 0.493424631987669\nEpsilon = 0.49337528952447024\nEpsilon = 0.4933259519955178\nEpsilon = 0.49327661940031825\nEpsilon = 0.4932272917383782\nEpsilon = 0.49317796900920435\nEpsilon = 0.4931286512123034\nEpsilon = 0.4930793383471822\nEpsilon = 0.4930300304133475\nEpsilon = 0.49298072741030613\nEpsilon = 0.4929314293375651\nEpsilon = 0.4928821361946314\nEpsilon = 0.49283284798101196\nEpsilon = 0.49278356469621387\nEpsilon = 0.49273428633974425\nEpsilon = 0.4926850129111103\nEpsilon = 0.49263574440981917\nEpsilon = 0.4925864808353782\nEpsilon = 0.4925372221872947\nEpsilon = 0.492487968465076\nAgent: ddqn_agent . Episode 395/2000. Number of steps to finish: 20. Loss: 13.967576026916504 Reward: -16.0\nEpsilon = 0.4924387196682295\nEpsilon = 0.4923894757962627\nEpsilon = 0.49234023684868305\nEpsilon = 0.4922910028249982\nEpsilon = 0.4922417737247157\nEpsilon = 0.49219254954734326\nEpsilon = 0.49214333029238855\nEpsilon = 0.4920941159593593\nEpsilon = 0.4920449065477634\nEpsilon = 0.49199570205710863\nEpsilon = 0.4919465024869029\nEpsilon = 0.49189730783665425\nEpsilon = 0.4918481181058706\nEpsilon = 0.49179893329406\nEpsilon = 0.49174975340073057\nEpsilon = 0.4917005784253905\nEpsilon = 0.49165140836754795\nEpsilon = 0.4916022432267112\nAgent: ddqn_agent . Episode 396/2000. Number of steps to finish: 18. Loss: 13.117498397827148 Reward: -6.0\nEpsilon = 0.49155308300238854\nEpsilon = 0.4915039276940883\nEpsilon = 0.49145477730131887\nEpsilon = 0.49140563182358876\nEpsilon = 0.4913564912604064\nEpsilon = 0.4913073556112804\nEpsilon = 0.4912582248757193\nEpsilon = 0.49120909905323173\nEpsilon = 0.4911599781433264\nEpsilon = 0.4911108621455121\nAgent: ddqn_agent . Episode 397/2000. Number of steps to finish: 10. Loss: 7.156097412109375 Reward: 2.0\nEpsilon = 0.49106175105929756\nEpsilon = 0.4910126448841916\nEpsilon = 0.4909635436197032\nEpsilon = 0.4909144472653412\nEpsilon = 0.4908653558206147\nEpsilon = 0.49081626928503264\nEpsilon = 0.4907671876581041\nEpsilon = 0.4907181109393383\nEpsilon = 0.4906690391282444\nEpsilon = 0.49061997222433157\nEpsilon = 0.49057091022710914\nAgent: ddqn_agent . Episode 398/2000. Number of steps to finish: 11. Loss: 8.034997940063477 Reward: 1.0\nEpsilon = 0.4905218531360864\nEpsilon = 0.49047280095077284\nEpsilon = 0.49042375367067775\nEpsilon = 0.4903747112953107\nEpsilon = 0.4903256738241812\nEpsilon = 0.49027664125679876\nEpsilon = 0.4902276135926731\nEpsilon = 0.4901785908313138\nEpsilon = 0.4901295729722307\nEpsilon = 0.4900805600149335\nEpsilon = 0.490031551958932\nEpsilon = 0.4899825488037361\nEpsilon = 0.48993355054885573\nEpsilon = 0.48988455719380086\nEpsilon = 0.4898355687380815\nEpsilon = 0.48978658518120766\nEpsilon = 0.48973760652268955\nEpsilon = 0.4896886327620373\nEpsilon = 0.48963966389876107\nEpsilon = 0.4895906999323712\nAgent: ddqn_agent . Episode 399/2000. Number of steps to finish: 20. Loss: 14.477914810180664 Reward: -12.0\nEpsilon = 0.48954174086237795\nEpsilon = 0.4894927866882917\nEpsilon = 0.4894438374096229\nEpsilon = 0.4893948930258819\nEpsilon = 0.48934595353657934\nEpsilon = 0.4892970189412257\nEpsilon = 0.48924808923933155\nEpsilon = 0.4891991644304076\nEpsilon = 0.4891502445139646\nEpsilon = 0.48910132948951324\nEpsilon = 0.4890524193565643\nEpsilon = 0.48900351411462867\nEpsilon = 0.4889546137632172\nEpsilon = 0.4889057183018409\nEpsilon = 0.4888568277300107\nEpsilon = 0.4888079420472377\nEpsilon = 0.488759061253033\nEpsilon = 0.4887101853469077\nEpsilon = 0.488661314328373\nEpsilon = 0.48861244819694016\nAgent: ddqn_agent . Episode 400/2000. Number of steps to finish: 20. Loss: 14.67282485961914 Reward: -12.0\nEpsilon = 0.4885635869521205\nEpsilon = 0.48851473059342526\nEpsilon = 0.48846587912036593\nEpsilon = 0.4884170325324539\nEpsilon = 0.48836819082920063\nEpsilon = 0.4883193540101177\nEpsilon = 0.4882705220747167\nEpsilon = 0.48822169502250923\nEpsilon = 0.488172872853007\nEpsilon = 0.4881240555657217\nEpsilon = 0.48807524316016515\nEpsilon = 0.48802643563584913\nEpsilon = 0.4879776329922855\nEpsilon = 0.4879288352289863\nEpsilon = 0.4878800423454634\nEpsilon = 0.48783125434122887\nEpsilon = 0.48778247121579477\nEpsilon = 0.4877336929686732\nEpsilon = 0.4876849195993763\nEpsilon = 0.48763615110741637\nAgent: ddqn_agent . Episode 401/2000. Number of steps to finish: 20. Loss: 14.359820365905762 Reward: -12.0\nEpsilon = 0.4875873874923056\nEpsilon = 0.4875386287535564\nEpsilon = 0.4874898748906811\nEpsilon = 0.487441125903192\nEpsilon = 0.4873923817906017\nEpsilon = 0.48734364255242263\nEpsilon = 0.4872949081881674\nEpsilon = 0.4872461786973486\nEpsilon = 0.48719745407947884\nEpsilon = 0.4871487343340709\nEpsilon = 0.4871000194606375\nEpsilon = 0.48705130945869146\nEpsilon = 0.4870026043277456\nEpsilon = 0.48695390406731287\nEpsilon = 0.4869052086769061\nEpsilon = 0.4868565181560384\nEpsilon = 0.4868078325042228\nEpsilon = 0.4867591517209724\nEpsilon = 0.4867104758058003\nEpsilon = 0.48666180475821974\nAgent: ddqn_agent . Episode 402/2000. Number of steps to finish: 20. Loss: 14.322185516357422 Reward: -18.0\nEpsilon = 0.4866131385777439\nEpsilon = 0.48656447726388613\nEpsilon = 0.48651582081615974\nEpsilon = 0.4864671692340781\nEpsilon = 0.48641852251715473\nEpsilon = 0.486369880664903\nEpsilon = 0.4863212436768365\nEpsilon = 0.4862726115524688\nEpsilon = 0.4862239842913136\nEpsilon = 0.48617536189288446\nEpsilon = 0.4861267443566952\nEpsilon = 0.4860781316822595\nEpsilon = 0.48602952386909126\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.48598092091670436\nEpsilon = 0.4859323228246127\nEpsilon = 0.4858837295923302\nEpsilon = 0.485835141219371\nEpsilon = 0.48578655770524903\nEpsilon = 0.4857379790494785\nEpsilon = 0.48568940525157356\nAgent: ddqn_agent . Episode 403/2000. Number of steps to finish: 20. Loss: 14.625309944152832 Reward: -12.0\nEpsilon = 0.48564083631104843\nEpsilon = 0.48559227222741735\nEpsilon = 0.4855437130001946\nEpsilon = 0.4854951586288946\nEpsilon = 0.4854466091130317\nEpsilon = 0.4853980644521204\nEpsilon = 0.4853495246456752\nEpsilon = 0.48530098969321067\nEpsilon = 0.48525245959424135\nEpsilon = 0.4852039343482819\nEpsilon = 0.48515541395484707\nEpsilon = 0.4851068984134516\nEpsilon = 0.48505838772361026\nEpsilon = 0.4850098818848379\nEpsilon = 0.4849613808966494\nEpsilon = 0.48491288475855976\nEpsilon = 0.4848643934700839\nEpsilon = 0.4848159070307369\nEpsilon = 0.4847674254400338\nEpsilon = 0.4847189486974898\nAgent: ddqn_agent . Episode 404/2000. Number of steps to finish: 20. Loss: 14.610614776611328 Reward: -12.0\nEpsilon = 0.4846704768026201\nEpsilon = 0.48462200975493985\nEpsilon = 0.48457354755396437\nEpsilon = 0.484525090199209\nEpsilon = 0.48447663769018906\nEpsilon = 0.48442819002642007\nEpsilon = 0.48437974720741744\nEpsilon = 0.4843313092326967\nEpsilon = 0.4842828761017734\nEpsilon = 0.48423444781416325\nAgent: ddqn_agent . Episode 405/2000. Number of steps to finish: 10. Loss: 7.202991962432861 Reward: 2.0\nEpsilon = 0.48418602436938185\nEpsilon = 0.4841376057669449\nEpsilon = 0.48408919200636824\nEpsilon = 0.48404078308716764\nEpsilon = 0.48399237900885894\nEpsilon = 0.48394397977095804\nEpsilon = 0.48389558537298094\nEpsilon = 0.48384719581444363\nEpsilon = 0.48379881109486217\nEpsilon = 0.48375043121375266\nEpsilon = 0.4837020561706313\nEpsilon = 0.48365368596501424\nEpsilon = 0.48360532059641775\nEpsilon = 0.4835569600643581\nEpsilon = 0.4835086043683517\nEpsilon = 0.4834602535079149\nEpsilon = 0.4834119074825641\nEpsilon = 0.4833635662918158\nEpsilon = 0.48331522993518666\nEpsilon = 0.48326689841219317\nAgent: ddqn_agent . Episode 406/2000. Number of steps to finish: 20. Loss: 14.26302719116211 Reward: -18.0\nEpsilon = 0.48321857172235194\nEpsilon = 0.4831702498651797\nEpsilon = 0.48312193284019317\nEpsilon = 0.4830736206469092\nEpsilon = 0.4830253132848445\nEpsilon = 0.482977010753516\nEpsilon = 0.4829287130524407\nEpsilon = 0.4828804201811354\nEpsilon = 0.4828321321391173\nEpsilon = 0.48278384892590337\nEpsilon = 0.4827355705410108\nEpsilon = 0.4826872969839567\nEpsilon = 0.4826390282542583\nEpsilon = 0.4825907643514329\nEpsilon = 0.48254250527499776\nEpsilon = 0.48249425102447024\nEpsilon = 0.4824460015993678\nEpsilon = 0.48239775699920784\nEpsilon = 0.48234951722350794\nEpsilon = 0.4823012822717856\nAgent: ddqn_agent . Episode 407/2000. Number of steps to finish: 20. Loss: 14.417431831359863 Reward: -12.0\nEpsilon = 0.4822530521435584\nEpsilon = 0.48220482683834404\nEpsilon = 0.4821566063556602\nEpsilon = 0.48210839069502465\nEpsilon = 0.4820601798559552\nEpsilon = 0.48201197383796957\nEpsilon = 0.48196377264058576\nEpsilon = 0.4819155762633217\nEpsilon = 0.4818673847056954\nEpsilon = 0.48181919796722483\nEpsilon = 0.4817710160474281\nEpsilon = 0.48172283894582335\nEpsilon = 0.48167466666192876\nEpsilon = 0.48162649919526257\nEpsilon = 0.48157833654534304\nEpsilon = 0.4815301787116885\nEpsilon = 0.4814820256938174\nEpsilon = 0.481433877491248\nEpsilon = 0.4813857341034989\nEpsilon = 0.48133759553008854\nAgent: ddqn_agent . Episode 408/2000. Number of steps to finish: 20. Loss: 14.326501846313477 Reward: -14.0\nEpsilon = 0.48128946177053555\nEpsilon = 0.4812413328243585\nEpsilon = 0.48119320869107607\nEpsilon = 0.48114508937020695\nEpsilon = 0.48109697486126995\nEpsilon = 0.4810488651637838\nEpsilon = 0.48100076027726746\nEpsilon = 0.4809526602012397\nEpsilon = 0.4809045649352196\nEpsilon = 0.4808564744787261\nEpsilon = 0.48080838883127824\nEpsilon = 0.4807603079923951\nEpsilon = 0.48071223196159585\nEpsilon = 0.4806641607383997\nEpsilon = 0.4806160943223258\nEpsilon = 0.4805680327128936\nEpsilon = 0.4805199759096223\nEpsilon = 0.48047192391203136\nEpsilon = 0.48042387671964015\nEpsilon = 0.4803758343319682\nAgent: ddqn_agent . Episode 409/2000. Number of steps to finish: 20. Loss: 14.065861701965332 Reward: -14.0\nEpsilon = 0.480327796748535\nEpsilon = 0.48027976396886013\nEpsilon = 0.48023173599246327\nEpsilon = 0.480183712818864\nEpsilon = 0.48013569444758214\nEpsilon = 0.4800876808781374\nEpsilon = 0.4800396721100496\nEpsilon = 0.4799916681428386\nEpsilon = 0.4799436689760243\nEpsilon = 0.4798956746091267\nEpsilon = 0.4798476850416658\nEpsilon = 0.47979970027316166\nEpsilon = 0.47975172030313434\nEpsilon = 0.47970374513110403\nAgent: ddqn_agent . Episode 410/2000. Number of steps to finish: 14. Loss: 9.800498008728027 Reward: -2.0\nEpsilon = 0.47965577475659094\nEpsilon = 0.4796078091791153\nEpsilon = 0.4795598483981974\nEpsilon = 0.47951189241335757\nEpsilon = 0.47946394122411623\nEpsilon = 0.4794159948299938\nEpsilon = 0.4793680532305108\nEpsilon = 0.47932011642518774\nEpsilon = 0.47927218441354524\nEpsilon = 0.4792242571951039\nEpsilon = 0.4791763347693844\nEpsilon = 0.4791284171359075\nEpsilon = 0.4790805042941939\nEpsilon = 0.4790325962437645\nEpsilon = 0.4789846929841401\nEpsilon = 0.47893679451484167\nEpsilon = 0.4788889008353902\nEpsilon = 0.47884101194530665\nEpsilon = 0.4787931278441121\nEpsilon = 0.4787452485313277\nAgent: ddqn_agent . Episode 411/2000. Number of steps to finish: 20. Loss: 14.330902099609375 Reward: -8.0\nEpsilon = 0.4786973740064746\nEpsilon = 0.478649504269074\nEpsilon = 0.4786016393186471\nEpsilon = 0.4785537791547152\nEpsilon = 0.47850592377679974\nEpsilon = 0.4784580731844221\nEpsilon = 0.47841022737710365\nEpsilon = 0.47836238635436595\nEpsilon = 0.47831455011573054\nEpsilon = 0.47826671866071896\nEpsilon = 0.47821889198885287\nEpsilon = 0.478171070099654\nEpsilon = 0.478123252992644\nEpsilon = 0.47807544066734475\nEpsilon = 0.478027633123278\nEpsilon = 0.47797983035996566\nEpsilon = 0.47793203237692966\nEpsilon = 0.477884239173692\nEpsilon = 0.47783645074977465\nEpsilon = 0.4777886671046997\nAgent: ddqn_agent . Episode 412/2000. Number of steps to finish: 20. Loss: 14.381648063659668 Reward: -18.0\nEpsilon = 0.4777408882379892\nEpsilon = 0.47769311414916543\nEpsilon = 0.4776453448377505\nEpsilon = 0.4775975803032667\nEpsilon = 0.47754982054523637\nEpsilon = 0.47750206556318187\nEpsilon = 0.4774543153566255\nEpsilon = 0.47740656992508984\nEpsilon = 0.47735882926809736\nEpsilon = 0.4773110933851706\nEpsilon = 0.47726336227583205\nEpsilon = 0.4772156359396045\nEpsilon = 0.4771679143760105\nEpsilon = 0.4771201975845729\nEpsilon = 0.47707248556481446\nEpsilon = 0.477024778316258\nEpsilon = 0.47697707583842636\nEpsilon = 0.4769293781308425\nEpsilon = 0.4768816851930294\nEpsilon = 0.4768339970245101\nAgent: ddqn_agent . Episode 413/2000. Number of steps to finish: 20. Loss: 14.804571151733398 Reward: -12.0\nEpsilon = 0.47678631362480767\nEpsilon = 0.4767386349934452\nEpsilon = 0.47669096112994586\nEpsilon = 0.4766432920338329\nEpsilon = 0.4765956277046295\nEpsilon = 0.476547968141859\nEpsilon = 0.47650031334504483\nEpsilon = 0.47645266331371033\nEpsilon = 0.47640501804737895\nEpsilon = 0.4763573775455742\nEpsilon = 0.47630974180781965\nEpsilon = 0.4762621108336389\nEpsilon = 0.47621448462255556\nEpsilon = 0.4761668631740933\nEpsilon = 0.47611924648777587\nEpsilon = 0.4760716345631271\nEpsilon = 0.4760240273996708\nEpsilon = 0.4759764249969308\nEpsilon = 0.47592882735443115\nEpsilon = 0.4758812344716957\nAgent: ddqn_agent . Episode 414/2000. Number of steps to finish: 20. Loss: 14.629642486572266 Reward: -12.0\nEpsilon = 0.47583364634824854\nEpsilon = 0.4757860629836137\nEpsilon = 0.47573848437731536\nEpsilon = 0.47569091052887763\nEpsilon = 0.4756433414378248\nEpsilon = 0.475595777103681\nEpsilon = 0.47554821752597065\nEpsilon = 0.47550066270421804\nEpsilon = 0.4754531126379476\nEpsilon = 0.4754055673266838\nEpsilon = 0.47535802676995115\nEpsilon = 0.4753104909672742\nEpsilon = 0.47526295991817746\nEpsilon = 0.47521543362218566\nEpsilon = 0.4751679120788234\nEpsilon = 0.4751203952876155\nEpsilon = 0.4750728832480868\nEpsilon = 0.47502537595976196\nEpsilon = 0.474977873422166\nEpsilon = 0.4749303756348238\nAgent: ddqn_agent . Episode 415/2000. Number of steps to finish: 20. Loss: 14.272932052612305 Reward: -10.0\nEpsilon = 0.47488288259726036\nEpsilon = 0.4748353943090006\nEpsilon = 0.4747879107695697\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.47474043197849275\nEpsilon = 0.4746929579352949\nEpsilon = 0.4746454886395014\nEpsilon = 0.47459802409063745\nEpsilon = 0.4745505642882284\nEpsilon = 0.47450310923179956\nEpsilon = 0.4744556589208764\nEpsilon = 0.4744082133549843\nEpsilon = 0.4743607725336488\nEpsilon = 0.4743133364563954\nEpsilon = 0.47426590512274974\nEpsilon = 0.4742184785322375\nEpsilon = 0.47417105668438425\nEpsilon = 0.47412363957871584\nEpsilon = 0.47407622721475795\nEpsilon = 0.47402881959203647\nEpsilon = 0.47398141671007726\nAgent: ddqn_agent . Episode 416/2000. Number of steps to finish: 20. Loss: 14.301441192626953 Reward: -12.0\nEpsilon = 0.4739340185684063\nEpsilon = 0.47388662516654945\nEpsilon = 0.4738392365040328\nEpsilon = 0.4737918525803824\nEpsilon = 0.4737444733951244\nEpsilon = 0.4736970989477849\nEpsilon = 0.4736497292378901\nEpsilon = 0.4736023642649663\nEpsilon = 0.47355500402853984\nEpsilon = 0.473507648528137\nEpsilon = 0.4734602977632842\nEpsilon = 0.4734129517335079\nEpsilon = 0.4733656104383345\nEpsilon = 0.4733182738772907\nEpsilon = 0.473270942049903\nEpsilon = 0.47322361495569804\nEpsilon = 0.47317629259420246\nEpsilon = 0.47312897496494305\nEpsilon = 0.47308166206744656\nEpsilon = 0.4730343539012398\nAgent: ddqn_agent . Episode 417/2000. Number of steps to finish: 20. Loss: 14.528642654418945 Reward: -16.0\nEpsilon = 0.4729870504658497\nEpsilon = 0.4729397517608031\nEpsilon = 0.47289245778562705\nEpsilon = 0.4728451685398485\nEpsilon = 0.4727978840229945\nEpsilon = 0.4727506042345922\nEpsilon = 0.47270332917416874\nEpsilon = 0.47265605884125134\nEpsilon = 0.4726087932353672\nEpsilon = 0.47256153235604365\nEpsilon = 0.47251427620280806\nAgent: ddqn_agent . Episode 418/2000. Number of steps to finish: 11. Loss: 7.4774065017700195 Reward: 1.0\nEpsilon = 0.47246702477518776\nEpsilon = 0.47241977807271024\nEpsilon = 0.472372536094903\nEpsilon = 0.4723252988412935\nEpsilon = 0.47227806631140934\nEpsilon = 0.4722308385047782\nEpsilon = 0.47218361542092774\nEpsilon = 0.47213639705938565\nEpsilon = 0.4720891834196797\nEpsilon = 0.4720419745013377\nEpsilon = 0.4719947703038876\nEpsilon = 0.47194757082685723\nEpsilon = 0.47190037606977453\nEpsilon = 0.4718531860321676\nEpsilon = 0.4718060007135644\nEpsilon = 0.471758820113493\nEpsilon = 0.47171164423148165\nEpsilon = 0.4716644730670585\nAgent: ddqn_agent . Episode 419/2000. Number of steps to finish: 18. Loss: 12.857316017150879 Reward: -6.0\nEpsilon = 0.4716173066197518\nEpsilon = 0.4715701448890898\nEpsilon = 0.4715229878746009\nEpsilon = 0.47147583557581346\nEpsilon = 0.4714286879922559\nEpsilon = 0.4713815451234567\nEpsilon = 0.47133440696894435\nEpsilon = 0.47128727352824745\nEpsilon = 0.47124014480089466\nEpsilon = 0.47119302078641456\nEpsilon = 0.47114590148433594\nEpsilon = 0.4710987868941875\nEpsilon = 0.4710516770154981\nEpsilon = 0.47100457184779654\nEpsilon = 0.4709574713906118\nEpsilon = 0.4709103756434727\nEpsilon = 0.47086328460590837\nEpsilon = 0.4708161982774478\nEpsilon = 0.47076911665762006\nEpsilon = 0.4707220397459543\nAgent: ddqn_agent . Episode 420/2000. Number of steps to finish: 20. Loss: 14.363096237182617 Reward: -14.0\nEpsilon = 0.4706749675419797\nEpsilon = 0.4706279000452255\nEpsilon = 0.470580837255221\nEpsilon = 0.47053377917149547\nEpsilon = 0.4704867257935783\nEpsilon = 0.47043967712099893\nEpsilon = 0.47039263315328683\nEpsilon = 0.4703455938899715\nEpsilon = 0.4702985593305825\nEpsilon = 0.47025152947464943\nEpsilon = 0.47020450432170197\nEpsilon = 0.4701574838712698\nEpsilon = 0.4701104681228827\nEpsilon = 0.47006345707607045\nEpsilon = 0.47001645073036286\nEpsilon = 0.4699694490852898\nEpsilon = 0.4699224521403813\nEpsilon = 0.46987545989516727\nEpsilon = 0.46982847234917774\nEpsilon = 0.4697814895019428\nAgent: ddqn_agent . Episode 421/2000. Number of steps to finish: 20. Loss: 14.183914184570312 Reward: -12.0\nEpsilon = 0.4697345113529926\nEpsilon = 0.4696875379018573\nEpsilon = 0.4696405691480671\nEpsilon = 0.46959360509115233\nEpsilon = 0.4695466457306432\nEpsilon = 0.46949969106607015\nEpsilon = 0.46945274109696356\nEpsilon = 0.46940579582285386\nEpsilon = 0.4693588552432716\nEpsilon = 0.46931191935774724\nEpsilon = 0.46926498816581147\nEpsilon = 0.46921806166699487\nAgent: ddqn_agent . Episode 422/2000. Number of steps to finish: 12. Loss: 8.526326179504395 Reward: 0.0\nEpsilon = 0.46917113986082815\nEpsilon = 0.46912422274684207\nEpsilon = 0.4690773103245674\nEpsilon = 0.46903040259353496\nEpsilon = 0.4689834995532756\nEpsilon = 0.4689366012033203\nEpsilon = 0.4688897075432\nEpsilon = 0.46884281857244564\nEpsilon = 0.4687959342905884\nEpsilon = 0.4687490546971594\nEpsilon = 0.46870217979168965\nEpsilon = 0.4686553095737105\nEpsilon = 0.46860844404275315\nEpsilon = 0.4685615831983489\nEpsilon = 0.46851472704002906\nEpsilon = 0.46846787556732505\nEpsilon = 0.4684210287797683\nEpsilon = 0.46837418667689035\nEpsilon = 0.46832734925822267\nEpsilon = 0.46828051652329683\nAgent: ddqn_agent . Episode 423/2000. Number of steps to finish: 20. Loss: 14.27370548248291 Reward: -14.0\nEpsilon = 0.4682336884716445\nEpsilon = 0.46818686510279733\nEpsilon = 0.46814004641628704\nEpsilon = 0.4680932324116454\nEpsilon = 0.4680464230884043\nEpsilon = 0.46799961844609544\nEpsilon = 0.46795281848425085\nEpsilon = 0.46790602320240243\nEpsilon = 0.4678592326000822\nEpsilon = 0.4678124466768222\nEpsilon = 0.46776566543215453\nEpsilon = 0.4677188888656113\nEpsilon = 0.4676721169767247\nEpsilon = 0.46762534976502707\nEpsilon = 0.4675785872300506\nEpsilon = 0.4675318293713276\nEpsilon = 0.46748507618839047\nEpsilon = 0.46743832768077165\nEpsilon = 0.46739158384800356\nEpsilon = 0.4673448446896188\nAgent: ddqn_agent . Episode 424/2000. Number of steps to finish: 20. Loss: 14.347831726074219 Reward: -10.0\nEpsilon = 0.46729811020514983\nEpsilon = 0.46725138039412933\nEpsilon = 0.46720465525608995\nEpsilon = 0.46715793479056433\nEpsilon = 0.4671112189970853\nEpsilon = 0.4670645078751856\nEpsilon = 0.4670178014243981\nEpsilon = 0.46697109964425565\nEpsilon = 0.46692440253429124\nEpsilon = 0.46687771009403783\nEpsilon = 0.4668310223230284\nEpsilon = 0.46678433922079615\nEpsilon = 0.46673766078687406\nEpsilon = 0.46669098702079537\nEpsilon = 0.46664431792209327\nEpsilon = 0.4665976534903011\nEpsilon = 0.46655099372495207\nEpsilon = 0.46650433862557955\nEpsilon = 0.466457688191717\nEpsilon = 0.4664110424228978\nAgent: ddqn_agent . Episode 425/2000. Number of steps to finish: 20. Loss: 14.078649520874023 Reward: -10.0\nEpsilon = 0.46636440131865553\nEpsilon = 0.46631776487852367\nEpsilon = 0.46627113310203583\nEpsilon = 0.4662245059887256\nEpsilon = 0.4661778835381267\nEpsilon = 0.4661312657497729\nEpsilon = 0.466084652623198\nEpsilon = 0.46603804415793565\nEpsilon = 0.46599144035351986\nEpsilon = 0.46594484120948454\nEpsilon = 0.4658982467253636\nEpsilon = 0.4658516569006911\nEpsilon = 0.465805071735001\nEpsilon = 0.46575849122782753\nEpsilon = 0.46571191537870477\nEpsilon = 0.4656653441871669\nEpsilon = 0.4656187776527482\nEpsilon = 0.4655722157749829\nEpsilon = 0.46552565855340544\nEpsilon = 0.4654791059875501\nAgent: ddqn_agent . Episode 426/2000. Number of steps to finish: 20. Loss: 14.268599510192871 Reward: -12.0\nEpsilon = 0.46543255807695133\nEpsilon = 0.46538601482114367\nEpsilon = 0.46533947621966154\nEpsilon = 0.4652929422720396\nEpsilon = 0.4652464129778124\nEpsilon = 0.4651998883365146\nEpsilon = 0.46515336834768095\nEpsilon = 0.4651068530108462\nEpsilon = 0.4650603423255451\nEpsilon = 0.46501383629131254\nEpsilon = 0.46496733490768344\nEpsilon = 0.46492083817419266\nEpsilon = 0.46487434609037526\nEpsilon = 0.4648278586557662\nEpsilon = 0.46478137586990065\nEpsilon = 0.4647348977323137\nEpsilon = 0.4646884242425405\nEpsilon = 0.4646419554001162\nEpsilon = 0.4645954912045762\nEpsilon = 0.46454903165545575\nAgent: ddqn_agent . Episode 427/2000. Number of steps to finish: 20. Loss: 13.991482734680176 Reward: -8.0\nEpsilon = 0.46450257675229023\nEpsilon = 0.464456126494615\nEpsilon = 0.46440968088196555\nEpsilon = 0.46436323991387735\nEpsilon = 0.464316803589886\nEpsilon = 0.46427037190952697\nEpsilon = 0.46422394487233604\nEpsilon = 0.4641775224778488\nEpsilon = 0.46413110472560104\nEpsilon = 0.4640846916151285\nEpsilon = 0.464038283145967\nEpsilon = 0.4639918793176524\nEpsilon = 0.46394548012972064\nEpsilon = 0.46389908558170767\nEpsilon = 0.4638526956731495\nEpsilon = 0.4638063104035822\nEpsilon = 0.4637599297725419\nEpsilon = 0.46371355377956464\nEpsilon = 0.4636671824241867\nEpsilon = 0.46362081570594427\nAgent: ddqn_agent . Episode 428/2000. Number of steps to finish: 20. Loss: 13.988880157470703 Reward: -10.0\nEpsilon = 0.46357445362437366\nEpsilon = 0.46352809617901125\nEpsilon = 0.46348174336939335\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.46343539519505644\nEpsilon = 0.46338905165553695\nEpsilon = 0.4633427127503714\nEpsilon = 0.4632963784790964\nEpsilon = 0.4632500488412485\nEpsilon = 0.4632037238363644\nEpsilon = 0.46315740346398077\nEpsilon = 0.46311108772363435\nEpsilon = 0.463064776614862\nEpsilon = 0.46301847013720054\nEpsilon = 0.46297216829018684\nEpsilon = 0.4629258710733578\nEpsilon = 0.4628795784862505\nEpsilon = 0.4628332905284019\nEpsilon = 0.46278700719934907\nEpsilon = 0.46274072849862913\nEpsilon = 0.4626944544257793\nAgent: ddqn_agent . Episode 429/2000. Number of steps to finish: 20. Loss: 14.198478698730469 Reward: -20.0\nEpsilon = 0.4626481849803367\nEpsilon = 0.4626019201618387\nEpsilon = 0.4625556599698225\nEpsilon = 0.46250940440382554\nEpsilon = 0.4624631534633852\nEpsilon = 0.46241690714803885\nEpsilon = 0.462370665457324\nEpsilon = 0.4623244283907783\nEpsilon = 0.46227819594793923\nEpsilon = 0.46223196812834444\nEpsilon = 0.4621857449315316\nEpsilon = 0.4621395263570384\nEpsilon = 0.4620933124044027\nEpsilon = 0.46204710307316227\nEpsilon = 0.46200089836285496\nEpsilon = 0.46195469827301866\nEpsilon = 0.46190850280319135\nEpsilon = 0.46186231195291105\nEpsilon = 0.46181612572171576\nEpsilon = 0.46176994410914357\nAgent: ddqn_agent . Episode 430/2000. Number of steps to finish: 20. Loss: 14.204591751098633 Reward: -12.0\nEpsilon = 0.4617237671147327\nEpsilon = 0.4616775947380212\nEpsilon = 0.4616314269785474\nEpsilon = 0.4615852638358496\nEpsilon = 0.461539105309466\nEpsilon = 0.4614929513989351\nEpsilon = 0.4614468021037952\nEpsilon = 0.4614006574235848\nEpsilon = 0.4613545173578425\nEpsilon = 0.46130838190610673\nEpsilon = 0.46126225106791613\nEpsilon = 0.4612161248428093\nEpsilon = 0.46117000323032503\nEpsilon = 0.461123886230002\nEpsilon = 0.461077773841379\nEpsilon = 0.46103166606399487\nEpsilon = 0.46098556289738846\nAgent: ddqn_agent . Episode 431/2000. Number of steps to finish: 17. Loss: 12.034008979797363 Reward: -5.0\nEpsilon = 0.4609394643410987\nEpsilon = 0.4608933703946646\nEpsilon = 0.4608472810576251\nEpsilon = 0.4608011963295194\nEpsilon = 0.4607551162098864\nEpsilon = 0.4607090406982654\nEpsilon = 0.4606629697941956\nEpsilon = 0.4606169034972162\nEpsilon = 0.46057084180686647\nEpsilon = 0.46052478472268576\nEpsilon = 0.4604787322442135\nEpsilon = 0.46043268437098905\nEpsilon = 0.46038664110255195\nEpsilon = 0.4603406024384417\nEpsilon = 0.4602945683781979\nEpsilon = 0.4602485389213601\nEpsilon = 0.4602025140674679\nEpsilon = 0.46015649381606116\nEpsilon = 0.46011047816667955\nEpsilon = 0.4600644671188629\nAgent: ddqn_agent . Episode 432/2000. Number of steps to finish: 20. Loss: 14.092595100402832 Reward: -12.0\nEpsilon = 0.460018460672151\nEpsilon = 0.4599724588260838\nEpsilon = 0.4599264615802012\nEpsilon = 0.45988046893404316\nEpsilon = 0.4598344808871498\nEpsilon = 0.4597884974390611\nEpsilon = 0.4597425185893172\nEpsilon = 0.45969654433745827\nEpsilon = 0.4596505746830245\nEpsilon = 0.4596046096255562\nEpsilon = 0.45955864916459366\nEpsilon = 0.4595126932996772\nEpsilon = 0.45946674203034726\nEpsilon = 0.4594207953561442\nEpsilon = 0.4593748532766086\nEpsilon = 0.45932891579128093\nEpsilon = 0.4592829828997018\nEpsilon = 0.45923705460141184\nEpsilon = 0.4591911308959517\nEpsilon = 0.4591452117828621\nAgent: ddqn_agent . Episode 433/2000. Number of steps to finish: 20. Loss: 14.125940322875977 Reward: -10.0\nEpsilon = 0.4590992972616838\nEpsilon = 0.45905338733195766\nEpsilon = 0.4590074819932245\nEpsilon = 0.4589615812450252\nEpsilon = 0.45891568508690067\nEpsilon = 0.45886979351839197\nEpsilon = 0.45882390653904015\nEpsilon = 0.45877802414838625\nEpsilon = 0.4587321463459714\nEpsilon = 0.4586862731313368\nEpsilon = 0.45864040450402366\nEpsilon = 0.4585945404635733\nEpsilon = 0.45854868100952695\nEpsilon = 0.458502826141426\nEpsilon = 0.4584569758588119\nEpsilon = 0.45841113016122603\nEpsilon = 0.4583652890482099\nEpsilon = 0.45831945251930506\nEpsilon = 0.45827362057405313\nEpsilon = 0.45822779321199575\nAgent: ddqn_agent . Episode 434/2000. Number of steps to finish: 20. Loss: 14.295421600341797 Reward: -10.0\nEpsilon = 0.45818197043267456\nEpsilon = 0.4581361522356313\nEpsilon = 0.45809033862040777\nEpsilon = 0.45804452958654573\nEpsilon = 0.4579987251335871\nEpsilon = 0.45795292526107373\nEpsilon = 0.4579071299685476\nEpsilon = 0.4578613392555507\nEpsilon = 0.4578155531216252\nEpsilon = 0.45776977156631304\nEpsilon = 0.4577239945891564\nEpsilon = 0.4576782221896975\nEpsilon = 0.4576324543674785\nEpsilon = 0.45758669112204176\nEpsilon = 0.45754093245292954\nEpsilon = 0.45749517835968423\nEpsilon = 0.45744942884184825\nEpsilon = 0.4574036838989641\nEpsilon = 0.4573579435305742\nEpsilon = 0.4573122077362211\nAgent: ddqn_agent . Episode 435/2000. Number of steps to finish: 20. Loss: 14.525641441345215 Reward: -12.0\nEpsilon = 0.4572664765154475\nEpsilon = 0.45722074986779593\nEpsilon = 0.45717502779280916\nEpsilon = 0.4571293102900299\nEpsilon = 0.4570835973590009\nEpsilon = 0.457037888999265\nEpsilon = 0.4569921852103651\nEpsilon = 0.45694648599184406\nEpsilon = 0.4569007913432449\nEpsilon = 0.4568551012641106\nEpsilon = 0.4568094157539842\nEpsilon = 0.4567637348124088\nEpsilon = 0.45671805843892754\nEpsilon = 0.4566723866330837\nEpsilon = 0.4566267193944204\nEpsilon = 0.45658105672248095\nEpsilon = 0.4565353986168087\nEpsilon = 0.456489745076947\nEpsilon = 0.4564440961024393\nEpsilon = 0.45639845169282905\nAgent: ddqn_agent . Episode 436/2000. Number of steps to finish: 20. Loss: 14.162467002868652 Reward: -12.0\nEpsilon = 0.4563528118476598\nEpsilon = 0.456307176566475\nEpsilon = 0.4562615458488184\nEpsilon = 0.45621591969423353\nEpsilon = 0.4561702981022641\nEpsilon = 0.4561246810724539\nEpsilon = 0.45607906860434666\nEpsilon = 0.45603346069748624\nEpsilon = 0.4559878573514165\nEpsilon = 0.4559422585656814\nEpsilon = 0.45589666433982484\nEpsilon = 0.45585107467339087\nEpsilon = 0.45580548956592354\nEpsilon = 0.45575990901696695\nEpsilon = 0.45571433302606523\nEpsilon = 0.45566876159276265\nEpsilon = 0.4556231947166034\nEpsilon = 0.4555776323971317\nEpsilon = 0.455532074633892\nEpsilon = 0.4554865214264286\nAgent: ddqn_agent . Episode 437/2000. Number of steps to finish: 20. Loss: 14.316798210144043 Reward: -12.0\nEpsilon = 0.45544097277428597\nEpsilon = 0.45539542867700855\nEpsilon = 0.4553498891341409\nEpsilon = 0.45530435414522746\nEpsilon = 0.45525882370981297\nEpsilon = 0.45521329782744197\nEpsilon = 0.4551677764976592\nEpsilon = 0.45512225972000947\nEpsilon = 0.4550767474940375\nEpsilon = 0.45503123981928806\nEpsilon = 0.4549857366953061\nEpsilon = 0.4549402381216366\nEpsilon = 0.45489474409782443\nEpsilon = 0.45484925462341463\nEpsilon = 0.4548037696979523\nEpsilon = 0.4547582893209825\nEpsilon = 0.4547128134920504\nEpsilon = 0.4546673422107012\nEpsilon = 0.45462187547648014\nEpsilon = 0.4545764132889325\nAgent: ddqn_agent . Episode 438/2000. Number of steps to finish: 20. Loss: 14.305622100830078 Reward: -8.0\nEpsilon = 0.4545309556476036\nEpsilon = 0.45448550255203884\nEpsilon = 0.45444005400178367\nEpsilon = 0.4543946099963835\nEpsilon = 0.4543491705353839\nEpsilon = 0.4543037356183303\nEpsilon = 0.4542583052447685\nEpsilon = 0.45421287941424404\nEpsilon = 0.4541674581263026\nEpsilon = 0.45412204138049\nEpsilon = 0.45407662917635194\nEpsilon = 0.45403122151343434\nEpsilon = 0.453985818391283\nEpsilon = 0.4539404198094439\nEpsilon = 0.45389502576746293\nEpsilon = 0.45384963626488617\nEpsilon = 0.4538042513012597\nEpsilon = 0.45375887087612954\nEpsilon = 0.45371349498904195\nEpsilon = 0.45366812363954306\nAgent: ddqn_agent . Episode 439/2000. Number of steps to finish: 20. Loss: 14.586055755615234 Reward: -16.0\nEpsilon = 0.4536227568271791\nEpsilon = 0.4535773945514964\nEpsilon = 0.4535320368120413\nEpsilon = 0.4534866836083601\nEpsilon = 0.4534413349399993\nEpsilon = 0.4533959908065053\nEpsilon = 0.4533506512074246\nEpsilon = 0.4533053161423039\nEpsilon = 0.4532599856106897\nEpsilon = 0.45321465961212865\nEpsilon = 0.45316933814616744\nEpsilon = 0.45312402121235285\nEpsilon = 0.4530787088102316\nEpsilon = 0.4530334009393506\nEpsilon = 0.4529880975992566\nEpsilon = 0.4529427987894967\nEpsilon = 0.4528975045096178\nEpsilon = 0.4528522147591668\nEpsilon = 0.4528069295376909\nEpsilon = 0.45276164884473713\nAgent: ddqn_agent . Episode 440/2000. Number of steps to finish: 20. Loss: 14.764374732971191 Reward: -8.0\nEpsilon = 0.45271637267985265\nEpsilon = 0.4526711010425847\nEpsilon = 0.4526258339324804\nEpsilon = 0.45258057134908714\nEpsilon = 0.45253531329195223\nEpsilon = 0.45249005976062306\nEpsilon = 0.452444810754647\nEpsilon = 0.45239956627357153\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.45235432631694417\nEpsilon = 0.4523090908843125\nEpsilon = 0.45226385997522406\nEpsilon = 0.45221863358922654\nEpsilon = 0.4521734117258676\nEpsilon = 0.45212819438469504\nEpsilon = 0.45208298156525656\nEpsilon = 0.45203777326710004\nEpsilon = 0.45199256948977334\nEpsilon = 0.4519473702328244\nEpsilon = 0.4519021754958011\nEpsilon = 0.4518569852782515\nAgent: ddqn_agent . Episode 441/2000. Number of steps to finish: 20. Loss: 14.204131126403809 Reward: -12.0\nEpsilon = 0.4518117995797237\nEpsilon = 0.4517666183997658\nEpsilon = 0.45172144173792583\nEpsilon = 0.45167626959375207\nEpsilon = 0.4516311019667927\nEpsilon = 0.451585938856596\nEpsilon = 0.45154078026271033\nEpsilon = 0.4514956261846841\nEpsilon = 0.4514504766220656\nEpsilon = 0.45140533157440343\nEpsilon = 0.451360191041246\nEpsilon = 0.45131505502214186\nEpsilon = 0.45126992351663964\nEpsilon = 0.451224796524288\nEpsilon = 0.45117967404463555\nEpsilon = 0.4511345560772311\nEpsilon = 0.45108944262162337\nAgent: ddqn_agent . Episode 442/2000. Number of steps to finish: 17. Loss: 11.983592987060547 Reward: -5.0\nEpsilon = 0.4510443336773612\nEpsilon = 0.45099922924399344\nEpsilon = 0.45095412932106904\nEpsilon = 0.4509090339081369\nEpsilon = 0.4508639430047461\nEpsilon = 0.45081885661044563\nEpsilon = 0.4507737747247846\nEpsilon = 0.45072869734731213\nEpsilon = 0.4506836244775774\nEpsilon = 0.45063855611512965\nEpsilon = 0.45059349225951817\nEpsilon = 0.45054843291029223\nEpsilon = 0.4505033780670012\nEpsilon = 0.4504583277291945\nEpsilon = 0.45041328189642155\nEpsilon = 0.45036824056823194\nEpsilon = 0.4503232037441751\nEpsilon = 0.4502781714238007\nEpsilon = 0.45023314360665834\nEpsilon = 0.45018812029229766\nAgent: ddqn_agent . Episode 443/2000. Number of steps to finish: 20. Loss: 14.465771675109863 Reward: -10.0\nEpsilon = 0.4501431014802684\nEpsilon = 0.4500980871701204\nEpsilon = 0.4500530773614034\nEpsilon = 0.4500080720536672\nEpsilon = 0.44996307124646184\nEpsilon = 0.4499180749393372\nEpsilon = 0.44987308313184327\nEpsilon = 0.44982809582353006\nEpsilon = 0.4497831130139477\nEpsilon = 0.4497381347026463\nEpsilon = 0.44969316088917605\nEpsilon = 0.4496481915730871\nEpsilon = 0.44960322675392983\nEpsilon = 0.44955826643125446\nEpsilon = 0.44951331060461136\nEpsilon = 0.4494683592735509\nEpsilon = 0.4494234124376235\nEpsilon = 0.44937847009637977\nEpsilon = 0.4493335322493701\nEpsilon = 0.4492885988961452\nAgent: ddqn_agent . Episode 444/2000. Number of steps to finish: 20. Loss: 14.456334114074707 Reward: -12.0\nEpsilon = 0.4492436700362556\nEpsilon = 0.449198745669252\nEpsilon = 0.4491538257946851\nEpsilon = 0.44910891041210566\nEpsilon = 0.44906399952106446\nEpsilon = 0.4490190931211124\nEpsilon = 0.44897419121180027\nEpsilon = 0.4489292937926791\nEpsilon = 0.44888440086329984\nEpsilon = 0.4488395124232135\nEpsilon = 0.4487946284719712\nEpsilon = 0.448749749009124\nEpsilon = 0.4487048740342231\nEpsilon = 0.4486600035468197\nEpsilon = 0.448615137546465\nEpsilon = 0.4485702760327103\nEpsilon = 0.44852541900510706\nEpsilon = 0.44848056646320655\nAgent: ddqn_agent . Episode 445/2000. Number of steps to finish: 18. Loss: 12.953458786010742 Reward: -6.0\nEpsilon = 0.4484357184065602\nEpsilon = 0.44839087483471957\nEpsilon = 0.4483460357472361\nEpsilon = 0.44830120114366134\nEpsilon = 0.448256371023547\nEpsilon = 0.44821154538644464\nEpsilon = 0.448166724231906\nEpsilon = 0.44812190755948283\nEpsilon = 0.4480770953687269\nEpsilon = 0.44803228765919006\nEpsilon = 0.44798748443042413\nEpsilon = 0.4479426856819811\nEpsilon = 0.4478978914134129\nEpsilon = 0.44785310162427155\nEpsilon = 0.4478083163141091\nEpsilon = 0.4477635354824777\nEpsilon = 0.44771875912892944\nEpsilon = 0.44767398725301655\nEpsilon = 0.44762921985429127\nEpsilon = 0.44758445693230586\nAgent: ddqn_agent . Episode 446/2000. Number of steps to finish: 20. Loss: 13.886444091796875 Reward: -12.0\nEpsilon = 0.4475396984866126\nEpsilon = 0.44749494451676397\nEpsilon = 0.4474501950223123\nEpsilon = 0.44740545000281007\nEpsilon = 0.4473607094578098\nEpsilon = 0.447315973386864\nEpsilon = 0.4472712417895253\nEpsilon = 0.44722651466534635\nEpsilon = 0.4471817920138798\nEpsilon = 0.44713707383467843\nEpsilon = 0.447092360127295\nEpsilon = 0.44704765089128223\nEpsilon = 0.4470029461261931\nEpsilon = 0.4469582458315805\nEpsilon = 0.44691355000699734\nEpsilon = 0.44686885865199666\nEpsilon = 0.4468241717661315\nEpsilon = 0.4467794893489549\nEpsilon = 0.44673481140002\nEpsilon = 0.44669013791888\nAgent: ddqn_agent . Episode 447/2000. Number of steps to finish: 20. Loss: 13.986223220825195 Reward: -14.0\nEpsilon = 0.4466454689050881\nEpsilon = 0.4466008043581976\nEpsilon = 0.4465561442777618\nEpsilon = 0.446511488663334\nEpsilon = 0.4464668375144677\nEpsilon = 0.44642219083071627\nEpsilon = 0.4463775486116332\nEpsilon = 0.44633291085677207\nEpsilon = 0.4462882775656864\nEpsilon = 0.4462436487379298\nEpsilon = 0.44619902437305603\nEpsilon = 0.4461544044706187\nEpsilon = 0.4461097890301717\nEpsilon = 0.44606517805126866\nEpsilon = 0.44602057153346353\nEpsilon = 0.4459759694763102\nEpsilon = 0.44593137187936255\nEpsilon = 0.44588677874217464\nEpsilon = 0.4458421900643004\nEpsilon = 0.445797605845294\nAgent: ddqn_agent . Episode 448/2000. Number of steps to finish: 20. Loss: 14.36551570892334 Reward: -16.0\nEpsilon = 0.44575302608470946\nEpsilon = 0.445708450782101\nEpsilon = 0.4456638799370228\nEpsilon = 0.44561931354902906\nEpsilon = 0.4455747516176742\nEpsilon = 0.44553019414251244\nEpsilon = 0.4454856411230982\nEpsilon = 0.4454410925589859\nEpsilon = 0.44539654844973\nEpsilon = 0.445352008794885\nEpsilon = 0.4453074735940055\nAgent: ddqn_agent . Episode 449/2000. Number of steps to finish: 11. Loss: 7.812074184417725 Reward: 1.0\nEpsilon = 0.44526294284664614\nEpsilon = 0.4452184165523615\nEpsilon = 0.4451738947107063\nEpsilon = 0.4451293773212352\nEpsilon = 0.4450848643835031\nEpsilon = 0.44504035589706475\nEpsilon = 0.44499585186147506\nEpsilon = 0.4449513522762889\nEpsilon = 0.44490685714106126\nEpsilon = 0.44486236645534716\nEpsilon = 0.4448178802187016\nEpsilon = 0.44477339843067976\nEpsilon = 0.44472892109083667\nEpsilon = 0.4446844481987276\nEpsilon = 0.44463997975390773\nEpsilon = 0.44459551575593237\nEpsilon = 0.44455105620435675\nEpsilon = 0.4445066010987363\nEpsilon = 0.4444621504386264\nEpsilon = 0.44441770422358257\nAgent: ddqn_agent . Episode 450/2000. Number of steps to finish: 20. Loss: 13.932056427001953 Reward: -12.0\nEpsilon = 0.4443732624531602\nEpsilon = 0.4443288251269149\nEpsilon = 0.4442843922444022\nEpsilon = 0.4442399638051778\nEpsilon = 0.4441955398087973\nEpsilon = 0.4441511202548164\nEpsilon = 0.44410670514279094\nEpsilon = 0.4440622944722767\nEpsilon = 0.44401788824282945\nEpsilon = 0.4439734864540052\nEpsilon = 0.4439290891053598\nEpsilon = 0.44388469619644927\nEpsilon = 0.44384030772682964\nEpsilon = 0.44379592369605697\nEpsilon = 0.4437515441036874\nEpsilon = 0.44370716894927703\nEpsilon = 0.4436627982323821\nEpsilon = 0.44361843195255884\nEpsilon = 0.4435740701093636\nEpsilon = 0.4435297127023527\nAgent: ddqn_agent . Episode 451/2000. Number of steps to finish: 20. Loss: 14.50001335144043 Reward: -16.0\nEpsilon = 0.44348535973108244\nEpsilon = 0.4434410111951093\nEpsilon = 0.4433966670939898\nEpsilon = 0.44335232742728037\nEpsilon = 0.44330799219453765\nEpsilon = 0.4432636613953182\nEpsilon = 0.44321933502917865\nEpsilon = 0.44317501309567575\nEpsilon = 0.4431306955943662\nEpsilon = 0.44308638252480675\nEpsilon = 0.44304207388655426\nEpsilon = 0.4429977696791656\nEpsilon = 0.4429534699021977\nEpsilon = 0.44290917455520745\nAgent: ddqn_agent . Episode 452/2000. Number of steps to finish: 14. Loss: 10.139341354370117 Reward: -2.0\nEpsilon = 0.44286488363775195\nEpsilon = 0.4428205971493882\nEpsilon = 0.4427763150896733\nEpsilon = 0.44273203745816436\nEpsilon = 0.4426877642544185\nEpsilon = 0.4426434954779931\nEpsilon = 0.4425992311284453\nEpsilon = 0.44255497120533244\nEpsilon = 0.44251071570821193\nEpsilon = 0.44246646463664113\nEpsilon = 0.44242221799017745\nEpsilon = 0.44237797576837845\nEpsilon = 0.4423337379708016\nEpsilon = 0.44228950459700456\nEpsilon = 0.44224527564654487\nEpsilon = 0.4422010511189802\nEpsilon = 0.4421568310138683\nEpsilon = 0.44211261533076696\nEpsilon = 0.44206840406923387\nEpsilon = 0.44202419722882696\nAgent: ddqn_agent . Episode 453/2000. Number of steps to finish: 20. Loss: 13.7960786819458 Reward: -8.0\nEpsilon = 0.4419799948091041\nEpsilon = 0.4419357968096232\nEpsilon = 0.44189160322994225\nEpsilon = 0.44184741406961925\nEpsilon = 0.4418032293282123\nEpsilon = 0.4417590490052795\nEpsilon = 0.441714873100379\nEpsilon = 0.441670701613069\nEpsilon = 0.4416265345429077\nEpsilon = 0.4415823718894534\nEpsilon = 0.4415382136522644\nEpsilon = 0.4414940598308992\nEpsilon = 0.4414499104249161\nEpsilon = 0.4414057654338736\nEpsilon = 0.4413616248573302\nEpsilon = 0.4413174886948445\nEpsilon = 0.441273356945975\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.44122922961028044\nEpsilon = 0.4411851066873194\nEpsilon = 0.4411409881766507\nAgent: ddqn_agent . Episode 454/2000. Number of steps to finish: 20. Loss: 14.348286628723145 Reward: -8.0\nEpsilon = 0.44109687407783305\nEpsilon = 0.4410527643904253\nEpsilon = 0.44100865911398623\nEpsilon = 0.44096455824807484\nEpsilon = 0.44092046179225003\nEpsilon = 0.4408763697460708\nEpsilon = 0.4408322821090962\nEpsilon = 0.4407881988808853\nEpsilon = 0.44074412006099717\nEpsilon = 0.44070004564899107\nEpsilon = 0.44065597564442616\nEpsilon = 0.44061191004686173\nAgent: ddqn_agent . Episode 455/2000. Number of steps to finish: 12. Loss: 8.382624626159668 Reward: 0.0\nEpsilon = 0.44056784885585704\nEpsilon = 0.4405237920709715\nEpsilon = 0.4404797396917644\nEpsilon = 0.44043569171779523\nEpsilon = 0.44039164814862347\nEpsilon = 0.4403476089838086\nEpsilon = 0.4403035742229102\nEpsilon = 0.44025954386548793\nEpsilon = 0.4402155179111014\nEpsilon = 0.4401714963593103\nEpsilon = 0.4401274792096744\nEpsilon = 0.4400834664617534\nEpsilon = 0.4400394581151072\nEpsilon = 0.4399954541692957\nEpsilon = 0.43995145462387875\nEpsilon = 0.43990745947841636\nEpsilon = 0.4398634687324685\nEpsilon = 0.43981948238559526\nEpsilon = 0.4397755004373567\nEpsilon = 0.439731522887313\nAgent: ddqn_agent . Episode 456/2000. Number of steps to finish: 20. Loss: 14.410083770751953 Reward: -12.0\nEpsilon = 0.4396875497350243\nEpsilon = 0.4396435809800508\nEpsilon = 0.4395996166219528\nEpsilon = 0.4395556566602906\nEpsilon = 0.4395117010946246\nEpsilon = 0.43946774992451515\nEpsilon = 0.43942380314952273\nEpsilon = 0.43937986076920776\nEpsilon = 0.43933592278313083\nEpsilon = 0.4392919891908525\nEpsilon = 0.4392480599919334\nAgent: ddqn_agent . Episode 457/2000. Number of steps to finish: 11. Loss: 7.891637325286865 Reward: 1.0\nEpsilon = 0.4392041351859342\nEpsilon = 0.4391602147724156\nEpsilon = 0.4391162987509384\nEpsilon = 0.4390723871210633\nEpsilon = 0.43902847988235116\nEpsilon = 0.43898457703436294\nEpsilon = 0.4389406785766595\nEpsilon = 0.4388967845088019\nEpsilon = 0.438852894830351\nEpsilon = 0.438809009540868\nEpsilon = 0.4387651286399139\nEpsilon = 0.43872125212704993\nEpsilon = 0.43867738000183726\nEpsilon = 0.43863351226383707\nEpsilon = 0.4385896489126107\nEpsilon = 0.4385457899477195\nEpsilon = 0.4385019353687247\nEpsilon = 0.43845808517518786\nEpsilon = 0.43841423936667034\nEpsilon = 0.4383703979427337\nAgent: ddqn_agent . Episode 458/2000. Number of steps to finish: 20. Loss: 14.028205871582031 Reward: -18.0\nEpsilon = 0.4383265609029394\nEpsilon = 0.4382827282468491\nEpsilon = 0.43823889997402443\nEpsilon = 0.438195076084027\nEpsilon = 0.4381512565764186\nEpsilon = 0.43810744145076097\nEpsilon = 0.4380636307066159\nEpsilon = 0.43801982434354525\nEpsilon = 0.4379760223611109\nEpsilon = 0.4379322247588748\nEpsilon = 0.4378884315363989\nEpsilon = 0.43784464269324525\nEpsilon = 0.4378008582289759\nEpsilon = 0.437757078143153\nEpsilon = 0.4377133024353387\nEpsilon = 0.43766953110509516\nEpsilon = 0.4376257641519847\nEpsilon = 0.4375820015755695\nEpsilon = 0.4375382433754119\nEpsilon = 0.4374944895510744\nAgent: ddqn_agent . Episode 459/2000. Number of steps to finish: 20. Loss: 14.120583534240723 Reward: -12.0\nEpsilon = 0.4374507401021193\nEpsilon = 0.4374069950281091\nEpsilon = 0.4373632543286063\nEpsilon = 0.4373195180031734\nEpsilon = 0.4372757860513731\nEpsilon = 0.43723205847276797\nEpsilon = 0.4371883352669207\nEpsilon = 0.43714461643339403\nEpsilon = 0.4371009019717507\nEpsilon = 0.43705719188155356\nEpsilon = 0.4370134861623654\nEpsilon = 0.43696978481374915\nEpsilon = 0.43692608783526776\nEpsilon = 0.43688239522648425\nEpsilon = 0.4368387069869616\nEpsilon = 0.4367950231162629\nEpsilon = 0.4367513436139513\nAgent: ddqn_agent . Episode 460/2000. Number of steps to finish: 17. Loss: 12.069206237792969 Reward: -5.0\nEpsilon = 0.4367076684795899\nEpsilon = 0.43666399771274195\nEpsilon = 0.4366203313129707\nEpsilon = 0.4365766692798394\nEpsilon = 0.4365330116129114\nEpsilon = 0.4364893583117501\nEpsilon = 0.4364457093759189\nEpsilon = 0.43640206480498134\nEpsilon = 0.4363584245985008\nEpsilon = 0.43631478875604096\nEpsilon = 0.43627115727716537\nEpsilon = 0.43622753016143767\nEpsilon = 0.43618390740842156\nEpsilon = 0.4361402890176807\nEpsilon = 0.43609667498877897\nEpsilon = 0.4360530653212801\nEpsilon = 0.43600946001474794\nEpsilon = 0.43596585906874646\nEpsilon = 0.4359222624828396\nEpsilon = 0.4358786702565913\nAgent: ddqn_agent . Episode 461/2000. Number of steps to finish: 20. Loss: 13.925614356994629 Reward: -12.0\nEpsilon = 0.4358350823895657\nEpsilon = 0.43579149888132673\nEpsilon = 0.4357479197314386\nEpsilon = 0.43570434493946547\nEpsilon = 0.43566077450497154\nEpsilon = 0.43561720842752105\nEpsilon = 0.4355736467066783\nEpsilon = 0.43553008934200765\nEpsilon = 0.43548653633307344\nEpsilon = 0.4354429876794401\nEpsilon = 0.43539944338067216\nEpsilon = 0.43535590343633407\nEpsilon = 0.4353123678459904\nEpsilon = 0.43526883660920584\nEpsilon = 0.4352253097255449\nEpsilon = 0.43518178719457234\nEpsilon = 0.4351382690158529\nEpsilon = 0.4350947551889513\nEpsilon = 0.4350512457134324\nEpsilon = 0.43500774058886105\nAgent: ddqn_agent . Episode 462/2000. Number of steps to finish: 20. Loss: 14.17320442199707 Reward: -14.0\nEpsilon = 0.43496423981480214\nEpsilon = 0.43492074339082065\nEpsilon = 0.4348772513164816\nEpsilon = 0.43483376359134995\nEpsilon = 0.43479028021499083\nEpsilon = 0.43474680118696934\nEpsilon = 0.4347033265068507\nEpsilon = 0.4346598561742\nEpsilon = 0.43461639018858256\nEpsilon = 0.4345729285495637\nEpsilon = 0.43452947125670877\nEpsilon = 0.4344860183095831\nEpsilon = 0.43444256970775214\nEpsilon = 0.43439912545078135\nEpsilon = 0.4343556855382363\nEpsilon = 0.4343122499696825\nEpsilon = 0.43426881874468554\nEpsilon = 0.43422539186281106\nEpsilon = 0.4341819693236248\nEpsilon = 0.4341385511266924\nAgent: ddqn_agent . Episode 463/2000. Number of steps to finish: 20. Loss: 14.423212051391602 Reward: -10.0\nEpsilon = 0.4340951372715798\nEpsilon = 0.4340517277578526\nEpsilon = 0.4340083225850768\nEpsilon = 0.4339649217528183\nEpsilon = 0.43392152526064304\nEpsilon = 0.43387813310811696\nEpsilon = 0.43383474529480615\nEpsilon = 0.43379136182027667\nEpsilon = 0.43374798268409465\nEpsilon = 0.43370460788582627\nEpsilon = 0.4336612374250377\nEpsilon = 0.4336178713012952\nAgent: ddqn_agent . Episode 464/2000. Number of steps to finish: 12. Loss: 9.011970520019531 Reward: 0.0\nEpsilon = 0.4335745095141651\nEpsilon = 0.4335311520632137\nEpsilon = 0.4334877989480074\nEpsilon = 0.43344445016811256\nEpsilon = 0.43340110572309576\nEpsilon = 0.4333577656125234\nEpsilon = 0.4333144298359622\nEpsilon = 0.4332710983929786\nEpsilon = 0.43322777128313933\nEpsilon = 0.433184448506011\nEpsilon = 0.4331411300611604\nEpsilon = 0.4330978159481543\nEpsilon = 0.43305450616655944\nEpsilon = 0.4330112007159428\nEpsilon = 0.4329678995958712\nEpsilon = 0.4329246028059116\nEpsilon = 0.43288131034563104\nEpsilon = 0.4328380222145965\nEpsilon = 0.43279473841237504\nAgent: ddqn_agent . Episode 465/2000. Number of steps to finish: 19. Loss: 13.45670223236084 Reward: -7.0\nEpsilon = 0.4327514589385338\nEpsilon = 0.4327081837926399\nEpsilon = 0.4326649129742607\nEpsilon = 0.43262164648296325\nEpsilon = 0.43257838431831497\nEpsilon = 0.4325351264798831\nEpsilon = 0.4324918729672351\nEpsilon = 0.4324486237799384\nEpsilon = 0.4324053789175604\nEpsilon = 0.4323621383796687\nEpsilon = 0.43231890216583074\nEpsilon = 0.43227567027561414\nEpsilon = 0.4322324427085866\nEpsilon = 0.4321892194643157\nEpsilon = 0.4321460005423693\nEpsilon = 0.43210278594231505\nEpsilon = 0.43205957566372083\nEpsilon = 0.4320163697061545\nEpsilon = 0.43197316806918384\nEpsilon = 0.43192997075237693\nAgent: ddqn_agent . Episode 466/2000. Number of steps to finish: 20. Loss: 13.79913330078125 Reward: -14.0\nEpsilon = 0.4318867777553017\nEpsilon = 0.4318435890775262\nEpsilon = 0.4318004047186184\nEpsilon = 0.43175722467814653\nEpsilon = 0.4317140489556787\nEpsilon = 0.43167087755078315\nEpsilon = 0.43162771046302806\nEpsilon = 0.43158454769198173\nEpsilon = 0.4315413892372125\nEpsilon = 0.43149823509828883\nEpsilon = 0.431455085274779\nEpsilon = 0.43141193976625153\nEpsilon = 0.4313687985722749\nEpsilon = 0.43132566169241765\nEpsilon = 0.4312825291262484\nEpsilon = 0.4312394008733358\nEpsilon = 0.43119627693324847\nEpsilon = 0.43115315730555515\nEpsilon = 0.4311100419898246\nEpsilon = 0.43106693098562565\nAgent: ddqn_agent . Episode 467/2000. Number of steps to finish: 20. Loss: 14.63725471496582 Reward: -14.0\nEpsilon = 0.4310238242925271\nEpsilon = 0.43098072191009784\nEpsilon = 0.43093762383790685\nEpsilon = 0.4308945300755231\nEpsilon = 0.4308514406225155\nEpsilon = 0.43080835547845325\nEpsilon = 0.4307652746429054\nEpsilon = 0.43072219811544116\nEpsilon = 0.4306791258956296\nEpsilon = 0.43063605798304005\nEpsilon = 0.43059299437724174\nEpsilon = 0.43054993507780404\nEpsilon = 0.4305068800842963\nEpsilon = 0.43046382939628786\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.43042078301334824\nEpsilon = 0.43037774093504694\nEpsilon = 0.43033470316095346\nEpsilon = 0.43029166969063737\nEpsilon = 0.4302486405236683\nEpsilon = 0.4302056156596159\nAgent: ddqn_agent . Episode 468/2000. Number of steps to finish: 20. Loss: 13.939615249633789 Reward: -12.0\nEpsilon = 0.43016259509804994\nEpsilon = 0.43011957883854013\nEpsilon = 0.43007656688065626\nEpsilon = 0.4300335592239682\nEpsilon = 0.4299905558680458\nEpsilon = 0.429947556812459\nEpsilon = 0.42990456205677774\nEpsilon = 0.4298615716005721\nEpsilon = 0.42981858544341206\nEpsilon = 0.4297756035848677\nEpsilon = 0.42973262602450923\nEpsilon = 0.4296896527619068\nEpsilon = 0.4296466837966306\nEpsilon = 0.42960371912825096\nEpsilon = 0.42956075875633815\nEpsilon = 0.4295178026804625\nEpsilon = 0.4294748509001945\nEpsilon = 0.42943190341510445\nEpsilon = 0.42938896022476297\nEpsilon = 0.4293460213287405\nAgent: ddqn_agent . Episode 469/2000. Number of steps to finish: 20. Loss: 14.045714378356934 Reward: -12.0\nEpsilon = 0.4293030867266076\nEpsilon = 0.42926015641793497\nEpsilon = 0.4292172304022932\nEpsilon = 0.429174308679253\nEpsilon = 0.42913139124838506\nEpsilon = 0.4290884781092602\nEpsilon = 0.42904556926144927\nEpsilon = 0.42900266470452314\nEpsilon = 0.42895976443805267\nEpsilon = 0.42891686846160887\nEpsilon = 0.4288739767747627\nEpsilon = 0.4288310893770852\nEpsilon = 0.4287882062681475\nEpsilon = 0.4287453274475207\nEpsilon = 0.42870245291477593\nEpsilon = 0.42865958266948445\nEpsilon = 0.4286167167112175\nEpsilon = 0.42857385503954637\nEpsilon = 0.4285309976540424\nAgent: ddqn_agent . Episode 470/2000. Number of steps to finish: 19. Loss: 13.267780303955078 Reward: -7.0\nEpsilon = 0.42848814455427703\nEpsilon = 0.4284452957398216\nEpsilon = 0.4284024512102476\nEpsilon = 0.4283596109651266\nEpsilon = 0.4283167750040301\nEpsilon = 0.4282739433265297\nEpsilon = 0.42823111593219704\nEpsilon = 0.42818829282060383\nEpsilon = 0.4281454739913218\nEpsilon = 0.4281026594439227\nEpsilon = 0.4280598491779783\nEpsilon = 0.4280170431930605\nEpsilon = 0.4279742414887412\nEpsilon = 0.4279314440645923\nEpsilon = 0.42788865092018585\nEpsilon = 0.42784586205509384\nEpsilon = 0.42780307746888835\nEpsilon = 0.42776029716114145\nEpsilon = 0.42771752113142536\nAgent: ddqn_agent . Episode 471/2000. Number of steps to finish: 19. Loss: 13.318214416503906 Reward: -7.0\nEpsilon = 0.4276747493793122\nEpsilon = 0.4276319819043743\nEpsilon = 0.4275892187061839\nEpsilon = 0.42754645978431327\nEpsilon = 0.42750370513833486\nEpsilon = 0.42746095476782103\nEpsilon = 0.42741820867234426\nEpsilon = 0.42737546685147704\nEpsilon = 0.4273327293047919\nEpsilon = 0.4272899960318614\nEpsilon = 0.42724726703225824\nEpsilon = 0.427204542305555\nEpsilon = 0.42716182185132445\nEpsilon = 0.4271191056691393\nEpsilon = 0.42707639375857237\nEpsilon = 0.4270336861191965\nEpsilon = 0.4269909827505846\nEpsilon = 0.4269482836523096\nEpsilon = 0.42690558882394436\nEpsilon = 0.42686289826506196\nAgent: ddqn_agent . Episode 472/2000. Number of steps to finish: 20. Loss: 14.146051406860352 Reward: -14.0\nEpsilon = 0.42682021197523545\nEpsilon = 0.42677752995403795\nEpsilon = 0.42673485220104257\nEpsilon = 0.4266921787158225\nEpsilon = 0.4266495094979509\nEpsilon = 0.4266068445470011\nEpsilon = 0.4265641838625464\nEpsilon = 0.42652152744416016\nEpsilon = 0.42647887529141576\nEpsilon = 0.42643622740388665\nEpsilon = 0.4263935837811463\nEpsilon = 0.42635094442276816\nEpsilon = 0.4263083093283259\nEpsilon = 0.4262656784973931\nEpsilon = 0.42622305192954335\nEpsilon = 0.4261804296243504\nEpsilon = 0.42613781158138797\nEpsilon = 0.42609519780022986\nEpsilon = 0.42605258828044984\nEpsilon = 0.4260099830216218\nAgent: ddqn_agent . Episode 473/2000. Number of steps to finish: 20. Loss: 14.02349853515625 Reward: -14.0\nEpsilon = 0.42596738202331963\nEpsilon = 0.4259247852851173\nEpsilon = 0.4258821928065888\nEpsilon = 0.42583960458730813\nEpsilon = 0.4257970206268494\nEpsilon = 0.4257544409247867\nEpsilon = 0.42571186548069423\nEpsilon = 0.4256692942941462\nEpsilon = 0.4256267273647168\nEpsilon = 0.42558416469198035\nEpsilon = 0.42554160627551113\nEpsilon = 0.4254990521148836\nEpsilon = 0.4254565022096721\nEpsilon = 0.4254139565594512\nEpsilon = 0.42537141516379523\nEpsilon = 0.42532887802227887\nEpsilon = 0.42528634513447666\nEpsilon = 0.42524381649996323\nEpsilon = 0.42520129211831326\nEpsilon = 0.4251587719891014\nAgent: ddqn_agent . Episode 474/2000. Number of steps to finish: 20. Loss: 14.177395820617676 Reward: -20.0\nEpsilon = 0.4251162561119025\nEpsilon = 0.4250737444862913\nEpsilon = 0.4250312371118427\nEpsilon = 0.42498873398813153\nEpsilon = 0.42494623511473273\nEpsilon = 0.42490374049122126\nEpsilon = 0.42486125011717213\nEpsilon = 0.4248187639921604\nEpsilon = 0.4247762821157612\nEpsilon = 0.4247338044875496\nEpsilon = 0.42469133110710083\nEpsilon = 0.42464886197399015\nEpsilon = 0.42460639708779274\nEpsilon = 0.424563936448084\nEpsilon = 0.4245214800544392\nEpsilon = 0.42447902790643377\nEpsilon = 0.42443658000364315\nEpsilon = 0.42439413634564277\nEpsilon = 0.4243516969320082\nEpsilon = 0.424309261762315\nAgent: ddqn_agent . Episode 475/2000. Number of steps to finish: 20. Loss: 14.192730903625488 Reward: -18.0\nEpsilon = 0.42426683083613875\nEpsilon = 0.4242244041530551\nEpsilon = 0.4241819817126398\nEpsilon = 0.4241395635144686\nEpsilon = 0.4240971495581171\nEpsilon = 0.42405473984316133\nEpsilon = 0.424012334369177\nEpsilon = 0.4239699331357401\nEpsilon = 0.42392753614242656\nEpsilon = 0.42388514338881234\nEpsilon = 0.4238427548744735\nEpsilon = 0.42380037059898606\nEpsilon = 0.42375799056192615\nEpsilon = 0.42371561476286995\nEpsilon = 0.42367324320139366\nEpsilon = 0.42363087587707354\nEpsilon = 0.42358851278948584\nEpsilon = 0.4235461539382069\nEpsilon = 0.42350379932281307\nEpsilon = 0.4234614489428808\nAgent: ddqn_agent . Episode 476/2000. Number of steps to finish: 20. Loss: 14.328362464904785 Reward: -10.0\nEpsilon = 0.4234191027979865\nEpsilon = 0.4233767608877067\nEpsilon = 0.42333442321161796\nEpsilon = 0.42329208976929683\nEpsilon = 0.42324976056031993\nEpsilon = 0.4232074355842639\nEpsilon = 0.42316511484070546\nEpsilon = 0.4231227983292214\nEpsilon = 0.42308048604938847\nEpsilon = 0.4230381780007835\nEpsilon = 0.42299587418298346\nEpsilon = 0.42295357459556515\nEpsilon = 0.4229112792381056\nEpsilon = 0.4228689881101818\nEpsilon = 0.4228267012113708\nEpsilon = 0.42278441854124965\nEpsilon = 0.42274214009939554\nEpsilon = 0.4226998658853856\nEpsilon = 0.4226575958987971\nEpsilon = 0.4226153301392072\nAgent: ddqn_agent . Episode 477/2000. Number of steps to finish: 20. Loss: 13.832587242126465 Reward: -12.0\nEpsilon = 0.4225730686061933\nEpsilon = 0.4225308112993327\nEpsilon = 0.42248855821820275\nEpsilon = 0.4224463093623809\nEpsilon = 0.4224040647314447\nEpsilon = 0.42236182432497155\nEpsilon = 0.42231958814253906\nEpsilon = 0.4222773561837248\nEpsilon = 0.42223512844810646\nEpsilon = 0.4221929049352616\nEpsilon = 0.4221506856447681\nEpsilon = 0.4221084705762036\nEpsilon = 0.42206625972914597\nEpsilon = 0.42202405310317304\nEpsilon = 0.4219818506978627\nEpsilon = 0.4219396525127929\nEpsilon = 0.4218974585475416\nEpsilon = 0.4218552688016869\nEpsilon = 0.4218130832748067\nEpsilon = 0.42177090196647926\nAgent: ddqn_agent . Episode 478/2000. Number of steps to finish: 20. Loss: 14.176593780517578 Reward: -12.0\nEpsilon = 0.4217287248762826\nEpsilon = 0.42168655200379496\nEpsilon = 0.4216443833485946\nEpsilon = 0.42160221891025973\nEpsilon = 0.4215600586883687\nEpsilon = 0.4215179026824999\nEpsilon = 0.42147575089223166\nEpsilon = 0.42143360331714247\nEpsilon = 0.42139145995681077\nEpsilon = 0.4213493208108151\nEpsilon = 0.42130718587873406\nEpsilon = 0.4212650551601462\nEpsilon = 0.4212229286546302\nEpsilon = 0.42118080636176475\nAgent: ddqn_agent . Episode 479/2000. Number of steps to finish: 14. Loss: 9.936826705932617 Reward: -2.0\nEpsilon = 0.4211386882811286\nEpsilon = 0.4210965744123005\nEpsilon = 0.42105446475485925\nEpsilon = 0.42101235930838377\nEpsilon = 0.4209702580724529\nEpsilon = 0.42092816104664565\nEpsilon = 0.420886068230541\nEpsilon = 0.42084397962371795\nEpsilon = 0.42080189522575556\nEpsilon = 0.420759815036233\nEpsilon = 0.42071773905472937\nEpsilon = 0.4206756672808239\nEpsilon = 0.4206335997140958\nEpsilon = 0.4205915363541244\nEpsilon = 0.42054947720048896\nEpsilon = 0.42050742225276894\nEpsilon = 0.42046537151054364\nEpsilon = 0.4204233249733926\nEpsilon = 0.42038128264089525\nEpsilon = 0.42033924451263116\nAgent: ddqn_agent . Episode 480/2000. Number of steps to finish: 20. Loss: 14.523201942443848 Reward: -14.0\nEpsilon = 0.4202972105881799\nEpsilon = 0.4202551808671211\nEpsilon = 0.4202131553490344\nEpsilon = 0.42017113403349954\nEpsilon = 0.4201291169200962\nEpsilon = 0.4200871040084042\nEpsilon = 0.4200450952980034\nEpsilon = 0.4200030907884736\nEpsilon = 0.4199610904793947\nEpsilon = 0.4199190943703468\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.41987710246090976\nEpsilon = 0.41983511475066365\nEpsilon = 0.4197931312391886\nEpsilon = 0.41975115192606466\nAgent: ddqn_agent . Episode 481/2000. Number of steps to finish: 14. Loss: 10.183113098144531 Reward: -2.0\nEpsilon = 0.41970917681087205\nEpsilon = 0.41966720589319095\nEpsilon = 0.41962523917260164\nEpsilon = 0.4195832766486844\nEpsilon = 0.41954131832101954\nEpsilon = 0.41949936418918743\nEpsilon = 0.4194574142527685\nEpsilon = 0.4194154685113432\nEpsilon = 0.4193735269644921\nEpsilon = 0.4193315896117957\nEpsilon = 0.4192896564528345\nEpsilon = 0.41924772748718925\nEpsilon = 0.4192058027144405\nEpsilon = 0.4191638821341691\nEpsilon = 0.41912196574595567\nEpsilon = 0.4190800535493811\nEpsilon = 0.4190381455440262\nEpsilon = 0.4189962417294718\nEpsilon = 0.41895434210529886\nEpsilon = 0.41891244667108835\nAgent: ddqn_agent . Episode 482/2000. Number of steps to finish: 20. Loss: 14.497739791870117 Reward: -18.0\nEpsilon = 0.41887055542642126\nEpsilon = 0.4188286683708786\nEpsilon = 0.4187867855040415\nEpsilon = 0.41874490682549115\nEpsilon = 0.4187030323348086\nEpsilon = 0.4186611620315751\nEpsilon = 0.41861929591537195\nEpsilon = 0.4185774339857804\nEpsilon = 0.4185355762423818\nEpsilon = 0.41849372268475754\nAgent: ddqn_agent . Episode 483/2000. Number of steps to finish: 10. Loss: 7.0315165519714355 Reward: 2.0\nEpsilon = 0.41845187331248906\nEpsilon = 0.4184100281251578\nEpsilon = 0.4183681871223453\nEpsilon = 0.4183263503036331\nEpsilon = 0.4182845176686027\nEpsilon = 0.41824268921683583\nEpsilon = 0.41820086494791414\nEpsilon = 0.41815904486141936\nEpsilon = 0.4181172289569332\nEpsilon = 0.4180754172340375\nEpsilon = 0.4180336096923141\nEpsilon = 0.4179918063313449\nEpsilon = 0.4179500071507118\nEpsilon = 0.41790821214999674\nAgent: ddqn_agent . Episode 484/2000. Number of steps to finish: 14. Loss: 9.945723533630371 Reward: -2.0\nEpsilon = 0.41786642132878177\nEpsilon = 0.4178246346866489\nEpsilon = 0.41778285222318023\nEpsilon = 0.4177410739379579\nEpsilon = 0.4176992998305641\nEpsilon = 0.417657529900581\nEpsilon = 0.417615764147591\nEpsilon = 0.4175740025711762\nEpsilon = 0.4175322451709191\nEpsilon = 0.417490491946402\nEpsilon = 0.41744874289720735\nEpsilon = 0.41740699802291764\nEpsilon = 0.41736525732311536\nEpsilon = 0.41732352079738305\nEpsilon = 0.41728178844530334\nEpsilon = 0.4172400602664588\nEpsilon = 0.4171983362604322\nEpsilon = 0.4171566164268061\nEpsilon = 0.41711490076516344\nEpsilon = 0.4170731892750869\nAgent: ddqn_agent . Episode 485/2000. Number of steps to finish: 20. Loss: 14.628789901733398 Reward: -10.0\nEpsilon = 0.4170314819561594\nEpsilon = 0.4169897788079638\nEpsilon = 0.416948079830083\nEpsilon = 0.4169063850221\nEpsilon = 0.4168646943835978\nEpsilon = 0.41682300791415944\nEpsilon = 0.416781325613368\nEpsilon = 0.41673964748080666\nEpsilon = 0.4166979735160586\nEpsilon = 0.416656303718707\nAgent: ddqn_agent . Episode 486/2000. Number of steps to finish: 10. Loss: 7.348334312438965 Reward: 2.0\nEpsilon = 0.4166146380883351\nEpsilon = 0.41657297662452625\nEpsilon = 0.4165313193268638\nEpsilon = 0.41648966619493116\nEpsilon = 0.41644801722831165\nEpsilon = 0.41640637242658884\nEpsilon = 0.4163647317893462\nEpsilon = 0.41632309531616724\nEpsilon = 0.4162814630066356\nEpsilon = 0.41623983486033495\nEpsilon = 0.41619821087684894\nEpsilon = 0.41615659105576125\nEpsilon = 0.41611497539665565\nEpsilon = 0.416073363899116\nEpsilon = 0.41603175656272606\nEpsilon = 0.4159901533870698\nEpsilon = 0.4159485543717311\nEpsilon = 0.415906959516294\nEpsilon = 0.41586536882034236\nEpsilon = 0.41582378228346034\nAgent: ddqn_agent . Episode 487/2000. Number of steps to finish: 20. Loss: 14.343818664550781 Reward: -12.0\nEpsilon = 0.415782199905232\nEpsilon = 0.4157406216852415\nEpsilon = 0.41569904762307297\nEpsilon = 0.4156574777183107\nEpsilon = 0.4156159119705389\nEpsilon = 0.4155743503793418\nEpsilon = 0.4155327929443039\nEpsilon = 0.41549123966500945\nEpsilon = 0.41544969054104297\nEpsilon = 0.41540814557198885\nEpsilon = 0.41536660475743165\nEpsilon = 0.4153250680969559\nEpsilon = 0.4152835355901462\nEpsilon = 0.41524200723658716\nEpsilon = 0.4152004830358635\nEpsilon = 0.4151589629875599\nEpsilon = 0.41511744709126114\nEpsilon = 0.415075935346552\nEpsilon = 0.41503442775301735\nEpsilon = 0.41499292431024204\nAgent: ddqn_agent . Episode 488/2000. Number of steps to finish: 20. Loss: 14.413090705871582 Reward: -14.0\nEpsilon = 0.414951425017811\nEpsilon = 0.4149099298753092\nEpsilon = 0.4148684388823217\nEpsilon = 0.4148269520384335\nEpsilon = 0.4147854693432297\nEpsilon = 0.41474399079629537\nEpsilon = 0.4147025163972157\nEpsilon = 0.41466104614557603\nEpsilon = 0.4146195800409615\nEpsilon = 0.4145781180829574\nEpsilon = 0.4145366602711491\nEpsilon = 0.414495206605122\nEpsilon = 0.4144537570844615\nEpsilon = 0.4144123117087531\nEpsilon = 0.4143708704775822\nEpsilon = 0.41432943339053446\nEpsilon = 0.41428800044719544\nEpsilon = 0.41424657164715073\nEpsilon = 0.414205146989986\nEpsilon = 0.414163726475287\nAgent: ddqn_agent . Episode 489/2000. Number of steps to finish: 20. Loss: 14.071635246276855 Reward: -10.0\nEpsilon = 0.41412231010263945\nEpsilon = 0.41408089787162916\nEpsilon = 0.414039489781842\nEpsilon = 0.41399808583286385\nEpsilon = 0.41395668602428054\nEpsilon = 0.4139152903556781\nEpsilon = 0.41387389882664255\nEpsilon = 0.41383251143675986\nEpsilon = 0.4137911281856162\nEpsilon = 0.41374974907279766\nEpsilon = 0.4137083740978904\nEpsilon = 0.4136670032604806\nEpsilon = 0.41362563656015455\nEpsilon = 0.41358427399649855\nEpsilon = 0.4135429155690989\nEpsilon = 0.413501561277542\nEpsilon = 0.41346021112141423\nEpsilon = 0.4134188651003021\nEpsilon = 0.4133775232137921\nEpsilon = 0.4133361854614707\nAgent: ddqn_agent . Episode 490/2000. Number of steps to finish: 20. Loss: 14.200531005859375 Reward: -10.0\nEpsilon = 0.4132948518429246\nEpsilon = 0.4132535223577403\nEpsilon = 0.41321219700550454\nEpsilon = 0.413170875785804\nEpsilon = 0.41312955869822543\nEpsilon = 0.4130882457423556\nEpsilon = 0.4130469369177814\nEpsilon = 0.4130056322240896\nEpsilon = 0.4129643316608672\nEpsilon = 0.41292303522770113\nEpsilon = 0.41288174292417834\nEpsilon = 0.41284045474988595\nAgent: ddqn_agent . Episode 491/2000. Number of steps to finish: 12. Loss: 8.333504676818848 Reward: 0.0\nEpsilon = 0.41279917070441097\nEpsilon = 0.41275789078734054\nEpsilon = 0.4127166149982618\nEpsilon = 0.412675343336762\nEpsilon = 0.41263407580242834\nEpsilon = 0.4125928123948481\nEpsilon = 0.4125515531136086\nEpsilon = 0.41251029795829725\nEpsilon = 0.4124690469285014\nEpsilon = 0.4124278000238085\nEpsilon = 0.41238655724380613\nEpsilon = 0.41234531858808177\nEpsilon = 0.412304084056223\nEpsilon = 0.4122628536478174\nEpsilon = 0.4122216273624526\nEpsilon = 0.4121804051997164\nEpsilon = 0.4121391871591964\nEpsilon = 0.4120979732404805\nEpsilon = 0.41205676344315645\nEpsilon = 0.41201555776681215\nAgent: ddqn_agent . Episode 492/2000. Number of steps to finish: 20. Loss: 14.573153495788574 Reward: -10.0\nEpsilon = 0.41197435621103545\nEpsilon = 0.41193315877541437\nEpsilon = 0.4118919654595368\nEpsilon = 0.4118507762629909\nEpsilon = 0.4118095911853646\nEpsilon = 0.41176841022624605\nEpsilon = 0.41172723338522343\nEpsilon = 0.4116860606618849\nEpsilon = 0.4116448920558187\nEpsilon = 0.4116037275666131\nEpsilon = 0.4115625671938565\nEpsilon = 0.4115214109371371\nEpsilon = 0.4114802587960434\nEpsilon = 0.41143911077016376\nEpsilon = 0.41139796685908675\nEpsilon = 0.41135682706240084\nEpsilon = 0.4113156913796946\nEpsilon = 0.4112745598105566\nEpsilon = 0.41123343235457555\nEpsilon = 0.4111923090113401\nAgent: ddqn_agent . Episode 493/2000. Number of steps to finish: 20. Loss: 13.954858779907227 Reward: -14.0\nEpsilon = 0.41115118978043896\nEpsilon = 0.41111007466146093\nEpsilon = 0.4110689636539948\nEpsilon = 0.41102785675762943\nEpsilon = 0.41098675397195367\nEpsilon = 0.41094565529655647\nEpsilon = 0.4109045607310268\nEpsilon = 0.41086347027495373\nEpsilon = 0.41082238392792625\nEpsilon = 0.4107813016895335\nEpsilon = 0.4107402235593645\nEpsilon = 0.4106991495370086\nEpsilon = 0.4106580796220549\nEpsilon = 0.4106170138140927\nEpsilon = 0.4105759521127113\nEpsilon = 0.4105348945175\nEpsilon = 0.41049384102804826\nEpsilon = 0.41045279164394544\nEpsilon = 0.41041174636478106\nEpsilon = 0.41037070519014457\nAgent: ddqn_agent . Episode 494/2000. Number of steps to finish: 20. Loss: 14.567184448242188 Reward: -10.0\nEpsilon = 0.41032966811962557\nEpsilon = 0.4102886351528136\nEpsilon = 0.41024760628929835\nEpsilon = 0.4102065815286694\nEpsilon = 0.41016556087051653\nEpsilon = 0.41012454431442946\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.410083531859998\nEpsilon = 0.41004252350681203\nEpsilon = 0.4100015192544614\nAgent: ddqn_agent . Episode 495/2000. Number of steps to finish: 9. Loss: 6.604800224304199 Reward: 3.0\nEpsilon = 0.40996051910253595\nEpsilon = 0.4099195230506257\nEpsilon = 0.4098785310983206\nEpsilon = 0.40983754324521077\nEpsilon = 0.4097965594908862\nEpsilon = 0.40975557983493716\nEpsilon = 0.40971460427695366\nEpsilon = 0.40967363281652597\nEpsilon = 0.40963266545324434\nEpsilon = 0.409591702186699\nEpsilon = 0.4095507430164803\nEpsilon = 0.4095097879421787\nEpsilon = 0.4094688369633845\nEpsilon = 0.40942789007968816\nEpsilon = 0.4093869472906802\nEpsilon = 0.40934600859595116\nEpsilon = 0.4093050739950916\nEpsilon = 0.4092641434876921\nEpsilon = 0.4092232170733433\nEpsilon = 0.409182294751636\nAgent: ddqn_agent . Episode 496/2000. Number of steps to finish: 20. Loss: 14.538827896118164 Reward: -12.0\nEpsilon = 0.4091413765221608\nEpsilon = 0.4091004623845086\nEpsilon = 0.4090595523382701\nEpsilon = 0.4090186463830363\nEpsilon = 0.408977744518398\nEpsilon = 0.4089368467439462\nEpsilon = 0.4088959530592718\nEpsilon = 0.4088550634639659\nEpsilon = 0.4088141779576195\nEpsilon = 0.40877329653982375\nEpsilon = 0.4087324192101698\nEpsilon = 0.40869154596824875\nEpsilon = 0.4086506768136519\nEpsilon = 0.40860981174597055\nEpsilon = 0.40856895076479594\nEpsilon = 0.40852809386971944\nAgent: ddqn_agent . Episode 497/2000. Number of steps to finish: 16. Loss: 11.586219787597656 Reward: -4.0\nEpsilon = 0.40848724106033246\nEpsilon = 0.4084463923362264\nEpsilon = 0.40840554769699283\nEpsilon = 0.4083647071422231\nEpsilon = 0.4083238706715089\nEpsilon = 0.40828303828444173\nEpsilon = 0.4082422099806133\nEpsilon = 0.40820138575961523\nEpsilon = 0.40816056562103925\nEpsilon = 0.40811974956447716\nEpsilon = 0.40807893758952074\nEpsilon = 0.4080381296957618\nEpsilon = 0.40799732588279225\nEpsilon = 0.407956526150204\nEpsilon = 0.407915730497589\nEpsilon = 0.40787493892453924\nEpsilon = 0.4078341514306468\nEpsilon = 0.4077933680155037\nEpsilon = 0.4077525886787022\nEpsilon = 0.4077118134198343\nAgent: ddqn_agent . Episode 498/2000. Number of steps to finish: 20. Loss: 14.130315780639648 Reward: -10.0\nEpsilon = 0.4076710422384923\nEpsilon = 0.40763027513426847\nEpsilon = 0.40758951210675504\nEpsilon = 0.40754875315554434\nEpsilon = 0.40750799828022877\nEpsilon = 0.40746724748040075\nEpsilon = 0.40742650075565273\nEpsilon = 0.40738575810557714\nEpsilon = 0.4073450195297666\nEpsilon = 0.40730428502781363\nEpsilon = 0.40726355459931085\nEpsilon = 0.4072228282438509\nEpsilon = 0.40718210596102655\nEpsilon = 0.40714138775043046\nEpsilon = 0.4071006736116554\nEpsilon = 0.40705996354429425\nEpsilon = 0.40701925754793983\nEpsilon = 0.40697855562218505\nEpsilon = 0.40693785776662283\nEpsilon = 0.4068971639808462\nAgent: ddqn_agent . Episode 499/2000. Number of steps to finish: 20. Loss: 14.327680587768555 Reward: -18.0\nEpsilon = 0.4068564742644481\nEpsilon = 0.40681578861702167\nEpsilon = 0.40677510703815994\nEpsilon = 0.40673442952745614\nEpsilon = 0.4066937560845034\nEpsilon = 0.406653086708895\nEpsilon = 0.40661242140022413\nEpsilon = 0.4065717601580841\nEpsilon = 0.4065311029820683\nEpsilon = 0.4064904498717701\nEpsilon = 0.4064498008267829\nEpsilon = 0.40640915584670023\nEpsilon = 0.4063685149311156\nEpsilon = 0.4063278780796225\nEpsilon = 0.40628724529181454\nEpsilon = 0.40624661656728533\nEpsilon = 0.4062059919056286\nEpsilon = 0.40616537130643804\nEpsilon = 0.4061247547693074\nEpsilon = 0.40608414229383044\nAgent: ddqn_agent . Episode 500/2000. Number of steps to finish: 20. Loss: 14.426464080810547 Reward: -10.0\nEpsilon = 0.40604353387960107\nEpsilon = 0.40600292952621314\nEpsilon = 0.40596232923326053\nEpsilon = 0.40592173300033724\nEpsilon = 0.4058811408270372\nEpsilon = 0.4058405527129545\nEpsilon = 0.4057999686576832\nEpsilon = 0.40575938866081745\nEpsilon = 0.40571881272195137\nEpsilon = 0.40567824084067916\nEpsilon = 0.4056376730165951\nEpsilon = 0.40559710924929343\nEpsilon = 0.4055565495383685\nEpsilon = 0.4055159938834147\nEpsilon = 0.4054754422840264\nEpsilon = 0.405434894739798\nEpsilon = 0.405394351250324\nEpsilon = 0.40535381181519897\nEpsilon = 0.40531327643401743\nEpsilon = 0.405272745106374\nAgent: ddqn_agent . Episode 501/2000. Number of steps to finish: 20. Loss: 14.022482872009277 Reward: -20.0\nEpsilon = 0.40523221783186336\nEpsilon = 0.4051916946100802\nEpsilon = 0.4051511754406192\nEpsilon = 0.40511066032307513\nEpsilon = 0.4050701492570428\nEpsilon = 0.4050296422421171\nEpsilon = 0.40498913927789293\nEpsilon = 0.40494864036396516\nEpsilon = 0.40490814549992876\nEpsilon = 0.4048676546853788\nEpsilon = 0.40482716791991025\nEpsilon = 0.4047866852031183\nEpsilon = 0.404746206534598\nEpsilon = 0.40470573191394454\nEpsilon = 0.40466526134075315\nEpsilon = 0.40462479481461905\nEpsilon = 0.4045843323351376\nEpsilon = 0.40454387390190405\nEpsilon = 0.40450341951451385\nEpsilon = 0.4044629691725624\nAgent: ddqn_agent . Episode 502/2000. Number of steps to finish: 20. Loss: 13.792657852172852 Reward: -20.0\nEpsilon = 0.4044225228756452\nEpsilon = 0.4043820806233576\nEpsilon = 0.4043416424152953\nEpsilon = 0.40430120825105376\nEpsilon = 0.40426077813022865\nEpsilon = 0.40422035205241563\nEpsilon = 0.4041799300172104\nEpsilon = 0.4041395120242087\nEpsilon = 0.4040990980730063\nEpsilon = 0.404058688163199\nAgent: ddqn_agent . Episode 503/2000. Number of steps to finish: 10. Loss: 6.9871039390563965 Reward: 2.0\nEpsilon = 0.4040182822943827\nEpsilon = 0.40397788046615324\nEpsilon = 0.40393748267810664\nEpsilon = 0.40389708892983883\nEpsilon = 0.40385669922094586\nEpsilon = 0.40381631355102376\nEpsilon = 0.40377593191966865\nEpsilon = 0.4037355543264767\nEpsilon = 0.40369518077104405\nEpsilon = 0.4036548112529669\nEpsilon = 0.40361444577184163\nAgent: ddqn_agent . Episode 504/2000. Number of steps to finish: 11. Loss: 7.830777168273926 Reward: 1.0\nEpsilon = 0.40357408432726444\nEpsilon = 0.4035337269188317\nEpsilon = 0.40349337354613984\nEpsilon = 0.40345302420878526\nEpsilon = 0.4034126789063644\nEpsilon = 0.40337233763847374\nEpsilon = 0.4033320004047099\nEpsilon = 0.4032916672046694\nEpsilon = 0.40325133803794894\nEpsilon = 0.4032110129041451\nEpsilon = 0.4031706918028547\nEpsilon = 0.4031303747336744\nEpsilon = 0.40309006169620104\nEpsilon = 0.4030497526900314\nEpsilon = 0.4030094477147624\nEpsilon = 0.4029691467699909\nEpsilon = 0.4029288498553139\nEpsilon = 0.4028885569703284\nEpsilon = 0.40284826811463137\nEpsilon = 0.4028079832878199\nAgent: ddqn_agent . Episode 505/2000. Number of steps to finish: 20. Loss: 14.161454200744629 Reward: -14.0\nEpsilon = 0.4027677024894911\nEpsilon = 0.4027274257192422\nEpsilon = 0.4026871529766703\nEpsilon = 0.40264688426137263\nEpsilon = 0.4026066195729465\nEpsilon = 0.40256635891098924\nEpsilon = 0.40252610227509816\nEpsilon = 0.40248584966487067\nEpsilon = 0.4024456010799042\nEpsilon = 0.4024053565197962\nEpsilon = 0.4023651159841442\nEpsilon = 0.4023248794725458\nEpsilon = 0.40228464698459854\nEpsilon = 0.40224441851990006\nEpsilon = 0.4022041940780481\nEpsilon = 0.40216397365864026\nEpsilon = 0.4021237572612744\nEpsilon = 0.4020835448855483\nEpsilon = 0.4020433365310598\nEpsilon = 0.4020031321974067\nAgent: ddqn_agent . Episode 506/2000. Number of steps to finish: 20. Loss: 14.25037670135498 Reward: -10.0\nEpsilon = 0.40196293188418697\nEpsilon = 0.40192273559099856\nEpsilon = 0.40188254331743944\nEpsilon = 0.4018423550631077\nEpsilon = 0.40180217082760145\nEpsilon = 0.4017619906105187\nEpsilon = 0.40172181441145766\nEpsilon = 0.4016816422300165\nEpsilon = 0.40164147406579354\nEpsilon = 0.40160130991838694\nEpsilon = 0.4015611497873951\nEpsilon = 0.40152099367241634\nEpsilon = 0.4014808415730491\nEpsilon = 0.4014406934888918\nEpsilon = 0.4014005494195429\nEpsilon = 0.40136040936460093\nEpsilon = 0.4013202733236645\nEpsilon = 0.4012801412963321\nEpsilon = 0.4012400132822025\nEpsilon = 0.4011998892808743\nAgent: ddqn_agent . Episode 507/2000. Number of steps to finish: 20. Loss: 14.243361473083496 Reward: -14.0\nEpsilon = 0.40115976929194624\nEpsilon = 0.40111965331501703\nEpsilon = 0.40107954134968554\nEpsilon = 0.4010394333955506\nEpsilon = 0.40099932945221106\nEpsilon = 0.4009592295192658\nEpsilon = 0.4009191335963139\nEpsilon = 0.4008790416829543\nEpsilon = 0.400838953778786\nEpsilon = 0.4007988698834081\nEpsilon = 0.4007587899964198\nEpsilon = 0.40071871411742016\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.4006786422460084\nEpsilon = 0.4006385743817838\nEpsilon = 0.40059851052434564\nEpsilon = 0.4005584506732932\nEpsilon = 0.4005183948282259\nEpsilon = 0.4004783429887431\nEpsilon = 0.4004382951544442\nEpsilon = 0.40039825132492873\nAgent: ddqn_agent . Episode 508/2000. Number of steps to finish: 20. Loss: 14.29493522644043 Reward: -10.0\nEpsilon = 0.4003582114997962\nEpsilon = 0.40031817567864625\nEpsilon = 0.4002781438610784\nEpsilon = 0.4002381160466923\nEpsilon = 0.4001980922350876\nEpsilon = 0.4001580724258641\nEpsilon = 0.40011805661862154\nEpsilon = 0.40007804481295967\nEpsilon = 0.4000380370084784\nEpsilon = 0.39999803320477756\nEpsilon = 0.3999580334014571\nEpsilon = 0.39991803759811695\nEpsilon = 0.39987804579435715\nEpsilon = 0.39983805798977773\nEpsilon = 0.3997980741839788\nEpsilon = 0.3997580943765604\nEpsilon = 0.39971811856712275\nEpsilon = 0.39967814675526603\nEpsilon = 0.3996381789405905\nEpsilon = 0.39959821512269644\nAgent: ddqn_agent . Episode 509/2000. Number of steps to finish: 20. Loss: 14.141672134399414 Reward: -14.0\nEpsilon = 0.39955825530118416\nEpsilon = 0.39951829947565404\nEpsilon = 0.3994783476457065\nEpsilon = 0.3994383998109419\nEpsilon = 0.39939845597096085\nEpsilon = 0.39935851612536377\nEpsilon = 0.3993185802737512\nEpsilon = 0.39927864841572386\nEpsilon = 0.3992387205508823\nEpsilon = 0.3991987966788272\nEpsilon = 0.3991588767991593\nEpsilon = 0.39911896091147936\nEpsilon = 0.3990790490153882\nEpsilon = 0.39903914111048666\nEpsilon = 0.39899923719637564\nEpsilon = 0.39895933727265603\nEpsilon = 0.39891944133892876\nEpsilon = 0.39887954939479486\nEpsilon = 0.3988396614398554\nEpsilon = 0.3987997774737114\nAgent: ddqn_agent . Episode 510/2000. Number of steps to finish: 20. Loss: 14.390157699584961 Reward: -12.0\nEpsilon = 0.3987598974959641\nEpsilon = 0.3987200215062145\nEpsilon = 0.3986801495040639\nEpsilon = 0.39864028148911346\nEpsilon = 0.39860041746096453\nEpsilon = 0.3985605574192184\nEpsilon = 0.3985207013634765\nEpsilon = 0.39848084929334016\nEpsilon = 0.3984410012084108\nEpsilon = 0.39840115710829\nEpsilon = 0.39836131699257915\nEpsilon = 0.3983214808608799\nEpsilon = 0.3982816487127938\nEpsilon = 0.39824182054792256\nEpsilon = 0.3982019963658678\nAgent: ddqn_agent . Episode 511/2000. Number of steps to finish: 15. Loss: 10.318617820739746 Reward: -3.0\nEpsilon = 0.3981621761662312\nEpsilon = 0.39812235994861456\nEpsilon = 0.3980825477126197\nEpsilon = 0.39804273945784846\nEpsilon = 0.3980029351839027\nEpsilon = 0.39796313489038426\nEpsilon = 0.3979233385768952\nEpsilon = 0.3978835462430375\nEpsilon = 0.3978437578884132\nEpsilon = 0.39780397351262436\nEpsilon = 0.3977641931152731\nEpsilon = 0.3977244166959616\nEpsilon = 0.397684644254292\nEpsilon = 0.39764487578986657\nEpsilon = 0.3976051113022876\nEpsilon = 0.39756535079115735\nEpsilon = 0.39752559425607825\nEpsilon = 0.39748584169665263\nEpsilon = 0.39744609311248297\nEpsilon = 0.39740634850317175\nAgent: ddqn_agent . Episode 512/2000. Number of steps to finish: 20. Loss: 14.590606689453125 Reward: -18.0\nEpsilon = 0.39736660786832145\nEpsilon = 0.3973268712075346\nEpsilon = 0.39728713852041386\nEpsilon = 0.3972474098065618\nEpsilon = 0.3972076850655812\nEpsilon = 0.3971679642970746\nEpsilon = 0.3971282475006449\nEpsilon = 0.39708853467589483\nEpsilon = 0.39704882582242723\nEpsilon = 0.397009120939845\nEpsilon = 0.396969420027751\nEpsilon = 0.3969297230857482\nEpsilon = 0.39689003011343965\nEpsilon = 0.39685034111042833\nEpsilon = 0.39681065607631727\nEpsilon = 0.3967709750107096\nAgent: ddqn_agent . Episode 513/2000. Number of steps to finish: 16. Loss: 11.165698051452637 Reward: -4.0\nEpsilon = 0.39673129791320855\nEpsilon = 0.39669162478341724\nEpsilon = 0.3966519556209389\nEpsilon = 0.3966122904253768\nEpsilon = 0.3965726291963343\nEpsilon = 0.39653297193341464\nEpsilon = 0.3964933186362213\nEpsilon = 0.39645366930435766\nEpsilon = 0.39641402393742725\nEpsilon = 0.3963743825350335\nEpsilon = 0.39633474509677996\nEpsilon = 0.39629511162227027\nEpsilon = 0.39625548211110806\nEpsilon = 0.39621585656289693\nEpsilon = 0.39617623497724064\nEpsilon = 0.3961366173537429\nEpsilon = 0.3960970036920075\nEpsilon = 0.3960573939916383\nEpsilon = 0.39601778825223916\nEpsilon = 0.39597818647341393\nAgent: ddqn_agent . Episode 514/2000. Number of steps to finish: 20. Loss: 14.415261268615723 Reward: -12.0\nEpsilon = 0.3959385886547666\nEpsilon = 0.3958989947959011\nEpsilon = 0.3958594048964215\nEpsilon = 0.3958198189559319\nEpsilon = 0.3957802369740363\nEpsilon = 0.3957406589503389\nEpsilon = 0.39570108488444383\nEpsilon = 0.3956615147759554\nEpsilon = 0.3956219486244778\nEpsilon = 0.39558238642961535\nEpsilon = 0.3955428281909724\nEpsilon = 0.3955032739081533\nEpsilon = 0.3954637235807625\nEpsilon = 0.3954241772084044\nEpsilon = 0.39538463479068353\nEpsilon = 0.3953450963272045\nAgent: ddqn_agent . Episode 515/2000. Number of steps to finish: 16. Loss: 11.572001457214355 Reward: -4.0\nEpsilon = 0.39530556181757176\nEpsilon = 0.39526603126139\nEpsilon = 0.39522650465826387\nEpsilon = 0.39518698200779806\nEpsilon = 0.39514746330959727\nEpsilon = 0.3951079485632663\nEpsilon = 0.39506843776841\nEpsilon = 0.39502893092463315\nEpsilon = 0.3949894280315407\nEpsilon = 0.39494992908873755\nEpsilon = 0.39491043409582866\nEpsilon = 0.3948709430524191\nEpsilon = 0.39483145595811386\nEpsilon = 0.39479197281251804\nEpsilon = 0.3947524936152368\nEpsilon = 0.39471301836587525\nEpsilon = 0.3946735470640387\nEpsilon = 0.3946340797093323\nEpsilon = 0.39459461630136133\nEpsilon = 0.3945551568397312\nAgent: ddqn_agent . Episode 516/2000. Number of steps to finish: 20. Loss: 14.29157543182373 Reward: -14.0\nEpsilon = 0.39451570132404723\nEpsilon = 0.3944762497539148\nEpsilon = 0.39443680212893945\nEpsilon = 0.39439735844872653\nEpsilon = 0.39435791871288167\nEpsilon = 0.3943184829210104\nEpsilon = 0.3942790510727183\nEpsilon = 0.3942396231676111\nEpsilon = 0.3942001992052943\nEpsilon = 0.3941607791853738\nEpsilon = 0.39412136310745527\nEpsilon = 0.3940819509711445\nEpsilon = 0.3940425427760474\nEpsilon = 0.39400313852176977\nEpsilon = 0.3939637382079176\nEpsilon = 0.3939243418340968\nEpsilon = 0.3938849493999134\nEpsilon = 0.3938455609049734\nEpsilon = 0.39380617634888293\nEpsilon = 0.39376679573124806\nAgent: ddqn_agent . Episode 517/2000. Number of steps to finish: 20. Loss: 13.801061630249023 Reward: -12.0\nEpsilon = 0.39372741905167497\nEpsilon = 0.3936880463097698\nEpsilon = 0.3936486775051388\nEpsilon = 0.3936093126373883\nEpsilon = 0.3935699517061246\nEpsilon = 0.393530594710954\nEpsilon = 0.39349124165148286\nEpsilon = 0.3934518925273177\nEpsilon = 0.39341254733806497\nEpsilon = 0.39337320608333115\nEpsilon = 0.3933338687627228\nEpsilon = 0.3932945353758465\nEpsilon = 0.39325520592230895\nEpsilon = 0.39321588040171673\nEpsilon = 0.3931765588136766\nEpsilon = 0.3931372411577952\nEpsilon = 0.39309792743367944\nEpsilon = 0.3930586176409361\nEpsilon = 0.393019311779172\nEpsilon = 0.3929800098479941\nAgent: ddqn_agent . Episode 518/2000. Number of steps to finish: 20. Loss: 14.14824390411377 Reward: -14.0\nEpsilon = 0.3929407118470093\nEpsilon = 0.3929014177758246\nEpsilon = 0.39286212763404704\nEpsilon = 0.39282284142128365\nEpsilon = 0.3927835591371415\nEpsilon = 0.3927442807812278\nEpsilon = 0.3927050063531497\nEpsilon = 0.3926657358525144\nEpsilon = 0.39262646927892914\nEpsilon = 0.39258720663200125\nEpsilon = 0.39254794791133807\nEpsilon = 0.39250869311654696\nEpsilon = 0.3924694422472353\nEpsilon = 0.39243019530301054\nEpsilon = 0.39239095228348025\nEpsilon = 0.3923517131882519\nEpsilon = 0.3923124780169331\nEpsilon = 0.3922732467691314\nEpsilon = 0.39223401944445446\nEpsilon = 0.39219479604251\nAgent: ddqn_agent . Episode 519/2000. Number of steps to finish: 20. Loss: 14.398241996765137 Reward: -12.0\nEpsilon = 0.3921555765629058\nEpsilon = 0.3921163610052495\nEpsilon = 0.39207714936914895\nEpsilon = 0.392037941654212\nEpsilon = 0.3919987378600466\nEpsilon = 0.3919595379862606\nEpsilon = 0.39192034203246195\nEpsilon = 0.39188114999825874\nEpsilon = 0.3918419618832589\nEpsilon = 0.3918027776870706\nEpsilon = 0.3917635974093019\nEpsilon = 0.39172442104956096\nEpsilon = 0.391685248607456\nEpsilon = 0.39164608008259527\nEpsilon = 0.39160691547458704\nEpsilon = 0.39156775478303957\nEpsilon = 0.3915285980075613\nEpsilon = 0.3914894451477605\nEpsilon = 0.39145029620324573\nEpsilon = 0.3914111511736254\nAgent: ddqn_agent . Episode 520/2000. Number of steps to finish: 20. Loss: 14.164857864379883 Reward: -14.0\nEpsilon = 0.391372010058508\nEpsilon = 0.3913328728575022\nEpsilon = 0.39129373957021646\nEpsilon = 0.39125461019625946\nEpsilon = 0.3912154847352398\nEpsilon = 0.3911763631867663\nEpsilon = 0.3911372455504476\nEpsilon = 0.39109813182589254\nEpsilon = 0.39105902201271\nEpsilon = 0.39101991611050874\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.3909808141188977\nEpsilon = 0.3909417160374858\nEpsilon = 0.39090262186588204\nEpsilon = 0.39086353160369547\nEpsilon = 0.3908244452505351\nEpsilon = 0.39078536280601006\nEpsilon = 0.39074628426972946\nEpsilon = 0.3907072096413025\nEpsilon = 0.39066813892033836\nEpsilon = 0.3906290721064463\nAgent: ddqn_agent . Episode 521/2000. Number of steps to finish: 20. Loss: 14.668281555175781 Reward: -10.0\nEpsilon = 0.3905900091992357\nEpsilon = 0.39055095019831576\nEpsilon = 0.3905118951032959\nEpsilon = 0.3904728439137856\nEpsilon = 0.39043379662939426\nEpsilon = 0.39039475324973133\nEpsilon = 0.39035571377440637\nEpsilon = 0.39031667820302895\nEpsilon = 0.39027764653520863\nEpsilon = 0.3902386187705551\nEpsilon = 0.39019959490867806\nEpsilon = 0.3901605749491872\nEpsilon = 0.3901215588916923\nEpsilon = 0.39008254673580317\nEpsilon = 0.3900435384811296\nEpsilon = 0.3900045341272815\nEpsilon = 0.38996553367386877\nEpsilon = 0.3899265371205014\nEpsilon = 0.38988754446678936\nEpsilon = 0.38984855571234267\nAgent: ddqn_agent . Episode 522/2000. Number of steps to finish: 20. Loss: 14.261360168457031 Reward: -18.0\nEpsilon = 0.3898095708567714\nEpsilon = 0.38977058989968577\nEpsilon = 0.3897316128406958\nEpsilon = 0.38969263967941176\nEpsilon = 0.3896536704154438\nEpsilon = 0.38961470504840223\nEpsilon = 0.38957574357789737\nEpsilon = 0.3895367860035396\nEpsilon = 0.38949783232493923\nEpsilon = 0.38945888254170674\nEpsilon = 0.3894199366534526\nEpsilon = 0.38938099465978726\nEpsilon = 0.3893420565603213\nEpsilon = 0.38930312235466524\nEpsilon = 0.3892641920424298\nEpsilon = 0.38922526562322557\nEpsilon = 0.3891863430966633\nEpsilon = 0.3891474244623536\nEpsilon = 0.3891085097199074\nEpsilon = 0.38906959886893544\nAgent: ddqn_agent . Episode 523/2000. Number of steps to finish: 20. Loss: 14.53470516204834 Reward: -14.0\nEpsilon = 0.38903069190904854\nEpsilon = 0.38899178883985763\nEpsilon = 0.38895288966097363\nEpsilon = 0.3889139943720075\nEpsilon = 0.38887510297257033\nEpsilon = 0.3888362154622731\nEpsilon = 0.38879733184072685\nEpsilon = 0.3887584521075428\nEpsilon = 0.38871957626233206\nEpsilon = 0.3886807043047058\nEpsilon = 0.38864183623427534\nEpsilon = 0.3886029720506519\nEpsilon = 0.38856411175344685\nEpsilon = 0.3885252553422715\nAgent: ddqn_agent . Episode 524/2000. Number of steps to finish: 14. Loss: 9.747507095336914 Reward: -2.0\nEpsilon = 0.3884864028167373\nEpsilon = 0.3884475541764556\nEpsilon = 0.38840870942103795\nEpsilon = 0.38836986855009586\nEpsilon = 0.38833103156324084\nEpsilon = 0.3882921984600845\nEpsilon = 0.3882533692402385\nEpsilon = 0.3882145439033145\nEpsilon = 0.3881757224489242\nEpsilon = 0.3881369048766793\nEpsilon = 0.38809809118619165\nEpsilon = 0.388059281377073\nAgent: ddqn_agent . Episode 525/2000. Number of steps to finish: 12. Loss: 8.75302791595459 Reward: 0.0\nEpsilon = 0.3880204754489353\nEpsilon = 0.3879816734013904\nEpsilon = 0.38794287523405024\nEpsilon = 0.38790408094652684\nEpsilon = 0.3878652905384322\nEpsilon = 0.3878265040093784\nEpsilon = 0.3877877213589775\nEpsilon = 0.3877489425868416\nEpsilon = 0.38771016769258293\nEpsilon = 0.3876713966758137\nEpsilon = 0.38763262953614613\nEpsilon = 0.38759386627319253\nEpsilon = 0.3875551068865652\nEpsilon = 0.38751635137587653\nEpsilon = 0.387477599740739\nEpsilon = 0.3874388519807649\nEpsilon = 0.3874001080955668\nEpsilon = 0.38736136808475724\nEpsilon = 0.38732263194794875\nEpsilon = 0.38728389968475396\nAgent: ddqn_agent . Episode 526/2000. Number of steps to finish: 20. Loss: 14.324172019958496 Reward: -12.0\nEpsilon = 0.3872451712947855\nEpsilon = 0.387206446777656\nEpsilon = 0.38716772613297823\nEpsilon = 0.38712900936036493\nEpsilon = 0.3870902964594289\nEpsilon = 0.38705158742978296\nEpsilon = 0.38701288227103997\nEpsilon = 0.3869741809828129\nEpsilon = 0.38693548356471463\nEpsilon = 0.3868967900163582\nEpsilon = 0.38685810033735657\nEpsilon = 0.3868194145273228\nEpsilon = 0.3867807325858701\nEpsilon = 0.3867420545126115\nEpsilon = 0.3867033803071602\nEpsilon = 0.3866647099691295\nEpsilon = 0.38662604349813257\nEpsilon = 0.38658738089378275\nEpsilon = 0.38654872215569336\nEpsilon = 0.3865100672834778\nAgent: ddqn_agent . Episode 527/2000. Number of steps to finish: 20. Loss: 14.017997741699219 Reward: -10.0\nEpsilon = 0.38647141627674947\nEpsilon = 0.3864327691351218\nEpsilon = 0.3863941258582083\nEpsilon = 0.3863554864456225\nEpsilon = 0.3863168508969779\nEpsilon = 0.3862782192118882\nEpsilon = 0.386239591389967\nEpsilon = 0.386200967430828\nEpsilon = 0.3861623473340849\nEpsilon = 0.3861237310993515\nEpsilon = 0.38608511872624157\nEpsilon = 0.386046510214369\nEpsilon = 0.3860079055633475\nEpsilon = 0.3859693047727912\nEpsilon = 0.3859307078423139\nEpsilon = 0.3858921147715297\nEpsilon = 0.38585352556005253\nEpsilon = 0.3858149402074965\nEpsilon = 0.38577635871347576\nEpsilon = 0.3857377810776044\nAgent: ddqn_agent . Episode 528/2000. Number of steps to finish: 20. Loss: 14.718415260314941 Reward: -14.0\nEpsilon = 0.3856992072994967\nEpsilon = 0.38566063737876677\nEpsilon = 0.3856220713150289\nEpsilon = 0.3855835091078974\nEpsilon = 0.38554495075698664\nEpsilon = 0.38550639626191097\nEpsilon = 0.38546784562228475\nEpsilon = 0.38542929883772253\nEpsilon = 0.3853907559078388\nEpsilon = 0.385352216832248\nEpsilon = 0.3853136816105648\nEpsilon = 0.3852751502424037\nEpsilon = 0.38523662272737946\nEpsilon = 0.3851980990651067\nEpsilon = 0.3851595792552002\nEpsilon = 0.3851210632972747\nAgent: ddqn_agent . Episode 529/2000. Number of steps to finish: 16. Loss: 11.5348539352417 Reward: -4.0\nEpsilon = 0.385082551190945\nEpsilon = 0.3850440429358259\nEpsilon = 0.3850055385315323\nEpsilon = 0.38496703797767917\nEpsilon = 0.38492854127388143\nEpsilon = 0.38489004841975405\nEpsilon = 0.38485155941491206\nEpsilon = 0.38481307425897054\nEpsilon = 0.38477459295154465\nEpsilon = 0.3847361154922495\nEpsilon = 0.38469764188070027\nEpsilon = 0.3846591721165122\nEpsilon = 0.38462070619930055\nAgent: ddqn_agent . Episode 530/2000. Number of steps to finish: 13. Loss: 8.886524200439453 Reward: -1.0\nEpsilon = 0.38458224412868064\nEpsilon = 0.3845437859042678\nEpsilon = 0.3845053315256774\nEpsilon = 0.38446688099252485\nEpsilon = 0.3844284343044256\nEpsilon = 0.3843899914609951\nEpsilon = 0.38435155246184904\nEpsilon = 0.38431311730660284\nEpsilon = 0.3842746859948722\nEpsilon = 0.3842362585262727\nEpsilon = 0.3841978349004201\nAgent: ddqn_agent . Episode 531/2000. Number of steps to finish: 11. Loss: 7.953423976898193 Reward: 1.0\nEpsilon = 0.38415941511693\nEpsilon = 0.3841209991754183\nEpsilon = 0.38408258707550075\nEpsilon = 0.3840441788167932\nEpsilon = 0.38400577439891154\nEpsilon = 0.38396737382147167\nEpsilon = 0.3839289770840895\nEpsilon = 0.3838905841863811\nEpsilon = 0.3838521951279625\nEpsilon = 0.3838138099084497\nEpsilon = 0.3837754285274589\nEpsilon = 0.38373705098460614\nEpsilon = 0.3836986772795077\nEpsilon = 0.38366030741177976\nEpsilon = 0.38362194138103856\nEpsilon = 0.38358357918690045\nEpsilon = 0.38354522082898174\nEpsilon = 0.38350686630689884\nEpsilon = 0.3834685156202682\nEpsilon = 0.38343016876870617\nAgent: ddqn_agent . Episode 532/2000. Number of steps to finish: 20. Loss: 14.24935531616211 Reward: -12.0\nEpsilon = 0.3833918257518293\nEpsilon = 0.3833534865692541\nEpsilon = 0.3833151512205972\nEpsilon = 0.3832768197054751\nEpsilon = 0.3832384920235046\nEpsilon = 0.38320016817430225\nEpsilon = 0.38316184815748483\nEpsilon = 0.3831235319726691\nEpsilon = 0.3830852196194718\nEpsilon = 0.3830469110975098\nEpsilon = 0.38300860640640005\nEpsilon = 0.3829703055457594\nEpsilon = 0.3829320085152048\nEpsilon = 0.3828937153143533\nEpsilon = 0.38285542594282185\nEpsilon = 0.3828171404002276\nEpsilon = 0.38277885868618755\nEpsilon = 0.3827405808003189\nEpsilon = 0.3827023067422389\nEpsilon = 0.38266403651156466\nAgent: ddqn_agent . Episode 533/2000. Number of steps to finish: 20. Loss: 14.354447364807129 Reward: -10.0\nEpsilon = 0.3826257701079135\nEpsilon = 0.3825875075309027\nEpsilon = 0.3825492487801496\nEpsilon = 0.3825109938552716\nEpsilon = 0.38247274275588605\nEpsilon = 0.38243449548161046\nEpsilon = 0.3823962520320623\nEpsilon = 0.38235801240685907\nEpsilon = 0.3823197766056184\nEpsilon = 0.3822815446279578\nEpsilon = 0.382243316473495\nEpsilon = 0.38220509214184767\nEpsilon = 0.38216687163263346\nEpsilon = 0.3821286549454702\nEpsilon = 0.38209044207997567\nEpsilon = 0.3820522330357677\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.38201402781246413\nEpsilon = 0.38197582640968286\nEpsilon = 0.3819376288270419\nEpsilon = 0.3818994350641592\nAgent: ddqn_agent . Episode 534/2000. Number of steps to finish: 20. Loss: 13.911453247070312 Reward: -12.0\nEpsilon = 0.3818612451206528\nEpsilon = 0.3818230589961407\nEpsilon = 0.3817848766902411\nEpsilon = 0.38174669820257207\nEpsilon = 0.3817085235327518\nEpsilon = 0.38167035268039856\nEpsilon = 0.3816321856451305\nEpsilon = 0.381594022426566\nEpsilon = 0.3815558630243233\nEpsilon = 0.3815177074380209\nEpsilon = 0.3814795556672771\nEpsilon = 0.38144140771171037\nEpsilon = 0.3814032635709392\nEpsilon = 0.3813651232445821\nEpsilon = 0.3813269867322577\nEpsilon = 0.3812888540335845\nEpsilon = 0.38125072514818115\nEpsilon = 0.3812126000756663\nEpsilon = 0.38117447881565875\nEpsilon = 0.3811363613677772\nAgent: ddqn_agent . Episode 535/2000. Number of steps to finish: 20. Loss: 14.029459953308105 Reward: -16.0\nEpsilon = 0.3810982477316404\nEpsilon = 0.38106013790686727\nEpsilon = 0.3810220318930766\nEpsilon = 0.3809839296898873\nEpsilon = 0.38094583129691834\nEpsilon = 0.38090773671378864\nEpsilon = 0.38086964594011724\nEpsilon = 0.38083155897552323\nEpsilon = 0.3807934758196257\nEpsilon = 0.38075539647204376\nEpsilon = 0.3807173209323966\nEpsilon = 0.3806792492003033\nEpsilon = 0.3806411812753833\nEpsilon = 0.3806031171572558\nEpsilon = 0.3805650568455401\nEpsilon = 0.38052700033985554\nEpsilon = 0.38048894763982155\nEpsilon = 0.38045089874505755\nEpsilon = 0.38041285365518307\nEpsilon = 0.38037481236981757\nAgent: ddqn_agent . Episode 536/2000. Number of steps to finish: 20. Loss: 13.726305961608887 Reward: -16.0\nEpsilon = 0.3803367748885806\nEpsilon = 0.38029874121109175\nEpsilon = 0.38026071133697065\nEpsilon = 0.38022268526583697\nEpsilon = 0.3801846629973104\nEpsilon = 0.38014664453101066\nEpsilon = 0.3801086298665576\nEpsilon = 0.3800706190035709\nEpsilon = 0.38003261194167054\nEpsilon = 0.37999460868047635\nEpsilon = 0.37995660921960833\nEpsilon = 0.3799186135586864\nEpsilon = 0.3798806216973305\nEpsilon = 0.3798426336351608\nEpsilon = 0.3798046493717973\nEpsilon = 0.3797666689068601\nEpsilon = 0.3797286922399694\nEpsilon = 0.37969071937074544\nEpsilon = 0.37965275029880835\nEpsilon = 0.3796147850237785\nAgent: ddqn_agent . Episode 537/2000. Number of steps to finish: 20. Loss: 14.698261260986328 Reward: -8.0\nEpsilon = 0.3795768235452761\nEpsilon = 0.37953886586292157\nEpsilon = 0.3795009119763353\nEpsilon = 0.3794629618851377\nEpsilon = 0.37942501558894914\nEpsilon = 0.3793870730873903\nEpsilon = 0.37934913438008155\nEpsilon = 0.37931119946664354\nEpsilon = 0.3792732683466969\nEpsilon = 0.3792353410198622\nEpsilon = 0.37919741748576025\nEpsilon = 0.3791594977440117\nEpsilon = 0.37912158179423733\nEpsilon = 0.3790836696360579\nEpsilon = 0.3790457612690943\nEpsilon = 0.3790078566929674\nEpsilon = 0.3789699559072981\nEpsilon = 0.3789320589117074\nEpsilon = 0.37889416570581624\nEpsilon = 0.3788562762892457\nAgent: ddqn_agent . Episode 538/2000. Number of steps to finish: 20. Loss: 14.295550346374512 Reward: -12.0\nEpsilon = 0.37881839066161677\nEpsilon = 0.3787805088225506\nEpsilon = 0.37874263077166836\nEpsilon = 0.3787047565085912\nEpsilon = 0.37866688603294035\nEpsilon = 0.37862901934433707\nEpsilon = 0.37859115644240265\nEpsilon = 0.3785532973267584\nEpsilon = 0.37851544199702575\nEpsilon = 0.37847759045282603\nEpsilon = 0.3784397426937808\nEpsilon = 0.3784018987195114\nEpsilon = 0.37836405852963945\nEpsilon = 0.37832622212378647\nEpsilon = 0.3782883895015741\nEpsilon = 0.37825056066262397\nEpsilon = 0.3782127356065577\nEpsilon = 0.37817491433299705\nEpsilon = 0.37813709684156377\nEpsilon = 0.3780992831318796\nAgent: ddqn_agent . Episode 539/2000. Number of steps to finish: 20. Loss: 14.24095630645752 Reward: -14.0\nEpsilon = 0.3780614732035664\nEpsilon = 0.37802366705624607\nEpsilon = 0.37798586468954043\nEpsilon = 0.37794806610307147\nEpsilon = 0.37791027129646115\nEpsilon = 0.3778724802693315\nEpsilon = 0.37783469302130457\nEpsilon = 0.37779690955200246\nEpsilon = 0.37775912986104726\nEpsilon = 0.37772135394806117\nEpsilon = 0.37768358181266637\nEpsilon = 0.3776458134544851\nEpsilon = 0.37760804887313965\nEpsilon = 0.3775702880682523\nEpsilon = 0.3775325310394455\nEpsilon = 0.37749477778634155\nEpsilon = 0.3774570283085629\nEpsilon = 0.3774192826057321\nEpsilon = 0.37738154067747154\nEpsilon = 0.37734380252340377\nAgent: ddqn_agent . Episode 540/2000. Number of steps to finish: 20. Loss: 14.192809104919434 Reward: -20.0\nEpsilon = 0.37730606814315143\nEpsilon = 0.3772683375363371\nEpsilon = 0.37723061070258346\nEpsilon = 0.3771928876415132\nEpsilon = 0.37715516835274904\nEpsilon = 0.37711745283591375\nEpsilon = 0.37707974109063014\nEpsilon = 0.37704203311652107\nEpsilon = 0.3770043289132094\nEpsilon = 0.3769666284803181\nEpsilon = 0.37692893181747006\nEpsilon = 0.37689123892428833\nEpsilon = 0.3768535498003959\nEpsilon = 0.37681586444541587\nEpsilon = 0.37677818285897136\nEpsilon = 0.37674050504068546\nEpsilon = 0.37670283099018137\nEpsilon = 0.37666516070708234\nEpsilon = 0.37662749419101166\nEpsilon = 0.3765898314415926\nAgent: ddqn_agent . Episode 541/2000. Number of steps to finish: 20. Loss: 14.32139778137207 Reward: -12.0\nEpsilon = 0.3765521724584484\nEpsilon = 0.37651451724120255\nEpsilon = 0.3764768657894784\nEpsilon = 0.3764392181028995\nEpsilon = 0.3764015741810892\nEpsilon = 0.37636393402367113\nEpsilon = 0.37632629763026876\nEpsilon = 0.37628866500050573\nEpsilon = 0.3762510361340057\nEpsilon = 0.3762134110303923\nEpsilon = 0.3761757896892893\nEpsilon = 0.37613817211032036\nEpsilon = 0.37610055829310934\nEpsilon = 0.37606294823728004\nEpsilon = 0.3760253419424563\nEpsilon = 0.3759877394082621\nEpsilon = 0.37595014063432125\nEpsilon = 0.37591254562025783\nEpsilon = 0.3758749543656958\nEpsilon = 0.3758373668702592\nAgent: ddqn_agent . Episode 542/2000. Number of steps to finish: 20. Loss: 13.932690620422363 Reward: -14.0\nEpsilon = 0.3757997831335722\nEpsilon = 0.3757622031552589\nEpsilon = 0.37572462693494335\nEpsilon = 0.37568705447224987\nEpsilon = 0.3756494857668026\nEpsilon = 0.375611920818226\nEpsilon = 0.37557435962614416\nEpsilon = 0.37553680219018154\nEpsilon = 0.3754992485099625\nEpsilon = 0.3754616985851115\nEpsilon = 0.375424152415253\nEpsilon = 0.3753866100000115\nEpsilon = 0.3753490713390115\nEpsilon = 0.37531153643187765\nEpsilon = 0.37527400527823446\nEpsilon = 0.37523647787770664\nEpsilon = 0.3751989542299189\nEpsilon = 0.3751614343344959\nEpsilon = 0.37512391819106244\nEpsilon = 0.37508640579924335\nAgent: ddqn_agent . Episode 543/2000. Number of steps to finish: 20. Loss: 13.527336120605469 Reward: -12.0\nEpsilon = 0.3750488971586634\nEpsilon = 0.37501139226894753\nEpsilon = 0.3749738911297206\nEpsilon = 0.37493639374060767\nEpsilon = 0.3748989001012336\nEpsilon = 0.3748614102112235\nEpsilon = 0.3748239240702024\nEpsilon = 0.37478644167779535\nEpsilon = 0.3747489630336276\nEpsilon = 0.37471148813732424\nAgent: ddqn_agent . Episode 544/2000. Number of steps to finish: 10. Loss: 7.088103294372559 Reward: 2.0\nEpsilon = 0.3746740169885105\nEpsilon = 0.37463654958681164\nEpsilon = 0.374599085931853\nEpsilon = 0.3745616260232598\nEpsilon = 0.3745241698606575\nEpsilon = 0.37448671744367146\nEpsilon = 0.3744492687719271\nEpsilon = 0.3744118238450499\nEpsilon = 0.3743743826626654\nEpsilon = 0.3743369452243991\nEpsilon = 0.37429951152987667\nEpsilon = 0.3742620815787237\nEpsilon = 0.3742246553705658\nEpsilon = 0.37418723290502875\nEpsilon = 0.37414981418173826\nEpsilon = 0.3741123992003201\nEpsilon = 0.37407498796040006\nEpsilon = 0.374037580461604\nEpsilon = 0.37400017670355784\nEpsilon = 0.3739627766858875\nAgent: ddqn_agent . Episode 545/2000. Number of steps to finish: 20. Loss: 14.182931900024414 Reward: -16.0\nEpsilon = 0.37392538040821893\nEpsilon = 0.37388798787017813\nEpsilon = 0.3738505990713911\nEpsilon = 0.373813214011484\nEpsilon = 0.37377583269008285\nEpsilon = 0.37373845510681386\nEpsilon = 0.3737010812613032\nEpsilon = 0.3736637111531771\nEpsilon = 0.3736263447820618\nAgent: ddqn_agent . Episode 546/2000. Number of steps to finish: 9. Loss: 6.7869768142700195 Reward: 3.0\nEpsilon = 0.3735889821475836\nEpsilon = 0.37355162324936886\nEpsilon = 0.3735142680870439\nEpsilon = 0.3734769166602352\nEpsilon = 0.37343956896856917\nEpsilon = 0.3734022250116723\nEpsilon = 0.3733648847891711\nEpsilon = 0.3733275483006922\nEpsilon = 0.3732902155458621\nEpsilon = 0.37325288652430755\nEpsilon = 0.37321556123565514\nEpsilon = 0.37317823967953156\nEpsilon = 0.37314092185556363\nEpsilon = 0.3731036077633781\nEpsilon = 0.3730662974026018\nEpsilon = 0.3730289907728615\nEpsilon = 0.37299168787378423\nEpsilon = 0.37295438870499686\nEpsilon = 0.3729170932661264\nEpsilon = 0.3728798015567998\nAgent: ddqn_agent . Episode 547/2000. Number of steps to finish: 20. Loss: 13.911895751953125 Reward: -12.0\nEpsilon = 0.3728425135766441\nEpsilon = 0.3728052293252864\nEpsilon = 0.3727679488023539\nEpsilon = 0.3727306720074737\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.3726933989402729\nEpsilon = 0.3726561296003789\nEpsilon = 0.3726188639874189\nEpsilon = 0.37258160210102015\nEpsilon = 0.37254434394081004\nEpsilon = 0.372507089506416\nEpsilon = 0.3724698387974653\nEpsilon = 0.37243259181358557\nEpsilon = 0.3723953485544042\nEpsilon = 0.37235810901954874\nEpsilon = 0.3723208732086468\nEpsilon = 0.37228364112132595\nEpsilon = 0.3722464127572138\nEpsilon = 0.3722091881159381\nEpsilon = 0.37217196719712653\nEpsilon = 0.3721347500004068\nAgent: ddqn_agent . Episode 548/2000. Number of steps to finish: 20. Loss: 14.643597602844238 Reward: -14.0\nEpsilon = 0.3720975365254068\nEpsilon = 0.37206032677175427\nEpsilon = 0.3720231207390771\nEpsilon = 0.37198591842700324\nEpsilon = 0.37194871983516054\nEpsilon = 0.37191152496317703\nEpsilon = 0.3718743338106807\nEpsilon = 0.37183714637729964\nEpsilon = 0.3717999626626619\nEpsilon = 0.3717627826663957\nEpsilon = 0.371725606388129\nEpsilon = 0.3716884338274902\nEpsilon = 0.3716512649841075\nEpsilon = 0.37161409985760907\nEpsilon = 0.3715769384476233\nEpsilon = 0.3715397807537785\nEpsilon = 0.37150262677570317\nEpsilon = 0.3714654765130256\nEpsilon = 0.3714283299653743\nEpsilon = 0.37139118713237773\nAgent: ddqn_agent . Episode 549/2000. Number of steps to finish: 20. Loss: 14.301612854003906 Reward: -14.0\nEpsilon = 0.3713540480136645\nEpsilon = 0.3713169126088631\nEpsilon = 0.3712797809176022\nEpsilon = 0.3712426529395105\nEpsilon = 0.3712055286742165\nEpsilon = 0.3711684081213491\nEpsilon = 0.37113129128053696\nEpsilon = 0.3710941781514089\nEpsilon = 0.37105706873359373\nEpsilon = 0.37101996302672036\nEpsilon = 0.37098286103041767\nEpsilon = 0.3709457627443146\nEpsilon = 0.3709086681680402\nEpsilon = 0.3708715773012234\nEpsilon = 0.3708344901434933\nEpsilon = 0.37079740669447897\nEpsilon = 0.3707603269538095\nEpsilon = 0.37072325092111413\nEpsilon = 0.37068617859602204\nEpsilon = 0.3706491099781624\nAgent: ddqn_agent . Episode 550/2000. Number of steps to finish: 20. Loss: 13.954277038574219 Reward: -14.0\nEpsilon = 0.3706120450671646\nEpsilon = 0.3705749838626579\nEpsilon = 0.37053792636427163\nEpsilon = 0.3705008725716352\nEpsilon = 0.37046382248437804\nEpsilon = 0.3704267761021296\nEpsilon = 0.37038973342451936\nEpsilon = 0.3703526944511769\nEpsilon = 0.3703156591817318\nEpsilon = 0.37027862761581365\nEpsilon = 0.3702415997530521\nEpsilon = 0.3702045755930768\nEpsilon = 0.37016755513551747\nEpsilon = 0.3701305383800039\nEpsilon = 0.3700935253261659\nEpsilon = 0.3700565159736333\nEpsilon = 0.37001951032203595\nEpsilon = 0.36998250837100377\nEpsilon = 0.3699455101201667\nEpsilon = 0.36990851556915466\nAgent: ddqn_agent . Episode 551/2000. Number of steps to finish: 20. Loss: 14.071175575256348 Reward: -16.0\nEpsilon = 0.36987152471759777\nEpsilon = 0.369834537565126\nEpsilon = 0.3697975541113695\nEpsilon = 0.3697605743559584\nEpsilon = 0.3697235982985228\nEpsilon = 0.36968662593869295\nEpsilon = 0.3696496572760991\nEpsilon = 0.3696126923103715\nEpsilon = 0.3695757310411405\nEpsilon = 0.3695387734680364\nEpsilon = 0.36950181959068956\nEpsilon = 0.3694648694087305\nEpsilon = 0.36942792292178966\nEpsilon = 0.3693909801294975\nEpsilon = 0.36935404103148456\nEpsilon = 0.3693171056273814\nEpsilon = 0.36928017391681867\nEpsilon = 0.369243245899427\nEpsilon = 0.369206321574837\nEpsilon = 0.3691694009426795\nAgent: ddqn_agent . Episode 552/2000. Number of steps to finish: 20. Loss: 14.363214492797852 Reward: -12.0\nEpsilon = 0.36913248400258525\nEpsilon = 0.369095570754185\nEpsilon = 0.36905866119710956\nEpsilon = 0.36902175533098985\nEpsilon = 0.36898485315545676\nEpsilon = 0.36894795467014124\nEpsilon = 0.3689110598746742\nEpsilon = 0.3688741687686867\nEpsilon = 0.3688372813518099\nEpsilon = 0.3688003976236747\nEpsilon = 0.36876351758391235\nEpsilon = 0.368726641232154\nEpsilon = 0.36868976856803076\nEpsilon = 0.368652899591174\nEpsilon = 0.36861603430121487\nEpsilon = 0.36857917269778473\nEpsilon = 0.36854231478051497\nEpsilon = 0.36850546054903693\nEpsilon = 0.36846861000298203\nAgent: ddqn_agent . Episode 553/2000. Number of steps to finish: 19. Loss: 13.468098640441895 Reward: -7.0\nEpsilon = 0.3684317631419817\nEpsilon = 0.36839491996566753\nEpsilon = 0.36835808047367097\nEpsilon = 0.3683212446656236\nEpsilon = 0.36828441254115707\nEpsilon = 0.36824758409990294\nEpsilon = 0.36821075934149294\nEpsilon = 0.3681739382655588\nEpsilon = 0.3681371208717322\nEpsilon = 0.36810030715964503\nEpsilon = 0.36806349712892905\nEpsilon = 0.3680266907792162\nEpsilon = 0.3679898881101383\nEpsilon = 0.3679530891213273\nEpsilon = 0.36791629381241514\nEpsilon = 0.3678795021830339\nEpsilon = 0.3678427142328156\nEpsilon = 0.36780592996139233\nEpsilon = 0.3677691493683962\nEpsilon = 0.3677323724534594\nAgent: ddqn_agent . Episode 554/2000. Number of steps to finish: 20. Loss: 13.770722389221191 Reward: -14.0\nEpsilon = 0.3676955992162141\nEpsilon = 0.3676588296562925\nEpsilon = 0.36762206377332685\nEpsilon = 0.3675853015669495\nEpsilon = 0.36754854303679285\nEpsilon = 0.3675117881824892\nEpsilon = 0.36747503700367096\nEpsilon = 0.3674382894999706\nEpsilon = 0.3674015456710206\nEpsilon = 0.3673648055164535\nEpsilon = 0.36732806903590187\nEpsilon = 0.3672913362289983\nEpsilon = 0.3672546070953754\nEpsilon = 0.36721788163466584\nEpsilon = 0.3671811598465024\nEpsilon = 0.36714444173051775\nEpsilon = 0.3671077272863447\nEpsilon = 0.36707101651361607\nEpsilon = 0.3670343094119647\nEpsilon = 0.3669976059810235\nAgent: ddqn_agent . Episode 555/2000. Number of steps to finish: 20. Loss: 14.204374313354492 Reward: -18.0\nEpsilon = 0.3669609062204254\nEpsilon = 0.36692421012980336\nEpsilon = 0.3668875177087904\nEpsilon = 0.36685082895701954\nEpsilon = 0.36681414387412387\nEpsilon = 0.36677746245973647\nEpsilon = 0.3667407847134905\nEpsilon = 0.3667041106350191\nEpsilon = 0.36666744022395564\nEpsilon = 0.3666307734799332\nEpsilon = 0.3665941104025852\nEpsilon = 0.36655745099154496\nEpsilon = 0.3665207952464458\nEpsilon = 0.3664841431669212\nEpsilon = 0.3664474947526045\nAgent: ddqn_agent . Episode 556/2000. Number of steps to finish: 15. Loss: 10.704702377319336 Reward: -3.0\nEpsilon = 0.3664108500031293\nEpsilon = 0.366374208918129\nEpsilon = 0.36633757149723717\nEpsilon = 0.36630093774008743\nEpsilon = 0.3662643076463134\nEpsilon = 0.3662276812155488\nEpsilon = 0.3661910584474272\nEpsilon = 0.3661544393415825\nEpsilon = 0.36611782389764835\nEpsilon = 0.36608121211525857\nEpsilon = 0.36604460399404704\nEpsilon = 0.36600799953364765\nEpsilon = 0.3659713987336943\nEpsilon = 0.3659348015938209\nEpsilon = 0.3658982081136615\nEpsilon = 0.3658616182928502\nEpsilon = 0.3658250321310209\nEpsilon = 0.3657884496278078\nEpsilon = 0.36575187078284505\nEpsilon = 0.36571529559576677\nAgent: ddqn_agent . Episode 557/2000. Number of steps to finish: 20. Loss: 14.110126495361328 Reward: -20.0\nEpsilon = 0.3656787240662072\nEpsilon = 0.3656421561938006\nEpsilon = 0.3656055919781812\nEpsilon = 0.3655690314189834\nEpsilon = 0.3655324745158415\nEpsilon = 0.3654959212683899\nEpsilon = 0.3654593716762631\nEpsilon = 0.36542282573909546\nEpsilon = 0.3653862834565216\nEpsilon = 0.36534974482817595\nEpsilon = 0.36531320985369314\nEpsilon = 0.3652766785327078\nEpsilon = 0.3652401508648545\nEpsilon = 0.36520362684976804\nEpsilon = 0.36516710648708306\nEpsilon = 0.36513058977643437\nAgent: ddqn_agent . Episode 558/2000. Number of steps to finish: 16. Loss: 11.097954750061035 Reward: -4.0\nEpsilon = 0.36509407671745675\nEpsilon = 0.365057567309785\nEpsilon = 0.365021061553054\nEpsilon = 0.3649845594468987\nEpsilon = 0.36494806099095406\nEpsilon = 0.364911566184855\nEpsilon = 0.3648750750282365\nEpsilon = 0.36483858752073367\nEpsilon = 0.3648021036619816\nAgent: ddqn_agent . Episode 559/2000. Number of steps to finish: 9. Loss: 6.253460884094238 Reward: 3.0\nEpsilon = 0.36476562345161545\nEpsilon = 0.3647291468892703\nEpsilon = 0.3646926739745813\nEpsilon = 0.36465620470718385\nEpsilon = 0.36461973908671313\nEpsilon = 0.36458327711280447\nEpsilon = 0.3645468187850932\nEpsilon = 0.3645103641032147\nEpsilon = 0.3644739130668044\nEpsilon = 0.36443746567549773\nEpsilon = 0.3644010219289302\nEpsilon = 0.3643645818267373\nEpsilon = 0.3643281453685546\nEpsilon = 0.36429171255401777\nEpsilon = 0.36425528338276236\nEpsilon = 0.3642188578544241\nEpsilon = 0.36418243596863864\nEpsilon = 0.3641460177250418\nEpsilon = 0.3641096031232693\nEpsilon = 0.36407319216295697\nAgent: ddqn_agent . Episode 560/2000. Number of steps to finish: 20. Loss: 14.450545310974121 Reward: -14.0\nEpsilon = 0.3640367848437407\nEpsilon = 0.36400038116525635\nEpsilon = 0.36396398112713985\nEpsilon = 0.36392758472902714\nEpsilon = 0.36389119197055425\nEpsilon = 0.3638548028513572\nEpsilon = 0.36381841737107207\nEpsilon = 0.36378203552933497\nEpsilon = 0.363745657325782\nEpsilon = 0.36370928276004943\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.3636729118317734\nEpsilon = 0.36363654454059025\nEpsilon = 0.3636001808861362\nEpsilon = 0.3635638208680476\nEpsilon = 0.3635274644859608\nEpsilon = 0.3634911117395122\nEpsilon = 0.36345476262833826\nEpsilon = 0.3634184171520754\nEpsilon = 0.3633820753103602\nEpsilon = 0.3633457371028292\nAgent: ddqn_agent . Episode 561/2000. Number of steps to finish: 20. Loss: 14.36652660369873 Reward: -14.0\nEpsilon = 0.3633094025291189\nEpsilon = 0.363273071588866\nEpsilon = 0.36323674428170716\nEpsilon = 0.363200420607279\nEpsilon = 0.36316410056521825\nEpsilon = 0.3631277841551617\nEpsilon = 0.3630914713767462\nEpsilon = 0.36305516222960854\nEpsilon = 0.3630188567133856\nEpsilon = 0.36298255482771424\nEpsilon = 0.36294625657223145\nEpsilon = 0.3629099619465742\nEpsilon = 0.36287367095037953\nEpsilon = 0.3628373835832845\nEpsilon = 0.36280109984492614\nEpsilon = 0.3627648197349417\nEpsilon = 0.3627285432529682\nEpsilon = 0.36269227039864294\nEpsilon = 0.36265600117160307\nEpsilon = 0.3626197355714859\nAgent: ddqn_agent . Episode 562/2000. Number of steps to finish: 20. Loss: 14.024839401245117 Reward: -12.0\nEpsilon = 0.36258347359792875\nEpsilon = 0.362547215250569\nEpsilon = 0.3625109605290439\nEpsilon = 0.36247470943299104\nEpsilon = 0.36243846196204776\nEpsilon = 0.36240221811585155\nEpsilon = 0.36236597789403996\nEpsilon = 0.36232974129625056\nEpsilon = 0.3622935083221209\nEpsilon = 0.36225727897128873\nEpsilon = 0.3622210532433916\nEpsilon = 0.36218483113806726\nEpsilon = 0.36214861265495346\nEpsilon = 0.362112397793688\nEpsilon = 0.3620761865539086\nEpsilon = 0.36203997893525325\nEpsilon = 0.36200377493735975\nEpsilon = 0.361967574559866\nEpsilon = 0.36193137780241\nEpsilon = 0.3618951846646298\nAgent: ddqn_agent . Episode 563/2000. Number of steps to finish: 20. Loss: 14.071002960205078 Reward: -18.0\nEpsilon = 0.3618589951461633\nEpsilon = 0.3618228092466487\nEpsilon = 0.36178662696572406\nEpsilon = 0.3617504483030275\nEpsilon = 0.3617142732581972\nEpsilon = 0.3616781018308714\nAgent: ddqn_agent . Episode 564/2000. Number of steps to finish: 6. Loss: 4.337804317474365 Reward: 6.0\nEpsilon = 0.36164193402068834\nEpsilon = 0.36160576982728626\nEpsilon = 0.3615696092503035\nEpsilon = 0.3615334522893785\nEpsilon = 0.36149729894414956\nEpsilon = 0.36146114921425515\nEpsilon = 0.36142500309933373\nEpsilon = 0.3613888605990238\nEpsilon = 0.3613527217129639\nEpsilon = 0.3613165864407926\nEpsilon = 0.36128045478214854\nEpsilon = 0.36124432673667034\nEpsilon = 0.3612082023039967\nEpsilon = 0.3611720814837663\nEpsilon = 0.3611359642756179\nEpsilon = 0.3610998506791903\nEpsilon = 0.3610637406941224\nEpsilon = 0.36102763432005297\nEpsilon = 0.36099153155662095\nEpsilon = 0.3609554324034653\nAgent: ddqn_agent . Episode 565/2000. Number of steps to finish: 20. Loss: 14.236565589904785 Reward: -10.0\nEpsilon = 0.3609193368602249\nEpsilon = 0.3608832449265389\nEpsilon = 0.36084715660204625\nEpsilon = 0.36081107188638606\nEpsilon = 0.36077499077919745\nEpsilon = 0.36073891328011953\nEpsilon = 0.36070283938879155\nEpsilon = 0.3606667691048527\nEpsilon = 0.3606307024279422\nEpsilon = 0.3605946393576994\nEpsilon = 0.36055857989376366\nAgent: ddqn_agent . Episode 566/2000. Number of steps to finish: 11. Loss: 7.758636951446533 Reward: 1.0\nEpsilon = 0.3605225240357743\nEpsilon = 0.36048647178337073\nEpsilon = 0.3604504231361924\nEpsilon = 0.3604143780938788\nEpsilon = 0.3603783366560694\nEpsilon = 0.36034229882240376\nEpsilon = 0.3603062645925215\nEpsilon = 0.3602702339660623\nEpsilon = 0.36023420694266567\nEpsilon = 0.3601981835219714\nEpsilon = 0.3601621637036192\nEpsilon = 0.36012614748724886\nEpsilon = 0.36009013487250013\nEpsilon = 0.36005412585901286\nEpsilon = 0.36001812044642695\nEpsilon = 0.3599821186343823\nEpsilon = 0.35994612042251883\nEpsilon = 0.3599101258104766\nEpsilon = 0.35987413479789554\nEpsilon = 0.3598381473844158\nAgent: ddqn_agent . Episode 567/2000. Number of steps to finish: 20. Loss: 14.012144088745117 Reward: -14.0\nEpsilon = 0.3598021635696773\nEpsilon = 0.35976618335332033\nEpsilon = 0.359730206734985\nEpsilon = 0.35969423371431153\nEpsilon = 0.3596582642909401\nEpsilon = 0.35962229846451105\nEpsilon = 0.3595863362346646\nEpsilon = 0.35955037760104114\nEpsilon = 0.35951442256328103\nEpsilon = 0.3594784711210247\nEpsilon = 0.35944252327391263\nEpsilon = 0.35940657902158524\nEpsilon = 0.3593706383636831\nEpsilon = 0.35933470129984674\nEpsilon = 0.35929876782971676\nEpsilon = 0.3592628379529338\nEpsilon = 0.3592269116691385\nEpsilon = 0.3591909889779716\nEpsilon = 0.3591550698790738\nAgent: ddqn_agent . Episode 568/2000. Number of steps to finish: 19. Loss: 13.434003829956055 Reward: -7.0\nEpsilon = 0.3591191543720859\nEpsilon = 0.3590832424566487\nEpsilon = 0.35904733413240303\nEpsilon = 0.3590114293989898\nEpsilon = 0.3589755282560499\nEpsilon = 0.35893963070322427\nEpsilon = 0.35890373674015397\nEpsilon = 0.35886784636647995\nEpsilon = 0.35883195958184333\nEpsilon = 0.3587960763858852\nEpsilon = 0.3587601967782466\nEpsilon = 0.3587243207585688\nEpsilon = 0.35868844832649294\nEpsilon = 0.3586525794816603\nEpsilon = 0.3586167142237121\nEpsilon = 0.3585808525522897\nEpsilon = 0.3585449944670345\nEpsilon = 0.35850913996758776\nEpsilon = 0.358473289053591\nEpsilon = 0.3584374417246856\nAgent: ddqn_agent . Episode 569/2000. Number of steps to finish: 20. Loss: 14.03243637084961 Reward: -12.0\nEpsilon = 0.35840159798051313\nEpsilon = 0.35836575782071506\nEpsilon = 0.358329921244933\nEpsilon = 0.3582940882528085\nEpsilon = 0.3582582588439832\nEpsilon = 0.3582224330180988\nEpsilon = 0.358186610774797\nEpsilon = 0.35815079211371953\nEpsilon = 0.3581149770345082\nEpsilon = 0.35807916553680474\nEpsilon = 0.3580433576202511\nEpsilon = 0.3580075532844891\nEpsilon = 0.35797175252916064\nEpsilon = 0.3579359553539077\nEpsilon = 0.3579001617583723\nEpsilon = 0.3578643717421965\nEpsilon = 0.35782858530502226\nEpsilon = 0.35779280244649175\nEpsilon = 0.35775702316624713\nEpsilon = 0.3577212474639305\nAgent: ddqn_agent . Episode 570/2000. Number of steps to finish: 20. Loss: 14.301515579223633 Reward: -14.0\nEpsilon = 0.35768547533918416\nEpsilon = 0.35764970679165026\nEpsilon = 0.3576139418209711\nEpsilon = 0.357578180426789\nEpsilon = 0.35754242260874636\nEpsilon = 0.3575066683664855\nEpsilon = 0.35747091769964884\nEpsilon = 0.3574351706078789\nEpsilon = 0.3573994270908181\nEpsilon = 0.35736368714810907\nEpsilon = 0.3573279507793943\nEpsilon = 0.3572922179843163\nEpsilon = 0.35725648876251787\nEpsilon = 0.3572207631136416\nEpsilon = 0.35718504103733023\nEpsilon = 0.3571493225332265\nEpsilon = 0.3571136076009732\nEpsilon = 0.35707789624021313\nEpsilon = 0.3570421884505891\nEpsilon = 0.357006484231744\nAgent: ddqn_agent . Episode 571/2000. Number of steps to finish: 20. Loss: 14.362846374511719 Reward: -12.0\nEpsilon = 0.35697078358332085\nEpsilon = 0.35693508650496253\nEpsilon = 0.356899392996312\nEpsilon = 0.35686370305701237\nEpsilon = 0.35682801668670666\nEpsilon = 0.356792333885038\nEpsilon = 0.3567566546516495\nEpsilon = 0.3567209789861844\nEpsilon = 0.35668530688828576\nEpsilon = 0.35664963835759694\nEpsilon = 0.35661397339376116\nEpsilon = 0.3565783119964218\nEpsilon = 0.3565426541652222\nEpsilon = 0.35650699989980567\nEpsilon = 0.3564713491998157\nEpsilon = 0.3564357020648957\nEpsilon = 0.35640005849468925\nEpsilon = 0.35636441848883976\nEpsilon = 0.35632878204699087\nEpsilon = 0.3562931491687862\nAgent: ddqn_agent . Episode 572/2000. Number of steps to finish: 20. Loss: 14.107539176940918 Reward: -12.0\nEpsilon = 0.35625751985386933\nEpsilon = 0.35622189410188393\nEpsilon = 0.35618627191247376\nEpsilon = 0.3561506532852825\nEpsilon = 0.356115038219954\nEpsilon = 0.356079426716132\nEpsilon = 0.3560438187734604\nEpsilon = 0.3560082143915831\nEpsilon = 0.3559726135701439\nEpsilon = 0.35593701630878694\nEpsilon = 0.35590142260715607\nEpsilon = 0.35586583246489534\nEpsilon = 0.35583024588164885\nEpsilon = 0.3557946628570607\nEpsilon = 0.355759083390775\nEpsilon = 0.3557235074824359\nEpsilon = 0.35568793513168767\nEpsilon = 0.3556523663381745\nEpsilon = 0.35561680110154065\nEpsilon = 0.3555812394214305\nAgent: ddqn_agent . Episode 573/2000. Number of steps to finish: 20. Loss: 14.465391159057617 Reward: -12.0\nEpsilon = 0.35554568129748837\nEpsilon = 0.3555101267293586\nEpsilon = 0.35547457571668567\nEpsilon = 0.355439028259114\nEpsilon = 0.3554034843562881\nEpsilon = 0.3553679440078525\nEpsilon = 0.3553324072134517\nEpsilon = 0.35529687397273035\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.35526134428533307\nEpsilon = 0.35522581815090454\nEpsilon = 0.35519029556908943\nEpsilon = 0.35515477653953254\nEpsilon = 0.3551192610618786\nEpsilon = 0.3550837491357724\nEpsilon = 0.35504824076085884\nEpsilon = 0.35501273593678273\nEpsilon = 0.35497723466318903\nEpsilon = 0.3549417369397227\nEpsilon = 0.3549062427660287\nEpsilon = 0.3548707521417521\nAgent: ddqn_agent . Episode 574/2000. Number of steps to finish: 20. Loss: 14.14439582824707 Reward: -10.0\nEpsilon = 0.35483526506653795\nEpsilon = 0.3547997815400313\nEpsilon = 0.35476430156187727\nEpsilon = 0.3547288251317211\nEpsilon = 0.35469335224920795\nEpsilon = 0.35465788291398304\nEpsilon = 0.35462241712569165\nEpsilon = 0.35458695488397907\nEpsilon = 0.3545514961884907\nEpsilon = 0.35451604103887185\nEpsilon = 0.35448058943476796\nEpsilon = 0.3544451413758245\nEpsilon = 0.35440969686168694\nEpsilon = 0.3543742558920008\nEpsilon = 0.3543388184664116\nEpsilon = 0.35430338458456495\nEpsilon = 0.3542679542461065\nEpsilon = 0.3542325274506819\nEpsilon = 0.3541971041979368\nEpsilon = 0.354161684487517\nAgent: ddqn_agent . Episode 575/2000. Number of steps to finish: 20. Loss: 14.45147705078125 Reward: -12.0\nEpsilon = 0.35412626831906824\nEpsilon = 0.35409085569223636\nEpsilon = 0.35405544660666716\nEpsilon = 0.3540200410620065\nEpsilon = 0.35398463905790034\nEpsilon = 0.3539492405939946\nEpsilon = 0.35391384566993517\nEpsilon = 0.3538784542853682\nEpsilon = 0.3538430664399396\nEpsilon = 0.3538076821332956\nEpsilon = 0.3537723013650823\nEpsilon = 0.3537369241349458\nEpsilon = 0.3537015504425323\nEpsilon = 0.3536661802874881\nEpsilon = 0.35363081366945937\nEpsilon = 0.35359545058809244\nEpsilon = 0.35356009104303365\nEpsilon = 0.35352473503392934\nEpsilon = 0.35348938256042595\nAgent: ddqn_agent . Episode 576/2000. Number of steps to finish: 19. Loss: 12.956340789794922 Reward: -7.0\nEpsilon = 0.3534540336221699\nEpsilon = 0.35341868821880773\nEpsilon = 0.35338334634998586\nEpsilon = 0.3533480080153509\nEpsilon = 0.35331267321454934\nEpsilon = 0.3532773419472279\nEpsilon = 0.35324201421303314\nEpsilon = 0.35320669001161187\nEpsilon = 0.35317136934261073\nEpsilon = 0.35313605220567645\nEpsilon = 0.3531007386004559\nEpsilon = 0.35306542852659584\nEpsilon = 0.3530301219837432\nEpsilon = 0.3529948189715448\nEpsilon = 0.3529595194896476\nEpsilon = 0.35292422353769864\nEpsilon = 0.35288893111534486\nEpsilon = 0.3528536422222333\nEpsilon = 0.3528183568580111\nEpsilon = 0.3527830750223253\nAgent: ddqn_agent . Episode 577/2000. Number of steps to finish: 20. Loss: 14.373531341552734 Reward: -16.0\nEpsilon = 0.3527477967148231\nEpsilon = 0.3527125219351516\nEpsilon = 0.35267725068295813\nEpsilon = 0.35264198295788984\nEpsilon = 0.35260671875959404\nEpsilon = 0.35257145808771806\nEpsilon = 0.3525362009419093\nEpsilon = 0.3525009473218151\nEpsilon = 0.35246569722708293\nEpsilon = 0.35243045065736023\nEpsilon = 0.3523952076122945\nEpsilon = 0.35235996809153325\nEpsilon = 0.3523247320947241\nEpsilon = 0.3522894996215146\nEpsilon = 0.35225427067155246\nEpsilon = 0.35221904524448533\nEpsilon = 0.3521838233399609\nEpsilon = 0.3521486049576269\nEpsilon = 0.35211339009713116\nEpsilon = 0.3520781787581215\nAgent: ddqn_agent . Episode 578/2000. Number of steps to finish: 20. Loss: 14.386004447937012 Reward: -20.0\nEpsilon = 0.35204297094024567\nEpsilon = 0.35200776664315164\nEpsilon = 0.35197256586648734\nEpsilon = 0.3519373686099007\nEpsilon = 0.3519021748730397\nEpsilon = 0.3518669846555524\nEpsilon = 0.35183179795708686\nEpsilon = 0.35179661477729113\nEpsilon = 0.3517614351158134\nEpsilon = 0.35172625897230186\nEpsilon = 0.3516910863464046\nEpsilon = 0.35165591723776995\nEpsilon = 0.35162075164604617\nEpsilon = 0.3515855895708816\nEpsilon = 0.3515504310119245\nEpsilon = 0.3515152759688233\nEpsilon = 0.35148012444122645\nEpsilon = 0.3514449764287823\nEpsilon = 0.35140983193113945\nEpsilon = 0.3513746909479463\nAgent: ddqn_agent . Episode 579/2000. Number of steps to finish: 20. Loss: 13.631685256958008 Reward: -14.0\nEpsilon = 0.35133955347885154\nEpsilon = 0.35130441952350366\nEpsilon = 0.3512692890815513\nEpsilon = 0.35123416215264314\nEpsilon = 0.3511990387364279\nEpsilon = 0.35116391883255427\nEpsilon = 0.351128802440671\nEpsilon = 0.35109368956042697\nEpsilon = 0.3510585801914709\nEpsilon = 0.35102347433345177\nEpsilon = 0.3509883719860184\nEpsilon = 0.3509532731488198\nEpsilon = 0.3509181778215049\nEpsilon = 0.35088308600372276\nEpsilon = 0.35084799769512237\nEpsilon = 0.3508129128953529\nEpsilon = 0.35077783160406334\nEpsilon = 0.3507427538209029\nEpsilon = 0.35070767954552085\nEpsilon = 0.3506726087775663\nAgent: ddqn_agent . Episode 580/2000. Number of steps to finish: 20. Loss: 14.110267639160156 Reward: -8.0\nEpsilon = 0.3506375415166885\nEpsilon = 0.35060247776253683\nEpsilon = 0.35056741751476056\nEpsilon = 0.3505323607730091\nEpsilon = 0.3504973075369318\nEpsilon = 0.3504622578061781\nEpsilon = 0.3504272115803975\nEpsilon = 0.35039216885923946\nEpsilon = 0.35035712964235355\nEpsilon = 0.35032209392938934\nEpsilon = 0.3502870617199964\nEpsilon = 0.3502520330138244\nEpsilon = 0.350217007810523\nEpsilon = 0.35018198610974194\nEpsilon = 0.350146967911131\nEpsilon = 0.35011195321433985\nEpsilon = 0.3500769420190184\nEpsilon = 0.3500419343248165\nEpsilon = 0.350006930131384\nEpsilon = 0.34997192943837085\nAgent: ddqn_agent . Episode 581/2000. Number of steps to finish: 20. Loss: 14.504546165466309 Reward: -8.0\nEpsilon = 0.34993693224542705\nEpsilon = 0.34990193855220253\nEpsilon = 0.3498669483583473\nEpsilon = 0.3498319616635115\nEpsilon = 0.34979697846734514\nEpsilon = 0.34976199876949843\nEpsilon = 0.34972702256962146\nEpsilon = 0.3496920498673645\nEpsilon = 0.3496570806623778\nEpsilon = 0.34962211495431156\nEpsilon = 0.34958715274281615\nEpsilon = 0.3495521940275419\nEpsilon = 0.3495172388081391\nEpsilon = 0.3494822870842583\nEpsilon = 0.3494473388555499\nEpsilon = 0.34941239412166436\nEpsilon = 0.3493774528822522\nEpsilon = 0.34934251513696396\nEpsilon = 0.3493075808854503\nEpsilon = 0.34927265012736175\nAgent: ddqn_agent . Episode 582/2000. Number of steps to finish: 20. Loss: 14.537574768066406 Reward: -16.0\nEpsilon = 0.349237722862349\nEpsilon = 0.34920279909006274\nEpsilon = 0.34916787881015376\nEpsilon = 0.34913296202227273\nEpsilon = 0.34909804872607053\nEpsilon = 0.34906313892119795\nEpsilon = 0.3490282326073058\nEpsilon = 0.34899332978404507\nEpsilon = 0.34895843045106667\nEpsilon = 0.34892353460802156\nEpsilon = 0.34888864225456073\nEpsilon = 0.34885375339033525\nEpsilon = 0.34881886801499623\nEpsilon = 0.3487839861281947\nEpsilon = 0.3487491077295819\nEpsilon = 0.3487142328188089\nEpsilon = 0.348679361395527\nEpsilon = 0.3486444934593875\nEpsilon = 0.34860962901004156\nEpsilon = 0.34857476804714055\nAgent: ddqn_agent . Episode 583/2000. Number of steps to finish: 20. Loss: 14.73570442199707 Reward: -14.0\nEpsilon = 0.34853991057033584\nEpsilon = 0.3485050565792788\nEpsilon = 0.3484702060736209\nEpsilon = 0.34843535905301354\nEpsilon = 0.34840051551710827\nEpsilon = 0.3483656754655566\nEpsilon = 0.34833083889801003\nEpsilon = 0.34829600581412024\nEpsilon = 0.3482611762135388\nEpsilon = 0.34822635009591746\nEpsilon = 0.34819152746090787\nEpsilon = 0.34815670830816176\nEpsilon = 0.34812189263733095\nEpsilon = 0.34808708044806724\nEpsilon = 0.34805227174002246\nEpsilon = 0.3480174665128485\nEpsilon = 0.3479826647661972\nEpsilon = 0.3479478664997206\nEpsilon = 0.3479130717130706\nAgent: ddqn_agent . Episode 584/2000. Number of steps to finish: 19. Loss: 13.277532577514648 Reward: -7.0\nEpsilon = 0.3478782804058993\nEpsilon = 0.34784349257785874\nEpsilon = 0.34780870822860094\nEpsilon = 0.34777392735777807\nEpsilon = 0.3477391499650423\nEpsilon = 0.3477043760500458\nEpsilon = 0.34766960561244076\nEpsilon = 0.3476348386518795\nEpsilon = 0.34760007516801433\nEpsilon = 0.34756531516049755\nAgent: ddqn_agent . Episode 585/2000. Number of steps to finish: 10. Loss: 7.179248809814453 Reward: 2.0\nEpsilon = 0.3475305586289815\nEpsilon = 0.34749580557311865\nEpsilon = 0.34746105599256133\nEpsilon = 0.3474263098869621\nEpsilon = 0.3473915672559734\nEpsilon = 0.34735682809924784\nEpsilon = 0.3473220924164379\nEpsilon = 0.34728736020719625\nEpsilon = 0.3472526314711755\nEpsilon = 0.3472179062080284\nEpsilon = 0.3471831844174076\nEpsilon = 0.3471484660989658\nEpsilon = 0.34711375125235594\nEpsilon = 0.3470790398772307\nEpsilon = 0.34704433197324297\nEpsilon = 0.34700962754004566\nEpsilon = 0.3469749265772917\nEpsilon = 0.34694022908463396\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.3469055350617255\nEpsilon = 0.3468708445082193\nAgent: ddqn_agent . Episode 586/2000. Number of steps to finish: 20. Loss: 14.11636734008789 Reward: -18.0\nEpsilon = 0.3468361574237685\nEpsilon = 0.3468014738080261\nEpsilon = 0.34676679366064533\nEpsilon = 0.34673211698127926\nEpsilon = 0.3466974437695811\nEpsilon = 0.3466627740252042\nEpsilon = 0.3466281077478017\nEpsilon = 0.3465934449370269\nEpsilon = 0.34655878559253317\nEpsilon = 0.3465241297139739\nEpsilon = 0.3464894773010025\nEpsilon = 0.3464548283532724\nEpsilon = 0.3464201828704371\nEpsilon = 0.34638554085215006\nEpsilon = 0.34635090229806487\nEpsilon = 0.34631626720783504\nEpsilon = 0.34628163558111424\nEpsilon = 0.34624700741755615\nEpsilon = 0.3462123827168144\nEpsilon = 0.34617776147854273\nAgent: ddqn_agent . Episode 587/2000. Number of steps to finish: 20. Loss: 14.102386474609375 Reward: -18.0\nEpsilon = 0.3461431437023949\nEpsilon = 0.34610852938802467\nEpsilon = 0.34607391853508584\nEpsilon = 0.34603931114323233\nEpsilon = 0.346004707212118\nEpsilon = 0.3459701067413968\nEpsilon = 0.3459355097307226\nEpsilon = 0.34590091617974955\nEpsilon = 0.34586632608813156\nEpsilon = 0.34583173945552276\nEpsilon = 0.3457971562815772\nEpsilon = 0.34576257656594905\nEpsilon = 0.3457280003082925\nEpsilon = 0.34569342750826165\nEpsilon = 0.34565885816551084\nEpsilon = 0.34562429227969427\nEpsilon = 0.3455897298504663\nEpsilon = 0.3455551708774813\nEpsilon = 0.3455206153603935\nEpsilon = 0.3454860632988575\nAgent: ddqn_agent . Episode 588/2000. Number of steps to finish: 20. Loss: 14.33336353302002 Reward: -12.0\nEpsilon = 0.3454515146925276\nEpsilon = 0.3454169695410584\nEpsilon = 0.3453824278441043\nEpsilon = 0.34534788960131985\nEpsilon = 0.3453133548123597\nEpsilon = 0.34527882347687844\nEpsilon = 0.34524429559453074\nEpsilon = 0.3452097711649713\nEpsilon = 0.34517525018785483\nEpsilon = 0.34514073266283607\nEpsilon = 0.3451062185895698\nEpsilon = 0.34507170796771086\nEpsilon = 0.3450372007969141\nEpsilon = 0.3450026970768344\nEpsilon = 0.3449681968071267\nEpsilon = 0.344933699987446\nEpsilon = 0.3448992066174473\nEpsilon = 0.34486471669678553\nEpsilon = 0.3448302302251159\nEpsilon = 0.3447957472020934\nAgent: ddqn_agent . Episode 589/2000. Number of steps to finish: 20. Loss: 14.219097137451172 Reward: -8.0\nEpsilon = 0.3447612676273732\nEpsilon = 0.3447267915006105\nEpsilon = 0.3446923188214604\nEpsilon = 0.34465784958957824\nEpsilon = 0.34462338380461927\nEpsilon = 0.3445889214662388\nEpsilon = 0.3445544625740922\nEpsilon = 0.3445200071278348\nEpsilon = 0.34448555512712203\nEpsilon = 0.3444511065716093\nEpsilon = 0.3444166614609522\nEpsilon = 0.3443822197948061\nEpsilon = 0.34434778157282664\nEpsilon = 0.34431334679466935\nEpsilon = 0.3442789154599899\nEpsilon = 0.3442444875684439\nEpsilon = 0.34421006311968705\nEpsilon = 0.3441756421133751\nEpsilon = 0.34414122454916374\nEpsilon = 0.3441068104267088\nAgent: ddqn_agent . Episode 590/2000. Number of steps to finish: 20. Loss: 13.623148918151855 Reward: -16.0\nEpsilon = 0.34407239974566617\nEpsilon = 0.3440379925056916\nEpsilon = 0.34400358870644104\nEpsilon = 0.3439691883475704\nEpsilon = 0.3439347914287356\nEpsilon = 0.34390039794959276\nEpsilon = 0.3438660079097978\nEpsilon = 0.34383162130900685\nEpsilon = 0.34379723814687596\nEpsilon = 0.34376285842306126\nAgent: ddqn_agent . Episode 591/2000. Number of steps to finish: 10. Loss: 6.973597526550293 Reward: 2.0\nEpsilon = 0.34372848213721896\nEpsilon = 0.34369410928900523\nEpsilon = 0.34365973987807635\nEpsilon = 0.34362537390408854\nEpsilon = 0.3435910113666981\nEpsilon = 0.3435566522655615\nEpsilon = 0.34352229660033495\nEpsilon = 0.3434879443706749\nEpsilon = 0.34345359557623784\nEpsilon = 0.34341925021668024\nEpsilon = 0.3433849082916586\nEpsilon = 0.3433505698008294\nEpsilon = 0.3433162347438493\nEpsilon = 0.34328190312037493\nEpsilon = 0.3432475749300629\nEpsilon = 0.34321325017256993\nEpsilon = 0.3431789288475527\nEpsilon = 0.3431446109546679\nEpsilon = 0.34311029649357244\nEpsilon = 0.3430759854639231\nAgent: ddqn_agent . Episode 592/2000. Number of steps to finish: 20. Loss: 13.859661102294922 Reward: -12.0\nEpsilon = 0.3430416778653767\nEpsilon = 0.34300737369759016\nEpsilon = 0.3429730729602204\nEpsilon = 0.34293877565292435\nEpsilon = 0.3429044817753591\nEpsilon = 0.34287019132718155\nEpsilon = 0.34283590430804883\nEpsilon = 0.34280162071761805\nEpsilon = 0.3427673405555463\nEpsilon = 0.34273306382149077\nEpsilon = 0.3426987905151086\nEpsilon = 0.34266452063605707\nEpsilon = 0.34263025418399345\nEpsilon = 0.3425959911585751\nEpsilon = 0.34256173155945924\nEpsilon = 0.3425274753863033\nEpsilon = 0.3424932226387647\nEpsilon = 0.3424589733165008\nEpsilon = 0.3424247274191692\nEpsilon = 0.34239048494642726\nAgent: ddqn_agent . Episode 593/2000. Number of steps to finish: 20. Loss: 14.376129150390625 Reward: -12.0\nEpsilon = 0.3423562458979326\nEpsilon = 0.34232201027334286\nEpsilon = 0.34228777807231553\nEpsilon = 0.3422535492945083\nEpsilon = 0.3422193239395788\nEpsilon = 0.3421851020071849\nEpsilon = 0.3421508834969842\nEpsilon = 0.3421166684086345\nEpsilon = 0.3420824567417936\nEpsilon = 0.3420482484961194\nEpsilon = 0.3420140436712698\nEpsilon = 0.3419798422669027\nEpsilon = 0.341945644282676\nEpsilon = 0.3419114497182477\nEpsilon = 0.3418772585732759\nEpsilon = 0.34184307084741855\nEpsilon = 0.3418088865403338\nEpsilon = 0.3417747056516798\nEpsilon = 0.3417405281811146\nEpsilon = 0.34170635412829653\nAgent: ddqn_agent . Episode 594/2000. Number of steps to finish: 20. Loss: 14.013771057128906 Reward: -18.0\nEpsilon = 0.3416721834928837\nEpsilon = 0.34163801627453444\nEpsilon = 0.341603852472907\nEpsilon = 0.3415696920876597\nEpsilon = 0.3415355351184509\nEpsilon = 0.3415013815649391\nEpsilon = 0.3414672314267826\nEpsilon = 0.3414330847036399\nEpsilon = 0.3413989413951696\nEpsilon = 0.34136480150103005\nEpsilon = 0.34133066502087994\nEpsilon = 0.34129653195437787\nEpsilon = 0.34126240230118243\nEpsilon = 0.3412282760609523\nEpsilon = 0.34119415323334623\nEpsilon = 0.3411600338180229\nEpsilon = 0.3411259178146411\nEpsilon = 0.3410918052228596\nEpsilon = 0.3410576960423373\nEpsilon = 0.3410235902727331\nAgent: ddqn_agent . Episode 595/2000. Number of steps to finish: 20. Loss: 14.248932838439941 Reward: -14.0\nEpsilon = 0.3409894879137058\nEpsilon = 0.34095538896491445\nEpsilon = 0.34092129342601796\nEpsilon = 0.3408872012966754\nEpsilon = 0.3408531125765457\nEpsilon = 0.34081902726528807\nEpsilon = 0.34078494536256154\nEpsilon = 0.3407508668680253\nEpsilon = 0.3407167917813385\nEpsilon = 0.3406827201021604\nEpsilon = 0.3406486518301502\nEpsilon = 0.34061458696496716\nEpsilon = 0.34058052550627066\nEpsilon = 0.34054646745372\nEpsilon = 0.34051241280697464\nEpsilon = 0.34047836156569394\nEpsilon = 0.3404443137295374\nEpsilon = 0.3404102692981644\nEpsilon = 0.3403762282712346\nEpsilon = 0.3403421906484075\nAgent: ddqn_agent . Episode 596/2000. Number of steps to finish: 20. Loss: 14.498208999633789 Reward: -10.0\nEpsilon = 0.34030815642934265\nEpsilon = 0.34027412561369974\nEpsilon = 0.3402400982011384\nEpsilon = 0.34020607419131826\nEpsilon = 0.3401720535838991\nEpsilon = 0.34013803637854073\nEpsilon = 0.34010402257490285\nEpsilon = 0.3400700121726454\nEpsilon = 0.3400360051714281\nEpsilon = 0.340002001570911\nAgent: ddqn_agent . Episode 597/2000. Number of steps to finish: 10. Loss: 7.223832607269287 Reward: 2.0\nEpsilon = 0.3399680013707539\nEpsilon = 0.33993400457061684\nEpsilon = 0.3399000111701598\nEpsilon = 0.33986602116904274\nEpsilon = 0.33983203456692584\nEpsilon = 0.33979805136346913\nEpsilon = 0.33976407155833277\nEpsilon = 0.3397300951511769\nEpsilon = 0.3396961221416618\nEpsilon = 0.33966215252944765\nEpsilon = 0.3396281863141947\nAgent: ddqn_agent . Episode 598/2000. Number of steps to finish: 11. Loss: 7.868603229522705 Reward: 1.0\nEpsilon = 0.3395942234955633\nEpsilon = 0.3395602640732137\nEpsilon = 0.3395263080468064\nEpsilon = 0.3394923554160017\nEpsilon = 0.3394584061804601\nEpsilon = 0.339424460339842\nEpsilon = 0.33939051789380803\nEpsilon = 0.33935657884201864\nEpsilon = 0.33932264318413446\nEpsilon = 0.33928871091981605\nEpsilon = 0.33925478204872406\nEpsilon = 0.3392208565705192\nEpsilon = 0.33918693448486215\nEpsilon = 0.33915301579141366\nEpsilon = 0.3391191004898345\nEpsilon = 0.33908518857978553\nEpsilon = 0.33905128006092755\nEpsilon = 0.33901737493292144\nEpsilon = 0.33898347319542815\nEpsilon = 0.3389495748481086\nAgent: ddqn_agent . Episode 599/2000. Number of steps to finish: 20. Loss: 14.365483283996582 Reward: -16.0\nEpsilon = 0.3389156798906238\nEpsilon = 0.33888178832263477\nEpsilon = 0.3388479001438025\nEpsilon = 0.33881401535378813\nEpsilon = 0.33878013395225276\nEpsilon = 0.33874625593885754\nEpsilon = 0.33871238131326364\nEpsilon = 0.33867851007513233\nEpsilon = 0.3386446422241248\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.3386107777599024\nEpsilon = 0.3385769166821264\nEpsilon = 0.3385430589904582\nEpsilon = 0.3385092046845592\nEpsilon = 0.33847535376409077\nEpsilon = 0.3384415062287144\nEpsilon = 0.33840766207809153\nAgent: ddqn_agent . Episode 600/2000. Number of steps to finish: 16. Loss: 11.463329315185547 Reward: -4.0\nEpsilon = 0.3383738213118837\nEpsilon = 0.3383399839297525\nEpsilon = 0.33830614993135955\nEpsilon = 0.3382723193163664\nEpsilon = 0.3382384920844348\nEpsilon = 0.3382046682352264\nEpsilon = 0.33817084776840284\nEpsilon = 0.338137030683626\nEpsilon = 0.3381032169805576\nEpsilon = 0.3380694066588596\nEpsilon = 0.3380355997181937\nEpsilon = 0.3380017961582219\nEpsilon = 0.3379679959786061\nEpsilon = 0.33793419917900824\nEpsilon = 0.33790040575909036\nEpsilon = 0.33786661571851445\nEpsilon = 0.3378328290569426\nEpsilon = 0.3377990457740369\nEpsilon = 0.3377652658694595\nEpsilon = 0.33773148934287256\nAgent: ddqn_agent . Episode 601/2000. Number of steps to finish: 20. Loss: 14.348433494567871 Reward: -14.0\nEpsilon = 0.3376977161939383\nEpsilon = 0.3376639464223189\nEpsilon = 0.33763018002767664\nEpsilon = 0.33759641700967385\nEpsilon = 0.3375626573679729\nEpsilon = 0.3375289011022361\nEpsilon = 0.3374951482121259\nEpsilon = 0.3374613986973047\nEpsilon = 0.33742765255743495\nEpsilon = 0.3373939097921792\nEpsilon = 0.3373601704012\nEpsilon = 0.3373264343841599\nEpsilon = 0.33729270174072146\nEpsilon = 0.33725897247054737\nEpsilon = 0.33722524657330033\nEpsilon = 0.337191524048643\nEpsilon = 0.33715780489623814\nEpsilon = 0.33712408911574854\nEpsilon = 0.33709037670683695\nEpsilon = 0.33705666766916625\nAgent: ddqn_agent . Episode 602/2000. Number of steps to finish: 20. Loss: 14.132766723632812 Reward: -10.0\nEpsilon = 0.33702296200239934\nEpsilon = 0.3369892597061991\nEpsilon = 0.3369555607802285\nEpsilon = 0.33692186522415046\nEpsilon = 0.336888173037628\nEpsilon = 0.33685448422032427\nEpsilon = 0.33682079877190224\nEpsilon = 0.33678711669202505\nEpsilon = 0.33675343798035584\nEpsilon = 0.3367197626365578\nEpsilon = 0.33668609066029415\nEpsilon = 0.33665242205122814\nEpsilon = 0.33661875680902303\nAgent: ddqn_agent . Episode 603/2000. Number of steps to finish: 13. Loss: 8.700385093688965 Reward: -1.0\nEpsilon = 0.33658509493334215\nEpsilon = 0.3365514364238488\nEpsilon = 0.33651778128020643\nEpsilon = 0.3364841295020784\nEpsilon = 0.3364504810891282\nEpsilon = 0.33641683604101924\nEpsilon = 0.33638319435741515\nEpsilon = 0.3363495560379794\nEpsilon = 0.3363159210823756\nEpsilon = 0.3362822894902674\nEpsilon = 0.33624866126131836\nEpsilon = 0.33621503639519223\nEpsilon = 0.33618141489155273\nEpsilon = 0.33614779675006357\nEpsilon = 0.33611418197038856\nEpsilon = 0.33608057055219154\nEpsilon = 0.3360469624951363\nEpsilon = 0.3360133577988868\nEpsilon = 0.3359797564631069\nEpsilon = 0.3359461584874606\nAgent: ddqn_agent . Episode 604/2000. Number of steps to finish: 20. Loss: 14.334552764892578 Reward: -8.0\nEpsilon = 0.3359125638716119\nEpsilon = 0.33587897261522476\nEpsilon = 0.3358453847179632\nEpsilon = 0.3358118001794914\nEpsilon = 0.33577821899947347\nEpsilon = 0.3357446411775735\nEpsilon = 0.33571106671345574\nEpsilon = 0.3356774956067844\nEpsilon = 0.33564392785722375\nEpsilon = 0.335610363464438\nEpsilon = 0.33557680242809157\nEpsilon = 0.33554324474784875\nEpsilon = 0.335509690423374\nEpsilon = 0.33547613945433163\nEpsilon = 0.3354425918403862\nEpsilon = 0.33540904758120216\nEpsilon = 0.33537550667644406\nEpsilon = 0.3353419691257764\nEpsilon = 0.3353084349288638\nEpsilon = 0.3352749040853709\nAgent: ddqn_agent . Episode 605/2000. Number of steps to finish: 20. Loss: 14.253475189208984 Reward: -12.0\nEpsilon = 0.3352413765949624\nEpsilon = 0.3352078524573029\nEpsilon = 0.3351743316720572\nEpsilon = 0.33514081423889\nEpsilon = 0.3351073001574661\nEpsilon = 0.33507378942745036\nEpsilon = 0.3350402820485076\nAgent: ddqn_agent . Episode 606/2000. Number of steps to finish: 7. Loss: 5.210625648498535 Reward: 5.0\nEpsilon = 0.33500677802030276\nEpsilon = 0.33497327734250076\nEpsilon = 0.33493978001476654\nEpsilon = 0.3349062860367651\nEpsilon = 0.3348727954081614\nEpsilon = 0.33483930812862056\nEpsilon = 0.3348058241978077\nEpsilon = 0.33477234361538794\nEpsilon = 0.3347388663810264\nEpsilon = 0.3347053924943883\nEpsilon = 0.33467192195513884\nEpsilon = 0.33463845476294335\nEpsilon = 0.33460499091746704\nEpsilon = 0.3345715304183753\nEpsilon = 0.33453807326533347\nEpsilon = 0.3345046194580069\nEpsilon = 0.3344711689960611\nEpsilon = 0.3344377218791615\nEpsilon = 0.3344042781069736\nEpsilon = 0.3343708376791629\nAgent: ddqn_agent . Episode 607/2000. Number of steps to finish: 20. Loss: 14.685685157775879 Reward: -16.0\nEpsilon = 0.334337400595395\nEpsilon = 0.3343039668553355\nEpsilon = 0.33427053645864996\nEpsilon = 0.3342371094050041\nEpsilon = 0.3342036856940636\nEpsilon = 0.3341702653254942\nEpsilon = 0.3341368482989617\nEpsilon = 0.3341034346141318\nEpsilon = 0.3340700242706704\nEpsilon = 0.33403661726824335\nEpsilon = 0.3340032136065165\nEpsilon = 0.3339698132851559\nEpsilon = 0.33393641630382737\nEpsilon = 0.333903022662197\nEpsilon = 0.3338696323599308\nEpsilon = 0.33383624539669476\nEpsilon = 0.3338028617721551\nEpsilon = 0.3337694814859779\nEpsilon = 0.3337361045378293\nEpsilon = 0.3337027309273755\nAgent: ddqn_agent . Episode 608/2000. Number of steps to finish: 20. Loss: 13.996538162231445 Reward: -14.0\nEpsilon = 0.33366936065428277\nEpsilon = 0.33363599371821734\nEpsilon = 0.3336026301188455\nEpsilon = 0.33356926985583363\nEpsilon = 0.33353591292884804\nEpsilon = 0.3335025593375552\nEpsilon = 0.3334692090816214\nEpsilon = 0.33343586216071325\nEpsilon = 0.33340251857449715\nEpsilon = 0.3333691783226397\nEpsilon = 0.33333584140480743\nEpsilon = 0.33330250782066695\nEpsilon = 0.3332691775698849\nEpsilon = 0.3332358506521279\nEpsilon = 0.3332025270670627\nEpsilon = 0.333169206814356\nEpsilon = 0.3331358898936746\nEpsilon = 0.33310257630468526\nEpsilon = 0.3330692660470548\nEpsilon = 0.3330359591204501\nAgent: ddqn_agent . Episode 609/2000. Number of steps to finish: 20. Loss: 14.352884292602539 Reward: -18.0\nEpsilon = 0.33300265552453806\nEpsilon = 0.33296935525898563\nEpsilon = 0.33293605832345974\nEpsilon = 0.3329027647176274\nEpsilon = 0.33286947444115567\nEpsilon = 0.3328361874937116\nEpsilon = 0.3328029038749622\nEpsilon = 0.3327696235845747\nEpsilon = 0.33273634662221624\nEpsilon = 0.332703072987554\nEpsilon = 0.33266980268025526\nEpsilon = 0.33263653569998725\nEpsilon = 0.33260327204641726\nEpsilon = 0.33257001171921263\nEpsilon = 0.33253675471804073\nEpsilon = 0.3325035010425689\nEpsilon = 0.33247025069246466\nEpsilon = 0.3324370036673954\nEpsilon = 0.3324037599670287\nEpsilon = 0.332370519591032\nAgent: ddqn_agent . Episode 610/2000. Number of steps to finish: 20. Loss: 13.790567398071289 Reward: -14.0\nEpsilon = 0.33233728253907285\nEpsilon = 0.33230404881081893\nEpsilon = 0.33227081840593786\nEpsilon = 0.33223759132409725\nEpsilon = 0.33220436756496485\nEpsilon = 0.33217114712820833\nEpsilon = 0.3321379300134955\nEpsilon = 0.33210471622049414\nEpsilon = 0.3320715057488721\nEpsilon = 0.33203829859829725\nEpsilon = 0.3320050947684374\nEpsilon = 0.33197189425896056\nEpsilon = 0.33193869706953466\nEpsilon = 0.3319055031998277\nEpsilon = 0.33187231264950773\nEpsilon = 0.3318391254182428\nEpsilon = 0.331805941505701\nEpsilon = 0.3317727609115504\nEpsilon = 0.33173958363545925\nEpsilon = 0.3317064096770957\nAgent: ddqn_agent . Episode 611/2000. Number of steps to finish: 20. Loss: 14.128664016723633 Reward: -12.0\nEpsilon = 0.331673239036128\nEpsilon = 0.33164007171222437\nEpsilon = 0.33160690770505313\nEpsilon = 0.3315737470142826\nEpsilon = 0.33154058963958116\nEpsilon = 0.3315074355806172\nEpsilon = 0.3314742848370591\nEpsilon = 0.3314411374085754\nEpsilon = 0.33140799329483456\nEpsilon = 0.3313748524955051\nEpsilon = 0.33134171501025556\nEpsilon = 0.33130858083875453\nEpsilon = 0.33127544998067066\nEpsilon = 0.3312423224356726\nEpsilon = 0.33120919820342903\nEpsilon = 0.3311760772836087\nEpsilon = 0.33114295967588037\nEpsilon = 0.3311098453799128\nEpsilon = 0.33107673439537477\nEpsilon = 0.3310436267219352\nAgent: ddqn_agent . Episode 612/2000. Number of steps to finish: 20. Loss: 14.663860321044922 Reward: -14.0\nEpsilon = 0.33101052235926304\nEpsilon = 0.3309774213070271\nEpsilon = 0.33094432356489645\nEpsilon = 0.33091122913253995\nEpsilon = 0.3308781380096267\nEpsilon = 0.3308450501958257\nEpsilon = 0.3308119656908061\nEpsilon = 0.33077888449423704\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.3307458066057876\nEpsilon = 0.330712732025127\nEpsilon = 0.3306796607519245\nEpsilon = 0.33064659278584935\nEpsilon = 0.33061352812657074\nEpsilon = 0.3305804667737581\nEpsilon = 0.3305474087270807\nEpsilon = 0.330514353986208\nEpsilon = 0.3304813025508094\nEpsilon = 0.3304482544205543\nEpsilon = 0.3304152095951123\nEpsilon = 0.33038216807415277\nAgent: ddqn_agent . Episode 613/2000. Number of steps to finish: 20. Loss: 14.575757026672363 Reward: -14.0\nEpsilon = 0.3303491298573454\nEpsilon = 0.33031609494435965\nEpsilon = 0.33028306333486523\nEpsilon = 0.33025003502853173\nEpsilon = 0.3302170100250289\nEpsilon = 0.3301839883240264\nEpsilon = 0.330150969925194\nEpsilon = 0.3301179548282015\nEpsilon = 0.3300849430327187\nEpsilon = 0.3300519345384154\nEpsilon = 0.3300189293449616\nEpsilon = 0.3299859274520271\nEpsilon = 0.3299529288592819\nEpsilon = 0.329919933566396\nEpsilon = 0.3298869415730393\nEpsilon = 0.32985395287888203\nEpsilon = 0.32982096748359413\nEpsilon = 0.3297879853868458\nEpsilon = 0.3297550065883071\nEpsilon = 0.32972203108764825\nAgent: ddqn_agent . Episode 614/2000. Number of steps to finish: 20. Loss: 14.278544425964355 Reward: -14.0\nEpsilon = 0.3296890588845395\nEpsilon = 0.32965608997865103\nEpsilon = 0.3296231243696532\nEpsilon = 0.32959016205721625\nEpsilon = 0.32955720304101055\nEpsilon = 0.32952424732070645\nEpsilon = 0.3294912948959744\nEpsilon = 0.3294583457664848\nEpsilon = 0.32942539993190817\nEpsilon = 0.329392457391915\nEpsilon = 0.3293595181461758\nEpsilon = 0.3293265821943612\nEpsilon = 0.32929364953614176\nAgent: ddqn_agent . Episode 615/2000. Number of steps to finish: 13. Loss: 9.195305824279785 Reward: -1.0\nEpsilon = 0.3292607201711881\nEpsilon = 0.329227794099171\nEpsilon = 0.3291948713197611\nEpsilon = 0.32916195183262914\nEpsilon = 0.3291290356374459\nEpsilon = 0.32909612273388217\nEpsilon = 0.3290632131216088\nEpsilon = 0.32903030680029666\nEpsilon = 0.3289974037696166\nEpsilon = 0.32896450402923966\nEpsilon = 0.3289316075788367\nEpsilon = 0.32889871441807883\nEpsilon = 0.32886582454663704\nEpsilon = 0.3288329379641824\nEpsilon = 0.328800054670386\nEpsilon = 0.32876717466491895\nEpsilon = 0.32873429794745246\nEpsilon = 0.3287014245176577\nEpsilon = 0.328668554375206\nEpsilon = 0.32863568751976846\nAgent: ddqn_agent . Episode 616/2000. Number of steps to finish: 20. Loss: 14.506711959838867 Reward: -14.0\nEpsilon = 0.3286028239510165\nEpsilon = 0.3285699636686214\nEpsilon = 0.32853710667225455\nEpsilon = 0.3285042529615873\nEpsilon = 0.32847140253629115\nEpsilon = 0.3284385553960375\nEpsilon = 0.3284057115404979\nEpsilon = 0.32837287096934387\nEpsilon = 0.32834003368224696\nEpsilon = 0.3283071996788787\nEpsilon = 0.3282743689589108\nEpsilon = 0.32824154152201496\nEpsilon = 0.3282087173678628\nEpsilon = 0.328175896496126\nEpsilon = 0.32814307890647637\nEpsilon = 0.3281102645985857\nEpsilon = 0.32807745357212587\nEpsilon = 0.3280446458267687\nEpsilon = 0.32801184136218603\nEpsilon = 0.3279790401780498\nAgent: ddqn_agent . Episode 617/2000. Number of steps to finish: 20. Loss: 14.333175659179688 Reward: -16.0\nEpsilon = 0.327946242274032\nEpsilon = 0.3279134476498046\nEpsilon = 0.32788065630503965\nEpsilon = 0.32784786823940915\nEpsilon = 0.3278150834525852\nEpsilon = 0.32778230194423996\nEpsilon = 0.32774952371404553\nEpsilon = 0.3277167487616741\nEpsilon = 0.327683977086798\nEpsilon = 0.3276512086890893\nEpsilon = 0.3276184435682204\nEpsilon = 0.3275856817238636\nEpsilon = 0.32755292315569123\nEpsilon = 0.3275201678633757\nEpsilon = 0.32748741584658936\nEpsilon = 0.3274546671050047\nEpsilon = 0.3274219216382942\nEpsilon = 0.3273891794461304\nEpsilon = 0.3273564405281858\nEpsilon = 0.327323704884133\nAgent: ddqn_agent . Episode 618/2000. Number of steps to finish: 20. Loss: 14.244084358215332 Reward: -12.0\nEpsilon = 0.3272909725136446\nEpsilon = 0.3272582434163932\nEpsilon = 0.32722551759205154\nEpsilon = 0.32719279504029236\nEpsilon = 0.3271600757607883\nEpsilon = 0.3271273597532122\nEpsilon = 0.3270946470172369\nEpsilon = 0.32706193755253515\nEpsilon = 0.3270292313587799\nEpsilon = 0.32699652843564403\nEpsilon = 0.32696382878280045\nEpsilon = 0.32693113239992216\nEpsilon = 0.32689843928668216\nEpsilon = 0.3268657494427535\nEpsilon = 0.3268330628678092\nEpsilon = 0.32680037956152247\nEpsilon = 0.3267676995235663\nEpsilon = 0.326735022753614\nEpsilon = 0.3267023492513386\nEpsilon = 0.3266696790164135\nAgent: ddqn_agent . Episode 619/2000. Number of steps to finish: 20. Loss: 14.747354507446289 Reward: -10.0\nEpsilon = 0.3266370120485118\nEpsilon = 0.32660434834730695\nEpsilon = 0.3265716879124722\nEpsilon = 0.32653903074368096\nEpsilon = 0.32650637684060657\nEpsilon = 0.32647372620292253\nEpsilon = 0.32644107883030227\nEpsilon = 0.32640843472241926\nEpsilon = 0.326375793878947\nEpsilon = 0.3263431562995591\nEpsilon = 0.3263105219839292\nEpsilon = 0.3262778909317308\nEpsilon = 0.32624526314263763\nEpsilon = 0.3262126386163234\nEpsilon = 0.32618001735246177\nEpsilon = 0.3261473993507265\nEpsilon = 0.32611478461079146\nAgent: ddqn_agent . Episode 620/2000. Number of steps to finish: 17. Loss: 12.477755546569824 Reward: -5.0\nEpsilon = 0.3260821731323304\nEpsilon = 0.3260495649150172\nEpsilon = 0.3260169599585257\nEpsilon = 0.32598435826252986\nEpsilon = 0.3259517598267036\nEpsilon = 0.32591916465072096\nEpsilon = 0.3258865727342559\nEpsilon = 0.3258539840769825\nEpsilon = 0.32582139867857485\nEpsilon = 0.325788816538707\nEpsilon = 0.32575623765705314\nEpsilon = 0.32572366203328745\nEpsilon = 0.32569108966708415\nEpsilon = 0.32565852055811745\nEpsilon = 0.32562595470606165\nEpsilon = 0.3255933921105911\nEpsilon = 0.32556083277138004\nEpsilon = 0.3255282766881029\nEpsilon = 0.3254957238604341\nEpsilon = 0.3254631742880481\nAgent: ddqn_agent . Episode 621/2000. Number of steps to finish: 20. Loss: 14.49085807800293 Reward: -16.0\nEpsilon = 0.3254306279706193\nEpsilon = 0.32539808490782224\nEpsilon = 0.32536554509933147\nEpsilon = 0.32533300854482156\nEpsilon = 0.3253004752439671\nEpsilon = 0.3252679451964427\nEpsilon = 0.32523541840192305\nEpsilon = 0.32520289486008286\nEpsilon = 0.32517037457059683\nEpsilon = 0.3251378575331398\nEpsilon = 0.3251053437473865\nEpsilon = 0.3250728332130117\nEpsilon = 0.32504032592969045\nEpsilon = 0.3250078218970975\nEpsilon = 0.3249753211149078\nEpsilon = 0.3249428235827963\nEpsilon = 0.324910329300438\nEpsilon = 0.324877838267508\nAgent: ddqn_agent . Episode 622/2000. Number of steps to finish: 18. Loss: 12.718059539794922 Reward: -6.0\nEpsilon = 0.32484535048368124\nEpsilon = 0.3248128659486329\nEpsilon = 0.32478038466203807\nEpsilon = 0.3247479066235719\nEpsilon = 0.3247154318329095\nEpsilon = 0.3246829602897262\nEpsilon = 0.3246504919936972\nEpsilon = 0.3246180269444979\nEpsilon = 0.3245855651418034\nEpsilon = 0.32455310658528924\nEpsilon = 0.3245206512746307\nEpsilon = 0.3244881992095032\nEpsilon = 0.3244557503895823\nEpsilon = 0.32442330481454335\nEpsilon = 0.3243908624840619\nEpsilon = 0.3243584233978135\nEpsilon = 0.32432598755547376\nEpsilon = 0.32429355495671824\nEpsilon = 0.32426112560122256\nEpsilon = 0.32422869948866245\nAgent: ddqn_agent . Episode 623/2000. Number of steps to finish: 20. Loss: 14.29262638092041 Reward: -18.0\nEpsilon = 0.3241962766187136\nEpsilon = 0.3241638569910517\nEpsilon = 0.3241314406053526\nEpsilon = 0.32409902746129204\nEpsilon = 0.32406661755854593\nEpsilon = 0.3240342108967901\nEpsilon = 0.3240018074757004\nEpsilon = 0.3239694072949528\nEpsilon = 0.32393701035422334\nEpsilon = 0.3239046166531879\nEpsilon = 0.3238722261915226\nEpsilon = 0.32383983896890345\nEpsilon = 0.32380745498500657\nEpsilon = 0.3237750742395081\nEpsilon = 0.3237426967320841\nEpsilon = 0.3237103224624109\nEpsilon = 0.32367795143016465\nEpsilon = 0.3236455836350216\nEpsilon = 0.32361321907665813\nEpsilon = 0.3235808577547505\nAgent: ddqn_agent . Episode 624/2000. Number of steps to finish: 20. Loss: 14.404228210449219 Reward: -14.0\nEpsilon = 0.32354849966897503\nEpsilon = 0.32351614481900814\nEpsilon = 0.32348379320452625\nEpsilon = 0.3234514448252058\nEpsilon = 0.3234190996807233\nEpsilon = 0.3233867577707552\nEpsilon = 0.32335441909497814\nEpsilon = 0.32332208365306864\nEpsilon = 0.32328975144470334\nEpsilon = 0.32325742246955885\nEpsilon = 0.3232250967273119\nEpsilon = 0.3231927742176392\nEpsilon = 0.32316045494021745\nEpsilon = 0.3231281388947234\nEpsilon = 0.3230958260808339\nEpsilon = 0.32306351649822584\nEpsilon = 0.32303121014657604\nEpsilon = 0.3229989070255614\nEpsilon = 0.32296660713485886\nEpsilon = 0.3229343104741454\nAgent: ddqn_agent . Episode 625/2000. Number of steps to finish: 20. Loss: 14.147575378417969 Reward: -10.0\nEpsilon = 0.32290201704309796\nEpsilon = 0.32286972684139364\nEpsilon = 0.3228374398687095\nEpsilon = 0.3228051561247226\nEpsilon = 0.3227728756091101\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.3227405983215492\nEpsilon = 0.3227083242617171\nEpsilon = 0.3226760534292909\nEpsilon = 0.322643785823948\nEpsilon = 0.3226115214453656\nEpsilon = 0.32257926029322104\nEpsilon = 0.3225470023671917\nEpsilon = 0.322514747666955\nEpsilon = 0.3224824961921883\nEpsilon = 0.32245024794256905\nEpsilon = 0.3224180029177748\nEpsilon = 0.322385761117483\nEpsilon = 0.3223535225413713\nEpsilon = 0.3223212871891172\nEpsilon = 0.3222890550603983\nAgent: ddqn_agent . Episode 626/2000. Number of steps to finish: 20. Loss: 14.823486328125 Reward: -14.0\nEpsilon = 0.32225682615489226\nEpsilon = 0.3222246004722768\nEpsilon = 0.32219237801222955\nEpsilon = 0.32216015877442833\nEpsilon = 0.3221279427585509\nEpsilon = 0.32209572996427505\nEpsilon = 0.3220635203912786\nEpsilon = 0.3220313140392395\nEpsilon = 0.3219991109078356\nEpsilon = 0.32196691099674485\nEpsilon = 0.3219347143056452\nEpsilon = 0.3219025208342146\nEpsilon = 0.3218703305821312\nEpsilon = 0.32183814354907303\nEpsilon = 0.32180595973471815\nEpsilon = 0.32177377913874466\nEpsilon = 0.3217416017608308\nEpsilon = 0.3217094276006547\nEpsilon = 0.32167725665789465\nEpsilon = 0.3216450889322289\nAgent: ddqn_agent . Episode 627/2000. Number of steps to finish: 20. Loss: 14.062176704406738 Reward: -12.0\nEpsilon = 0.32161292442333567\nEpsilon = 0.32158076313089334\nEpsilon = 0.32154860505458027\nEpsilon = 0.3215164501940748\nEpsilon = 0.3214842985490554\nEpsilon = 0.32145215011920053\nEpsilon = 0.3214200049041886\nEpsilon = 0.3213878629036982\nEpsilon = 0.3213557241174078\nEpsilon = 0.3213235885449961\nEpsilon = 0.3212914561861416\nEpsilon = 0.32125932704052296\nEpsilon = 0.3212272011078189\nEpsilon = 0.3211950783877081\nEpsilon = 0.32116295887986934\nEpsilon = 0.32113084258398134\nEpsilon = 0.32109872949972296\nEpsilon = 0.32106661962677296\nEpsilon = 0.3210345129648103\nEpsilon = 0.32100240951351383\nAgent: ddqn_agent . Episode 628/2000. Number of steps to finish: 20. Loss: 14.512528419494629 Reward: -12.0\nEpsilon = 0.3209703092725625\nEpsilon = 0.3209382122416352\nEpsilon = 0.32090611842041106\nEpsilon = 0.320874027808569\nEpsilon = 0.3208419404057881\nEpsilon = 0.32080985621174757\nEpsilon = 0.3207777752261264\nEpsilon = 0.32074569744860376\nEpsilon = 0.3207136228788589\nEpsilon = 0.320681551516571\nEpsilon = 0.32064948336141935\nEpsilon = 0.3206174184130832\nEpsilon = 0.32058535667124194\nEpsilon = 0.3205532981355748\nEpsilon = 0.3205212428057613\nEpsilon = 0.3204891906814807\nEpsilon = 0.3204571417624125\nEpsilon = 0.32042509604823627\nEpsilon = 0.32039305353863146\nEpsilon = 0.3203610142332776\nAgent: ddqn_agent . Episode 629/2000. Number of steps to finish: 20. Loss: 14.074615478515625 Reward: -18.0\nEpsilon = 0.3203289781318543\nEpsilon = 0.3202969452340411\nEpsilon = 0.3202649155395177\nEpsilon = 0.32023288904796376\nEpsilon = 0.320200865759059\nEpsilon = 0.3201688456724831\nEpsilon = 0.3201368287879159\nEpsilon = 0.3201048151050371\nEpsilon = 0.32007280462352655\nEpsilon = 0.3200407973430642\nEpsilon = 0.32000879326332987\nEpsilon = 0.31997679238400356\nEpsilon = 0.31994479470476517\nEpsilon = 0.31991280022529467\nEpsilon = 0.31988080894527215\nEpsilon = 0.31984882086437766\nEpsilon = 0.3198168359822912\nEpsilon = 0.31978485429869297\nEpsilon = 0.3197528758132631\nEpsilon = 0.3197209005256818\nAgent: ddqn_agent . Episode 630/2000. Number of steps to finish: 20. Loss: 14.22838306427002 Reward: -18.0\nEpsilon = 0.31968892843562924\nEpsilon = 0.31965695954278567\nEpsilon = 0.3196249938468314\nEpsilon = 0.3195930313474467\nEpsilon = 0.31956107204431194\nEpsilon = 0.31952911593710753\nEpsilon = 0.31949716302551384\nEpsilon = 0.3194652133092113\nEpsilon = 0.31943326678788037\nEpsilon = 0.3194013234612016\nEpsilon = 0.31936938332885545\nEpsilon = 0.3193374463905226\nEpsilon = 0.3193055126458835\nEpsilon = 0.3192735820946189\nEpsilon = 0.3192416547364095\nEpsilon = 0.3192097305709358\nEpsilon = 0.3191778095978787\nEpsilon = 0.31914589181691894\nEpsilon = 0.31911397722773727\nEpsilon = 0.31908206583001447\nAgent: ddqn_agent . Episode 631/2000. Number of steps to finish: 20. Loss: 14.419085502624512 Reward: -12.0\nEpsilon = 0.3190501576234315\nEpsilon = 0.31901825260766914\nEpsilon = 0.3189863507824084\nEpsilon = 0.31895445214733015\nEpsilon = 0.31892255670211545\nEpsilon = 0.31889066444644526\nEpsilon = 0.3188587753800006\nEpsilon = 0.31882688950246263\nEpsilon = 0.31879500681351236\nEpsilon = 0.318763127312831\nEpsilon = 0.31873125100009975\nEpsilon = 0.31869937787499975\nEpsilon = 0.31866750793721227\nEpsilon = 0.3186356411864186\nEpsilon = 0.3186037776222999\nEpsilon = 0.3185719172445377\nEpsilon = 0.31854006005281327\nEpsilon = 0.31850820604680796\nEpsilon = 0.31847635522620327\nEpsilon = 0.31844450759068066\nAgent: ddqn_agent . Episode 632/2000. Number of steps to finish: 20. Loss: 14.040824890136719 Reward: -12.0\nEpsilon = 0.3184126631399216\nEpsilon = 0.3183808218736076\nEpsilon = 0.31834898379142024\nEpsilon = 0.3183171488930411\nEpsilon = 0.3182853171781518\nEpsilon = 0.318253488646434\nEpsilon = 0.31822166329756935\nEpsilon = 0.3181898411312396\nEpsilon = 0.31815802214712646\nEpsilon = 0.31812620634491173\nEpsilon = 0.3180943937242772\nEpsilon = 0.3180625842849048\nEpsilon = 0.31803077802647634\nEpsilon = 0.3179989749486737\nEpsilon = 0.31796717505117883\nEpsilon = 0.3179353783336737\nEpsilon = 0.3179035847958403\nEpsilon = 0.31787179443736074\nEpsilon = 0.317840007257917\nEpsilon = 0.3178082232571912\nAgent: ddqn_agent . Episode 633/2000. Number of steps to finish: 20. Loss: 14.512150764465332 Reward: -14.0\nEpsilon = 0.3177764424348655\nEpsilon = 0.317744664790622\nEpsilon = 0.31771289032414296\nEpsilon = 0.31768111903511054\nEpsilon = 0.317649350923207\nEpsilon = 0.3176175859881147\nEpsilon = 0.31758582422951587\nEpsilon = 0.31755406564709293\nEpsilon = 0.31752231024052824\nEpsilon = 0.3174905580095042\nEpsilon = 0.31745880895370326\nEpsilon = 0.3174270630728079\nEpsilon = 0.3173953203665006\nEpsilon = 0.3173635808344639\nEpsilon = 0.31733184447638046\nEpsilon = 0.3173001112919328\nEpsilon = 0.3172683812808036\nEpsilon = 0.3172366544426755\nEpsilon = 0.3172049307772313\nEpsilon = 0.3171732102841536\nAgent: ddqn_agent . Episode 634/2000. Number of steps to finish: 20. Loss: 14.464007377624512 Reward: -12.0\nEpsilon = 0.31714149296312516\nEpsilon = 0.31710977881382885\nEpsilon = 0.31707806783594744\nEpsilon = 0.31704636002916387\nEpsilon = 0.3170146553931609\nEpsilon = 0.3169829539276216\nEpsilon = 0.31695125563222887\nEpsilon = 0.31691956050666564\nEpsilon = 0.31688786855061496\nEpsilon = 0.3168561797637599\nEpsilon = 0.3168244941457835\nEpsilon = 0.3167928116963689\nEpsilon = 0.31676113241519926\nEpsilon = 0.31672945630195776\nEpsilon = 0.3166977833563276\nEpsilon = 0.31666611357799196\nEpsilon = 0.3166344469666342\nEpsilon = 0.3166027835219375\nEpsilon = 0.31657112324358533\nEpsilon = 0.316539466131261\nAgent: ddqn_agent . Episode 635/2000. Number of steps to finish: 20. Loss: 13.847566604614258 Reward: -10.0\nEpsilon = 0.31650781218464785\nEpsilon = 0.3164761614034294\nEpsilon = 0.31644451378728905\nEpsilon = 0.31641286933591034\nEpsilon = 0.3163812280489767\nEpsilon = 0.31634958992617185\nEpsilon = 0.31631795496717924\nEpsilon = 0.31628632317168254\nEpsilon = 0.3162546945393654\nEpsilon = 0.31622306906991143\nEpsilon = 0.3161914467630044\nEpsilon = 0.3161598276183281\nEpsilon = 0.31612821163556626\nEpsilon = 0.3160965988144027\nEpsilon = 0.3160649891545213\nEpsilon = 0.3160333826556059\nEpsilon = 0.3160017793173403\nEpsilon = 0.31597017913940856\nEpsilon = 0.3159385821214946\nEpsilon = 0.31590698826328245\nAgent: ddqn_agent . Episode 636/2000. Number of steps to finish: 20. Loss: 14.137401580810547 Reward: -14.0\nEpsilon = 0.31587539756445615\nEpsilon = 0.3158438100246997\nEpsilon = 0.3158122256436972\nEpsilon = 0.31578064442113285\nEpsilon = 0.31574906635669076\nEpsilon = 0.31571749145005507\nEpsilon = 0.3156859197009101\nEpsilon = 0.31565435110894\nEpsilon = 0.3156227856738291\nEpsilon = 0.31559122339526174\nEpsilon = 0.3155596642729222\nEpsilon = 0.3155281083064949\nEpsilon = 0.31549655549566424\nEpsilon = 0.31546500584011467\nEpsilon = 0.31543345933953065\nEpsilon = 0.3154019159935967\nEpsilon = 0.31537037580199734\nEpsilon = 0.31533883876441715\nEpsilon = 0.3153073048805407\nEpsilon = 0.31527577415005265\nAgent: ddqn_agent . Episode 637/2000. Number of steps to finish: 20. Loss: 14.51061725616455 Reward: -14.0\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.31524424657263767\nEpsilon = 0.3152127221479804\nEpsilon = 0.3151812008757656\nEpsilon = 0.31514968275567806\nEpsilon = 0.3151181677874025\nEpsilon = 0.31508665597062374\nEpsilon = 0.3150551473050267\nEpsilon = 0.3150236417902962\nEpsilon = 0.31499213942611715\nEpsilon = 0.3149606402121745\nEpsilon = 0.3149291441481533\nEpsilon = 0.31489765123373853\nEpsilon = 0.3148661614686152\nEpsilon = 0.3148346748524683\nEpsilon = 0.3148031913849831\nEpsilon = 0.31477171106584456\nEpsilon = 0.314740233894738\nEpsilon = 0.3147087598713485\nEpsilon = 0.31467728899536135\nEpsilon = 0.3146458212664618\nAgent: ddqn_agent . Episode 638/2000. Number of steps to finish: 20. Loss: 14.231430053710938 Reward: -12.0\nEpsilon = 0.31461435668433513\nEpsilon = 0.31458289524866667\nEpsilon = 0.3145514369591418\nEpsilon = 0.3145199818154459\nEpsilon = 0.31448852981726433\nEpsilon = 0.3144570809642826\nEpsilon = 0.31442563525618616\nEpsilon = 0.31439419269266056\nEpsilon = 0.3143627532733913\nEpsilon = 0.314331316998064\nEpsilon = 0.3142998838663642\nEpsilon = 0.3142684538779776\nEpsilon = 0.31423702703258977\nEpsilon = 0.3142056033298865\nEpsilon = 0.3141741827695535\nEpsilon = 0.31414276535127655\nEpsilon = 0.3141113510747414\nEpsilon = 0.31407993993963396\nEpsilon = 0.31404853194564\nEpsilon = 0.3140171270924454\nAgent: ddqn_agent . Episode 639/2000. Number of steps to finish: 20. Loss: 14.127154350280762 Reward: -10.0\nEpsilon = 0.3139857253797361\nEpsilon = 0.31395432680719815\nEpsilon = 0.31392293137451743\nEpsilon = 0.31389153908138\nEpsilon = 0.31386014992747185\nEpsilon = 0.3138287639124791\nEpsilon = 0.31379738103608784\nEpsilon = 0.3137660012979842\nEpsilon = 0.3137346246978544\nEpsilon = 0.3137032512353846\nEpsilon = 0.3136718809102611\nEpsilon = 0.31364051372217006\nEpsilon = 0.31360914967079784\nEpsilon = 0.31357778875583076\nEpsilon = 0.31354643097695517\nEpsilon = 0.3135150763338575\nEpsilon = 0.3134837248262241\nEpsilon = 0.31345237645374147\nEpsilon = 0.3134210312160961\nAgent: ddqn_agent . Episode 640/2000. Number of steps to finish: 19. Loss: 13.759215354919434 Reward: -7.0\nEpsilon = 0.3133896891129745\nEpsilon = 0.3133583501440632\nEpsilon = 0.3133270143090488\nEpsilon = 0.3132956816076179\nEpsilon = 0.31326435203945713\nEpsilon = 0.3132330256042532\nEpsilon = 0.31320170230169275\nEpsilon = 0.3131703821314626\nEpsilon = 0.31313906509324946\nEpsilon = 0.3131077511867401\nEpsilon = 0.3130764404116214\nEpsilon = 0.3130451327675803\nEpsilon = 0.3130138282543035\nEpsilon = 0.3129825268714781\nEpsilon = 0.31295122861879093\nEpsilon = 0.31291993349592906\nEpsilon = 0.31288864150257945\nEpsilon = 0.3128573526384292\nEpsilon = 0.31282606690316533\nEpsilon = 0.312794784296475\nAgent: ddqn_agent . Episode 641/2000. Number of steps to finish: 20. Loss: 14.348501205444336 Reward: -8.0\nEpsilon = 0.3127635048180454\nEpsilon = 0.3127322284675636\nEpsilon = 0.31270095524471686\nEpsilon = 0.3126696851491924\nEpsilon = 0.3126384181806775\nEpsilon = 0.31260715433885944\nEpsilon = 0.3125758936234256\nEpsilon = 0.31254463603406324\nEpsilon = 0.3125133815704598\nEpsilon = 0.31248213023230276\nEpsilon = 0.3124508820192795\nEpsilon = 0.3124196369310776\nEpsilon = 0.3123883949673845\nEpsilon = 0.3123571561278878\nEpsilon = 0.31232592041227497\nEpsilon = 0.31229468782023373\nEpsilon = 0.3122634583514517\nEpsilon = 0.31223223200561656\nEpsilon = 0.312201008782416\nEpsilon = 0.3121697886815378\nAgent: ddqn_agent . Episode 642/2000. Number of steps to finish: 20. Loss: 14.413413047790527 Reward: -14.0\nEpsilon = 0.3121385717026696\nEpsilon = 0.31210735784549937\nEpsilon = 0.3120761471097148\nEpsilon = 0.31204493949500384\nEpsilon = 0.31201373500105434\nEpsilon = 0.31198253362755424\nEpsilon = 0.3119513353741915\nEpsilon = 0.31192014024065406\nEpsilon = 0.31188894822663\nEpsilon = 0.3118577593318073\nEpsilon = 0.3118265735558741\nEpsilon = 0.31179539089851854\nEpsilon = 0.31176421135942867\nEpsilon = 0.31173303493829274\nEpsilon = 0.3117018616347989\nEpsilon = 0.3116706914486354\nEpsilon = 0.31163952437949055\nEpsilon = 0.3116083604270526\nEpsilon = 0.3115771995910099\nEpsilon = 0.3115460418710508\nAgent: ddqn_agent . Episode 643/2000. Number of steps to finish: 20. Loss: 14.153324127197266 Reward: -12.0\nEpsilon = 0.31151488726686366\nEpsilon = 0.311483735778137\nEpsilon = 0.31145258740455917\nEpsilon = 0.31142144214581874\nEpsilon = 0.31139030000160417\nEpsilon = 0.311359160971604\nEpsilon = 0.31132802505550683\nEpsilon = 0.3112968922530013\nEpsilon = 0.31126576256377597\nEpsilon = 0.3112346359875196\nEpsilon = 0.31120351252392087\nEpsilon = 0.31117239217266845\nEpsilon = 0.3111412749334512\nEpsilon = 0.3111101608059578\nEpsilon = 0.3110790497898772\nEpsilon = 0.3110479418848982\nEpsilon = 0.3110168370907097\nEpsilon = 0.31098573540700064\nEpsilon = 0.3109546368334599\nEpsilon = 0.31092354136977657\nAgent: ddqn_agent . Episode 644/2000. Number of steps to finish: 20. Loss: 14.025836944580078 Reward: -18.0\nEpsilon = 0.3108924490156396\nEpsilon = 0.31086135977073803\nEpsilon = 0.31083027363476096\nEpsilon = 0.31079919060739747\nEpsilon = 0.3107681106883367\nEpsilon = 0.31073703387726787\nEpsilon = 0.31070596017388014\nEpsilon = 0.31067488957786277\nEpsilon = 0.310643822088905\nEpsilon = 0.3106127577066961\nEpsilon = 0.3105816964309254\nEpsilon = 0.3105506382612823\nEpsilon = 0.3105195831974562\nEpsilon = 0.31048853123913644\nEpsilon = 0.31045748238601256\nEpsilon = 0.310426436637774\nEpsilon = 0.3103953939941102\nEpsilon = 0.3103643544547108\nEpsilon = 0.31033331801926534\nEpsilon = 0.3103022846874634\nAgent: ddqn_agent . Episode 645/2000. Number of steps to finish: 20. Loss: 14.439695358276367 Reward: -20.0\nEpsilon = 0.31027125445899467\nEpsilon = 0.3102402273335488\nEpsilon = 0.3102092033108154\nEpsilon = 0.31017818239048434\nEpsilon = 0.3101471645722453\nEpsilon = 0.31011614985578806\nEpsilon = 0.31008513824080247\nEpsilon = 0.3100541297269784\nEpsilon = 0.3100231243140057\nEpsilon = 0.3099921220015743\nEpsilon = 0.3099611227893741\nEpsilon = 0.3099301266770952\nEpsilon = 0.3098991336644275\nEpsilon = 0.3098681437510611\nEpsilon = 0.309837156936686\nEpsilon = 0.30980617322099235\nEpsilon = 0.30977519260367026\nEpsilon = 0.3097442150844099\nEpsilon = 0.30971324066290146\nEpsilon = 0.3096822693388352\nAgent: ddqn_agent . Episode 646/2000. Number of steps to finish: 20. Loss: 14.245368003845215 Reward: -10.0\nEpsilon = 0.3096513011119013\nEpsilon = 0.3096203359817901\nEpsilon = 0.3095893739481919\nEpsilon = 0.3095584150107971\nEpsilon = 0.309527459169296\nEpsilon = 0.3094965064233791\nEpsilon = 0.3094655567727368\nEpsilon = 0.30943461021705954\nEpsilon = 0.3094036667560378\nEpsilon = 0.3093727263893622\nEpsilon = 0.3093417891167233\nEpsilon = 0.30931085493781163\nEpsilon = 0.30927992385231784\nEpsilon = 0.3092489958599326\nEpsilon = 0.3092180709603466\nEpsilon = 0.3091871491532506\nEpsilon = 0.30915623043833523\nEpsilon = 0.30912531481529143\nEpsilon = 0.3090944022838099\nEpsilon = 0.30906349284358153\nAgent: ddqn_agent . Episode 647/2000. Number of steps to finish: 20. Loss: 14.982495307922363 Reward: -16.0\nEpsilon = 0.30903258649429716\nEpsilon = 0.30900168323564775\nEpsilon = 0.3089707830673242\nEpsilon = 0.30893988598901745\nEpsilon = 0.30890899200041855\nEpsilon = 0.3088781011012185\nEpsilon = 0.30884721329110837\nEpsilon = 0.30881632856977925\nEpsilon = 0.3087854469369223\nEpsilon = 0.3087545683922286\nEpsilon = 0.30872369293538937\nEpsilon = 0.3086928205660958\nEpsilon = 0.3086619512840392\nEpsilon = 0.30863108508891085\nEpsilon = 0.308600221980402\nEpsilon = 0.30856936195820395\nEpsilon = 0.30853850502200814\nEpsilon = 0.3085076511715059\nEpsilon = 0.30847680040638875\nEpsilon = 0.3084459527263481\nAgent: ddqn_agent . Episode 648/2000. Number of steps to finish: 20. Loss: 14.414044380187988 Reward: -10.0\nEpsilon = 0.3084151081310755\nEpsilon = 0.30838426662026236\nEpsilon = 0.30835342819360034\nEpsilon = 0.308322592850781\nEpsilon = 0.30829176059149593\nEpsilon = 0.3082609314154368\nEpsilon = 0.3082301053222953\nEpsilon = 0.3081992823117631\nEpsilon = 0.3081684623835319\nEpsilon = 0.30813764553729356\nEpsilon = 0.30810683177273984\nEpsilon = 0.30807602108956256\nEpsilon = 0.3080452134874536\nEpsilon = 0.3080144089661049\nEpsilon = 0.30798360752520826\nEpsilon = 0.3079528091644557\nEpsilon = 0.3079220138835393\nEpsilon = 0.30789122168215094\nEpsilon = 0.30786043255998274\nEpsilon = 0.30782964651672673\nAgent: ddqn_agent . Episode 649/2000. Number of steps to finish: 20. Loss: 14.088231086730957 Reward: -18.0\nEpsilon = 0.3077988635520751\nEpsilon = 0.3077680836657199\nEpsilon = 0.30773730685735334\nEpsilon = 0.3077065331266676\nEpsilon = 0.307675762473355\nEpsilon = 0.30764499489710767\nAgent: ddqn_agent . Episode 650/2000. Number of steps to finish: 6. Loss: 4.303714275360107 Reward: 6.0\nEpsilon = 0.307614230397618\nEpsilon = 0.3075834689745782\nEpsilon = 0.30755271062768075\nEpsilon = 0.30752195535661797\nEpsilon = 0.3074912031610823\nEpsilon = 0.3074604540407662\nEpsilon = 0.3074297079953621\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.3073989650245626\nEpsilon = 0.30736822512806017\nEpsilon = 0.30733748830554736\nEpsilon = 0.3073067545567168\nEpsilon = 0.30727602388126113\nEpsilon = 0.307245296278873\nEpsilon = 0.3072145717492451\nEpsilon = 0.3071838502920702\nEpsilon = 0.30715313190704097\nEpsilon = 0.30712241659385026\nEpsilon = 0.30709170435219085\nEpsilon = 0.3070609951817556\nEpsilon = 0.30703028908223745\nAgent: ddqn_agent . Episode 651/2000. Number of steps to finish: 20. Loss: 14.448615074157715 Reward: -20.0\nEpsilon = 0.3069995860533292\nEpsilon = 0.3069688860947239\nEpsilon = 0.30693818920611443\nEpsilon = 0.3069074953871938\nEpsilon = 0.3068768046376551\nEpsilon = 0.30684611695719133\nEpsilon = 0.30681543234549563\nEpsilon = 0.3067847508022611\nEpsilon = 0.3067540723271809\nEpsilon = 0.30672339691994815\nEpsilon = 0.30669272458025615\nEpsilon = 0.3066620553077981\nEpsilon = 0.30663138910226734\nEpsilon = 0.30660072596335713\nEpsilon = 0.3065700658907608\nEpsilon = 0.30653940888417175\nEpsilon = 0.30650875494328333\nEpsilon = 0.306478104067789\nEpsilon = 0.30644745625738223\nEpsilon = 0.3064168115117565\nAgent: ddqn_agent . Episode 652/2000. Number of steps to finish: 20. Loss: 14.812531471252441 Reward: -12.0\nEpsilon = 0.3063861698306053\nEpsilon = 0.30635553121362225\nEpsilon = 0.30632489566050086\nEpsilon = 0.30629426317093483\nEpsilon = 0.30626363374461774\nEpsilon = 0.30623300738124326\nEpsilon = 0.30620238408050515\nEpsilon = 0.3061717638420971\nEpsilon = 0.3061411466657129\nEpsilon = 0.3061105325510463\nEpsilon = 0.30607992149779123\nEpsilon = 0.30604931350564146\nAgent: ddqn_agent . Episode 653/2000. Number of steps to finish: 12. Loss: 8.746243476867676 Reward: 0.0\nEpsilon = 0.3060187085742909\nEpsilon = 0.3059881067034335\nEpsilon = 0.30595750789276316\nEpsilon = 0.3059269121419739\nEpsilon = 0.3058963194507597\nEpsilon = 0.3058657298188146\nEpsilon = 0.3058351432458327\nEpsilon = 0.30580455973150816\nEpsilon = 0.305773979275535\nEpsilon = 0.30574340187760746\nEpsilon = 0.3057128275374197\nEpsilon = 0.30568225625466594\nEpsilon = 0.30565168802904047\nEpsilon = 0.30562112286023757\nEpsilon = 0.30559056074795155\nEpsilon = 0.30556000169187675\nEpsilon = 0.3055294456917076\nEpsilon = 0.30549889274713843\nEpsilon = 0.30546834285786373\nEpsilon = 0.30543779602357796\nAgent: ddqn_agent . Episode 654/2000. Number of steps to finish: 20. Loss: 14.277767181396484 Reward: -20.0\nEpsilon = 0.3054072522439756\nEpsilon = 0.3053767115187512\nEpsilon = 0.3053461738475993\nEpsilon = 0.30531563923021454\nEpsilon = 0.3052851076662915\nEpsilon = 0.3052545791555249\nEpsilon = 0.30522405369760935\nEpsilon = 0.3051935312922396\nEpsilon = 0.3051630119391104\nEpsilon = 0.3051324956379165\nEpsilon = 0.30510198238835273\nEpsilon = 0.3050714721901139\nEpsilon = 0.30504096504289485\nEpsilon = 0.30501046094639056\nEpsilon = 0.3049799599002959\nEpsilon = 0.3049494619043059\nEpsilon = 0.30491896695811543\nEpsilon = 0.30488847506141964\nEpsilon = 0.3048579862139135\nEpsilon = 0.30482750041529216\nAgent: ddqn_agent . Episode 655/2000. Number of steps to finish: 20. Loss: 14.015371322631836 Reward: -20.0\nEpsilon = 0.30479701766525064\nEpsilon = 0.30476653796348413\nEpsilon = 0.3047360613096878\nEpsilon = 0.3047055877035568\nEpsilon = 0.30467511714478646\nEpsilon = 0.304644649633072\nEpsilon = 0.3046141851681087\nEpsilon = 0.30458372374959186\nEpsilon = 0.3045532653772169\nEpsilon = 0.30452281005067916\nEpsilon = 0.3044923577696741\nEpsilon = 0.30446190853389715\nEpsilon = 0.30443146234304375\nEpsilon = 0.30440101919680945\nEpsilon = 0.3043705790948898\nEpsilon = 0.3043401420369803\nEpsilon = 0.30430970802277657\nEpsilon = 0.3042792770519743\nEpsilon = 0.3042488491242691\nEpsilon = 0.30421842423935663\nAgent: ddqn_agent . Episode 656/2000. Number of steps to finish: 20. Loss: 14.086540222167969 Reward: -10.0\nEpsilon = 0.3041880023969327\nEpsilon = 0.30415758359669304\nEpsilon = 0.3041271678383334\nEpsilon = 0.30409675512154954\nEpsilon = 0.3040663454460374\nEpsilon = 0.30403593881149277\nEpsilon = 0.30400553521761164\nEpsilon = 0.3039751346640899\nEpsilon = 0.3039447371506235\nEpsilon = 0.3039143426769084\nAgent: ddqn_agent . Episode 657/2000. Number of steps to finish: 10. Loss: 7.0242390632629395 Reward: 2.0\nEpsilon = 0.3038839512426407\nEpsilon = 0.30385356284751647\nEpsilon = 0.3038231774912317\nEpsilon = 0.3037927951734826\nEpsilon = 0.3037624158939653\nEpsilon = 0.3037320396523759\nEpsilon = 0.3037016664484106\nEpsilon = 0.3036712962817658\nEpsilon = 0.30364092915213764\nEpsilon = 0.3036105650592224\nEpsilon = 0.30358020400271646\nEpsilon = 0.3035498459823162\nEpsilon = 0.30351949099771797\nEpsilon = 0.3034891390486182\nEpsilon = 0.30345879013471333\nEpsilon = 0.30342844425569987\nEpsilon = 0.3033981014112743\nEpsilon = 0.3033677616011332\nEpsilon = 0.3033374248249731\nEpsilon = 0.3033070910824906\nAgent: ddqn_agent . Episode 658/2000. Number of steps to finish: 20. Loss: 14.287775993347168 Reward: -10.0\nEpsilon = 0.30327676037338236\nEpsilon = 0.303246432697345\nEpsilon = 0.3032161080540753\nEpsilon = 0.3031857864432699\nEpsilon = 0.30315546786462555\nEpsilon = 0.3031251523178391\nEpsilon = 0.3030948398026073\nEpsilon = 0.30306453031862707\nEpsilon = 0.3030342238655952\nEpsilon = 0.3030039204432086\nEpsilon = 0.3029736200511643\nEpsilon = 0.3029433226891592\nEpsilon = 0.30291302835689027\nEpsilon = 0.3028827370540546\nEpsilon = 0.3028524487803492\nEpsilon = 0.3028221635354712\nEpsilon = 0.30279188131911766\nEpsilon = 0.30276160213098574\nEpsilon = 0.3027313259707726\nAgent: ddqn_agent . Episode 659/2000. Number of steps to finish: 19. Loss: 13.45339298248291 Reward: -7.0\nEpsilon = 0.30270105283817555\nEpsilon = 0.3026707827328917\nEpsilon = 0.3026405156546184\nEpsilon = 0.30261025160305294\nEpsilon = 0.30257999057789264\nEpsilon = 0.3025497325788349\nEpsilon = 0.302519477605577\nEpsilon = 0.30248922565781644\nEpsilon = 0.3024589767352507\nEpsilon = 0.30242873083757715\nEpsilon = 0.3023984879644934\nEpsilon = 0.30236824811569696\nEpsilon = 0.3023380112908854\nEpsilon = 0.3023077774897563\nEpsilon = 0.3022775467120073\nEpsilon = 0.3022473189573361\nEpsilon = 0.30221709422544035\nEpsilon = 0.30218687251601783\nEpsilon = 0.3021566538287662\nEpsilon = 0.30212643816338336\nAgent: ddqn_agent . Episode 660/2000. Number of steps to finish: 20. Loss: 14.06286334991455 Reward: -12.0\nEpsilon = 0.30209622551956705\nEpsilon = 0.3020660158970151\nEpsilon = 0.3020358092954254\nEpsilon = 0.30200560571449586\nEpsilon = 0.3019754051539244\nEpsilon = 0.30194520761340904\nEpsilon = 0.3019150130926477\nEpsilon = 0.3018848215913384\nEpsilon = 0.30185463310917926\nEpsilon = 0.30182444764586835\nEpsilon = 0.30179426520110375\nEpsilon = 0.30176408577458363\nEpsilon = 0.3017339093660062\nEpsilon = 0.3017037359750696\nEpsilon = 0.3016735656014721\nEpsilon = 0.301643398244912\nEpsilon = 0.3016132339050875\nAgent: ddqn_agent . Episode 661/2000. Number of steps to finish: 17. Loss: 11.815707206726074 Reward: -5.0\nEpsilon = 0.301583072581697\nEpsilon = 0.3015529142744388\nEpsilon = 0.3015227589830114\nEpsilon = 0.3014926067071131\nEpsilon = 0.3014624574464424\nEpsilon = 0.30143231120069774\nEpsilon = 0.3014021679695777\nEpsilon = 0.3013720277527807\nEpsilon = 0.30134189055000543\nEpsilon = 0.3013117563609504\nEpsilon = 0.3012816251853143\nEpsilon = 0.3012514970227958\nEpsilon = 0.3012213718730935\nEpsilon = 0.3011912497359062\nEpsilon = 0.30116113061093264\nEpsilon = 0.30113101449787155\nEpsilon = 0.30110090139642176\nEpsilon = 0.3010707913062821\nEpsilon = 0.30104068422715147\nEpsilon = 0.3010105801587288\nAgent: ddqn_agent . Episode 662/2000. Number of steps to finish: 20. Loss: 13.981644630432129 Reward: -14.0\nEpsilon = 0.3009804791007129\nEpsilon = 0.30095038105280286\nEpsilon = 0.30092028601469756\nEpsilon = 0.30089019398609607\nEpsilon = 0.30086010496669746\nEpsilon = 0.3008300189562008\nEpsilon = 0.3007999359543052\nEpsilon = 0.30076985596070976\nEpsilon = 0.30073977897511367\nEpsilon = 0.30070970499721617\nEpsilon = 0.30067963402671644\nEpsilon = 0.3006495660633138\nEpsilon = 0.30061950110670743\nEpsilon = 0.30058943915659675\nEpsilon = 0.3005593802126811\nEpsilon = 0.30052932427465984\nEpsilon = 0.30049927134223237\nEpsilon = 0.30046922141509813\nEpsilon = 0.30043917449295665\nEpsilon = 0.30040913057550733\nAgent: ddqn_agent . Episode 663/2000. Number of steps to finish: 20. Loss: 14.322781562805176 Reward: -14.0\nEpsilon = 0.3003790896624498\nEpsilon = 0.3003490517534836\nEpsilon = 0.30031901684830825\nEpsilon = 0.3002889849466234\nEpsilon = 0.30025895604812874\nEpsilon = 0.30022893015252394\nEpsilon = 0.3001989072595087\nEpsilon = 0.3001688873687828\nEpsilon = 0.3001388704800459\nEpsilon = 0.3001088565929979\nEpsilon = 0.3000788457073386\nEpsilon = 0.30004883782276787\nEpsilon = 0.30001883293898557\nEpsilon = 0.2999888310556917\nAgent: ddqn_agent . Episode 664/2000. Number of steps to finish: 14. Loss: 9.903752326965332 Reward: -2.0\nEpsilon = 0.2999588321725861\nEpsilon = 0.29992883628936884\nEpsilon = 0.2998988434057399\nEpsilon = 0.29986885352139936\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.2998388666360472\nEpsilon = 0.2998088827493836\nEpsilon = 0.29977890186110867\nEpsilon = 0.29974892397092257\nEpsilon = 0.2997189490785255\nEpsilon = 0.29968897718361764\nEpsilon = 0.2996590082858993\nEpsilon = 0.2996290423850707\nEpsilon = 0.2995990794808322\nEpsilon = 0.2995691195728841\nEpsilon = 0.29953916266092684\nEpsilon = 0.29950920874466075\nEpsilon = 0.29947925782378626\nEpsilon = 0.2994493098980039\nEpsilon = 0.2994193649670141\nEpsilon = 0.2993894230305174\nAgent: ddqn_agent . Episode 665/2000. Number of steps to finish: 20. Loss: 14.03231430053711 Reward: -14.0\nEpsilon = 0.29935948408821433\nEpsilon = 0.2993295481398055\nEpsilon = 0.29929961518499154\nEpsilon = 0.299269685223473\nEpsilon = 0.2992397582549507\nEpsilon = 0.2992098342791252\nEpsilon = 0.29917991329569726\nEpsilon = 0.2991499953043677\nEpsilon = 0.2991200803048373\nEpsilon = 0.2990901682968068\nEpsilon = 0.2990602592799771\nEpsilon = 0.2990303532540491\nEpsilon = 0.2990004502187237\nEpsilon = 0.29897055017370183\nEpsilon = 0.29894065311868445\nEpsilon = 0.2989107590533726\nEpsilon = 0.29888086797746727\nEpsilon = 0.2988509798906695\nEpsilon = 0.29882109479268043\nEpsilon = 0.2987912126832012\nAgent: ddqn_agent . Episode 666/2000. Number of steps to finish: 20. Loss: 14.44761848449707 Reward: -12.0\nEpsilon = 0.29876133356193285\nEpsilon = 0.29873145742857665\nEpsilon = 0.2987015842828338\nEpsilon = 0.2986717141244055\nEpsilon = 0.29864184695299306\nEpsilon = 0.29861198276829776\nEpsilon = 0.29858212157002095\nEpsilon = 0.29855226335786395\nEpsilon = 0.29852240813152814\nEpsilon = 0.298492555890715\nEpsilon = 0.2984627066351259\nEpsilon = 0.2984328603644624\nEpsilon = 0.298403017078426\nEpsilon = 0.29837317677671815\nEpsilon = 0.29834333945904046\nEpsilon = 0.2983135051250946\nEpsilon = 0.2982836737745821\nEpsilon = 0.2982538454072046\nEpsilon = 0.2982240200226639\nEpsilon = 0.2981941976206616\nAgent: ddqn_agent . Episode 667/2000. Number of steps to finish: 20. Loss: 14.507049560546875 Reward: -18.0\nEpsilon = 0.2981643782008996\nEpsilon = 0.2981345617630795\nEpsilon = 0.2981047483069032\nEpsilon = 0.2980749378320725\nEpsilon = 0.2980451303382893\nEpsilon = 0.2980153258252555\nEpsilon = 0.29798552429267294\nEpsilon = 0.2979557257402437\nEpsilon = 0.29792593016766966\nEpsilon = 0.2978961375746529\nEpsilon = 0.29786634796089545\nEpsilon = 0.29783656132609937\nEpsilon = 0.29780677766996677\nEpsilon = 0.2977769969921998\nEpsilon = 0.2977472192925006\nEpsilon = 0.29771744457057137\nEpsilon = 0.2976876728261143\nEpsilon = 0.2976579040588317\nEpsilon = 0.2976281382684258\nEpsilon = 0.297598375454599\nAgent: ddqn_agent . Episode 668/2000. Number of steps to finish: 20. Loss: 13.974749565124512 Reward: -10.0\nEpsilon = 0.29756861561705356\nEpsilon = 0.29753885875549185\nEpsilon = 0.2975091048696163\nEpsilon = 0.29747935395912933\nEpsilon = 0.29744960602373344\nEpsilon = 0.29741986106313106\nEpsilon = 0.29739011907702473\nEpsilon = 0.297360380065117\nEpsilon = 0.2973306440271105\nEpsilon = 0.29730091096270783\nEpsilon = 0.2972711808716116\nEpsilon = 0.29724145375352445\nEpsilon = 0.2972117296081491\nEpsilon = 0.2971820084351883\nEpsilon = 0.2971522902343448\nEpsilon = 0.29712257500532135\nEpsilon = 0.2970928627478208\nEpsilon = 0.29706315346154605\nEpsilon = 0.2970334471461999\nEpsilon = 0.29700374380148525\nAgent: ddqn_agent . Episode 669/2000. Number of steps to finish: 20. Loss: 14.263666152954102 Reward: -18.0\nEpsilon = 0.2969740434271051\nEpsilon = 0.2969443460227624\nEpsilon = 0.2969146515881601\nEpsilon = 0.2968849601230013\nEpsilon = 0.296855271626989\nEpsilon = 0.2968255860998263\nEpsilon = 0.2967959035412163\nEpsilon = 0.2967662239508622\nEpsilon = 0.29673654732846716\nEpsilon = 0.29670687367373433\nEpsilon = 0.29667720298636696\nEpsilon = 0.29664753526606835\nEpsilon = 0.29661787051254174\nEpsilon = 0.29658820872549047\nEpsilon = 0.29655854990461794\nEpsilon = 0.29652889404962746\nEpsilon = 0.29649924116022247\nEpsilon = 0.29646959123610644\nEpsilon = 0.29643994427698284\nEpsilon = 0.29641030028255516\nAgent: ddqn_agent . Episode 670/2000. Number of steps to finish: 20. Loss: 14.254415512084961 Reward: -14.0\nEpsilon = 0.29638065925252693\nEpsilon = 0.29635102118660167\nEpsilon = 0.296321386084483\nEpsilon = 0.29629175394587454\nEpsilon = 0.29626212477048\nEpsilon = 0.29623249855800293\nEpsilon = 0.29620287530814715\nEpsilon = 0.29617325502061637\nEpsilon = 0.2961436376951143\nEpsilon = 0.2961140233313448\nEpsilon = 0.29608441192901164\nEpsilon = 0.29605480348781876\nEpsilon = 0.29602519800746996\nEpsilon = 0.29599559548766924\nEpsilon = 0.29596599592812045\nEpsilon = 0.29593639932852767\nEpsilon = 0.29590680568859484\nEpsilon = 0.295877215008026\nEpsilon = 0.2958476272865252\nEpsilon = 0.29581804252379656\nAgent: ddqn_agent . Episode 671/2000. Number of steps to finish: 20. Loss: 14.389669418334961 Reward: -16.0\nEpsilon = 0.2957884607195442\nEpsilon = 0.2957588818734722\nEpsilon = 0.29572930598528485\nEpsilon = 0.29569973305468633\nEpsilon = 0.2956701630813809\nEpsilon = 0.29564059606507276\nEpsilon = 0.29561103200546623\nEpsilon = 0.29558147090226566\nEpsilon = 0.29555191275517545\nEpsilon = 0.29552235756389994\nEpsilon = 0.29549280532814354\nEpsilon = 0.2954632560476107\nEpsilon = 0.29543370972200594\nEpsilon = 0.2954041663510337\nEpsilon = 0.2953746259343986\nEpsilon = 0.2953450884718052\nEpsilon = 0.295315553962958\nEpsilon = 0.2952860224075617\nEpsilon = 0.29525649380532093\nEpsilon = 0.2952269681559404\nAgent: ddqn_agent . Episode 672/2000. Number of steps to finish: 20. Loss: 14.234976768493652 Reward: -10.0\nEpsilon = 0.2951974454591248\nEpsilon = 0.2951679257145789\nEpsilon = 0.29513840892200743\nEpsilon = 0.29510889508111526\nEpsilon = 0.2950793841916072\nEpsilon = 0.295049876253188\nEpsilon = 0.2950203712655627\nEpsilon = 0.29499086922843615\nEpsilon = 0.2949613701415133\nEpsilon = 0.29493187400449916\nEpsilon = 0.29490238081709874\nEpsilon = 0.29487289057901706\nEpsilon = 0.29484340328995917\nEpsilon = 0.29481391894963016\nEpsilon = 0.2947844375577352\nEpsilon = 0.2947549591139794\nEpsilon = 0.29472548361806805\nEpsilon = 0.2946960110697062\nEpsilon = 0.2946665414685993\nEpsilon = 0.2946370748144524\nAgent: ddqn_agent . Episode 673/2000. Number of steps to finish: 20. Loss: 14.728135108947754 Reward: -14.0\nEpsilon = 0.2946076111069709\nEpsilon = 0.2945781503458602\nEpsilon = 0.2945486925308256\nEpsilon = 0.2945192376615725\nEpsilon = 0.2944897857378064\nEpsilon = 0.2944603367592326\nEpsilon = 0.29443089072555667\nEpsilon = 0.2944014476364841\nEpsilon = 0.2943720074917205\nEpsilon = 0.29434257029097133\nEpsilon = 0.2943131360339422\nEpsilon = 0.2942837047203388\nEpsilon = 0.2942542763498668\nEpsilon = 0.2942248509222318\nEpsilon = 0.29419542843713953\nEpsilon = 0.29416600889429584\nEpsilon = 0.2941365922934064\nEpsilon = 0.29410717863417707\nEpsilon = 0.2940777679163137\nEpsilon = 0.29404836013952207\nAgent: ddqn_agent . Episode 674/2000. Number of steps to finish: 20. Loss: 14.552610397338867 Reward: -14.0\nEpsilon = 0.29401895530350813\nEpsilon = 0.2939895534079778\nEpsilon = 0.29396015445263696\nEpsilon = 0.2939307584371917\nEpsilon = 0.293901365361348\nEpsilon = 0.29387197522481184\nEpsilon = 0.29384258802728935\nEpsilon = 0.29381320376848663\nEpsilon = 0.2937838224481098\nEpsilon = 0.293754444065865\nEpsilon = 0.2937250686214584\nEpsilon = 0.2936956961145963\nEpsilon = 0.2936663265449848\nEpsilon = 0.2936369599123303\nEpsilon = 0.2936075962163391\nEpsilon = 0.29357823545671746\nEpsilon = 0.2935488776331718\nEpsilon = 0.2935195227454085\nEpsilon = 0.29349017079313394\nEpsilon = 0.29346082177605465\nAgent: ddqn_agent . Episode 675/2000. Number of steps to finish: 20. Loss: 14.23469352722168 Reward: -12.0\nEpsilon = 0.29343147569387706\nEpsilon = 0.2934021325463077\nEpsilon = 0.2933727923330531\nEpsilon = 0.29334345505381976\nEpsilon = 0.29331412070831436\nEpsilon = 0.2932847892962435\nEpsilon = 0.2932554608173139\nEpsilon = 0.2932261352712322\nEpsilon = 0.2931968126577051\nEpsilon = 0.2931674929764393\nEpsilon = 0.29313817622714167\nEpsilon = 0.29310886240951894\nEpsilon = 0.29307955152327797\nEpsilon = 0.29305024356812565\nEpsilon = 0.2930209385437688\nEpsilon = 0.29299163644991444\nEpsilon = 0.29296233728626947\nEpsilon = 0.29293304105254087\nEpsilon = 0.29290374774843564\nEpsilon = 0.2928744573736608\nAgent: ddqn_agent . Episode 676/2000. Number of steps to finish: 20. Loss: 14.191462516784668 Reward: -20.0\nEpsilon = 0.2928451699279234\nEpsilon = 0.2928158854109306\nEpsilon = 0.2927866038223895\nEpsilon = 0.2927573251620073\nEpsilon = 0.2927280494294911\nEpsilon = 0.29269877662454813\nEpsilon = 0.2926695067468857\nEpsilon = 0.292640239796211\nEpsilon = 0.2926109757722314\nEpsilon = 0.29258171467465416\nEpsilon = 0.2925524565031867\nEpsilon = 0.29252320125753634\nEpsilon = 0.2924939489374106\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.29246469954251686\nEpsilon = 0.29243545307256263\nEpsilon = 0.2924062095272554\nEpsilon = 0.29237696890630266\nEpsilon = 0.29234773120941204\nEpsilon = 0.2923184964362911\nEpsilon = 0.29228926458664745\nAgent: ddqn_agent . Episode 677/2000. Number of steps to finish: 20. Loss: 14.29054069519043 Reward: -8.0\nEpsilon = 0.29226003566018877\nEpsilon = 0.29223080965662274\nEpsilon = 0.2922015865756571\nEpsilon = 0.2921723664169995\nEpsilon = 0.29214314918035783\nEpsilon = 0.2921139348654398\nEpsilon = 0.29208472347195324\nEpsilon = 0.292055514999606\nEpsilon = 0.2920263094481061\nEpsilon = 0.2919971068171613\nEpsilon = 0.2919679071064796\nEpsilon = 0.29193871031576896\nEpsilon = 0.2919095164447374\nEpsilon = 0.2918803254930929\nEpsilon = 0.2918511374605436\nEpsilon = 0.29182195234679753\nEpsilon = 0.29179277015156285\nEpsilon = 0.2917635908745477\nEpsilon = 0.2917344145154602\nEpsilon = 0.2917052410740087\nAgent: ddqn_agent . Episode 678/2000. Number of steps to finish: 20. Loss: 14.458257675170898 Reward: -14.0\nEpsilon = 0.2916760705499013\nEpsilon = 0.2916469029428463\nEpsilon = 0.291617738252552\nEpsilon = 0.29158857647872677\nEpsilon = 0.2915594176210789\nEpsilon = 0.2915302616793168\nEpsilon = 0.2915011086531489\nEpsilon = 0.2914719585422836\nEpsilon = 0.29144281134642935\nEpsilon = 0.2914136670652947\nEpsilon = 0.2913845256985882\nEpsilon = 0.29135538724601834\nEpsilon = 0.29132625170729376\nEpsilon = 0.291297119082123\nEpsilon = 0.2912679893702148\nEpsilon = 0.2912388625712778\nEpsilon = 0.29120973868502065\nEpsilon = 0.29118061771115217\nEpsilon = 0.29115149964938103\nEpsilon = 0.2911223844994161\nAgent: ddqn_agent . Episode 679/2000. Number of steps to finish: 20. Loss: 14.636913299560547 Reward: -14.0\nEpsilon = 0.29109327226096615\nEpsilon = 0.29106416293374004\nEpsilon = 0.29103505651744666\nEpsilon = 0.29100595301179494\nEpsilon = 0.29097685241649374\nEpsilon = 0.2909477547312521\nEpsilon = 0.290918659955779\nEpsilon = 0.2908895680897834\nEpsilon = 0.2908604791329744\nEpsilon = 0.2908313930850611\nEpsilon = 0.2908023099457526\nEpsilon = 0.29077322971475805\nEpsilon = 0.2907441523917866\nEpsilon = 0.2907150779765474\nEpsilon = 0.29068600646874976\nEpsilon = 0.29065693786810287\nEpsilon = 0.29062787217431607\nEpsilon = 0.29059880938709864\nEpsilon = 0.29056974950615994\nEpsilon = 0.29054069253120934\nAgent: ddqn_agent . Episode 680/2000. Number of steps to finish: 20. Loss: 14.678939819335938 Reward: -16.0\nEpsilon = 0.2905116384619562\nEpsilon = 0.29048258729811\nEpsilon = 0.2904535390393802\nEpsilon = 0.2904244936854763\nEpsilon = 0.29039545123610777\nEpsilon = 0.29036641169098415\nEpsilon = 0.29033737504981505\nEpsilon = 0.2903083413123101\nEpsilon = 0.29027931047817884\nEpsilon = 0.29025028254713103\nEpsilon = 0.29022125751887634\nEpsilon = 0.29019223539312444\nEpsilon = 0.2901632161695851\nEpsilon = 0.2901341998479682\nEpsilon = 0.2901051864279834\nEpsilon = 0.2900761759093406\nEpsilon = 0.29004716829174965\nEpsilon = 0.2900181635749205\nEpsilon = 0.289989161758563\nEpsilon = 0.2899601628423871\nAgent: ddqn_agent . Episode 681/2000. Number of steps to finish: 20. Loss: 14.194908142089844 Reward: -14.0\nEpsilon = 0.28993116682610287\nEpsilon = 0.2899021737094203\nEpsilon = 0.2898731834920493\nEpsilon = 0.2898441961737001\nEpsilon = 0.28981521175408276\nEpsilon = 0.28978623023290734\nEpsilon = 0.289757251609884\nEpsilon = 0.28972827588472305\nEpsilon = 0.2896993030571346\nEpsilon = 0.2896703331268289\nEpsilon = 0.2896413660935162\nEpsilon = 0.28961240195690685\nEpsilon = 0.28958344071671116\nEpsilon = 0.2895544823726395\nEpsilon = 0.2895255269244022\nEpsilon = 0.2894965743717098\nEpsilon = 0.2894676247142726\nEpsilon = 0.28943867795180117\nEpsilon = 0.289409734084006\nEpsilon = 0.2893807931105976\nAgent: ddqn_agent . Episode 682/2000. Number of steps to finish: 20. Loss: 14.068815231323242 Reward: -10.0\nEpsilon = 0.2893518550312866\nEpsilon = 0.28932291984578346\nEpsilon = 0.28929398755379887\nEpsilon = 0.2892650581550435\nEpsilon = 0.289236131649228\nEpsilon = 0.2892072080360631\nEpsilon = 0.28917828731525946\nEpsilon = 0.2891493694865279\nEpsilon = 0.28912045454957924\nEpsilon = 0.2890915425041243\nEpsilon = 0.28906263334987387\nEpsilon = 0.2890337270865389\nEpsilon = 0.28900482371383024\nEpsilon = 0.2889759232314589\nAgent: ddqn_agent . Episode 683/2000. Number of steps to finish: 14. Loss: 10.46927547454834 Reward: -2.0\nEpsilon = 0.2889470256391357\nEpsilon = 0.2889181309365718\nEpsilon = 0.28888923912347814\nEpsilon = 0.2888603501995658\nEpsilon = 0.28883146416454586\nEpsilon = 0.2888025810181294\nEpsilon = 0.28877370076002756\nEpsilon = 0.28874482338995155\nEpsilon = 0.28871594890761254\nEpsilon = 0.2886870773127218\nEpsilon = 0.28865820860499053\nEpsilon = 0.28862934278413005\nEpsilon = 0.28860047984985165\nEpsilon = 0.2885716198018667\nEpsilon = 0.2885427626398865\nEpsilon = 0.2885139083636225\nEpsilon = 0.28848505697278615\nEpsilon = 0.28845620846708886\nEpsilon = 0.28842736284624215\nEpsilon = 0.28839852010995753\nAgent: ddqn_agent . Episode 684/2000. Number of steps to finish: 20. Loss: 14.40830135345459 Reward: -14.0\nEpsilon = 0.2883696802579465\nEpsilon = 0.2883408432899207\nEpsilon = 0.28831200920559175\nEpsilon = 0.2882831780046712\nEpsilon = 0.2882543496868707\nEpsilon = 0.288225524251902\nEpsilon = 0.2881967016994768\nEpsilon = 0.2881678820293069\nEpsilon = 0.28813906524110394\nEpsilon = 0.28811025133457985\nEpsilon = 0.2880814403094464\nEpsilon = 0.28805263216541543\nEpsilon = 0.2880238269021989\nEpsilon = 0.2879950245195087\nEpsilon = 0.28796622501705677\nEpsilon = 0.28793742839455505\nEpsilon = 0.2879086346517156\nEpsilon = 0.28787984378825043\nEpsilon = 0.2878510558038716\nEpsilon = 0.2878222706982912\nAgent: ddqn_agent . Episode 685/2000. Number of steps to finish: 20. Loss: 14.663542747497559 Reward: -12.0\nEpsilon = 0.2877934884712214\nEpsilon = 0.2877647091223743\nEpsilon = 0.287735932651462\nEpsilon = 0.2877071590581969\nEpsilon = 0.2876783883422911\nEpsilon = 0.2876496205034569\nEpsilon = 0.28762085554140654\nEpsilon = 0.2875920934558524\nEpsilon = 0.2875633342465068\nEpsilon = 0.2875345779130822\nEpsilon = 0.2875058244552909\nEpsilon = 0.2874770738728454\nEpsilon = 0.2874483261654581\nEpsilon = 0.28741958133284157\nEpsilon = 0.2873908393747083\nAgent: ddqn_agent . Episode 686/2000. Number of steps to finish: 15. Loss: 10.630748748779297 Reward: -3.0\nEpsilon = 0.2873621002907708\nEpsilon = 0.2873333640807417\nEpsilon = 0.28730463074433366\nEpsilon = 0.2872759002812592\nEpsilon = 0.28724717269123107\nEpsilon = 0.28721844797396195\nEpsilon = 0.28718972612916455\nEpsilon = 0.28716100715655163\nEpsilon = 0.287132291055836\nEpsilon = 0.2871035778267304\nEpsilon = 0.2870748674689477\nEpsilon = 0.28704615998220084\nEpsilon = 0.2870174553662026\nEpsilon = 0.286988753620666\nEpsilon = 0.28696005474530395\nEpsilon = 0.28693135873982945\nEpsilon = 0.28690266560395544\nEpsilon = 0.28687397533739506\nEpsilon = 0.2868452879398613\nEpsilon = 0.2868166034110673\nAgent: ddqn_agent . Episode 687/2000. Number of steps to finish: 20. Loss: 14.215250015258789 Reward: -18.0\nEpsilon = 0.2867879217507262\nEpsilon = 0.28675924295855115\nEpsilon = 0.2867305670342553\nEpsilon = 0.2867018939775519\nEpsilon = 0.2866732237881542\nEpsilon = 0.28664455646577536\nEpsilon = 0.28661589201012877\nEpsilon = 0.28658723042092776\nEpsilon = 0.2865585716978857\nEpsilon = 0.2865299158407159\nEpsilon = 0.28650126284913185\nEpsilon = 0.28647261272284696\nEpsilon = 0.2864439654615747\nEpsilon = 0.28641532106502854\nEpsilon = 0.286386679532922\nEpsilon = 0.28635804086496874\nEpsilon = 0.2863294050608822\nEpsilon = 0.28630077212037613\nEpsilon = 0.2862721420431641\nEpsilon = 0.28624351482895977\nAgent: ddqn_agent . Episode 688/2000. Number of steps to finish: 20. Loss: 14.20048999786377 Reward: -14.0\nEpsilon = 0.2862148904774769\nEpsilon = 0.28618626898842914\nEpsilon = 0.2861576503615303\nEpsilon = 0.28612903459649414\nEpsilon = 0.2861004216930345\nEpsilon = 0.2860718116508652\nEpsilon = 0.2860432044697001\nEpsilon = 0.2860146001492531\nEpsilon = 0.2859859986892382\nEpsilon = 0.28595740008936926\nEpsilon = 0.28592880434936035\nEpsilon = 0.2859002114689254\nEpsilon = 0.2858716214477785\nEpsilon = 0.28584303428563373\nEpsilon = 0.2858144499822052\nEpsilon = 0.28578586853720694\nEpsilon = 0.2857572899503532\nEpsilon = 0.2857287142213582\nEpsilon = 0.2857001413499361\nEpsilon = 0.2856715713358011\nAgent: ddqn_agent . Episode 689/2000. Number of steps to finish: 20. Loss: 14.872518539428711 Reward: -14.0\nEpsilon = 0.28564300417866756\nEpsilon = 0.2856144398782497\nEpsilon = 0.28558587843426186\nEpsilon = 0.28555731984641847\nEpsilon = 0.28552876411443384\nEpsilon = 0.2855002112380224\nEpsilon = 0.2854716612168986\nEpsilon = 0.28544311405077694\nEpsilon = 0.28541456973937185\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.2853860282823979\nEpsilon = 0.28535748967956964\nEpsilon = 0.2853289539306017\nEpsilon = 0.28530042103520864\nEpsilon = 0.2852718909931051\nEpsilon = 0.28524336380400583\nEpsilon = 0.28521483946762544\nEpsilon = 0.28518631798367866\nEpsilon = 0.2851577993518803\nEpsilon = 0.2851292835719451\nEpsilon = 0.2851007706435879\nAgent: ddqn_agent . Episode 690/2000. Number of steps to finish: 20. Loss: 14.500638008117676 Reward: -18.0\nEpsilon = 0.28507226056652357\nEpsilon = 0.28504375334046694\nEpsilon = 0.2850152489651329\nEpsilon = 0.2849867474402364\nEpsilon = 0.28495824876549236\nEpsilon = 0.2849297529406158\nEpsilon = 0.28490125996532173\nEpsilon = 0.2848727698393252\nEpsilon = 0.2848442825623413\nEpsilon = 0.2848157981340851\nEpsilon = 0.2847873165542717\nEpsilon = 0.2847588378226163\nEpsilon = 0.28473036193883405\nEpsilon = 0.2847018889026402\nEpsilon = 0.28467341871374996\nEpsilon = 0.28464495137187856\nEpsilon = 0.2846164868767414\nEpsilon = 0.2845880252280537\nEpsilon = 0.28455956642553093\nEpsilon = 0.2845311104688884\nAgent: ddqn_agent . Episode 691/2000. Number of steps to finish: 20. Loss: 14.377967834472656 Reward: -12.0\nEpsilon = 0.2845026573578415\nEpsilon = 0.28447420709210575\nEpsilon = 0.2844457596713965\nEpsilon = 0.2844173150954294\nEpsilon = 0.2843888733639198\nEpsilon = 0.28436043447658343\nEpsilon = 0.2843319984331358\nEpsilon = 0.2843035652332925\nEpsilon = 0.28427513487676914\nEpsilon = 0.28424670736328145\nEpsilon = 0.2842182826925451\nEpsilon = 0.2841898608642759\nEpsilon = 0.2841614418781895\nEpsilon = 0.2841330257340017\nEpsilon = 0.28410461243142826\nEpsilon = 0.2840762019701851\nAgent: ddqn_agent . Episode 692/2000. Number of steps to finish: 16. Loss: 10.969525337219238 Reward: -4.0\nEpsilon = 0.2840477943499881\nEpsilon = 0.2840193895705531\nEpsilon = 0.2839909876315961\nEpsilon = 0.2839625885328329\nEpsilon = 0.28393419227397965\nEpsilon = 0.28390579885475226\nEpsilon = 0.2838774082748668\nEpsilon = 0.2838490205340393\nEpsilon = 0.2838206356319859\nEpsilon = 0.2837922535684227\nEpsilon = 0.2837638743430659\nEpsilon = 0.2837354979556316\nEpsilon = 0.283707124405836\nEpsilon = 0.2836787536933954\nEpsilon = 0.28365038581802604\nEpsilon = 0.28362202077944426\nEpsilon = 0.28359365857736635\nEpsilon = 0.28356529921150864\nEpsilon = 0.28353694268158747\nEpsilon = 0.2835085889873193\nAgent: ddqn_agent . Episode 693/2000. Number of steps to finish: 20. Loss: 14.626317024230957 Reward: -20.0\nEpsilon = 0.28348023812842055\nEpsilon = 0.2834518901046077\nEpsilon = 0.28342354491559724\nEpsilon = 0.2833952025611057\nEpsilon = 0.28336686304084957\nEpsilon = 0.2833385263545455\nEpsilon = 0.28331019250191003\nEpsilon = 0.2832818614826598\nEpsilon = 0.28325353329651154\nEpsilon = 0.2832252079431819\nEpsilon = 0.2831968854223876\nEpsilon = 0.28316856573384536\nEpsilon = 0.28314024887727196\nEpsilon = 0.28311193485238423\nEpsilon = 0.283083623658899\nEpsilon = 0.2830553152965331\nEpsilon = 0.28302700976500345\nEpsilon = 0.28299870706402697\nEpsilon = 0.28297040719332056\nEpsilon = 0.28294211015260123\nAgent: ddqn_agent . Episode 694/2000. Number of steps to finish: 20. Loss: 14.271483421325684 Reward: -18.0\nEpsilon = 0.282913815941586\nEpsilon = 0.28288552455999183\nEpsilon = 0.2828572360075358\nEpsilon = 0.28282895028393507\nEpsilon = 0.2828006673889067\nEpsilon = 0.2827723873221678\nEpsilon = 0.2827441100834356\nEpsilon = 0.28271583567242725\nEpsilon = 0.28268756408886003\nEpsilon = 0.28265929533245115\nEpsilon = 0.2826310294029179\nEpsilon = 0.2826027662999776\nEpsilon = 0.2825745060233476\nEpsilon = 0.2825462485727453\nEpsilon = 0.282517993947888\nEpsilon = 0.2824897421484932\nEpsilon = 0.2824614931742784\nEpsilon = 0.282433247024961\nEpsilon = 0.2824050037002585\nEpsilon = 0.28237676319988847\nAgent: ddqn_agent . Episode 695/2000. Number of steps to finish: 20. Loss: 14.554899215698242 Reward: -12.0\nEpsilon = 0.2823485255235685\nEpsilon = 0.2823202906710161\nEpsilon = 0.282292058641949\nEpsilon = 0.2822638294360848\nEpsilon = 0.2822356030531412\nEpsilon = 0.2822073794928359\nEpsilon = 0.28217915875488664\nEpsilon = 0.28215094083901116\nEpsilon = 0.28212272574492725\nEpsilon = 0.28209451347235276\nEpsilon = 0.2820663040210055\nEpsilon = 0.2820380973906034\nEpsilon = 0.28200989358086437\nEpsilon = 0.2819816925915063\nEpsilon = 0.28195349442224715\nEpsilon = 0.28192529907280495\nEpsilon = 0.2818971065428977\nEpsilon = 0.2818689168322434\nEpsilon = 0.2818407299405602\nEpsilon = 0.28181254586756616\nAgent: ddqn_agent . Episode 696/2000. Number of steps to finish: 20. Loss: 13.945894241333008 Reward: -20.0\nEpsilon = 0.2817843646129794\nEpsilon = 0.2817561861765181\nEpsilon = 0.2817280105579004\nEpsilon = 0.28169983775684465\nEpsilon = 0.28167166777306896\nEpsilon = 0.2816435006062917\nEpsilon = 0.28161533625623103\nEpsilon = 0.2815871747226054\nEpsilon = 0.28155901600513317\nEpsilon = 0.2815308601035327\nEpsilon = 0.28150270701752234\nEpsilon = 0.2814745567468206\nEpsilon = 0.2814464092911459\nEpsilon = 0.2814182646502168\nEpsilon = 0.28139012282375175\nEpsilon = 0.2813619838114694\nEpsilon = 0.2813338476130882\nEpsilon = 0.2813057142283269\nEpsilon = 0.2812775836569041\nEpsilon = 0.2812494558985384\nAgent: ddqn_agent . Episode 697/2000. Number of steps to finish: 20. Loss: 14.062573432922363 Reward: -18.0\nEpsilon = 0.28122133095294854\nEpsilon = 0.28119320881985327\nEpsilon = 0.2811650894989713\nEpsilon = 0.2811369729900214\nEpsilon = 0.2811088592927224\nEpsilon = 0.28108074840679315\nEpsilon = 0.2810526403319525\nEpsilon = 0.2810245350679193\nEpsilon = 0.28099643261441254\nEpsilon = 0.2809683329711511\nAgent: ddqn_agent . Episode 698/2000. Number of steps to finish: 10. Loss: 7.262090682983398 Reward: 2.0\nEpsilon = 0.28094023613785396\nEpsilon = 0.2809121421142402\nEpsilon = 0.28088405090002877\nEpsilon = 0.28085596249493877\nEpsilon = 0.28082787689868927\nEpsilon = 0.2807997941109994\nEpsilon = 0.2807717141315883\nEpsilon = 0.2807436369601751\nEpsilon = 0.2807155625964791\nEpsilon = 0.28068749104021945\nEpsilon = 0.28065942229111546\nEpsilon = 0.28063135634888636\nEpsilon = 0.2806032932132515\nEpsilon = 0.28057523288393016\nEpsilon = 0.2805471753606418\nEpsilon = 0.2805191206431057\nEpsilon = 0.2804910687310414\nEpsilon = 0.2804630196241683\nAgent: ddqn_agent . Episode 699/2000. Number of steps to finish: 18. Loss: 12.932180404663086 Reward: -6.0\nEpsilon = 0.2804349733222059\nEpsilon = 0.2804069298248737\nEpsilon = 0.2803788891318912\nEpsilon = 0.280350851242978\nEpsilon = 0.2803228161578537\nEpsilon = 0.2802947838762379\nEpsilon = 0.28026675439785026\nEpsilon = 0.2802387277224105\nEpsilon = 0.28021070384963825\nEpsilon = 0.2801826827792533\nEpsilon = 0.28015466451097537\nEpsilon = 0.2801266490445243\nEpsilon = 0.28009863637961985\nEpsilon = 0.2800706265159819\nEpsilon = 0.2800426194533303\nEpsilon = 0.280014615191385\nEpsilon = 0.27998661372986583\nEpsilon = 0.27995861506849284\nEpsilon = 0.279930619206986\nEpsilon = 0.2799026261450653\nAgent: ddqn_agent . Episode 700/2000. Number of steps to finish: 20. Loss: 14.909933090209961 Reward: -10.0\nEpsilon = 0.27987463588245076\nEpsilon = 0.27984664841886253\nEpsilon = 0.27981866375402065\nEpsilon = 0.2797906818876452\nEpsilon = 0.2797627028194565\nEpsilon = 0.27973472654917453\nEpsilon = 0.2797067530765196\nEpsilon = 0.27967878240121197\nEpsilon = 0.27965081452297186\nEpsilon = 0.27962284944151955\nEpsilon = 0.2795948871565754\nEpsilon = 0.27956692766785973\nEpsilon = 0.27953897097509295\nEpsilon = 0.2795110170779954\nEpsilon = 0.27948306597628764\nEpsilon = 0.27945511766969\nEpsilon = 0.279427172157923\nEpsilon = 0.2793992294407072\nEpsilon = 0.27937128951776313\nEpsilon = 0.27934335238881136\nAgent: ddqn_agent . Episode 701/2000. Number of steps to finish: 20. Loss: 14.28528118133545 Reward: -20.0\nEpsilon = 0.27931541805357246\nEpsilon = 0.2792874865117671\nEpsilon = 0.27925955776311595\nEpsilon = 0.2792316318073396\nEpsilon = 0.2792037086441589\nEpsilon = 0.2791757882732945\nEpsilon = 0.27914787069446717\nEpsilon = 0.27911995590739774\nEpsilon = 0.27909204391180703\nEpsilon = 0.27906413470741587\nEpsilon = 0.27903622829394514\nEpsilon = 0.27900832467111575\nEpsilon = 0.27898042383864863\nEpsilon = 0.2789525257962648\nEpsilon = 0.27892463054368516\nEpsilon = 0.2788967380806308\nEpsilon = 0.27886884840682274\nEpsilon = 0.27884096152198207\nEpsilon = 0.2788130774258299\nEpsilon = 0.27878519611808733\nAgent: ddqn_agent . Episode 702/2000. Number of steps to finish: 20. Loss: 14.360584259033203 Reward: -18.0\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.27875731759847555\nEpsilon = 0.2787294418667157\nEpsilon = 0.27870156892252906\nEpsilon = 0.2786736987656368\nEpsilon = 0.27864583139576027\nEpsilon = 0.2786179668126207\nEpsilon = 0.27859010501593945\nEpsilon = 0.27856224600543783\nEpsilon = 0.2785343897808373\nEpsilon = 0.2785065363418592\nEpsilon = 0.278478685688225\nEpsilon = 0.2784508378196562\nEpsilon = 0.27842299273587423\nEpsilon = 0.2783951504366006\nEpsilon = 0.278367310921557\nEpsilon = 0.27833947419046484\nEpsilon = 0.2783116402430458\nEpsilon = 0.27828380907902145\nEpsilon = 0.2782559806981135\nEpsilon = 0.27822815510004373\nAgent: ddqn_agent . Episode 703/2000. Number of steps to finish: 20. Loss: 14.675023078918457 Reward: -12.0\nEpsilon = 0.2782003322845337\nEpsilon = 0.2781725122513053\nEpsilon = 0.27814469500008016\nEpsilon = 0.27811688053058015\nEpsilon = 0.2780890688425271\nEpsilon = 0.27806125993564285\nEpsilon = 0.2780334538096493\nEpsilon = 0.27800565046426834\nEpsilon = 0.2779778498992219\nEpsilon = 0.277950052114232\nEpsilon = 0.2779222571090206\nEpsilon = 0.27789446488330966\nEpsilon = 0.2778666754368213\nEpsilon = 0.27783888876927765\nEpsilon = 0.2778111048804007\nEpsilon = 0.2777833237699127\nEpsilon = 0.2777555454375357\nEpsilon = 0.27772776988299197\nEpsilon = 0.2776999971060037\nEpsilon = 0.2776722271062931\nAgent: ddqn_agent . Episode 704/2000. Number of steps to finish: 20. Loss: 14.558773040771484 Reward: -14.0\nEpsilon = 0.27764445988358244\nEpsilon = 0.2776166954375941\nEpsilon = 0.2775889337680503\nEpsilon = 0.2775611748746735\nEpsilon = 0.27753341875718607\nEpsilon = 0.27750566541531035\nEpsilon = 0.2774779148487688\nEpsilon = 0.2774501670572839\nEpsilon = 0.2774224220405782\nEpsilon = 0.2773946797983742\nEpsilon = 0.27736694033039433\nEpsilon = 0.2773392036363613\nEpsilon = 0.2773114697159976\nEpsilon = 0.27728373856902605\nEpsilon = 0.27725601019516916\nEpsilon = 0.27722828459414967\nEpsilon = 0.27720056176569025\nEpsilon = 0.2771728417095137\nEpsilon = 0.2771451244253427\nEpsilon = 0.2771174099129002\nAgent: ddqn_agent . Episode 705/2000. Number of steps to finish: 20. Loss: 14.423423767089844 Reward: -14.0\nEpsilon = 0.27708969817190887\nEpsilon = 0.2770619892020917\nEpsilon = 0.27703428300317146\nEpsilon = 0.2770065795748711\nEpsilon = 0.27697887891691364\nEpsilon = 0.27695118102902194\nEpsilon = 0.27692348591091903\nEpsilon = 0.27689579356232796\nEpsilon = 0.27686810398297174\nEpsilon = 0.27684041717257346\nEpsilon = 0.2768127331308562\nEpsilon = 0.2767850518575431\nEpsilon = 0.27675737335235734\nEpsilon = 0.2767296976150221\nEpsilon = 0.2767020246452606\nEpsilon = 0.2766743544427961\nEpsilon = 0.2766466870073518\nEpsilon = 0.27661902233865104\nEpsilon = 0.2765913604364172\nEpsilon = 0.27656370130037355\nAgent: ddqn_agent . Episode 706/2000. Number of steps to finish: 20. Loss: 14.321662902832031 Reward: -10.0\nEpsilon = 0.27653604493024353\nEpsilon = 0.2765083913257505\nEpsilon = 0.2764807404866179\nEpsilon = 0.27645309241256927\nEpsilon = 0.276425447103328\nEpsilon = 0.2763978045586177\nEpsilon = 0.27637016477816184\nEpsilon = 0.27634252776168405\nEpsilon = 0.27631489350890787\nEpsilon = 0.276287262019557\nEpsilon = 0.27625963329335507\nEpsilon = 0.27623200733002573\nEpsilon = 0.2762043841292927\nEpsilon = 0.2761767636908798\nEpsilon = 0.2761491460145107\nEpsilon = 0.2761215310999092\nEpsilon = 0.27609391894679924\nEpsilon = 0.2760663095549046\nEpsilon = 0.2760387029239491\nEpsilon = 0.2760110990536567\nAgent: ddqn_agent . Episode 707/2000. Number of steps to finish: 20. Loss: 14.008731842041016 Reward: -14.0\nEpsilon = 0.2759834979437514\nEpsilon = 0.275955899593957\nEpsilon = 0.2759283040039976\nEpsilon = 0.2759007111735972\nEpsilon = 0.27587312110247986\nEpsilon = 0.27584553379036963\nEpsilon = 0.2758179492369906\nEpsilon = 0.2757903674420669\nEpsilon = 0.2757627884053227\nEpsilon = 0.27573521212648217\nEpsilon = 0.27570763860526953\nEpsilon = 0.275680067841409\nEpsilon = 0.27565249983462486\nEpsilon = 0.2756249345846414\nEpsilon = 0.2755973720911829\nEpsilon = 0.27556981235397376\nEpsilon = 0.27554225537273835\nEpsilon = 0.27551470114720106\nEpsilon = 0.2754871496770863\nEpsilon = 0.27545960096211863\nAgent: ddqn_agent . Episode 708/2000. Number of steps to finish: 20. Loss: 14.750703811645508 Reward: -20.0\nEpsilon = 0.27543205500202245\nEpsilon = 0.27540451179652226\nEpsilon = 0.2753769713453426\nEpsilon = 0.2753494336482081\nEpsilon = 0.27532189870484325\nEpsilon = 0.27529436651497274\nEpsilon = 0.2752668370783212\nEpsilon = 0.2752393103946134\nEpsilon = 0.27521178646357397\nEpsilon = 0.2751842652849276\nEpsilon = 0.27515674685839914\nEpsilon = 0.2751292311837133\nEpsilon = 0.27510171826059493\nEpsilon = 0.27507420808876887\nEpsilon = 0.27504670066796\nEpsilon = 0.2750191959978932\nEpsilon = 0.27499169407829344\nEpsilon = 0.27496419490888563\nEpsilon = 0.2749366984893947\nEpsilon = 0.2749092048195458\nAgent: ddqn_agent . Episode 709/2000. Number of steps to finish: 20. Loss: 13.914426803588867 Reward: -8.0\nEpsilon = 0.27488171389906385\nEpsilon = 0.27485422572767393\nEpsilon = 0.2748267403051012\nEpsilon = 0.27479925763107066\nEpsilon = 0.27477177770530753\nEpsilon = 0.274744300527537\nEpsilon = 0.27471682609748427\nEpsilon = 0.2746893544148745\nEpsilon = 0.27466188547943304\nEpsilon = 0.2746344192908851\nEpsilon = 0.274606955848956\nEpsilon = 0.2745794951533711\nEpsilon = 0.2745520372038558\nEpsilon = 0.2745245820001354\nEpsilon = 0.2744971295419354\nEpsilon = 0.2744696798289812\nEpsilon = 0.2744422328609983\nEpsilon = 0.2744147886377122\nEpsilon = 0.27438734715884844\nEpsilon = 0.27435990842413255\nAgent: ddqn_agent . Episode 710/2000. Number of steps to finish: 20. Loss: 14.255085945129395 Reward: -10.0\nEpsilon = 0.2743324724332901\nEpsilon = 0.2743050391860468\nEpsilon = 0.2742776086821282\nEpsilon = 0.27425018092125997\nEpsilon = 0.27422275590316786\nEpsilon = 0.27419533362757753\nEpsilon = 0.27416791409421476\nEpsilon = 0.2741404973028053\nEpsilon = 0.27411308325307504\nEpsilon = 0.27408567194474975\nEpsilon = 0.2740582633775553\nEpsilon = 0.27403085755121753\nEpsilon = 0.2740034544654624\nEpsilon = 0.2739760541200159\nEpsilon = 0.2739486565146039\nEpsilon = 0.2739212616489525\nEpsilon = 0.2738938695227876\nAgent: ddqn_agent . Episode 711/2000. Number of steps to finish: 17. Loss: 12.02382755279541 Reward: -5.0\nEpsilon = 0.2738664801358353\nEpsilon = 0.2738390934878217\nEpsilon = 0.27381170957847295\nEpsilon = 0.2737843284075151\nEpsilon = 0.27375694997467437\nEpsilon = 0.2737295742796769\nEpsilon = 0.273702201322249\nEpsilon = 0.27367483110211677\nEpsilon = 0.27364746361900655\nEpsilon = 0.27362009887264466\nEpsilon = 0.27359273686275737\nEpsilon = 0.27356537758907107\nEpsilon = 0.27353802105131214\nEpsilon = 0.27351066724920703\nEpsilon = 0.2734833161824821\nEpsilon = 0.27345596785086385\nEpsilon = 0.27342862225407877\nEpsilon = 0.2734012793918534\nEpsilon = 0.2733739392639142\nEpsilon = 0.27334660186998777\nAgent: ddqn_agent . Episode 712/2000. Number of steps to finish: 20. Loss: 14.370278358459473 Reward: -10.0\nEpsilon = 0.27331926720980076\nEpsilon = 0.27329193528307977\nEpsilon = 0.27326460608955144\nEpsilon = 0.2732372796289425\nEpsilon = 0.27320995590097963\nEpsilon = 0.2731826349053895\nEpsilon = 0.27315531664189896\nEpsilon = 0.2731280011102348\nEpsilon = 0.27310068831012374\nEpsilon = 0.27307337824129274\nEpsilon = 0.2730460709034686\nEpsilon = 0.2730187662963783\nEpsilon = 0.27299146441974864\nEpsilon = 0.2729641652733067\nEpsilon = 0.27293686885677937\nEpsilon = 0.27290957516989367\nEpsilon = 0.2728822842123767\nEpsilon = 0.27285499598395546\nEpsilon = 0.27282771048435706\nEpsilon = 0.27280042771330865\nAgent: ddqn_agent . Episode 713/2000. Number of steps to finish: 20. Loss: 14.484745025634766 Reward: -14.0\nEpsilon = 0.27277314767053734\nEpsilon = 0.2727458703557703\nEpsilon = 0.27271859576873475\nEpsilon = 0.2726913239091579\nEpsilon = 0.272664054776767\nEpsilon = 0.2726367883712893\nEpsilon = 0.27260952469245214\nEpsilon = 0.2725822637399829\nEpsilon = 0.2725550055136089\nEpsilon = 0.2725277500130575\nEpsilon = 0.2725004972380562\nEpsilon = 0.2724732471883324\nEpsilon = 0.27244599986361356\nEpsilon = 0.2724187552636272\nEpsilon = 0.2723915133881008\nEpsilon = 0.272364274236762\nEpsilon = 0.2723370378093383\nEpsilon = 0.2723098041055574\nEpsilon = 0.2722825731251468\nEpsilon = 0.2722553448678343\nAgent: ddqn_agent . Episode 714/2000. Number of steps to finish: 20. Loss: 14.279747009277344 Reward: -14.0\nEpsilon = 0.2722281193333475\nEpsilon = 0.27220089652141416\nEpsilon = 0.27217367643176205\nEpsilon = 0.2721464590641189\nEpsilon = 0.2721192444182125\nEpsilon = 0.2720920324937707\nEpsilon = 0.2720648232905213\nEpsilon = 0.2720376168081922\nEpsilon = 0.2720104130465114\nEpsilon = 0.27198321200520675\nEpsilon = 0.27195601368400624\nEpsilon = 0.27192881808263786\nEpsilon = 0.2719016252008296\nEpsilon = 0.27187443503830955\nEpsilon = 0.2718472475948057\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.27182006287004623\nEpsilon = 0.27179288086375925\nEpsilon = 0.27176570157567287\nEpsilon = 0.2717385250055153\nEpsilon = 0.27171135115301476\nAgent: ddqn_agent . Episode 715/2000. Number of steps to finish: 20. Loss: 14.387232780456543 Reward: -12.0\nEpsilon = 0.27168418001789946\nEpsilon = 0.2716570115998977\nEpsilon = 0.27162984589873773\nEpsilon = 0.27160268291414785\nEpsilon = 0.27157552264585644\nEpsilon = 0.27154836509359187\nEpsilon = 0.27152121025708253\nEpsilon = 0.2714940581360568\nEpsilon = 0.2714669087302432\nEpsilon = 0.2714397620393702\nEpsilon = 0.27141261806316624\nEpsilon = 0.27138547680135994\nEpsilon = 0.27135833825367983\nEpsilon = 0.2713312024198545\nEpsilon = 0.2713040692996125\nEpsilon = 0.2712769388926825\nEpsilon = 0.27124981119879327\nEpsilon = 0.27122268621767337\nEpsilon = 0.2711955639490516\nEpsilon = 0.2711684443926567\nAgent: ddqn_agent . Episode 716/2000. Number of steps to finish: 20. Loss: 14.208456039428711 Reward: -12.0\nEpsilon = 0.2711413275482174\nEpsilon = 0.2711142134154626\nEpsilon = 0.27108710199412106\nEpsilon = 0.27105999328392166\nEpsilon = 0.2710328872845933\nEpsilon = 0.27100578399586484\nEpsilon = 0.2709786834174652\nEpsilon = 0.27095158554912346\nEpsilon = 0.27092449039056854\nEpsilon = 0.2708973979415295\nEpsilon = 0.27087030820173535\nEpsilon = 0.2708432211709152\nEpsilon = 0.27081613684879813\nEpsilon = 0.27078905523511326\nEpsilon = 0.27076197632958976\nEpsilon = 0.2707349001319568\nEpsilon = 0.2707078266419436\nEpsilon = 0.2706807558592794\nEpsilon = 0.2706536877836935\nEpsilon = 0.2706266224149151\nAgent: ddqn_agent . Episode 717/2000. Number of steps to finish: 20. Loss: 14.475939750671387 Reward: -12.0\nEpsilon = 0.2705995597526736\nEpsilon = 0.27057249979669834\nEpsilon = 0.2705454425467187\nEpsilon = 0.270518388002464\nEpsilon = 0.2704913361636638\nEpsilon = 0.27046428703004743\nEpsilon = 0.27043724060134444\nEpsilon = 0.2704101968772843\nEpsilon = 0.2703831558575966\nEpsilon = 0.27035611754201083\nEpsilon = 0.2703290819302566\nEpsilon = 0.2703020490220636\nAgent: ddqn_agent . Episode 718/2000. Number of steps to finish: 12. Loss: 8.289840698242188 Reward: 0.0\nEpsilon = 0.2702750188171614\nEpsilon = 0.2702479913152797\nEpsilon = 0.2702209665161482\nEpsilon = 0.27019394441949657\nEpsilon = 0.2701669250250546\nEpsilon = 0.27013990833255214\nEpsilon = 0.27011289434171887\nEpsilon = 0.2700858830522847\nEpsilon = 0.27005887446397947\nEpsilon = 0.27003186857653305\nEpsilon = 0.2700048653896754\nEpsilon = 0.2699778649031364\nEpsilon = 0.2699508671166461\nEpsilon = 0.26992387202993445\nEpsilon = 0.26989687964273146\nAgent: ddqn_agent . Episode 719/2000. Number of steps to finish: 15. Loss: 10.861167907714844 Reward: -3.0\nEpsilon = 0.2698698899547672\nEpsilon = 0.26984290296577174\nEpsilon = 0.26981591867547516\nEpsilon = 0.2697889370836076\nEpsilon = 0.2697619581898993\nEpsilon = 0.2697349819940803\nEpsilon = 0.2697080084958809\nEpsilon = 0.2696810376950313\nEpsilon = 0.2696540695912618\nEpsilon = 0.2696271041843027\nEpsilon = 0.2696001414738843\nEpsilon = 0.2695731814597369\nEpsilon = 0.26954622414159096\nEpsilon = 0.2695192695191768\nEpsilon = 0.2694923175922249\nEpsilon = 0.2694653683604657\nEpsilon = 0.26943842182362965\nEpsilon = 0.26941147798144727\nEpsilon = 0.2693845368336491\nEpsilon = 0.26935759837996576\nAgent: ddqn_agent . Episode 720/2000. Number of steps to finish: 20. Loss: 14.365311622619629 Reward: -14.0\nEpsilon = 0.2693306626201278\nEpsilon = 0.2693037295538658\nEpsilon = 0.2692767991809104\nEpsilon = 0.26924987150099233\nEpsilon = 0.2692229465138422\nEpsilon = 0.2691960242191908\nEpsilon = 0.2691691046167689\nEpsilon = 0.2691421877063072\nEpsilon = 0.2691152734875366\nEpsilon = 0.26908836196018787\nEpsilon = 0.2690614531239919\nEpsilon = 0.26903454697867946\nEpsilon = 0.2690076435239816\nEpsilon = 0.2689807427596292\nEpsilon = 0.2689538446853532\nEpsilon = 0.2689269493008847\nEpsilon = 0.2689000566059546\nEpsilon = 0.26887316660029403\nEpsilon = 0.268846279283634\nEpsilon = 0.26881939465570565\nAgent: ddqn_agent . Episode 721/2000. Number of steps to finish: 20. Loss: 14.58449649810791 Reward: -16.0\nEpsilon = 0.26879251271624005\nEpsilon = 0.26876563346496846\nEpsilon = 0.26873875690162197\nEpsilon = 0.2687118830259318\nEpsilon = 0.2686850118376292\nEpsilon = 0.26865814333644544\nEpsilon = 0.2686312775221118\nEpsilon = 0.2686044143943596\nEpsilon = 0.26857755395292016\nEpsilon = 0.26855069619752486\nEpsilon = 0.2685238411279051\nAgent: ddqn_agent . Episode 722/2000. Number of steps to finish: 11. Loss: 7.760791301727295 Reward: 1.0\nEpsilon = 0.26849698874379235\nEpsilon = 0.26847013904491795\nEpsilon = 0.26844329203101347\nEpsilon = 0.2684164477018104\nEpsilon = 0.2683896060570402\nEpsilon = 0.26836276709643453\nEpsilon = 0.2683359308197249\nEpsilon = 0.2683090972266429\nEpsilon = 0.26828226631692026\nEpsilon = 0.26825543809028857\nEpsilon = 0.26822861254647956\nEpsilon = 0.2682017896852249\nEpsilon = 0.26817496950625636\nEpsilon = 0.26814815200930575\nEpsilon = 0.2681213371941048\nEpsilon = 0.2680945250603854\nEpsilon = 0.26806771560787934\nEpsilon = 0.26804090883631854\nEpsilon = 0.2680141047454349\nEpsilon = 0.26798730333496035\nAgent: ddqn_agent . Episode 723/2000. Number of steps to finish: 20. Loss: 14.32933521270752 Reward: -20.0\nEpsilon = 0.26796050460462684\nEpsilon = 0.26793370855416637\nEpsilon = 0.26790691518331095\nEpsilon = 0.2678801244917926\nEpsilon = 0.26785333647934345\nEpsilon = 0.26782655114569554\nEpsilon = 0.26779976849058096\nEpsilon = 0.2677729885137319\nEpsilon = 0.2677462112148805\nEpsilon = 0.267719436593759\nEpsilon = 0.26769266465009967\nEpsilon = 0.26766589538363467\nEpsilon = 0.2676391287940963\nEpsilon = 0.26761236488121687\nEpsilon = 0.2675856036447288\nEpsilon = 0.2675588450843643\nEpsilon = 0.2675320891998558\nEpsilon = 0.26750533599093584\nEpsilon = 0.26747858545733677\nEpsilon = 0.26745183759879104\nAgent: ddqn_agent . Episode 724/2000. Number of steps to finish: 20. Loss: 14.354864120483398 Reward: -14.0\nEpsilon = 0.26742509241503115\nEpsilon = 0.26739834990578965\nEpsilon = 0.26737161007079907\nEpsilon = 0.267344872909792\nEpsilon = 0.267318138422501\nEpsilon = 0.26729140660865874\nEpsilon = 0.26726467746799787\nEpsilon = 0.26723795100025105\nEpsilon = 0.267211227205151\nEpsilon = 0.2671845060824305\nEpsilon = 0.26715778763182224\nEpsilon = 0.26713107185305907\nEpsilon = 0.26710435874587374\nEpsilon = 0.26707764830999914\nEpsilon = 0.26705094054516815\nEpsilon = 0.26702423545111365\nEpsilon = 0.2669975330275685\nEpsilon = 0.26697083327426574\nEpsilon = 0.2669441361909383\nEpsilon = 0.26691744177731924\nAgent: ddqn_agent . Episode 725/2000. Number of steps to finish: 20. Loss: 14.866962432861328 Reward: -10.0\nEpsilon = 0.2668907500331415\nEpsilon = 0.2668640609581382\nEpsilon = 0.2668373745520424\nEpsilon = 0.2668106908145872\nEpsilon = 0.26678400974550576\nEpsilon = 0.2667573313445312\nEpsilon = 0.26673065561139675\nEpsilon = 0.2667039825458356\nEpsilon = 0.266677312147581\nEpsilon = 0.26665064441636627\nEpsilon = 0.2666239793519246\nEpsilon = 0.2665973169539894\nEpsilon = 0.266570657222294\nEpsilon = 0.26654400015657176\nEpsilon = 0.2665173457565561\nEpsilon = 0.26649069402198045\nEpsilon = 0.26646404495257825\nEpsilon = 0.266437398548083\nEpsilon = 0.26641075480822823\nEpsilon = 0.2663841137327474\nAgent: ddqn_agent . Episode 726/2000. Number of steps to finish: 20. Loss: 14.494431495666504 Reward: -12.0\nEpsilon = 0.2663574753213741\nEpsilon = 0.26633083957384196\nEpsilon = 0.26630420648988457\nEpsilon = 0.2662775760692356\nEpsilon = 0.26625094831162865\nEpsilon = 0.2662243232167975\nEpsilon = 0.26619770078447585\nEpsilon = 0.2661710810143974\nEpsilon = 0.266144463906296\nEpsilon = 0.26611784945990535\nEpsilon = 0.26609123767495935\nEpsilon = 0.26606462855119184\nEpsilon = 0.2660380220883367\nEpsilon = 0.2660114182861279\nEpsilon = 0.2659848171442993\nEpsilon = 0.26595821866258484\nEpsilon = 0.2659316228407186\nEpsilon = 0.2659050296784345\nEpsilon = 0.26587843917546666\nEpsilon = 0.2658518513315491\nAgent: ddqn_agent . Episode 727/2000. Number of steps to finish: 20. Loss: 14.132844924926758 Reward: -18.0\nEpsilon = 0.26582526614641594\nEpsilon = 0.2657986836198013\nEpsilon = 0.26577210375143934\nEpsilon = 0.2657455265410642\nEpsilon = 0.2657189519884101\nEpsilon = 0.2656923800932113\nEpsilon = 0.26566581085520197\nEpsilon = 0.26563924427411645\nEpsilon = 0.26561268034968905\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.2655861190816541\nEpsilon = 0.26555956046974594\nEpsilon = 0.265533004513699\nEpsilon = 0.2655064512132476\nEpsilon = 0.2654799005681263\nEpsilon = 0.2654533525780695\nEpsilon = 0.2654268072428117\nEpsilon = 0.26540026456208743\nEpsilon = 0.26537372453563124\nEpsilon = 0.2653471871631777\nEpsilon = 0.26532065244446135\nAgent: ddqn_agent . Episode 728/2000. Number of steps to finish: 20. Loss: 14.884747505187988 Reward: -12.0\nEpsilon = 0.2652941203792169\nEpsilon = 0.265267590967179\nEpsilon = 0.2652410642080823\nEpsilon = 0.2652145401016615\nEpsilon = 0.26518801864765135\nEpsilon = 0.2651614998457866\nEpsilon = 0.265134983695802\nEpsilon = 0.26510847019743244\nEpsilon = 0.2650819593504127\nEpsilon = 0.26505545115447765\nEpsilon = 0.2650289456093622\nEpsilon = 0.2650024427148013\nEpsilon = 0.2649759424705298\nEpsilon = 0.26494944487628275\nEpsilon = 0.2649229499317951\nEpsilon = 0.2648964576368019\nEpsilon = 0.26486996799103824\nEpsilon = 0.26484348099423916\nEpsilon = 0.2648169966461397\nEpsilon = 0.2647905149464751\nAgent: ddqn_agent . Episode 729/2000. Number of steps to finish: 20. Loss: 14.75124740600586 Reward: -14.0\nEpsilon = 0.26476403589498043\nEpsilon = 0.26473755949139094\nEpsilon = 0.2647110857354418\nEpsilon = 0.2646846146268682\nEpsilon = 0.2646581461654055\nEpsilon = 0.264631680350789\nEpsilon = 0.2646052171827539\nEpsilon = 0.26457875666103564\nEpsilon = 0.26455229878536957\nEpsilon = 0.264525843555491\nEpsilon = 0.2644993909711355\nEpsilon = 0.2644729410320384\nEpsilon = 0.26444649373793516\nEpsilon = 0.26442004908856137\nEpsilon = 0.2643936070836525\nEpsilon = 0.26436716772294416\nEpsilon = 0.2643407310061719\nEpsilon = 0.2643142969330713\nEpsilon = 0.26428786550337796\nEpsilon = 0.2642614367168276\nAgent: ddqn_agent . Episode 730/2000. Number of steps to finish: 20. Loss: 14.902656555175781 Reward: -10.0\nEpsilon = 0.26423501057315596\nEpsilon = 0.26420858707209866\nEpsilon = 0.26418216621339147\nEpsilon = 0.2641557479967701\nEpsilon = 0.26412933242197045\nEpsilon = 0.26410291948872827\nEpsilon = 0.2640765091967794\nEpsilon = 0.26405010154585973\nEpsilon = 0.2640236965357051\nEpsilon = 0.26399729416605155\nEpsilon = 0.26397089443663496\nEpsilon = 0.2639444973471913\nEpsilon = 0.26391810289745654\nEpsilon = 0.2638917110871668\nAgent: ddqn_agent . Episode 731/2000. Number of steps to finish: 14. Loss: 9.857039451599121 Reward: -2.0\nEpsilon = 0.2638653219160581\nEpsilon = 0.2638389353838665\nEpsilon = 0.2638125514903281\nEpsilon = 0.2637861702351791\nEpsilon = 0.26375979161815555\nEpsilon = 0.2637334156389937\nEpsilon = 0.2637070422974298\nEpsilon = 0.2636806715932001\nEpsilon = 0.26365430352604075\nEpsilon = 0.26362793809568813\nEpsilon = 0.26360157530187855\nEpsilon = 0.2635752151443484\nEpsilon = 0.263548857622834\nEpsilon = 0.2635225027370717\nEpsilon = 0.263496150486798\nEpsilon = 0.2634698008717493\nEpsilon = 0.2634434538916621\nEpsilon = 0.26341710954627295\nEpsilon = 0.26339076783531834\nEpsilon = 0.26336442875853483\nAgent: ddqn_agent . Episode 732/2000. Number of steps to finish: 20. Loss: 14.594429016113281 Reward: -14.0\nEpsilon = 0.26333809231565897\nEpsilon = 0.2633117585064274\nEpsilon = 0.26328542733057675\nEpsilon = 0.2632590987878437\nEpsilon = 0.2632327728779649\nEpsilon = 0.26320644960067713\nEpsilon = 0.2631801289557171\nEpsilon = 0.26315381094282153\nEpsilon = 0.26312749556172726\nEpsilon = 0.2631011828121711\nEpsilon = 0.2630748726938899\nEpsilon = 0.2630485652066205\nEpsilon = 0.26302226035009985\nEpsilon = 0.26299595812406484\nEpsilon = 0.26296965852825244\nEpsilon = 0.2629433615623996\nEpsilon = 0.26291706722624336\nEpsilon = 0.2628907755195207\nEpsilon = 0.2628644864419688\nEpsilon = 0.2628381999933246\nAgent: ddqn_agent . Episode 733/2000. Number of steps to finish: 20. Loss: 14.285483360290527 Reward: -8.0\nEpsilon = 0.2628119161733253\nEpsilon = 0.2627856349817079\nEpsilon = 0.26275935641820974\nEpsilon = 0.26273308048256794\nEpsilon = 0.26270680717451966\nEpsilon = 0.26268053649380224\nEpsilon = 0.2626542684401529\nEpsilon = 0.26262800301330885\nEpsilon = 0.26260174021300753\nEpsilon = 0.26257548003898623\nEpsilon = 0.26254922249098234\nEpsilon = 0.2625229675687332\nEpsilon = 0.2624967152719764\nEpsilon = 0.2624704656004492\nEpsilon = 0.2624442185538891\nEpsilon = 0.2624179741320337\nEpsilon = 0.26239173233462054\nEpsilon = 0.2623654931613871\nEpsilon = 0.2623392566120709\nEpsilon = 0.2623130226864097\nAgent: ddqn_agent . Episode 734/2000. Number of steps to finish: 20. Loss: 14.008832931518555 Reward: -14.0\nEpsilon = 0.26228679138414107\nEpsilon = 0.26226056270500264\nEpsilon = 0.26223433664873214\nEpsilon = 0.26220811321506726\nEpsilon = 0.26218189240374573\nEpsilon = 0.26215567421450536\nEpsilon = 0.2621294586470839\nEpsilon = 0.26210324570121923\nEpsilon = 0.2620770353766491\nEpsilon = 0.26205082767311144\nEpsilon = 0.26202462259034415\nEpsilon = 0.2619984201280851\nEpsilon = 0.2619722202860723\nEpsilon = 0.2619460230640437\nEpsilon = 0.2619198284617373\nEpsilon = 0.26189363647889113\nEpsilon = 0.26186744711524323\nEpsilon = 0.26184126037053174\nEpsilon = 0.2618150762444947\nEpsilon = 0.2617888947368702\nAgent: ddqn_agent . Episode 735/2000. Number of steps to finish: 20. Loss: 14.444403648376465 Reward: -14.0\nEpsilon = 0.2617627158473965\nEpsilon = 0.2617365395758118\nEpsilon = 0.2617103659218542\nEpsilon = 0.261684194885262\nEpsilon = 0.26165802646577346\nEpsilon = 0.2616318606631269\nEpsilon = 0.2616056974770606\nEpsilon = 0.2615795369073129\nEpsilon = 0.2615533789536222\nEpsilon = 0.26152722361572683\nEpsilon = 0.2615010708933653\nEpsilon = 0.26147492078627593\nEpsilon = 0.2614487732941973\nEpsilon = 0.26142262841686786\nEpsilon = 0.26139648615402616\nEpsilon = 0.26137034650541074\nEpsilon = 0.2613442094707602\nEpsilon = 0.26131807504981314\nEpsilon = 0.26129194324230814\nEpsilon = 0.26126581404798394\nAgent: ddqn_agent . Episode 736/2000. Number of steps to finish: 20. Loss: 14.709012985229492 Reward: -20.0\nEpsilon = 0.26123968746657916\nEpsilon = 0.26121356349783253\nEpsilon = 0.26118744214148276\nEpsilon = 0.2611613233972686\nEpsilon = 0.2611352072649289\nEpsilon = 0.2611090937442024\nEpsilon = 0.26108298283482795\nEpsilon = 0.26105687453654447\nEpsilon = 0.2610307688490908\nEpsilon = 0.26100466577220593\nEpsilon = 0.26097856530562874\nEpsilon = 0.26095246744909817\nEpsilon = 0.26092637220235326\nEpsilon = 0.260900279565133\nEpsilon = 0.2608741895371765\nEpsilon = 0.2608481021182228\nEpsilon = 0.26082201730801097\nEpsilon = 0.2607959351062802\nEpsilon = 0.26076985551276954\nEpsilon = 0.26074377852721825\nAgent: ddqn_agent . Episode 737/2000. Number of steps to finish: 20. Loss: 14.622310638427734 Reward: -12.0\nEpsilon = 0.2607177041493655\nEpsilon = 0.2606916323789506\nEpsilon = 0.2606655632157127\nEpsilon = 0.2606394966593911\nEpsilon = 0.2606134327097252\nEpsilon = 0.2605873713664542\nEpsilon = 0.2605613126293176\nEpsilon = 0.26053525649805465\nEpsilon = 0.2605092029724048\nEpsilon = 0.2604831520521076\nEpsilon = 0.2604571037369024\nEpsilon = 0.2604310580265287\nEpsilon = 0.26040501492072604\nEpsilon = 0.26037897441923397\nEpsilon = 0.26035293652179203\nEpsilon = 0.26032690122813984\nEpsilon = 0.26030086853801704\nEpsilon = 0.2602748384511632\nEpsilon = 0.2602488109673181\nEpsilon = 0.2602227860862214\nAgent: ddqn_agent . Episode 738/2000. Number of steps to finish: 20. Loss: 14.883220672607422 Reward: -14.0\nEpsilon = 0.2601967638076128\nEpsilon = 0.26017074413123203\nEpsilon = 0.2601447270568189\nEpsilon = 0.26011871258411323\nEpsilon = 0.26009270071285484\nEpsilon = 0.26006669144278355\nEpsilon = 0.2600406847736393\nEpsilon = 0.2600146807051619\nEpsilon = 0.2599886792370914\nEpsilon = 0.2599626803691677\nEpsilon = 0.2599366841011308\nEpsilon = 0.2599106904327207\nEpsilon = 0.2598846993636774\nEpsilon = 0.2598587108937411\nEpsilon = 0.2598327250226517\nEpsilon = 0.25980674175014945\nEpsilon = 0.2597807610759744\nEpsilon = 0.2597547829998668\nEpsilon = 0.25972880752156685\nEpsilon = 0.2597028346408147\nAgent: ddqn_agent . Episode 739/2000. Number of steps to finish: 20. Loss: 14.988851547241211 Reward: -14.0\nEpsilon = 0.2596768643573506\nEpsilon = 0.25965089667091484\nEpsilon = 0.2596249315812478\nEpsilon = 0.25959896908808966\nEpsilon = 0.25957300919118087\nEpsilon = 0.2595470518902618\nEpsilon = 0.25952109718507277\nEpsilon = 0.25949514507535426\nEpsilon = 0.25946919556084674\nEpsilon = 0.25944324864129065\nEpsilon = 0.2594173043164265\nEpsilon = 0.2593913625859949\nEpsilon = 0.2593654234497363\nEpsilon = 0.25933948690739134\nEpsilon = 0.2593135529587006\nEpsilon = 0.25928762160340474\nEpsilon = 0.2592616928412444\nEpsilon = 0.2592357666719603\nEpsilon = 0.2592098430952931\nEpsilon = 0.25918392211098357\nAgent: ddqn_agent . Episode 740/2000. Number of steps to finish: 20. Loss: 14.80549144744873 Reward: -14.0\nEpsilon = 0.25915800371877246\nEpsilon = 0.2591320879184006\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.2591061747096087\nEpsilon = 0.25908026409213775\nEpsilon = 0.2590543560657285\nEpsilon = 0.259028450630122\nEpsilon = 0.259002547785059\nEpsilon = 0.2589766475302805\nEpsilon = 0.25895074986552746\nEpsilon = 0.2589248547905409\nEpsilon = 0.2588989623050618\nEpsilon = 0.25887307240883134\nEpsilon = 0.25884718510159044\nEpsilon = 0.25882130038308027\nEpsilon = 0.258795418253042\nEpsilon = 0.25876953871121666\nEpsilon = 0.25874366175734553\nEpsilon = 0.2587177873911698\nEpsilon = 0.25869191561243066\nEpsilon = 0.25866604642086943\nAgent: ddqn_agent . Episode 741/2000. Number of steps to finish: 20. Loss: 14.237221717834473 Reward: -14.0\nEpsilon = 0.25864017981622733\nEpsilon = 0.25861431579824573\nEpsilon = 0.2585884543666659\nEpsilon = 0.25856259552122923\nEpsilon = 0.25853673926167714\nEpsilon = 0.258510885587751\nEpsilon = 0.2584850344991922\nEpsilon = 0.25845918599574225\nEpsilon = 0.2584333400771427\nEpsilon = 0.25840749674313496\nEpsilon = 0.25838165599346063\nEpsilon = 0.25835581782786127\nEpsilon = 0.2583299822460785\nEpsilon = 0.2583041492478539\nEpsilon = 0.2582783188329291\nEpsilon = 0.25825249100104586\nEpsilon = 0.25822666575194575\nEpsilon = 0.25820084308537056\nEpsilon = 0.25817502300106204\nEpsilon = 0.2581492054987619\nAgent: ddqn_agent . Episode 742/2000. Number of steps to finish: 20. Loss: 14.795900344848633 Reward: -16.0\nEpsilon = 0.25812339057821204\nEpsilon = 0.25809757823915425\nEpsilon = 0.25807176848133034\nEpsilon = 0.2580459613044822\nEpsilon = 0.25802015670835177\nEpsilon = 0.25799435469268095\nEpsilon = 0.2579685552572117\nEpsilon = 0.25794275840168596\nEpsilon = 0.2579169641258458\nEpsilon = 0.2578911724294332\nEpsilon = 0.2578653833121903\nEpsilon = 0.25783959677385904\nEpsilon = 0.25781381281418164\nAgent: ddqn_agent . Episode 743/2000. Number of steps to finish: 13. Loss: 9.120525360107422 Reward: -1.0\nEpsilon = 0.25778803143290024\nEpsilon = 0.25776225262975694\nEpsilon = 0.257736476404494\nEpsilon = 0.25771070275685354\nEpsilon = 0.25768493168657786\nEpsilon = 0.2576591631934092\nEpsilon = 0.25763339727708984\nEpsilon = 0.25760763393736213\nEpsilon = 0.2575818731739684\nEpsilon = 0.25755611498665104\nEpsilon = 0.2575303593751524\nEpsilon = 0.2575046063392149\nEpsilon = 0.257478855878581\nEpsilon = 0.25745310799299315\nEpsilon = 0.25742736268219385\nEpsilon = 0.25740161994592564\nEpsilon = 0.25737587978393106\nEpsilon = 0.25735014219595265\nEpsilon = 0.25732440718173305\nEpsilon = 0.25729867474101487\nAgent: ddqn_agent . Episode 744/2000. Number of steps to finish: 20. Loss: 14.292959213256836 Reward: -14.0\nEpsilon = 0.25727294487354074\nEpsilon = 0.2572472175790534\nEpsilon = 0.2572214928572955\nEpsilon = 0.2571957707080098\nEpsilon = 0.257170051130939\nEpsilon = 0.2571443341258259\nEpsilon = 0.2571186196924133\nEpsilon = 0.25709290783044403\nEpsilon = 0.257067198539661\nEpsilon = 0.257041491819807\nEpsilon = 0.25701578767062505\nEpsilon = 0.25699008609185797\nEpsilon = 0.2569643870832488\nAgent: ddqn_agent . Episode 745/2000. Number of steps to finish: 13. Loss: 9.413020133972168 Reward: -1.0\nEpsilon = 0.2569386906445405\nEpsilon = 0.2569129967754761\nEpsilon = 0.2568873054757985\nEpsilon = 0.25686161674525093\nEpsilon = 0.2568359305835764\nEpsilon = 0.256810246990518\nEpsilon = 0.25678456596581894\nEpsilon = 0.2567588875092224\nEpsilon = 0.25673321162047147\nEpsilon = 0.2567075382993094\nEpsilon = 0.2566818675454795\nEpsilon = 0.25665619935872497\nEpsilon = 0.2566305337387891\nEpsilon = 0.2566048706854152\nEpsilon = 0.2565792101983467\nEpsilon = 0.25655355227732685\nAgent: ddqn_agent . Episode 746/2000. Number of steps to finish: 16. Loss: 11.756463050842285 Reward: -4.0\nEpsilon = 0.2565278969220991\nEpsilon = 0.2565022441324069\nEpsilon = 0.25647659390799366\nEpsilon = 0.2564509462486029\nEpsilon = 0.256425301153978\nEpsilon = 0.2563996586238626\nEpsilon = 0.25637401865800025\nEpsilon = 0.25634838125613446\nEpsilon = 0.25632274641800884\nEpsilon = 0.25629711414336703\nEpsilon = 0.2562714844319527\nEpsilon = 0.2562458572835095\nEpsilon = 0.25622023269778116\nEpsilon = 0.2561946106745114\nEpsilon = 0.25616899121344394\nEpsilon = 0.2561433743143226\nEpsilon = 0.2561177599768912\nEpsilon = 0.2560921482008935\nEpsilon = 0.2560665389860734\nEpsilon = 0.2560409323321748\nAgent: ddqn_agent . Episode 747/2000. Number of steps to finish: 20. Loss: 14.066201210021973 Reward: -16.0\nEpsilon = 0.2560153282389416\nEpsilon = 0.2559897267061177\nEpsilon = 0.25596412773344707\nEpsilon = 0.25593853132067373\nEpsilon = 0.2559129374675417\nEpsilon = 0.25588734617379494\nEpsilon = 0.25586175743917755\nEpsilon = 0.2558361712634336\nEpsilon = 0.25581058764630726\nEpsilon = 0.25578500658754266\nEpsilon = 0.2557594280868839\nEpsilon = 0.25573385214407524\nEpsilon = 0.2557082787588608\nAgent: ddqn_agent . Episode 748/2000. Number of steps to finish: 13. Loss: 9.668049812316895 Reward: -1.0\nEpsilon = 0.2556827079309849\nEpsilon = 0.2556571396601918\nEpsilon = 0.2556315739462258\nEpsilon = 0.2556060107888312\nEpsilon = 0.25558045018775233\nEpsilon = 0.2555548921427336\nEpsilon = 0.2555293366535193\nEpsilon = 0.25550378371985394\nEpsilon = 0.25547823334148195\nEpsilon = 0.2554526855181478\nEpsilon = 0.255427140249596\nEpsilon = 0.25540159753557107\nEpsilon = 0.25537605737581753\nEpsilon = 0.25535051977007994\nEpsilon = 0.25532498471810294\nEpsilon = 0.25529945221963113\nEpsilon = 0.25527392227440915\nEpsilon = 0.2552483948821817\nEpsilon = 0.2552228700426935\nEpsilon = 0.25519734775568925\nAgent: ddqn_agent . Episode 749/2000. Number of steps to finish: 20. Loss: 14.638500213623047 Reward: -14.0\nEpsilon = 0.2551718280209137\nEpsilon = 0.2551463108381116\nEpsilon = 0.2551207962070278\nEpsilon = 0.2550952841274071\nEpsilon = 0.2550697745989944\nEpsilon = 0.2550442676215345\nEpsilon = 0.25501876319477235\nEpsilon = 0.25499326131845285\nEpsilon = 0.254967761992321\nEpsilon = 0.2549422652161218\nEpsilon = 0.2549167709896002\nEpsilon = 0.2548912793125012\nEpsilon = 0.25486579018456995\nEpsilon = 0.2548403036055515\nEpsilon = 0.2548148195751909\nEpsilon = 0.2547893380932334\nEpsilon = 0.25476385915942407\nEpsilon = 0.25473838277350813\nEpsilon = 0.2547129089352308\nEpsilon = 0.25468743764433727\nAgent: ddqn_agent . Episode 750/2000. Number of steps to finish: 20. Loss: 14.789000511169434 Reward: -10.0\nEpsilon = 0.25466196890057285\nEpsilon = 0.2546365027036828\nEpsilon = 0.2546110390534125\nEpsilon = 0.25458557794950715\nEpsilon = 0.2545601193917122\nEpsilon = 0.25453466337977304\nEpsilon = 0.25450920991343506\nEpsilon = 0.25448375899244374\nEpsilon = 0.2544583106165445\nEpsilon = 0.2544328647854828\nEpsilon = 0.2544074214990043\nEpsilon = 0.2543819807568544\nEpsilon = 0.25435654255877876\nEpsilon = 0.2543311069045229\nEpsilon = 0.2543056737938324\nEpsilon = 0.25428024322645304\nEpsilon = 0.2542548152021304\nEpsilon = 0.25422938972061015\nEpsilon = 0.2542039667816381\nAgent: ddqn_agent . Episode 751/2000. Number of steps to finish: 19. Loss: 13.598968505859375 Reward: -7.0\nEpsilon = 0.2541785463849599\nEpsilon = 0.2541531285303214\nEpsilon = 0.25412771321746835\nEpsilon = 0.2541023004461466\nEpsilon = 0.254076890216102\nEpsilon = 0.25405148252708043\nEpsilon = 0.2540260773788277\nEpsilon = 0.25400067477108984\nEpsilon = 0.25397527470361275\nEpsilon = 0.25394987717614237\nEpsilon = 0.2539244821884248\nEpsilon = 0.2538990897402059\nEpsilon = 0.2538736998312319\nEpsilon = 0.2538483124612488\nEpsilon = 0.25382292763000264\nEpsilon = 0.25379754533723964\nEpsilon = 0.25377216558270593\nEpsilon = 0.25374678836614767\nEpsilon = 0.25372141368731105\nEpsilon = 0.25369604154594233\nAgent: ddqn_agent . Episode 752/2000. Number of steps to finish: 20. Loss: 14.335402488708496 Reward: -12.0\nEpsilon = 0.2536706719417877\nEpsilon = 0.2536453048745935\nEpsilon = 0.25361994034410607\nEpsilon = 0.25359457835007165\nEpsilon = 0.25356921889223666\nEpsilon = 0.25354386197034745\nEpsilon = 0.2535185075841504\nEpsilon = 0.253493155733392\nEpsilon = 0.25346780641781863\nEpsilon = 0.2534424596371769\nEpsilon = 0.25341711539121314\nEpsilon = 0.253391773679674\nEpsilon = 0.25336643450230606\nEpsilon = 0.25334109785885583\nEpsilon = 0.25331576374906994\nEpsilon = 0.25329043217269503\nEpsilon = 0.25326510312947775\nEpsilon = 0.2532397766191648\nEpsilon = 0.2532144526415029\nEpsilon = 0.25318913119623876\nAgent: ddqn_agent . Episode 753/2000. Number of steps to finish: 20. Loss: 15.174361228942871 Reward: -16.0\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.2531638122831191\nEpsilon = 0.2531384959018908\nEpsilon = 0.2531131820523006\nEpsilon = 0.2530878707340954\nEpsilon = 0.253062561947022\nEpsilon = 0.2530372556908273\nEpsilon = 0.2530119519652582\nEpsilon = 0.2529866507700617\nEpsilon = 0.2529613521049847\nEpsilon = 0.2529360559697742\nEpsilon = 0.2529107623641772\nEpsilon = 0.2528854712879408\nEpsilon = 0.252860182740812\nEpsilon = 0.25283489672253795\nAgent: ddqn_agent . Episode 754/2000. Number of steps to finish: 14. Loss: 10.282780647277832 Reward: -2.0\nEpsilon = 0.2528096132328657\nEpsilon = 0.25278433227154246\nEpsilon = 0.2527590538383153\nEpsilon = 0.2527337779329315\nEpsilon = 0.2527085045551382\nEpsilon = 0.2526832337046827\nEpsilon = 0.25265796538131224\nEpsilon = 0.25263269958477413\nEpsilon = 0.25260743631481564\nEpsilon = 0.2525821755711842\nEpsilon = 0.25255691735362706\nEpsilon = 0.2525316616618917\nEpsilon = 0.25250640849572553\nEpsilon = 0.25248115785487596\nEpsilon = 0.2524559097390905\nEpsilon = 0.2524306641481166\nEpsilon = 0.2524054210817018\nEpsilon = 0.2523801805395936\nEpsilon = 0.25235494252153967\nEpsilon = 0.2523297070272875\nAgent: ddqn_agent . Episode 755/2000. Number of steps to finish: 20. Loss: 14.042202949523926 Reward: -12.0\nEpsilon = 0.2523044740565848\nEpsilon = 0.25227924360917914\nEpsilon = 0.2522540156848182\nEpsilon = 0.25222879028324974\nEpsilon = 0.2522035674042214\nEpsilon = 0.252178347047481\nEpsilon = 0.2521531292127762\nEpsilon = 0.25212791389985495\nEpsilon = 0.25210270110846494\nEpsilon = 0.2520774908383541\nEpsilon = 0.2520522830892703\nEpsilon = 0.2520270778609614\nEpsilon = 0.2520018751531753\nAgent: ddqn_agent . Episode 756/2000. Number of steps to finish: 13. Loss: 9.450613021850586 Reward: -1.0\nEpsilon = 0.25197667496565995\nEpsilon = 0.25195147729816336\nEpsilon = 0.25192628215043356\nEpsilon = 0.2519010895222185\nEpsilon = 0.25187589941326627\nEpsilon = 0.25185071182332497\nEpsilon = 0.25182552675214265\nEpsilon = 0.25180034419946745\nEpsilon = 0.2517751641650475\nEpsilon = 0.251749986648631\nEpsilon = 0.2517248116499662\nEpsilon = 0.25169963916880117\nEpsilon = 0.2516744692048843\nEpsilon = 0.2516493017579638\nEpsilon = 0.251624136827788\nEpsilon = 0.2515989744141052\nEpsilon = 0.25157381451666383\nEpsilon = 0.2515486571352122\nEpsilon = 0.25152350226949866\nEpsilon = 0.2514983499192717\nAgent: ddqn_agent . Episode 757/2000. Number of steps to finish: 20. Loss: 14.741936683654785 Reward: -16.0\nEpsilon = 0.2514732000842798\nEpsilon = 0.2514480527642714\nEpsilon = 0.25142290795899497\nEpsilon = 0.25139776566819905\nEpsilon = 0.25137262589163223\nEpsilon = 0.25134748862904305\nEpsilon = 0.25132235388018015\nEpsilon = 0.2512972216447921\nEpsilon = 0.25127209192262767\nEpsilon = 0.25124696471343544\nEpsilon = 0.25122184001696407\nEpsilon = 0.2511967178329624\nEpsilon = 0.25117159816117907\nEpsilon = 0.25114648100136294\nEpsilon = 0.2511213663532628\nEpsilon = 0.2510962542166275\nEpsilon = 0.25107114459120583\nEpsilon = 0.2510460374767467\nEpsilon = 0.25102093287299904\nEpsilon = 0.2509958307797117\nAgent: ddqn_agent . Episode 758/2000. Number of steps to finish: 20. Loss: 14.767077445983887 Reward: -16.0\nEpsilon = 0.25097073119663377\nEpsilon = 0.2509456341235141\nEpsilon = 0.25092053956010174\nEpsilon = 0.25089544750614573\nEpsilon = 0.2508703579613951\nEpsilon = 0.250845270925599\nEpsilon = 0.25082018639850645\nEpsilon = 0.25079510437986663\nEpsilon = 0.2507700248694286\nEpsilon = 0.2507449478669417\nEpsilon = 0.250719873372155\nEpsilon = 0.2506948013848178\nEpsilon = 0.25066973190467934\nEpsilon = 0.25064466493148885\nEpsilon = 0.2506196004649957\nEpsilon = 0.2505945385049492\nEpsilon = 0.2505694790510987\nEpsilon = 0.2505444221031936\nEpsilon = 0.2505193676609833\nEpsilon = 0.2504943157242172\nAgent: ddqn_agent . Episode 759/2000. Number of steps to finish: 20. Loss: 15.237829208374023 Reward: -10.0\nEpsilon = 0.25046926629264477\nEpsilon = 0.2504442193660155\nEpsilon = 0.2504191749440789\nEpsilon = 0.2503941330265845\nEpsilon = 0.2503690936132818\nEpsilon = 0.25034405670392046\nEpsilon = 0.2503190222982501\nEpsilon = 0.2502939903960203\nEpsilon = 0.2502689609969807\nEpsilon = 0.250243934100881\nEpsilon = 0.25021890970747096\nEpsilon = 0.2501938878165002\nEpsilon = 0.25016886842771857\nEpsilon = 0.2501438515408758\nEpsilon = 0.25011883715572175\nEpsilon = 0.2500938252720062\nEpsilon = 0.250068815889479\nEpsilon = 0.25004380900789\nEpsilon = 0.2500188046269892\nEpsilon = 0.24999380274652652\nAgent: ddqn_agent . Episode 760/2000. Number of steps to finish: 20. Loss: 14.885377883911133 Reward: -14.0\nEpsilon = 0.24996880336625188\nEpsilon = 0.24994380648591524\nEpsilon = 0.24991881210526665\nEpsilon = 0.24989382022405612\nEpsilon = 0.24986883084203373\nEpsilon = 0.24984384395894954\nEpsilon = 0.24981885957455366\nEpsilon = 0.2497938776885962\nEpsilon = 0.24976889830082735\nEpsilon = 0.24974392141099727\nEpsilon = 0.24971894701885616\nEpsilon = 0.24969397512415428\nEpsilon = 0.24966900572664186\nEpsilon = 0.2496440388260692\nEpsilon = 0.2496190744221866\nEpsilon = 0.24959411251474437\nEpsilon = 0.2495691531034929\nEpsilon = 0.24954419618818255\nEpsilon = 0.24951924176856374\nEpsilon = 0.2494942898443869\nAgent: ddqn_agent . Episode 761/2000. Number of steps to finish: 20. Loss: 14.906067848205566 Reward: -20.0\nEpsilon = 0.24946934041540247\nEpsilon = 0.24944439348136094\nEpsilon = 0.2494194490420128\nEpsilon = 0.2493945070971086\nEpsilon = 0.24936956764639887\nEpsilon = 0.24934463068963422\nEpsilon = 0.24931969622656527\nEpsilon = 0.2492947642569426\nEpsilon = 0.24926983478051692\nEpsilon = 0.24924490779703887\nEpsilon = 0.24921998330625916\nEpsilon = 0.24919506130792854\nEpsilon = 0.24917014180179775\nEpsilon = 0.24914522478761758\nEpsilon = 0.2491203102651388\nEpsilon = 0.2490953982341123\nEpsilon = 0.2490704886942889\nEpsilon = 0.24904558164541948\nEpsilon = 0.24902067708725495\nEpsilon = 0.24899577501954623\nAgent: ddqn_agent . Episode 762/2000. Number of steps to finish: 20. Loss: 14.126836776733398 Reward: -14.0\nEpsilon = 0.24897087544204427\nEpsilon = 0.24894597835450008\nEpsilon = 0.24892108375666464\nEpsilon = 0.24889619164828897\nEpsilon = 0.24887130202912414\nEpsilon = 0.24884641489892123\nEpsilon = 0.24882153025743134\nEpsilon = 0.2487966481044056\nEpsilon = 0.24877176843959517\nEpsilon = 0.2487468912627512\nEpsilon = 0.24872201657362494\nEpsilon = 0.24869714437196758\nEpsilon = 0.2486722746575304\nEpsilon = 0.24864740743006464\nEpsilon = 0.24862254268932163\nEpsilon = 0.2485976804350527\nEpsilon = 0.2485728206670092\nEpsilon = 0.2485479633849425\nEpsilon = 0.24852310858860402\nEpsilon = 0.24849825627774516\nAgent: ddqn_agent . Episode 763/2000. Number of steps to finish: 20. Loss: 14.541902542114258 Reward: -12.0\nEpsilon = 0.24847340645211738\nEpsilon = 0.24844855911147218\nEpsilon = 0.24842371425556103\nEpsilon = 0.24839887188413548\nEpsilon = 0.24837403199694708\nEpsilon = 0.24834919459374738\nEpsilon = 0.248324359674288\nEpsilon = 0.24829952723832058\nEpsilon = 0.24827469728559676\nEpsilon = 0.2482498698158682\nEpsilon = 0.2482250448288866\nAgent: ddqn_agent . Episode 764/2000. Number of steps to finish: 11. Loss: 8.15393352508545 Reward: 1.0\nEpsilon = 0.24820022232440372\nEpsilon = 0.24817540230217128\nEpsilon = 0.24815058476194107\nEpsilon = 0.2481257697034649\nEpsilon = 0.24810095712649455\nEpsilon = 0.2480761470307819\nEpsilon = 0.24805133941607882\nEpsilon = 0.24802653428213722\nEpsilon = 0.248001731628709\nEpsilon = 0.24797693145554614\nEpsilon = 0.24795213376240058\nEpsilon = 0.24792733854902435\nEpsilon = 0.24790254581516946\nEpsilon = 0.24787775556058794\nEpsilon = 0.24785296778503188\nEpsilon = 0.2478281824882534\nEpsilon = 0.24780339967000456\nEpsilon = 0.24777861933003756\nEpsilon = 0.24775384146810456\nEpsilon = 0.24772906608395776\nAgent: ddqn_agent . Episode 765/2000. Number of steps to finish: 20. Loss: 14.875418663024902 Reward: -18.0\nEpsilon = 0.24770429317734935\nEpsilon = 0.24767952274803162\nEpsilon = 0.24765475479575683\nEpsilon = 0.24762998932027724\nEpsilon = 0.2476052263213452\nEpsilon = 0.24758046579871307\nEpsilon = 0.2475557077521332\nEpsilon = 0.24753095218135798\nEpsilon = 0.24750619908613986\nEpsilon = 0.24748144846623124\nEpsilon = 0.24745670032138462\nEpsilon = 0.24743195465135248\nEpsilon = 0.24740721145588734\nEpsilon = 0.24738247073474176\nEpsilon = 0.2473577324876683\nEpsilon = 0.24733299671441952\nEpsilon = 0.2473082634147481\nEpsilon = 0.24728353258840663\nEpsilon = 0.2472588042351478\nEpsilon = 0.24723407835472427\nAgent: ddqn_agent . Episode 766/2000. Number of steps to finish: 20. Loss: 14.477163314819336 Reward: -18.0\nEpsilon = 0.2472093549468888\nEpsilon = 0.24718463401139412\nEpsilon = 0.247159915547993\nEpsilon = 0.2471351995564382\nEpsilon = 0.24711048603648256\nEpsilon = 0.24708577498787893\nEpsilon = 0.24706106641038014\nEpsilon = 0.2470363603037391\nEpsilon = 0.24701165666770872\nEpsilon = 0.24698695550204194\nEpsilon = 0.24696225680649175\nEpsilon = 0.2469375605808111\nEpsilon = 0.24691286682475302\nEpsilon = 0.24688817553807055\nEpsilon = 0.24686348672051675\nEpsilon = 0.2468388003718447\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.2468141164918075\nEpsilon = 0.24678943508015833\nEpsilon = 0.2467647561366503\nEpsilon = 0.24674007966103664\nAgent: ddqn_agent . Episode 767/2000. Number of steps to finish: 20. Loss: 14.598542213439941 Reward: -10.0\nEpsilon = 0.24671540565307054\nEpsilon = 0.24669073411250525\nEpsilon = 0.246666065039094\nEpsilon = 0.2466413984325901\nEpsilon = 0.24661673429274683\nEpsilon = 0.24659207261931756\nEpsilon = 0.24656741341205562\nEpsilon = 0.24654275667071443\nEpsilon = 0.24651810239504737\nEpsilon = 0.24649345058480787\nEpsilon = 0.2464688012397494\nEpsilon = 0.24644415435962544\nEpsilon = 0.24641950994418949\nEpsilon = 0.24639486799319507\nEpsilon = 0.24637022850639576\nEpsilon = 0.24634559148354512\nEpsilon = 0.24632095692439676\nEpsilon = 0.2462963248287043\nEpsilon = 0.24627169519622144\nEpsilon = 0.24624706802670182\nAgent: ddqn_agent . Episode 768/2000. Number of steps to finish: 20. Loss: 14.4259033203125 Reward: -10.0\nEpsilon = 0.24622244331989915\nEpsilon = 0.24619782107556715\nEpsilon = 0.2461732012934596\nEpsilon = 0.24614858397333025\nEpsilon = 0.24612396911493292\nEpsilon = 0.24609935671802144\nEpsilon = 0.24607474678234964\nEpsilon = 0.24605013930767142\nEpsilon = 0.24602553429374066\nEpsilon = 0.2460009317403113\nEpsilon = 0.24597633164713725\nEpsilon = 0.24595173401397255\nEpsilon = 0.24592713884057116\nEpsilon = 0.2459025461266871\nEpsilon = 0.24587795587207445\nEpsilon = 0.24585336807648725\nEpsilon = 0.2458287827396796\nEpsilon = 0.24580419986140561\nEpsilon = 0.2457796194414195\nEpsilon = 0.24575504147947536\nAgent: ddqn_agent . Episode 769/2000. Number of steps to finish: 20. Loss: 14.710700035095215 Reward: -18.0\nEpsilon = 0.2457304659753274\nEpsilon = 0.24570589292872988\nEpsilon = 0.245681322339437\nEpsilon = 0.24565675420720307\nEpsilon = 0.24563218853178234\nEpsilon = 0.24560762531292918\nEpsilon = 0.24558306455039788\nEpsilon = 0.24555850624394285\nEpsilon = 0.24553395039331846\nEpsilon = 0.24550939699827914\nEpsilon = 0.24548484605857931\nEpsilon = 0.24546029757397345\nEpsilon = 0.24543575154421604\nEpsilon = 0.24541120796906163\nEpsilon = 0.2453866668482647\nEpsilon = 0.24536212818157987\nEpsilon = 0.24533759196876173\nEpsilon = 0.24531305820956487\nEpsilon = 0.24528852690374392\nEpsilon = 0.24526399805105356\nAgent: ddqn_agent . Episode 770/2000. Number of steps to finish: 20. Loss: 15.32675838470459 Reward: -16.0\nEpsilon = 0.24523947165124846\nEpsilon = 0.24521494770408334\nEpsilon = 0.24519042620931292\nEpsilon = 0.24516590716669198\nEpsilon = 0.2451413905759753\nEpsilon = 0.2451168764369177\nEpsilon = 0.245092364749274\nEpsilon = 0.24506785551279908\nEpsilon = 0.2450433487272478\nEpsilon = 0.2450188443923751\nEpsilon = 0.24499434250793586\nEpsilon = 0.24496984307368508\nEpsilon = 0.2449453460893777\nEpsilon = 0.24492085155476878\nEpsilon = 0.2448963594696133\nEpsilon = 0.24487186983366635\nEpsilon = 0.244847382646683\nEpsilon = 0.24482289790841832\nEpsilon = 0.24479841561862747\nEpsilon = 0.2447739357770656\nAgent: ddqn_agent . Episode 771/2000. Number of steps to finish: 20. Loss: 13.96378231048584 Reward: -16.0\nEpsilon = 0.2447494583834879\nEpsilon = 0.24472498343764956\nEpsilon = 0.2447005109393058\nEpsilon = 0.24467604088821188\nEpsilon = 0.24465157328412307\nEpsilon = 0.24462710812679467\nEpsilon = 0.24460264541598198\nEpsilon = 0.2445781851514404\nEpsilon = 0.24455372733292524\nEpsilon = 0.24452927196019195\nEpsilon = 0.24450481903299592\nEpsilon = 0.24448036855109262\nEpsilon = 0.2444559205142375\nEpsilon = 0.2444314749221861\nEpsilon = 0.24440703177469386\nEpsilon = 0.2443825910715164\nEpsilon = 0.24435815281240925\nEpsilon = 0.244333716997128\nEpsilon = 0.24430928362542828\nEpsilon = 0.24428485269706574\nAgent: ddqn_agent . Episode 772/2000. Number of steps to finish: 20. Loss: 14.663168907165527 Reward: -14.0\nEpsilon = 0.24426042421179603\nEpsilon = 0.24423599816937486\nEpsilon = 0.24421157456955792\nEpsilon = 0.24418715341210095\nEpsilon = 0.24416273469675975\nEpsilon = 0.2441383184232901\nEpsilon = 0.24411390459144777\nEpsilon = 0.24408949320098863\nEpsilon = 0.24406508425166853\nEpsilon = 0.24404067774324337\nEpsilon = 0.24401627367546905\nEpsilon = 0.2439918720481015\nEpsilon = 0.24396747286089668\nEpsilon = 0.2439430761136106\nEpsilon = 0.24391868180599924\nEpsilon = 0.24389428993781864\nEpsilon = 0.24386990050882487\nEpsilon = 0.24384551351877398\nEpsilon = 0.2438211289674221\nEpsilon = 0.24379674685452538\nAgent: ddqn_agent . Episode 773/2000. Number of steps to finish: 20. Loss: 14.461747169494629 Reward: -10.0\nEpsilon = 0.24377236717983994\nEpsilon = 0.24374798994312197\nEpsilon = 0.24372361514412766\nEpsilon = 0.24369924278261326\nEpsilon = 0.243674872858335\nEpsilon = 0.24365050537104918\nEpsilon = 0.24362614032051208\nEpsilon = 0.24360177770648003\nEpsilon = 0.2435774175287094\nEpsilon = 0.24355305978695654\nEpsilon = 0.24352870448097785\nEpsilon = 0.24350435161052975\nEpsilon = 0.2434800011753687\nEpsilon = 0.24345565317525117\nEpsilon = 0.24343130760993364\nAgent: ddqn_agent . Episode 774/2000. Number of steps to finish: 15. Loss: 11.025941848754883 Reward: -3.0\nEpsilon = 0.24340696447917265\nEpsilon = 0.24338262378272474\nEpsilon = 0.24335828552034647\nEpsilon = 0.24333394969179445\nEpsilon = 0.24330961629682527\nEpsilon = 0.24328528533519558\nEpsilon = 0.24326095680666207\nEpsilon = 0.2432366307109814\nEpsilon = 0.2432123070479103\nEpsilon = 0.24318798581720552\nEpsilon = 0.2431636670186238\nEpsilon = 0.24313935065192194\nEpsilon = 0.24311503671685675\nEpsilon = 0.24309072521318506\nEpsilon = 0.24306641614066374\nEpsilon = 0.24304210949904967\nEpsilon = 0.24301780528809977\nEpsilon = 0.24299350350757096\nEpsilon = 0.2429692041572202\nEpsilon = 0.24294490723680448\nAgent: ddqn_agent . Episode 775/2000. Number of steps to finish: 20. Loss: 14.0067777633667 Reward: -14.0\nEpsilon = 0.24292061274608082\nEpsilon = 0.2428963206848062\nEpsilon = 0.24287203105273772\nEpsilon = 0.24284774384963245\nEpsilon = 0.2428234590752475\nEpsilon = 0.24279917672933998\nEpsilon = 0.24277489681166706\nEpsilon = 0.24275061932198588\nEpsilon = 0.24272634426005368\nEpsilon = 0.24270207162562768\nEpsilon = 0.24267780141846512\nEpsilon = 0.2426535336383233\nEpsilon = 0.24262926828495945\nEpsilon = 0.24260500535813095\nEpsilon = 0.24258074485759515\nEpsilon = 0.2425564867831094\nEpsilon = 0.2425322311344311\nEpsilon = 0.24250797791131765\nEpsilon = 0.2424837271135265\nEpsilon = 0.24245947874081517\nAgent: ddqn_agent . Episode 776/2000. Number of steps to finish: 20. Loss: 14.49472427368164 Reward: -16.0\nEpsilon = 0.24243523279294107\nEpsilon = 0.2424109892696618\nEpsilon = 0.24238674817073483\nEpsilon = 0.24236250949591776\nEpsilon = 0.24233827324496818\nEpsilon = 0.24231403941764368\nEpsilon = 0.24228980801370192\nEpsilon = 0.24226557903290055\nEpsilon = 0.24224135247499726\nEpsilon = 0.24221712833974976\nEpsilon = 0.24219290662691578\nEpsilon = 0.2421686873362531\nEpsilon = 0.24214447046751947\nEpsilon = 0.2421202560204727\nEpsilon = 0.24209604399487067\nEpsilon = 0.2420718343904712\nEpsilon = 0.24204762720703216\nEpsilon = 0.24202342244431146\nEpsilon = 0.24199922010206704\nEpsilon = 0.24197502018005684\nAgent: ddqn_agent . Episode 777/2000. Number of steps to finish: 20. Loss: 14.752765655517578 Reward: -16.0\nEpsilon = 0.24195082267803883\nEpsilon = 0.24192662759577102\nEpsilon = 0.24190243493301145\nEpsilon = 0.24187824468951816\nEpsilon = 0.2418540568650492\nEpsilon = 0.24182987145936272\nEpsilon = 0.24180568847221678\nEpsilon = 0.24178150790336955\nEpsilon = 0.24175732975257921\nEpsilon = 0.24173315401960396\nEpsilon = 0.241708980704202\nEpsilon = 0.24168480980613158\nEpsilon = 0.24166064132515097\nEpsilon = 0.24163647526101845\nEpsilon = 0.24161231161349234\nEpsilon = 0.241588150382331\nEpsilon = 0.24156399156729277\nEpsilon = 0.24153983516813604\nEpsilon = 0.24151568118461922\nEpsilon = 0.24149152961650075\nAgent: ddqn_agent . Episode 778/2000. Number of steps to finish: 20. Loss: 14.891546249389648 Reward: -14.0\nEpsilon = 0.2414673804635391\nEpsilon = 0.24144323372549276\nEpsilon = 0.2414190894021202\nEpsilon = 0.24139494749318\nEpsilon = 0.2413708079984307\nEpsilon = 0.24134667091763085\nEpsilon = 0.2413225362505391\nEpsilon = 0.24129840399691405\nEpsilon = 0.24127427415651437\nEpsilon = 0.24125014672909872\nEpsilon = 0.2412260217144258\nEpsilon = 0.24120189911225437\nEpsilon = 0.24117777892234316\nEpsilon = 0.24115366114445091\nEpsilon = 0.24112954577833648\nEpsilon = 0.24110543282375865\nEpsilon = 0.24108132228047627\nEpsilon = 0.2410572141482482\nEpsilon = 0.2410331084268334\nEpsilon = 0.24100900511599072\nAgent: ddqn_agent . Episode 779/2000. Number of steps to finish: 20. Loss: 14.16476058959961 Reward: -14.0\nEpsilon = 0.2409849042154791\nEpsilon = 0.24096080572505757\nEpsilon = 0.24093670964448508\nEpsilon = 0.24091261597352062\nEpsilon = 0.24088852471192326\nEpsilon = 0.24086443585945208\nEpsilon = 0.24084034941586613\nEpsilon = 0.24081626538092454\nEpsilon = 0.24079218375438646\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.24076810453601102\nEpsilon = 0.24074402772555742\nEpsilon = 0.24071995332278487\nEpsilon = 0.2406958813274526\nEpsilon = 0.24067181173931984\nEpsilon = 0.24064774455814592\nEpsilon = 0.2406236797836901\nEpsilon = 0.24059961741571173\nEpsilon = 0.24057555745397016\nEpsilon = 0.24055149989822477\nEpsilon = 0.24052744474823495\nAgent: ddqn_agent . Episode 780/2000. Number of steps to finish: 20. Loss: 14.310561180114746 Reward: -18.0\nEpsilon = 0.24050339200376014\nEpsilon = 0.24047934166455978\nEpsilon = 0.24045529373039332\nEpsilon = 0.24043124820102027\nEpsilon = 0.24040720507620017\nEpsilon = 0.24038316435569254\nEpsilon = 0.24035912603925697\nEpsilon = 0.24033509012665305\nEpsilon = 0.24031105661764038\nEpsilon = 0.2402870255119786\nEpsilon = 0.24026299680942742\nEpsilon = 0.2402389705097465\nEpsilon = 0.24021494661269552\nEpsilon = 0.24019092511803425\nEpsilon = 0.24016690602552246\nEpsilon = 0.2401428893349199\nEpsilon = 0.24011887504598642\nEpsilon = 0.24009486315848183\nEpsilon = 0.240070853672166\nEpsilon = 0.24004684658679878\nAgent: ddqn_agent . Episode 781/2000. Number of steps to finish: 20. Loss: 14.50771713256836 Reward: -12.0\nEpsilon = 0.2400228419021401\nEpsilon = 0.23999883961794988\nEpsilon = 0.2399748397339881\nEpsilon = 0.2399508422500147\nEpsilon = 0.23992684716578971\nEpsilon = 0.23990285448107312\nEpsilon = 0.239878864195625\nEpsilon = 0.23985487630920546\nEpsilon = 0.23983089082157455\nEpsilon = 0.23980690773249239\nEpsilon = 0.23978292704171913\nEpsilon = 0.23975894874901496\nEpsilon = 0.23973497285414005\nEpsilon = 0.23971099935685464\nEpsilon = 0.23968702825691895\nEpsilon = 0.23966305955409326\nEpsilon = 0.23963909324813784\nEpsilon = 0.23961512933881304\nEpsilon = 0.23959116782587916\nAgent: ddqn_agent . Episode 782/2000. Number of steps to finish: 19. Loss: 14.111723899841309 Reward: -7.0\nEpsilon = 0.2395672087090966\nEpsilon = 0.23954325198822568\nEpsilon = 0.23951929766302685\nEpsilon = 0.23949534573326053\nEpsilon = 0.2394713961986872\nEpsilon = 0.23944744905906734\nEpsilon = 0.23942350431416143\nEpsilon = 0.23939956196373002\nEpsilon = 0.23937562200753365\nEpsilon = 0.2393516844453329\nEpsilon = 0.23932774927688835\nEpsilon = 0.23930381650196067\nEpsilon = 0.23927988612031048\nEpsilon = 0.23925595813169845\nEpsilon = 0.2392320325358853\nEpsilon = 0.2392081093326317\nEpsilon = 0.23918418852169845\nEpsilon = 0.2391602701028463\nEpsilon = 0.239136354075836\nEpsilon = 0.23911244044042843\nAgent: ddqn_agent . Episode 783/2000. Number of steps to finish: 20. Loss: 14.338447570800781 Reward: -14.0\nEpsilon = 0.23908852919638438\nEpsilon = 0.23906462034346473\nEpsilon = 0.2390407138814304\nEpsilon = 0.23901680981004225\nEpsilon = 0.23899290812906124\nEpsilon = 0.23896900883824834\nEpsilon = 0.2389451119373645\nEpsilon = 0.23892121742617078\nEpsilon = 0.23889732530442817\nEpsilon = 0.23887343557189772\nEpsilon = 0.23884954822834054\nEpsilon = 0.2388256632735177\nEpsilon = 0.23880178070719035\nEpsilon = 0.23877790052911962\nEpsilon = 0.2387540227390667\nEpsilon = 0.2387301473367928\nEpsilon = 0.23870627432205913\nEpsilon = 0.23868240369462693\nEpsilon = 0.23865853545425747\nEpsilon = 0.23863466960071206\nAgent: ddqn_agent . Episode 784/2000. Number of steps to finish: 20. Loss: 14.955511093139648 Reward: -14.0\nEpsilon = 0.238610806133752\nEpsilon = 0.23858694505313863\nEpsilon = 0.23856308635863333\nEpsilon = 0.23853923004999747\nEpsilon = 0.23851537612699247\nEpsilon = 0.23849152458937978\nEpsilon = 0.23846767543692085\nEpsilon = 0.23844382866937716\nEpsilon = 0.23841998428651023\nEpsilon = 0.2383961422880816\nEpsilon = 0.2383723026738528\nEpsilon = 0.2383484654435854\nEpsilon = 0.23832463059704104\nEpsilon = 0.23830079813398133\nEpsilon = 0.23827696805416793\nEpsilon = 0.2382531403573625\nEpsilon = 0.23822931504332676\nEpsilon = 0.23820549211182243\nEpsilon = 0.23818167156261125\nEpsilon = 0.23815785339545498\nAgent: ddqn_agent . Episode 785/2000. Number of steps to finish: 20. Loss: 14.973759651184082 Reward: -14.0\nEpsilon = 0.23813403761011542\nEpsilon = 0.23811022420635441\nEpsilon = 0.23808641318393378\nEpsilon = 0.23806260454261538\nEpsilon = 0.23803879828216112\nEpsilon = 0.2380149944023329\nEpsilon = 0.23799119290289267\nEpsilon = 0.23796739378360238\nEpsilon = 0.23794359704422402\nEpsilon = 0.2379198026845196\nEpsilon = 0.23789601070425115\nEpsilon = 0.23787222110318074\nEpsilon = 0.2378484338810704\nEpsilon = 0.2378246490376823\nEpsilon = 0.23780086657277855\nEpsilon = 0.23777708648612128\nEpsilon = 0.23775330877747267\nEpsilon = 0.23772953344659492\nEpsilon = 0.23770576049325026\nEpsilon = 0.23768198991720094\nAgent: ddqn_agent . Episode 786/2000. Number of steps to finish: 20. Loss: 14.454076766967773 Reward: -10.0\nEpsilon = 0.23765822171820922\nEpsilon = 0.2376344558960374\nEpsilon = 0.2376106924504478\nEpsilon = 0.23758693138120277\nEpsilon = 0.23756317268806465\nEpsilon = 0.23753941637079584\nEpsilon = 0.23751566242915875\nEpsilon = 0.23749191086291585\nEpsilon = 0.23746816167182958\nEpsilon = 0.2374444148556624\nEpsilon = 0.23742067041417683\nEpsilon = 0.2373969283471354\nEpsilon = 0.2373731886543007\nEpsilon = 0.23734945133543528\nEpsilon = 0.23732571639030173\nAgent: ddqn_agent . Episode 787/2000. Number of steps to finish: 15. Loss: 11.140203475952148 Reward: -3.0\nEpsilon = 0.2373019838186627\nEpsilon = 0.23727825362028082\nEpsilon = 0.2372545257949188\nEpsilon = 0.23723080034233932\nEpsilon = 0.23720707726230508\nEpsilon = 0.23718335655457887\nEpsilon = 0.2371596382189234\nEpsilon = 0.23713592225510152\nEpsilon = 0.23711220866287602\nEpsilon = 0.23708849744200974\nEpsilon = 0.23706478859226554\nEpsilon = 0.23704108211340633\nEpsilon = 0.23701737800519498\nEpsilon = 0.23699367626739445\nEpsilon = 0.2369699768997677\nEpsilon = 0.23694627990207773\nEpsilon = 0.23692258527408752\nEpsilon = 0.2368988930155601\nEpsilon = 0.23687520312625857\nEpsilon = 0.23685151560594594\nAgent: ddqn_agent . Episode 788/2000. Number of steps to finish: 20. Loss: 14.766840934753418 Reward: -20.0\nEpsilon = 0.23682783045438535\nEpsilon = 0.23680414767133992\nEpsilon = 0.23678046725657279\nEpsilon = 0.23675678920984713\nEpsilon = 0.23673311353092616\nEpsilon = 0.23670944021957308\nEpsilon = 0.23668576927555113\nEpsilon = 0.23666210069862356\nEpsilon = 0.2366384344885537\nEpsilon = 0.23661477064510486\nEpsilon = 0.23659110916804035\nEpsilon = 0.23656745005712354\nEpsilon = 0.23654379331211783\nEpsilon = 0.23652013893278662\nEpsilon = 0.23649648691889333\nEpsilon = 0.23647283727020144\nEpsilon = 0.2364491899864744\nEpsilon = 0.23642554506747576\nEpsilon = 0.23640190251296903\nEpsilon = 0.23637826232271772\nAgent: ddqn_agent . Episode 789/2000. Number of steps to finish: 20. Loss: 14.99853515625 Reward: -10.0\nEpsilon = 0.23635462449648545\nEpsilon = 0.2363309890340358\nEpsilon = 0.23630735593513238\nEpsilon = 0.23628372519953889\nEpsilon = 0.23626009682701893\nEpsilon = 0.23623647081733623\nEpsilon = 0.2362128471702545\nEpsilon = 0.23618922588553748\nEpsilon = 0.23616560696294892\nEpsilon = 0.23614199040225262\nEpsilon = 0.23611837620321238\nEpsilon = 0.23609476436559207\nEpsilon = 0.2360711548891555\nEpsilon = 0.2360475477736666\nEpsilon = 0.23602394301888924\nEpsilon = 0.23600034062458736\nAgent: ddqn_agent . Episode 790/2000. Number of steps to finish: 16. Loss: 11.56633472442627 Reward: -4.0\nEpsilon = 0.2359767405905249\nEpsilon = 0.23595314291646585\nEpsilon = 0.23592954760217422\nEpsilon = 0.235905954647414\nEpsilon = 0.23588236405194926\nEpsilon = 0.23585877581554407\nEpsilon = 0.2358351899379625\nEpsilon = 0.23581160641896873\nEpsilon = 0.23578802525832684\nEpsilon = 0.23576444645580102\nEpsilon = 0.23574087001115546\nEpsilon = 0.23571729592415436\nEpsilon = 0.23569372419456194\nEpsilon = 0.23567015482214249\nEpsilon = 0.23564658780666028\nEpsilon = 0.2356230231478796\nEpsilon = 0.23559946084556482\nEpsilon = 0.23557590089948027\nEpsilon = 0.23555234330939032\nEpsilon = 0.2355287880750594\nAgent: ddqn_agent . Episode 791/2000. Number of steps to finish: 20. Loss: 15.543212890625 Reward: -14.0\nEpsilon = 0.23550523519625188\nEpsilon = 0.23548168467273226\nEpsilon = 0.23545813650426498\nEpsilon = 0.23543459069061456\nEpsilon = 0.23541104723154552\nEpsilon = 0.23538750612682235\nEpsilon = 0.23536396737620968\nEpsilon = 0.23534043097947208\nEpsilon = 0.23531689693637414\nEpsilon = 0.2352933652466805\nEpsilon = 0.23526983591015585\nEpsilon = 0.23524630892656484\nEpsilon = 0.2352227842956722\nEpsilon = 0.23519926201724264\nEpsilon = 0.23517574209104092\nEpsilon = 0.23515222451683182\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.23512870929438015\nEpsilon = 0.2351051964234507\nEpsilon = 0.23508168590380837\nEpsilon = 0.235058177735218\nAgent: ddqn_agent . Episode 792/2000. Number of steps to finish: 20. Loss: 14.924283981323242 Reward: -18.0\nEpsilon = 0.23503467191744448\nEpsilon = 0.23501116845025274\nEpsilon = 0.23498766733340773\nEpsilon = 0.2349641685666744\nEpsilon = 0.23494067214981773\nEpsilon = 0.23491717808260276\nEpsilon = 0.23489368636479452\nEpsilon = 0.23487019699615805\nEpsilon = 0.23484670997645843\nEpsilon = 0.2348232253054608\nEpsilon = 0.23479974298293024\nEpsilon = 0.23477626300863194\nEpsilon = 0.23475278538233108\nEpsilon = 0.23472931010379283\nEpsilon = 0.23470583717278246\nEpsilon = 0.2346823665890652\nEpsilon = 0.23465889835240628\nEpsilon = 0.23463543246257104\nEpsilon = 0.2346119689193248\nEpsilon = 0.23458850772243287\nAgent: ddqn_agent . Episode 793/2000. Number of steps to finish: 20. Loss: 14.896804809570312 Reward: -10.0\nEpsilon = 0.23456504887166063\nEpsilon = 0.23454159236677347\nEpsilon = 0.23451813820753678\nEpsilon = 0.23449468639371604\nEpsilon = 0.23447123692507668\nEpsilon = 0.23444778980138417\nEpsilon = 0.23442434502240403\nEpsilon = 0.2344009025879018\nEpsilon = 0.23437746249764302\nEpsilon = 0.23435402475139325\nEpsilon = 0.23433058934891812\nEpsilon = 0.23430715628998322\nEpsilon = 0.23428372557435423\nEpsilon = 0.2342602972017968\nEpsilon = 0.23423687117207664\nEpsilon = 0.23421344748495943\nEpsilon = 0.23419002614021092\nEpsilon = 0.2341666071375969\nEpsilon = 0.23414319047688315\nEpsilon = 0.23411977615783547\nAgent: ddqn_agent . Episode 794/2000. Number of steps to finish: 20. Loss: 14.705072402954102 Reward: -14.0\nEpsilon = 0.23409636418021967\nEpsilon = 0.23407295454380164\nEpsilon = 0.23404954724834726\nEpsilon = 0.23402614229362242\nEpsilon = 0.23400273967939306\nEpsilon = 0.23397933940542512\nEpsilon = 0.23395594147148457\nEpsilon = 0.23393254587733742\nEpsilon = 0.2339091526227497\nEpsilon = 0.23388576170748743\nEpsilon = 0.2338623731313167\nEpsilon = 0.23383898689400356\nAgent: ddqn_agent . Episode 795/2000. Number of steps to finish: 12. Loss: 9.085099220275879 Reward: 0.0\nEpsilon = 0.23381560299531418\nEpsilon = 0.23379222143501466\nEpsilon = 0.23376884221287117\nEpsilon = 0.2337454653286499\nEpsilon = 0.23372209078211703\nEpsilon = 0.2336987185730388\nEpsilon = 0.2336753487011815\nEpsilon = 0.2336519811663114\nEpsilon = 0.23362861596819476\nEpsilon = 0.23360525310659794\nEpsilon = 0.23358189258128728\nEpsilon = 0.23355853439202914\nEpsilon = 0.23353517853858993\nEpsilon = 0.23351182502073609\nEpsilon = 0.233488473838234\nEpsilon = 0.2334651249908502\nEpsilon = 0.23344177847835113\nEpsilon = 0.2334184343005033\nEpsilon = 0.23339509245707324\nEpsilon = 0.23337175294782753\nAgent: ddqn_agent . Episode 796/2000. Number of steps to finish: 20. Loss: 14.78840446472168 Reward: -18.0\nEpsilon = 0.23334841577253274\nEpsilon = 0.2333250809309555\nEpsilon = 0.23330174842286241\nEpsilon = 0.23327841824802012\nEpsilon = 0.23325509040619533\nEpsilon = 0.23323176489715472\nEpsilon = 0.23320844172066502\nEpsilon = 0.23318512087649296\nEpsilon = 0.2331618023644053\nEpsilon = 0.23313848618416888\nEpsilon = 0.23311517233555046\nEpsilon = 0.2330918608183169\nEpsilon = 0.23306855163223508\nEpsilon = 0.23304524477707186\nEpsilon = 0.23302194025259415\nEpsilon = 0.2329986380585689\nEpsilon = 0.23297533819476304\nEpsilon = 0.23295204066094358\nEpsilon = 0.23292874545687747\nEpsilon = 0.23290545258233178\nAgent: ddqn_agent . Episode 797/2000. Number of steps to finish: 20. Loss: 14.60755729675293 Reward: -12.0\nEpsilon = 0.23288216203707354\nEpsilon = 0.23285887382086984\nEpsilon = 0.23283558793348774\nEpsilon = 0.23281230437469438\nEpsilon = 0.23278902314425692\nEpsilon = 0.2327657442419425\nEpsilon = 0.2327424676675183\nEpsilon = 0.23271919342075156\nEpsilon = 0.2326959215014095\nEpsilon = 0.23267265190925937\nEpsilon = 0.23264938464406845\nEpsilon = 0.23262611970560404\nEpsilon = 0.23260285709363349\nEpsilon = 0.23257959680792412\nEpsilon = 0.23255633884824334\nEpsilon = 0.2325330832143585\nEpsilon = 0.23250982990603708\nEpsilon = 0.23248657892304647\nEpsilon = 0.23246333026515417\nEpsilon = 0.23244008393212767\nAgent: ddqn_agent . Episode 798/2000. Number of steps to finish: 20. Loss: 14.737581253051758 Reward: -14.0\nEpsilon = 0.23241683992373446\nEpsilon = 0.2323935982397421\nEpsilon = 0.2323703588799181\nEpsilon = 0.2323471218440301\nEpsilon = 0.2323238871318457\nEpsilon = 0.23230065474313252\nEpsilon = 0.23227742467765822\nEpsilon = 0.23225419693519045\nEpsilon = 0.23223097151549693\nEpsilon = 0.23220774841834538\nEpsilon = 0.23218452764350356\nEpsilon = 0.2321613091907392\nEpsilon = 0.23213809305982014\nEpsilon = 0.23211487925051416\nEpsilon = 0.2320916677625891\nEpsilon = 0.23206845859581285\nEpsilon = 0.23204525174995327\nEpsilon = 0.2320220472247783\nEpsilon = 0.2319988450200558\nEpsilon = 0.2319756451355538\nAgent: ddqn_agent . Episode 799/2000. Number of steps to finish: 20. Loss: 14.888500213623047 Reward: -12.0\nEpsilon = 0.23195244757104025\nEpsilon = 0.23192925232628314\nEpsilon = 0.23190605940105052\nEpsilon = 0.23188286879511041\nEpsilon = 0.23185968050823091\nEpsilon = 0.2318364945401801\nEpsilon = 0.23181331089072607\nEpsilon = 0.231790129559637\nEpsilon = 0.23176695054668103\nEpsilon = 0.23174377385162637\nEpsilon = 0.2317205994742412\nEpsilon = 0.23169742741429378\nEpsilon = 0.23167425767155236\nEpsilon = 0.2316510902457852\nEpsilon = 0.23162792513676062\nEpsilon = 0.23160476234424693\nEpsilon = 0.2315816018680125\nEpsilon = 0.2315584437078257\nEpsilon = 0.2315352878634549\nEpsilon = 0.23151213433466855\nAgent: ddqn_agent . Episode 800/2000. Number of steps to finish: 20. Loss: 14.405248641967773 Reward: -20.0\nEpsilon = 0.2314889831212351\nEpsilon = 0.23146583422292297\nEpsilon = 0.23144268763950068\nEpsilon = 0.23141954337073672\nEpsilon = 0.23139640141639967\nEpsilon = 0.23137326177625803\nEpsilon = 0.2313501244500804\nAgent: ddqn_agent . Episode 801/2000. Number of steps to finish: 7. Loss: 5.562626361846924 Reward: 5.0\nEpsilon = 0.2313269894376354\nEpsilon = 0.23130385673869164\nEpsilon = 0.23128072635301777\nEpsilon = 0.23125759828038248\nEpsilon = 0.23123447252055443\nEpsilon = 0.23121134907330237\nEpsilon = 0.23118822793839505\nEpsilon = 0.2311651091156012\nEpsilon = 0.23114199260468965\nEpsilon = 0.2311188784054292\nEpsilon = 0.23109576651758865\nEpsilon = 0.2310726569409369\nEpsilon = 0.2310495496752428\nEpsilon = 0.23102644472027528\nEpsilon = 0.23100334207580325\nEpsilon = 0.23098024174159568\nEpsilon = 0.23095714371742151\nEpsilon = 0.23093404800304979\nEpsilon = 0.23091095459824948\nEpsilon = 0.23088786350278967\nAgent: ddqn_agent . Episode 802/2000. Number of steps to finish: 20. Loss: 14.353181838989258 Reward: -20.0\nEpsilon = 0.2308647747164394\nEpsilon = 0.23084168823896775\nEpsilon = 0.23081860407014385\nEpsilon = 0.23079552220973684\nEpsilon = 0.23077244265751587\nEpsilon = 0.2307493654132501\nEpsilon = 0.23072629047670878\nEpsilon = 0.23070321784766112\nAgent: ddqn_agent . Episode 803/2000. Number of steps to finish: 8. Loss: 5.763296604156494 Reward: 4.0\nEpsilon = 0.23068014752587637\nEpsilon = 0.2306570795111238\nEpsilon = 0.2306340138031727\nEpsilon = 0.2306109504017924\nEpsilon = 0.2305878893067522\nEpsilon = 0.23056483051782153\nEpsilon = 0.23054177403476975\nEpsilon = 0.23051871985736627\nEpsilon = 0.23049566798538054\nEpsilon = 0.23047261841858202\nEpsilon = 0.23044957115674017\nEpsilon = 0.2304265261996245\nEpsilon = 0.23040348354700455\nEpsilon = 0.23038044319864984\nEpsilon = 0.23035740515432998\nEpsilon = 0.23033436941381455\nEpsilon = 0.23031133597687317\nAgent: ddqn_agent . Episode 804/2000. Number of steps to finish: 17. Loss: 12.587181091308594 Reward: -5.0\nEpsilon = 0.23028830484327548\nEpsilon = 0.23026527601279115\nEpsilon = 0.23024224948518987\nEpsilon = 0.23021922526024136\nEpsilon = 0.23019620333771534\nEpsilon = 0.23017318371738157\nEpsilon = 0.23015016639900984\nEpsilon = 0.23012715138236994\nEpsilon = 0.2301041386672317\nEpsilon = 0.230081128253365\nEpsilon = 0.23005812014053967\nEpsilon = 0.2300351143285256\nEpsilon = 0.23001211081709275\nEpsilon = 0.22998910960601104\nEpsilon = 0.22996611069505044\nEpsilon = 0.22994311408398094\nEpsilon = 0.22992011977257254\nEpsilon = 0.2298971277605953\nEpsilon = 0.22987413804781923\nEpsilon = 0.22985115063401446\nAgent: ddqn_agent . Episode 805/2000. Number of steps to finish: 20. Loss: 15.364909172058105 Reward: -20.0\nEpsilon = 0.22982816551895105\nEpsilon = 0.22980518270239916\nEpsilon = 0.22978220218412893\nEpsilon = 0.22975922396391052\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.22973624804151413\nEpsilon = 0.22971327441670997\nEpsilon = 0.2296903030892683\nEpsilon = 0.22966733405895937\nEpsilon = 0.22964436732555346\nEpsilon = 0.2296214028888209\nEpsilon = 0.22959844074853203\nEpsilon = 0.22957548090445717\nEpsilon = 0.22955252335636672\nEpsilon = 0.22952956810403108\nEpsilon = 0.22950661514722068\nEpsilon = 0.22948366448570595\nEpsilon = 0.2294607161192574\nEpsilon = 0.22943777004764546\nEpsilon = 0.2294148262706407\nEpsilon = 0.22939188478801364\nAgent: ddqn_agent . Episode 806/2000. Number of steps to finish: 20. Loss: 15.06346321105957 Reward: -20.0\nEpsilon = 0.22936894559953486\nEpsilon = 0.2293460087049749\nEpsilon = 0.2293230741041044\nEpsilon = 0.229300141796694\nEpsilon = 0.22927721178251434\nEpsilon = 0.22925428406133608\nEpsilon = 0.22923135863292995\nEpsilon = 0.22920843549706665\nEpsilon = 0.22918551465351694\nEpsilon = 0.22916259610205159\nEpsilon = 0.22913967984244138\nEpsilon = 0.22911676587445715\nEpsilon = 0.2290938541978697\nEpsilon = 0.2290709448124499\nEpsilon = 0.22904803771796867\nEpsilon = 0.22902513291419688\nEpsilon = 0.22900223040090545\nEpsilon = 0.22897933017786537\nEpsilon = 0.22895643224484757\nEpsilon = 0.2289335366016231\nAgent: ddqn_agent . Episode 807/2000. Number of steps to finish: 20. Loss: 14.297921180725098 Reward: -20.0\nEpsilon = 0.22891064324796295\nEpsilon = 0.22888775218363816\nEpsilon = 0.2288648634084198\nEpsilon = 0.22884197692207897\nEpsilon = 0.22881909272438677\nEpsilon = 0.22879621081511434\nEpsilon = 0.22877333119403284\nEpsilon = 0.22875045386091344\nEpsilon = 0.22872757881552735\nEpsilon = 0.2287047060576458\nEpsilon = 0.22868183558704006\nEpsilon = 0.22865896740348135\nEpsilon = 0.228636101506741\nEpsilon = 0.22861323789659033\nEpsilon = 0.22859037657280068\nEpsilon = 0.2285675175351434\nEpsilon = 0.2285446607833899\nEpsilon = 0.22852180631731156\nEpsilon = 0.22849895413667984\nEpsilon = 0.22847610424126616\nAgent: ddqn_agent . Episode 808/2000. Number of steps to finish: 20. Loss: 14.613594055175781 Reward: -12.0\nEpsilon = 0.22845325663084204\nEpsilon = 0.22843041130517897\nEpsilon = 0.22840756826404846\nEpsilon = 0.22838472750722205\nEpsilon = 0.22836188903447133\nEpsilon = 0.22833905284556788\nEpsilon = 0.22831621894028334\nEpsilon = 0.2282933873183893\nEpsilon = 0.22827055797965745\nEpsilon = 0.2282477309238595\nEpsilon = 0.22822490615076713\nEpsilon = 0.22820208366015204\nEpsilon = 0.22817926345178602\nEpsilon = 0.22815644552544084\nEpsilon = 0.2281336298808883\nEpsilon = 0.22811081651790022\nEpsilon = 0.22808800543624844\nEpsilon = 0.22806519663570482\nEpsilon = 0.22804239011604124\nEpsilon = 0.22801958587702964\nAgent: ddqn_agent . Episode 809/2000. Number of steps to finish: 20. Loss: 14.611077308654785 Reward: -10.0\nEpsilon = 0.22799678391844194\nEpsilon = 0.2279739842400501\nEpsilon = 0.2279511868416261\nEpsilon = 0.22792839172294194\nEpsilon = 0.22790559888376966\nEpsilon = 0.22788280832388128\nEpsilon = 0.2278600200430489\nEpsilon = 0.22783723404104458\nEpsilon = 0.22781445031764047\nEpsilon = 0.2277916688726087\nEpsilon = 0.22776888970572146\nEpsilon = 0.2277461128167509\nEpsilon = 0.22772333820546922\nEpsilon = 0.2277005658716487\nEpsilon = 0.22767779581506153\nEpsilon = 0.22765502803548002\nEpsilon = 0.22763226253267646\nEpsilon = 0.22760949930642319\nEpsilon = 0.22758673835649254\nEpsilon = 0.2275639796826569\nAgent: ddqn_agent . Episode 810/2000. Number of steps to finish: 20. Loss: 14.402444839477539 Reward: -20.0\nEpsilon = 0.22754122328468865\nEpsilon = 0.22751846916236018\nEpsilon = 0.22749571731544393\nEpsilon = 0.2274729677437124\nEpsilon = 0.22745022044693802\nEpsilon = 0.22742747542489333\nEpsilon = 0.22740473267735084\nEpsilon = 0.22738199220408312\nEpsilon = 0.2273592540048627\nEpsilon = 0.22733651807946223\nEpsilon = 0.2273137844276543\nEpsilon = 0.22729105304921152\nEpsilon = 0.22726832394390659\nEpsilon = 0.2272455971115122\nEpsilon = 0.22722287255180107\nEpsilon = 0.2272001502645459\nEpsilon = 0.22717743024951945\nEpsilon = 0.2271547125064945\nEpsilon = 0.22713199703524384\nEpsilon = 0.22710928383554033\nAgent: ddqn_agent . Episode 811/2000. Number of steps to finish: 20. Loss: 14.87297534942627 Reward: -14.0\nEpsilon = 0.22708657290715678\nEpsilon = 0.22706386424986608\nEpsilon = 0.2270411578634411\nEpsilon = 0.22701845374765475\nEpsilon = 0.22699575190228\nEpsilon = 0.22697305232708978\nEpsilon = 0.22695035502185706\nEpsilon = 0.2269276599863549\nEpsilon = 0.22690496722035625\nEpsilon = 0.2268822767236342\nEpsilon = 0.22685958849596183\nEpsilon = 0.22683690253711222\nEpsilon = 0.2268142188468585\nEpsilon = 0.22679153742497382\nEpsilon = 0.22676885827123133\nEpsilon = 0.2267461813854042\nEpsilon = 0.22672350676726566\nEpsilon = 0.22670083441658895\nEpsilon = 0.2266781643331473\nEpsilon = 0.226655496516714\nAgent: ddqn_agent . Episode 812/2000. Number of steps to finish: 20. Loss: 15.048213005065918 Reward: -14.0\nEpsilon = 0.22663283096706233\nEpsilon = 0.22661016768396564\nEpsilon = 0.22658750666719724\nEpsilon = 0.22656484791653053\nEpsilon = 0.22654219143173887\nEpsilon = 0.2265195372125957\nEpsilon = 0.22649688525887446\nEpsilon = 0.22647423557034857\nEpsilon = 0.22645158814679153\nEpsilon = 0.22642894298797686\nEpsilon = 0.22640630009367807\nEpsilon = 0.2263836594636687\nEpsilon = 0.22636102109772233\nEpsilon = 0.22633838499561257\nEpsilon = 0.226315751157113\nEpsilon = 0.2262931195819973\nEpsilon = 0.2262704902700391\nEpsilon = 0.2262478632210121\nEpsilon = 0.22622523843469\nEpsilon = 0.22620261591084653\nAgent: ddqn_agent . Episode 813/2000. Number of steps to finish: 20. Loss: 14.54500675201416 Reward: -10.0\nEpsilon = 0.22617999564925545\nEpsilon = 0.22615737764969052\nEpsilon = 0.22613476191192555\nEpsilon = 0.22611214843573435\nEpsilon = 0.22608953722089078\nEpsilon = 0.2260669282671687\nEpsilon = 0.22604432157434198\nEpsilon = 0.22602171714218455\nEpsilon = 0.22599911497047034\nEpsilon = 0.2259765150589733\nEpsilon = 0.2259539174074674\nEpsilon = 0.22593132201572666\nEpsilon = 0.2259087288835251\nEpsilon = 0.22588613801063673\nEpsilon = 0.22586354939683567\nEpsilon = 0.225840963041896\nEpsilon = 0.2258183789455918\nEpsilon = 0.22579579710769726\nEpsilon = 0.2257732175279865\nEpsilon = 0.2257506402062337\nAgent: ddqn_agent . Episode 814/2000. Number of steps to finish: 20. Loss: 14.549854278564453 Reward: -10.0\nEpsilon = 0.22572806514221308\nEpsilon = 0.22570549233569887\nEpsilon = 0.2256829217864653\nEpsilon = 0.22566035349428665\nEpsilon = 0.22563778745893723\nEpsilon = 0.22561522368019132\nEpsilon = 0.2255926621578233\nEpsilon = 0.22557010289160753\nEpsilon = 0.22554754588131837\nEpsilon = 0.22552499112673025\nEpsilon = 0.22550243862761757\nEpsilon = 0.22547988838375482\nEpsilon = 0.22545734039491644\nEpsilon = 0.22543479466087696\nEpsilon = 0.22541225118141087\nEpsilon = 0.22538970995629273\nEpsilon = 0.2253671709852971\nEpsilon = 0.22534463426819856\nEpsilon = 0.22532209980477175\nEpsilon = 0.22529956759479128\nAgent: ddqn_agent . Episode 815/2000. Number of steps to finish: 20. Loss: 14.981545448303223 Reward: -12.0\nEpsilon = 0.2252770376380318\nEpsilon = 0.22525450993426802\nEpsilon = 0.22523198448327458\nEpsilon = 0.22520946128482625\nEpsilon = 0.22518694033869777\nEpsilon = 0.2251644216446639\nEpsilon = 0.22514190520249944\nEpsilon = 0.2251193910119792\nEpsilon = 0.225096879072878\nEpsilon = 0.22507436938497072\nEpsilon = 0.22505186194803223\nEpsilon = 0.22502935676183744\nEpsilon = 0.22500685382616126\nEpsilon = 0.22498435314077864\nEpsilon = 0.22496185470546456\nEpsilon = 0.22493935851999403\nEpsilon = 0.22491686458414203\nEpsilon = 0.22489437289768363\nEpsilon = 0.22487188346039386\nEpsilon = 0.2248493962720478\nAgent: ddqn_agent . Episode 816/2000. Number of steps to finish: 20. Loss: 14.271058082580566 Reward: -12.0\nEpsilon = 0.2248269113324206\nEpsilon = 0.22480442864128736\nEpsilon = 0.22478194819842323\nEpsilon = 0.22475947000360338\nEpsilon = 0.22473699405660302\nEpsilon = 0.22471452035719736\nEpsilon = 0.22469204890516165\nEpsilon = 0.22466957970027113\nEpsilon = 0.2246471127423011\nEpsilon = 0.22462464803102686\nEpsilon = 0.22460218556622374\nEpsilon = 0.22457972534766713\nEpsilon = 0.22455726737513237\nEpsilon = 0.22453481164839487\nEpsilon = 0.22451235816723003\nEpsilon = 0.2244899069314133\nEpsilon = 0.22446745794072015\nEpsilon = 0.22444501119492608\nEpsilon = 0.22442256669380659\nEpsilon = 0.2244001244371372\nAgent: ddqn_agent . Episode 817/2000. Number of steps to finish: 20. Loss: 15.255780220031738 Reward: -18.0\nEpsilon = 0.2243776844246935\nEpsilon = 0.22435524665625103\nEpsilon = 0.2243328111315854\nEpsilon = 0.22431037785047225\nEpsilon = 0.2242879468126872\nEpsilon = 0.22426551801800595\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.22424309146620416\nEpsilon = 0.22422066715705755\nEpsilon = 0.22419824509034184\nEpsilon = 0.2241758252658328\nEpsilon = 0.22415340768330622\nEpsilon = 0.2241309923425379\nEpsilon = 0.22410857924330363\nEpsilon = 0.22408616838537931\nEpsilon = 0.2240637597685408\nAgent: ddqn_agent . Episode 818/2000. Number of steps to finish: 15. Loss: 10.68001937866211 Reward: -3.0\nEpsilon = 0.22404135339256392\nEpsilon = 0.22401894925722468\nEpsilon = 0.22399654736229896\nEpsilon = 0.22397414770756274\nEpsilon = 0.223951750292792\nEpsilon = 0.2239293551177627\nEpsilon = 0.22390696218225095\nEpsilon = 0.22388457148603272\nEpsilon = 0.2238621830288841\nEpsilon = 0.22383979681058122\nEpsilon = 0.22381741283090018\nEpsilon = 0.2237950310896171\nEpsilon = 0.22377265158650814\nEpsilon = 0.2237502743213495\nEpsilon = 0.22372789929391737\nEpsilon = 0.22370552650398798\nEpsilon = 0.2236831559513376\nEpsilon = 0.22366078763574246\nEpsilon = 0.22363842155697888\nEpsilon = 0.22361605771482318\nAgent: ddqn_agent . Episode 819/2000. Number of steps to finish: 20. Loss: 14.330621719360352 Reward: -12.0\nEpsilon = 0.2235936961090517\nEpsilon = 0.2235713367394408\nEpsilon = 0.22354897960576686\nEpsilon = 0.2235266247078063\nEpsilon = 0.22350427204533552\nEpsilon = 0.223481921618131\nEpsilon = 0.22345957342596917\nEpsilon = 0.22343722746862657\nEpsilon = 0.2234148837458797\nEpsilon = 0.22339254225750513\nEpsilon = 0.22337020300327937\nEpsilon = 0.22334786598297904\nEpsilon = 0.22332553119638074\nEpsilon = 0.2233031986432611\nEpsilon = 0.22328086832339678\nEpsilon = 0.22325854023656444\nEpsilon = 0.2232362143825408\nEpsilon = 0.22321389076110254\nEpsilon = 0.22319156937202644\nEpsilon = 0.22316925021508924\nAgent: ddqn_agent . Episode 820/2000. Number of steps to finish: 20. Loss: 14.504268646240234 Reward: -14.0\nEpsilon = 0.22314693329006774\nEpsilon = 0.22312461859673874\nEpsilon = 0.22310230613487908\nEpsilon = 0.22307999590426558\nEpsilon = 0.22305768790467514\nEpsilon = 0.22303538213588467\nEpsilon = 0.22301307859767108\nEpsilon = 0.22299077728981131\nEpsilon = 0.22296847821208235\nEpsilon = 0.22294618136426114\nEpsilon = 0.2229238867461247\nEpsilon = 0.2229015943574501\nEpsilon = 0.22287930419801436\nEpsilon = 0.22285701626759455\nEpsilon = 0.22283473056596778\nEpsilon = 0.22281244709291118\nEpsilon = 0.22279016584820188\nEpsilon = 0.22276788683161705\nEpsilon = 0.2227456100429339\nEpsilon = 0.2227233354819296\nAgent: ddqn_agent . Episode 821/2000. Number of steps to finish: 20. Loss: 14.572094917297363 Reward: -16.0\nEpsilon = 0.22270106314838142\nEpsilon = 0.2226787930420666\nEpsilon = 0.22265652516276238\nEpsilon = 0.2226342595102461\nEpsilon = 0.22261199608429508\nEpsilon = 0.22258973488468664\nEpsilon = 0.2225674759111982\nEpsilon = 0.22254521916360706\nEpsilon = 0.2225229646416907\nEpsilon = 0.22250071234522653\nEpsilon = 0.222478462273992\nEpsilon = 0.22245621442776461\nEpsilon = 0.22243396880632185\nEpsilon = 0.22241172540944124\nEpsilon = 0.2223894842369003\nEpsilon = 0.22236724528847662\nEpsilon = 0.2223450085639478\nEpsilon = 0.2223227740630914\nEpsilon = 0.2223005417856851\nEpsilon = 0.22227831173150653\nAgent: ddqn_agent . Episode 822/2000. Number of steps to finish: 20. Loss: 14.113119125366211 Reward: -14.0\nEpsilon = 0.22225608390033338\nEpsilon = 0.22223385829194336\nEpsilon = 0.22221163490611417\nEpsilon = 0.22218941374262358\nEpsilon = 0.22216719480124933\nEpsilon = 0.2221449780817692\nEpsilon = 0.22212276358396102\nEpsilon = 0.22210055130760262\nEpsilon = 0.22207834125247186\nEpsilon = 0.22205613341834662\nEpsilon = 0.2220339278050048\nEpsilon = 0.2220117244122243\nEpsilon = 0.22198952323978308\nEpsilon = 0.2219673242874591\nEpsilon = 0.22194512755503037\nEpsilon = 0.22192293304227487\nEpsilon = 0.22190074074897065\nEpsilon = 0.22187855067489576\nEpsilon = 0.22185636281982826\nEpsilon = 0.22183417718354628\nAgent: ddqn_agent . Episode 823/2000. Number of steps to finish: 20. Loss: 14.576448440551758 Reward: -14.0\nEpsilon = 0.22181199376582794\nEpsilon = 0.22178981256645136\nEpsilon = 0.22176763358519472\nEpsilon = 0.2217454568218362\nEpsilon = 0.221723282276154\nEpsilon = 0.22170110994792638\nEpsilon = 0.2216789398369316\nEpsilon = 0.22165677194294792\nEpsilon = 0.22163460626575363\nEpsilon = 0.22161244280512704\nEpsilon = 0.22159028156084654\nEpsilon = 0.22156812253269045\nEpsilon = 0.2215459657204372\nEpsilon = 0.22152381112386516\nEpsilon = 0.22150165874275277\nEpsilon = 0.22147950857687848\nEpsilon = 0.22145736062602078\nEpsilon = 0.22143521488995818\nEpsilon = 0.2214130713684692\nEpsilon = 0.22139093006133234\nAgent: ddqn_agent . Episode 824/2000. Number of steps to finish: 20. Loss: 14.5297212600708 Reward: -10.0\nEpsilon = 0.2213687909683262\nEpsilon = 0.22134665408922938\nEpsilon = 0.22132451942382045\nEpsilon = 0.22130238697187807\nEpsilon = 0.22128025673318089\nEpsilon = 0.22125812870750758\nEpsilon = 0.22123600289463682\nEpsilon = 0.22121387929434735\nEpsilon = 0.22119175790641793\nEpsilon = 0.22116963873062728\nEpsilon = 0.22114752176675423\nEpsilon = 0.22112540701457756\nEpsilon = 0.2211032944738761\nEpsilon = 0.22108118414442873\nEpsilon = 0.2210590760260143\nEpsilon = 0.2210369701184117\nEpsilon = 0.22101486642139986\nEpsilon = 0.22099276493475772\nEpsilon = 0.22097066565826426\nEpsilon = 0.22094856859169842\nAgent: ddqn_agent . Episode 825/2000. Number of steps to finish: 20. Loss: 15.19952392578125 Reward: -18.0\nEpsilon = 0.22092647373483926\nEpsilon = 0.22090438108746577\nEpsilon = 0.22088229064935702\nEpsilon = 0.22086020242029208\nEpsilon = 0.22083811640005005\nEpsilon = 0.22081603258841004\nEpsilon = 0.2207939509851512\nEpsilon = 0.22077187159005268\nEpsilon = 0.2207497944028937\nEpsilon = 0.2207277194234534\nEpsilon = 0.22070564665151104\nEpsilon = 0.22068357608684588\nEpsilon = 0.2206615077292372\nEpsilon = 0.2206394415784643\nEpsilon = 0.22061737763430644\nEpsilon = 0.22059531589654302\nEpsilon = 0.22057325636495337\nEpsilon = 0.2205511990393169\nEpsilon = 0.22052914391941295\nEpsilon = 0.220507091005021\nAgent: ddqn_agent . Episode 826/2000. Number of steps to finish: 20. Loss: 14.955294609069824 Reward: -12.0\nEpsilon = 0.2204850402959205\nEpsilon = 0.2204629917918909\nEpsilon = 0.22044094549271173\nEpsilon = 0.22041890139816248\nEpsilon = 0.22039685950802265\nEpsilon = 0.22037481982207185\nEpsilon = 0.22035278234008965\nEpsilon = 0.22033074706185565\nEpsilon = 0.22030871398714946\nEpsilon = 0.22028668311575075\nEpsilon = 0.22026465444743917\nEpsilon = 0.22024262798199443\nEpsilon = 0.22022060371919622\nEpsilon = 0.2201985816588243\nEpsilon = 0.22017656180065842\nEpsilon = 0.22015454414447835\nEpsilon = 0.22013252869006392\nEpsilon = 0.22011051543719493\nEpsilon = 0.2200885043856512\nEpsilon = 0.22006649553521263\nAgent: ddqn_agent . Episode 827/2000. Number of steps to finish: 20. Loss: 14.912252426147461 Reward: -18.0\nEpsilon = 0.2200444888856591\nEpsilon = 0.22002248443677055\nEpsilon = 0.22000048218832688\nEpsilon = 0.21997848214010804\nEpsilon = 0.21995648429189404\nEpsilon = 0.21993448864346485\nEpsilon = 0.21991249519460052\nEpsilon = 0.21989050394508106\nEpsilon = 0.21986851489468656\nEpsilon = 0.2198465280431971\nEpsilon = 0.21982454339039278\nEpsilon = 0.21980256093605374\nEpsilon = 0.21978058067996015\nEpsilon = 0.21975860262189215\nEpsilon = 0.21973662676162997\nEpsilon = 0.2197146530989538\nEpsilon = 0.21969268163364392\nEpsilon = 0.21967071236548055\nEpsilon = 0.219648745294244\nEpsilon = 0.21962678041971456\nAgent: ddqn_agent . Episode 828/2000. Number of steps to finish: 20. Loss: 14.746814727783203 Reward: -14.0\nEpsilon = 0.2196048177416726\nEpsilon = 0.21958285725989843\nEpsilon = 0.21956089897417244\nEpsilon = 0.21953894288427503\nEpsilon = 0.21951698898998662\nEpsilon = 0.21949503729108763\nEpsilon = 0.21947308778735852\nEpsilon = 0.21945114047857978\nEpsilon = 0.21942919536453193\nEpsilon = 0.21940725244499548\nEpsilon = 0.21938531171975098\nEpsilon = 0.21936337318857901\nEpsilon = 0.21934143685126015\nEpsilon = 0.21931950270757503\nEpsilon = 0.21929757075730427\nEpsilon = 0.21927564100022853\nEpsilon = 0.2192537134361285\nEpsilon = 0.2192317880647849\nEpsilon = 0.21920986488597843\nEpsilon = 0.21918794389948984\nAgent: ddqn_agent . Episode 829/2000. Number of steps to finish: 20. Loss: 14.870402336120605 Reward: -10.0\nEpsilon = 0.2191660251050999\nEpsilon = 0.2191441085025894\nEpsilon = 0.21912219409173914\nEpsilon = 0.21910028187232997\nEpsilon = 0.21907837184414272\nEpsilon = 0.2190564640069583\nEpsilon = 0.2190345583605576\nEpsilon = 0.21901265490472155\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.21899075363923107\nEpsilon = 0.21896885456386714\nEpsilon = 0.21894695767841074\nEpsilon = 0.21892506298264292\nEpsilon = 0.21890317047634467\nEpsilon = 0.21888128015929703\nEpsilon = 0.2188593920312811\nEpsilon = 0.21883750609207797\nEpsilon = 0.21881562234146876\nEpsilon = 0.2187937407792346\nEpsilon = 0.2187718614051567\nEpsilon = 0.21874998421901617\nAgent: ddqn_agent . Episode 830/2000. Number of steps to finish: 20. Loss: 15.180488586425781 Reward: -18.0\nEpsilon = 0.21872810922059427\nEpsilon = 0.2187062364096722\nEpsilon = 0.21868436578603126\nEpsilon = 0.21866249734945267\nEpsilon = 0.21864063109971774\nEpsilon = 0.21861876703660776\nEpsilon = 0.2185969051599041\nEpsilon = 0.2185750454693881\nEpsilon = 0.21855318796484116\nEpsilon = 0.21853133264604468\nEpsilon = 0.21850947951278007\nEpsilon = 0.2184876285648288\nEpsilon = 0.21846577980197232\nEpsilon = 0.21844393322399214\nEpsilon = 0.21842208883066974\nEpsilon = 0.21840024662178667\nEpsilon = 0.21837840659712449\nEpsilon = 0.21835656875646478\nEpsilon = 0.21833473309958915\nEpsilon = 0.2183128996262792\nAgent: ddqn_agent . Episode 831/2000. Number of steps to finish: 20. Loss: 13.98760986328125 Reward: -18.0\nEpsilon = 0.21829106833631656\nEpsilon = 0.21826923922948294\nEpsilon = 0.21824741230556\nEpsilon = 0.21822558756432944\nEpsilon = 0.218203765005573\nEpsilon = 0.21818194462907245\nEpsilon = 0.21816012643460955\nEpsilon = 0.2181383104219661\nEpsilon = 0.21811649659092391\nEpsilon = 0.21809468494126483\nEpsilon = 0.2180728754727707\nEpsilon = 0.21805106818522343\nEpsilon = 0.21802926307840492\nEpsilon = 0.21800746015209707\nEpsilon = 0.21798565940608186\nEpsilon = 0.21796386084014124\nEpsilon = 0.21794206445405723\nEpsilon = 0.21792027024761182\nEpsilon = 0.21789847822058706\nEpsilon = 0.217876688372765\nAgent: ddqn_agent . Episode 832/2000. Number of steps to finish: 20. Loss: 14.732291221618652 Reward: -18.0\nEpsilon = 0.2178549007039277\nEpsilon = 0.21783311521385731\nEpsilon = 0.21781133190233593\nEpsilon = 0.2177895507691457\nEpsilon = 0.21776777181406878\nEpsilon = 0.21774599503688738\nEpsilon = 0.21772422043738368\nEpsilon = 0.21770244801533994\nEpsilon = 0.2176806777705384\nEpsilon = 0.21765890970276136\nEpsilon = 0.21763714381179108\nEpsilon = 0.2176153800974099\nEpsilon = 0.21759361855940018\nEpsilon = 0.21757185919754424\nEpsilon = 0.21755010201162447\nEpsilon = 0.21752834700142332\nEpsilon = 0.2175065941667232\nEpsilon = 0.2174848435073065\nEpsilon = 0.21746309502295577\nEpsilon = 0.21744134871345347\nAgent: ddqn_agent . Episode 833/2000. Number of steps to finish: 20. Loss: 14.697123527526855 Reward: -14.0\nEpsilon = 0.21741960457858212\nEpsilon = 0.21739786261812427\nEpsilon = 0.21737612283186247\nEpsilon = 0.21735438521957928\nEpsilon = 0.21733264978105732\nEpsilon = 0.21731091651607923\nEpsilon = 0.21728918542442763\nEpsilon = 0.2172674565058852\nEpsilon = 0.2172457297602346\nEpsilon = 0.21722400518725857\nEpsilon = 0.21720228278673984\nEpsilon = 0.21718056255846116\nEpsilon = 0.21715884450220532\nEpsilon = 0.2171371286177551\nEpsilon = 0.21711541490489333\nEpsilon = 0.21709370336340283\nEpsilon = 0.2170719939930665\nEpsilon = 0.2170502867936672\nEpsilon = 0.21702858176498782\nEpsilon = 0.21700687890681133\nAgent: ddqn_agent . Episode 834/2000. Number of steps to finish: 20. Loss: 14.538056373596191 Reward: -12.0\nEpsilon = 0.21698517821892066\nEpsilon = 0.21696347970109878\nEpsilon = 0.21694178335312866\nEpsilon = 0.21692008917479336\nEpsilon = 0.2168983971658759\nEpsilon = 0.2168767073261593\nEpsilon = 0.2168550196554267\nEpsilon = 0.21683333415346115\nEpsilon = 0.2168116508200458\nEpsilon = 0.2167899696549638\nEpsilon = 0.2167682906579983\nEpsilon = 0.2167466138289325\nEpsilon = 0.21672493916754962\nEpsilon = 0.21670326667363288\nEpsilon = 0.21668159634696552\nEpsilon = 0.21665992818733082\nEpsilon = 0.2166382621945121\nEpsilon = 0.21661659836829264\nEpsilon = 0.2165949367084558\nEpsilon = 0.21657327721478498\nAgent: ddqn_agent . Episode 835/2000. Number of steps to finish: 20. Loss: 14.57471752166748 Reward: -18.0\nEpsilon = 0.2165516198870635\nEpsilon = 0.2165299647250748\nEpsilon = 0.2165083117286023\nEpsilon = 0.21648666089742943\nEpsilon = 0.21646501223133968\nEpsilon = 0.21644336573011655\nEpsilon = 0.21642172139354354\nEpsilon = 0.2164000792214042\nEpsilon = 0.21637843921348207\nEpsilon = 0.21635680136956073\nEpsilon = 0.21633516568942376\nEpsilon = 0.21631353217285482\nEpsilon = 0.21629190081963753\nEpsilon = 0.21627027162955556\nEpsilon = 0.2162486446023926\nEpsilon = 0.21622701973793237\nEpsilon = 0.2162053970359586\nEpsilon = 0.216183776496255\nEpsilon = 0.21616215811860537\nEpsilon = 0.2161405419027935\nAgent: ddqn_agent . Episode 836/2000. Number of steps to finish: 20. Loss: 14.749907493591309 Reward: -16.0\nEpsilon = 0.21611892784860323\nEpsilon = 0.21609731595581838\nEpsilon = 0.2160757062242228\nEpsilon = 0.2160540986536004\nEpsilon = 0.21603249324373505\nEpsilon = 0.21601088999441068\nEpsilon = 0.21598928890541125\nEpsilon = 0.21596768997652072\nEpsilon = 0.21594609320752306\nEpsilon = 0.2159244985982023\nEpsilon = 0.21590290614834248\nEpsilon = 0.21588131585772766\nEpsilon = 0.2158597277261419\nEpsilon = 0.2158381417533693\nEpsilon = 0.21581655793919396\nEpsilon = 0.21579497628340005\nEpsilon = 0.2157733967857717\nEpsilon = 0.21575181944609312\nEpsilon = 0.21573024426414852\nEpsilon = 0.21570867123972212\nAgent: ddqn_agent . Episode 837/2000. Number of steps to finish: 20. Loss: 14.359635353088379 Reward: -16.0\nEpsilon = 0.21568710037259814\nEpsilon = 0.2156655316625609\nEpsilon = 0.21564396510939463\nEpsilon = 0.2156224007128837\nEpsilon = 0.2156008384728124\nEpsilon = 0.21557927838896512\nEpsilon = 0.21555772046112623\nEpsilon = 0.21553616468908013\nEpsilon = 0.21551461107261122\nEpsilon = 0.21549305961150395\nEpsilon = 0.2154715103055428\nEpsilon = 0.21544996315451223\nEpsilon = 0.21542841815819677\nEpsilon = 0.21540687531638095\nEpsilon = 0.21538533462884932\nEpsilon = 0.21536379609538644\nEpsilon = 0.21534225971577692\nEpsilon = 0.21532072548980535\nEpsilon = 0.21529919341725637\nEpsilon = 0.21527766349791463\nAgent: ddqn_agent . Episode 838/2000. Number of steps to finish: 20. Loss: 14.752687454223633 Reward: -14.0\nEpsilon = 0.21525613573156485\nEpsilon = 0.2152346101179917\nEpsilon = 0.21521308665697989\nEpsilon = 0.21519156534831418\nEpsilon = 0.21517004619177935\nEpsilon = 0.21514852918716018\nEpsilon = 0.21512701433424147\nEpsilon = 0.21510550163280806\nEpsilon = 0.21508399108264478\nEpsilon = 0.21506248268353653\nEpsilon = 0.21504097643526818\nEpsilon = 0.21501947233762464\nEpsilon = 0.21499797039039087\nEpsilon = 0.21497647059335184\nEpsilon = 0.2149549729462925\nEpsilon = 0.21493347744899788\nEpsilon = 0.21491198410125298\nEpsilon = 0.21489049290284284\nEpsilon = 0.21486900385355256\nEpsilon = 0.2148475169531672\nAgent: ddqn_agent . Episode 839/2000. Number of steps to finish: 20. Loss: 14.780804634094238 Reward: -16.0\nEpsilon = 0.2148260322014719\nEpsilon = 0.21480454959825174\nEpsilon = 0.21478306914329193\nEpsilon = 0.2147615908363776\nEpsilon = 0.21474011467729398\nEpsilon = 0.21471864066582624\nEpsilon = 0.21469716880175965\nEpsilon = 0.2146756990848795\nEpsilon = 0.214654231514971\nEpsilon = 0.21463276609181953\nEpsilon = 0.21461130281521035\nEpsilon = 0.21458984168492884\nEpsilon = 0.21456838270076034\nEpsilon = 0.21454692586249027\nEpsilon = 0.21452547116990403\nEpsilon = 0.21450401862278703\nEpsilon = 0.21448256822092476\nEpsilon = 0.21446111996410266\nEpsilon = 0.21443967385210624\nEpsilon = 0.21441822988472103\nAgent: ddqn_agent . Episode 840/2000. Number of steps to finish: 20. Loss: 15.448394775390625 Reward: -12.0\nEpsilon = 0.21439678806173257\nEpsilon = 0.2143753483829264\nEpsilon = 0.2143539108480881\nEpsilon = 0.2143324754570033\nEpsilon = 0.2143110422094576\nEpsilon = 0.21428961110523664\nEpsilon = 0.21426818214412613\nEpsilon = 0.21424675532591173\nEpsilon = 0.21422533065037913\nEpsilon = 0.2142039081173141\nEpsilon = 0.21418248772650236\nEpsilon = 0.21416106947772973\nEpsilon = 0.21413965337078195\nEpsilon = 0.21411823940544486\nEpsilon = 0.2140968275815043\nEpsilon = 0.21407541789874615\nEpsilon = 0.2140540103569563\nEpsilon = 0.21403260495592058\nEpsilon = 0.21401120169542498\nEpsilon = 0.21398980057525543\nAgent: ddqn_agent . Episode 841/2000. Number of steps to finish: 20. Loss: 14.963886260986328 Reward: -16.0\nEpsilon = 0.21396840159519792\nEpsilon = 0.21394700475503842\nEpsilon = 0.21392561005456293\nEpsilon = 0.21390421749355748\nEpsilon = 0.21388282707180814\nEpsilon = 0.21386143878910097\nEpsilon = 0.21384005264522205\nEpsilon = 0.21381866863995752\nEpsilon = 0.21379728677309354\nEpsilon = 0.21377590704441624\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.2137545294537118\nEpsilon = 0.21373315400076642\nEpsilon = 0.21371178068536634\nEpsilon = 0.2136904095072978\nEpsilon = 0.21366904046634708\nEpsilon = 0.21364767356230044\nEpsilon = 0.2136263087949442\nEpsilon = 0.21360494616406472\nEpsilon = 0.21358358566944832\nEpsilon = 0.21356222731088137\nAgent: ddqn_agent . Episode 842/2000. Number of steps to finish: 20. Loss: 14.738118171691895 Reward: -18.0\nEpsilon = 0.21354087108815029\nEpsilon = 0.21351951700104146\nEpsilon = 0.21349816504934135\nEpsilon = 0.21347681523283643\nEpsilon = 0.21345546755131314\nEpsilon = 0.21343412200455802\nEpsilon = 0.21341277859235758\nEpsilon = 0.21339143731449833\nEpsilon = 0.21337009817076688\nEpsilon = 0.2133487611609498\nEpsilon = 0.21332742628483373\nEpsilon = 0.21330609354220526\nEpsilon = 0.21328476293285104\nEpsilon = 0.21326343445655777\nEpsilon = 0.21324210811311212\nEpsilon = 0.21322078390230081\nEpsilon = 0.21319946182391059\nEpsilon = 0.21317814187772818\nEpsilon = 0.21315682406354042\nEpsilon = 0.21313550838113407\nAgent: ddqn_agent . Episode 843/2000. Number of steps to finish: 20. Loss: 14.721988677978516 Reward: -16.0\nEpsilon = 0.21311419483029595\nEpsilon = 0.21309288341081292\nEpsilon = 0.21307157412247185\nEpsilon = 0.2130502669650596\nEpsilon = 0.2130289619383631\nEpsilon = 0.21300765904216926\nEpsilon = 0.21298635827626505\nEpsilon = 0.21296505964043744\nEpsilon = 0.21294376313447339\nEpsilon = 0.21292246875815993\nEpsilon = 0.21290117651128412\nEpsilon = 0.212879886393633\nEpsilon = 0.21285859840499363\nEpsilon = 0.21283731254515315\nEpsilon = 0.21281602881389863\nEpsilon = 0.21279474721101724\nEpsilon = 0.21277346773629613\nEpsilon = 0.2127521903895225\nEpsilon = 0.21273091517048356\nEpsilon = 0.21270964207896653\nAgent: ddqn_agent . Episode 844/2000. Number of steps to finish: 20. Loss: 15.217114448547363 Reward: -14.0\nEpsilon = 0.21268837111475863\nEpsilon = 0.21266710227764715\nEpsilon = 0.2126458355674194\nEpsilon = 0.21262457098386264\nEpsilon = 0.21260330852676426\nEpsilon = 0.2125820481959116\nEpsilon = 0.212560789991092\nEpsilon = 0.21253953391209288\nEpsilon = 0.21251827995870168\nEpsilon = 0.2124970281307058\nEpsilon = 0.21247577842789275\nEpsilon = 0.21245453085004998\nEpsilon = 0.212433285396965\nEpsilon = 0.2124120420684253\nEpsilon = 0.21239080086421844\nEpsilon = 0.21236956178413202\nEpsilon = 0.2123483248279536\nEpsilon = 0.2123270899954708\nEpsilon = 0.21230585728647128\nEpsilon = 0.21228462670074263\nAgent: ddqn_agent . Episode 845/2000. Number of steps to finish: 20. Loss: 14.211244583129883 Reward: -10.0\nEpsilon = 0.21226339823807255\nEpsilon = 0.21224217189824876\nEpsilon = 0.21222094768105892\nEpsilon = 0.21219972558629083\nEpsilon = 0.2121785056137322\nEpsilon = 0.21215728776317083\nEpsilon = 0.21213607203439452\nEpsilon = 0.21211485842719108\nEpsilon = 0.21209364694134836\nEpsilon = 0.21207243757665423\nEpsilon = 0.21205123033289655\nEpsilon = 0.21203002520986328\nEpsilon = 0.2120088222073423\nEpsilon = 0.21198762132512156\nEpsilon = 0.21196642256298906\nEpsilon = 0.21194522592073275\nEpsilon = 0.21192403139814067\nEpsilon = 0.21190283899500087\nEpsilon = 0.21188164871110138\nAgent: ddqn_agent . Episode 846/2000. Number of steps to finish: 19. Loss: 13.735084533691406 Reward: -7.0\nEpsilon = 0.21186046054623028\nEpsilon = 0.21183927450017567\nEpsilon = 0.21181809057272566\nEpsilon = 0.21179690876366838\nEpsilon = 0.21177572907279202\nEpsilon = 0.21175455149988473\nEpsilon = 0.21173337604473474\nEpsilon = 0.2117122027071303\nEpsilon = 0.21169103148685958\nEpsilon = 0.2116698623837109\nEpsilon = 0.21164869539747255\nEpsilon = 0.21162753052793282\nEpsilon = 0.21160636777488004\nEpsilon = 0.21158520713810255\nEpsilon = 0.21156404861738876\nEpsilon = 0.211542892212527\nEpsilon = 0.21152173792330575\nEpsilon = 0.21150058574951341\nEpsilon = 0.21147943569093847\nEpsilon = 0.21145828774736938\nAgent: ddqn_agent . Episode 847/2000. Number of steps to finish: 20. Loss: 14.42733383178711 Reward: -16.0\nEpsilon = 0.21143714191859464\nEpsilon = 0.2114159982044028\nEpsilon = 0.21139485660458235\nEpsilon = 0.2113737171189219\nEpsilon = 0.21135257974721\nEpsilon = 0.21133144448923527\nEpsilon = 0.21131031134478634\nEpsilon = 0.21128918031365188\nEpsilon = 0.21126805139562052\nEpsilon = 0.21124692459048097\nEpsilon = 0.21122579989802193\nEpsilon = 0.21120467731803214\nEpsilon = 0.21118355685030032\nEpsilon = 0.2111624384946153\nEpsilon = 0.21114132225076584\nEpsilon = 0.21112020811854076\nEpsilon = 0.21109909609772892\nEpsilon = 0.21107798618811915\nEpsilon = 0.21105687838950035\nEpsilon = 0.2110357727016614\nAgent: ddqn_agent . Episode 848/2000. Number of steps to finish: 20. Loss: 15.044124603271484 Reward: -16.0\nEpsilon = 0.21101466912439124\nEpsilon = 0.2109935676574788\nEpsilon = 0.21097246830071303\nEpsilon = 0.21095137105388295\nEpsilon = 0.21093027591677757\nEpsilon = 0.2109091828891859\nEpsilon = 0.21088809197089697\nEpsilon = 0.21086700316169987\nEpsilon = 0.2108459164613837\nEpsilon = 0.21082483186973758\nEpsilon = 0.2108037493865506\nEpsilon = 0.21078266901161197\nEpsilon = 0.21076159074471082\nEpsilon = 0.21074051458563636\nEpsilon = 0.2107194405341778\nEpsilon = 0.2106983685901244\nEpsilon = 0.21067729875326538\nAgent: ddqn_agent . Episode 849/2000. Number of steps to finish: 17. Loss: 12.69144344329834 Reward: -5.0\nEpsilon = 0.21065623102339007\nEpsilon = 0.21063516540028773\nEpsilon = 0.2106141018837477\nEpsilon = 0.21059304047355934\nEpsilon = 0.21057198116951198\nEpsilon = 0.21055092397139502\nEpsilon = 0.21052986887899788\nEpsilon = 0.21050881589210999\nEpsilon = 0.21048776501052077\nEpsilon = 0.21046671623401972\nEpsilon = 0.21044566956239633\nEpsilon = 0.2104246249954401\nEpsilon = 0.21040358253294056\nEpsilon = 0.21038254217468727\nEpsilon = 0.21036150392046982\nEpsilon = 0.21034046777007778\nEpsilon = 0.21031943372330078\nEpsilon = 0.21029840177992845\nEpsilon = 0.21027737193975046\nEpsilon = 0.21025634420255648\nAgent: ddqn_agent . Episode 850/2000. Number of steps to finish: 20. Loss: 14.795544624328613 Reward: -12.0\nEpsilon = 0.21023531856813624\nEpsilon = 0.21021429503627942\nEpsilon = 0.21019327360677578\nEpsilon = 0.2101722542794151\nEpsilon = 0.21015123705398717\nEpsilon = 0.21013022193028177\nEpsilon = 0.21010920890808873\nEpsilon = 0.21008819798719794\nEpsilon = 0.21006718916739922\nEpsilon = 0.21004618244848247\nEpsilon = 0.21002517783023764\nEpsilon = 0.2100041753124546\nEpsilon = 0.20998317489492335\nEpsilon = 0.20996217657743385\nEpsilon = 0.2099411803597761\nEpsilon = 0.20992018624174014\nEpsilon = 0.20989919422311595\nEpsilon = 0.20987820430369364\nEpsilon = 0.20985721648326328\nEpsilon = 0.20983623076161495\nAgent: ddqn_agent . Episode 851/2000. Number of steps to finish: 20. Loss: 14.316014289855957 Reward: -12.0\nEpsilon = 0.2098152471385388\nEpsilon = 0.20979426561382494\nEpsilon = 0.20977328618726357\nEpsilon = 0.20975230885864485\nEpsilon = 0.20973133362775898\nEpsilon = 0.2097103604943962\nEpsilon = 0.20968938945834675\nEpsilon = 0.20966842051940093\nEpsilon = 0.209647453677349\nEpsilon = 0.20962648893198127\nEpsilon = 0.20960552628308807\nEpsilon = 0.20958456573045975\nEpsilon = 0.2095636072738867\nEpsilon = 0.20954265091315932\nEpsilon = 0.209521696648068\nEpsilon = 0.2095007444784032\nEpsilon = 0.20947979440395537\nEpsilon = 0.20945884642451498\nEpsilon = 0.20943790053987255\nEpsilon = 0.20941695674981856\nAgent: ddqn_agent . Episode 852/2000. Number of steps to finish: 20. Loss: 15.0514497756958 Reward: -12.0\nEpsilon = 0.20939601505414357\nEpsilon = 0.20937507545263817\nEpsilon = 0.20935413794509292\nEpsilon = 0.20933320253129842\nEpsilon = 0.2093122692110453\nEpsilon = 0.2092913379841242\nEpsilon = 0.20927040885032577\nEpsilon = 0.20924948180944075\nEpsilon = 0.20922855686125982\nEpsilon = 0.2092076340055737\nEpsilon = 0.20918671324217317\nEpsilon = 0.20916579457084894\nEpsilon = 0.20914487799139186\nEpsilon = 0.20912396350359272\nEpsilon = 0.20910305110724237\nEpsilon = 0.20908214080213164\nEpsilon = 0.20906123258805143\nEpsilon = 0.20904032646479262\nEpsilon = 0.20901942243214613\nEpsilon = 0.20899852048990292\nAgent: ddqn_agent . Episode 853/2000. Number of steps to finish: 20. Loss: 15.125856399536133 Reward: -14.0\nEpsilon = 0.20897762063785394\nEpsilon = 0.20895672287579015\nEpsilon = 0.20893582720350257\nEpsilon = 0.20891493362078223\nEpsilon = 0.20889404212742016\nEpsilon = 0.20887315272320742\nEpsilon = 0.2088522654079351\nEpsilon = 0.20883138018139433\nEpsilon = 0.2088104970433762\nEpsilon = 0.20878961599367188\nEpsilon = 0.20876873703207252\nEpsilon = 0.20874786015836932\nEpsilon = 0.20872698537235349\nEpsilon = 0.20870611267381625\nEpsilon = 0.20868524206254888\nEpsilon = 0.20866437353834263\nEpsilon = 0.2086435071009888\nEpsilon = 0.2086226427502787\nEpsilon = 0.20860178048600367\nEpsilon = 0.20858092030795508\nAgent: ddqn_agent . Episode 854/2000. Number of steps to finish: 20. Loss: 14.993087768554688 Reward: -10.0\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.2085600622159243\nEpsilon = 0.2085392062097027\nEpsilon = 0.20851835228908175\nEpsilon = 0.20849750045385285\nEpsilon = 0.20847665070380747\nEpsilon = 0.2084558030387371\nEpsilon = 0.2084349574584332\nEpsilon = 0.20841411396268736\nEpsilon = 0.2083932725512911\nEpsilon = 0.20837243322403595\nEpsilon = 0.20835159598071354\nEpsilon = 0.20833076082111546\nEpsilon = 0.20830992774503335\nEpsilon = 0.20828909675225885\nEpsilon = 0.20826826784258362\nEpsilon = 0.20824744101579937\nEpsilon = 0.20822661627169778\nEpsilon = 0.2082057936100706\nEpsilon = 0.2081849730307096\nEpsilon = 0.20816415453340653\nAgent: ddqn_agent . Episode 855/2000. Number of steps to finish: 20. Loss: 14.75698184967041 Reward: -16.0\nEpsilon = 0.20814333811795319\nEpsilon = 0.2081225237841414\nEpsilon = 0.208101711531763\nEpsilon = 0.2080809013606098\nEpsilon = 0.20806009327047376\nEpsilon = 0.2080392872611467\nEpsilon = 0.2080184833324206\nEpsilon = 0.20799768148408734\nEpsilon = 0.20797688171593895\nEpsilon = 0.20795608402776736\nEpsilon = 0.2079352884193646\nEpsilon = 0.20791449489052266\nEpsilon = 0.2078937034410336\nEpsilon = 0.2078729140706895\nEpsilon = 0.20785212677928244\nEpsilon = 0.20783134156660452\nEpsilon = 0.20781055843244786\nEpsilon = 0.20778977737660462\nEpsilon = 0.20776899839886695\nEpsilon = 0.20774822149902705\nAgent: ddqn_agent . Episode 856/2000. Number of steps to finish: 20. Loss: 14.521641731262207 Reward: -16.0\nEpsilon = 0.20772744667687715\nEpsilon = 0.20770667393220946\nEpsilon = 0.20768590326481623\nEpsilon = 0.20766513467448974\nEpsilon = 0.2076443681610223\nEpsilon = 0.2076236037242062\nEpsilon = 0.2076028413638338\nEpsilon = 0.20758208107969742\nEpsilon = 0.20756132287158946\nEpsilon = 0.2075405667393023\nEpsilon = 0.20751981268262837\nEpsilon = 0.2074990607013601\nEpsilon = 0.20747831079528997\nEpsilon = 0.20745756296421045\nEpsilon = 0.20743681720791401\nEpsilon = 0.20741607352619323\nEpsilon = 0.20739533191884063\nEpsilon = 0.20737459238564876\nEpsilon = 0.2073538549264102\nEpsilon = 0.20733311954091757\nAgent: ddqn_agent . Episode 857/2000. Number of steps to finish: 20. Loss: 14.728114128112793 Reward: -18.0\nEpsilon = 0.20731238622896347\nEpsilon = 0.20729165499034058\nEpsilon = 0.20727092582484155\nEpsilon = 0.20725019873225906\nEpsilon = 0.20722947371238584\nEpsilon = 0.2072087507650146\nEpsilon = 0.20718802988993812\nEpsilon = 0.20716731108694914\nEpsilon = 0.20714659435584043\nEpsilon = 0.20712587969640484\nEpsilon = 0.20710516710843518\nEpsilon = 0.20708445659172434\nEpsilon = 0.20706374814606518\nEpsilon = 0.20704304177125057\nEpsilon = 0.20702233746707346\nEpsilon = 0.20700163523332676\nEpsilon = 0.20698093506980345\nEpsilon = 0.20696023697629648\nEpsilon = 0.20693954095259887\nEpsilon = 0.2069188469985036\nAgent: ddqn_agent . Episode 858/2000. Number of steps to finish: 20. Loss: 14.193942070007324 Reward: -12.0\nEpsilon = 0.20689815511380374\nEpsilon = 0.20687746529829237\nEpsilon = 0.20685677755176254\nEpsilon = 0.20683609187400737\nEpsilon = 0.20681540826481998\nEpsilon = 0.2067947267239935\nEpsilon = 0.2067740472513211\nEpsilon = 0.20675336984659595\nEpsilon = 0.2067326945096113\nEpsilon = 0.20671202124016033\nEpsilon = 0.20669135003803632\nEpsilon = 0.2066706809030325\nEpsilon = 0.2066500138349422\nEpsilon = 0.2066293488335587\nEpsilon = 0.20660868589867537\nEpsilon = 0.2065880250300855\nEpsilon = 0.20656736622758248\nEpsilon = 0.20654670949095974\nEpsilon = 0.20652605482001066\nEpsilon = 0.20650540221452865\nAgent: ddqn_agent . Episode 859/2000. Number of steps to finish: 20. Loss: 14.675625801086426 Reward: -16.0\nEpsilon = 0.2064847516743072\nEpsilon = 0.20646410319913977\nEpsilon = 0.20644345678881987\nEpsilon = 0.20642281244314098\nEpsilon = 0.20640217016189666\nEpsilon = 0.20638152994488046\nEpsilon = 0.20636089179188596\nEpsilon = 0.20634025570270678\nEpsilon = 0.20631962167713652\nEpsilon = 0.20629898971496882\nEpsilon = 0.20627835981599732\nEpsilon = 0.20625773198001573\nEpsilon = 0.20623710620681773\nEpsilon = 0.20621648249619706\nEpsilon = 0.20619586084794744\nEpsilon = 0.20617524126186265\nEpsilon = 0.20615462373773646\nEpsilon = 0.20613400827536268\nEpsilon = 0.20611339487453514\nEpsilon = 0.2060927835350477\nAgent: ddqn_agent . Episode 860/2000. Number of steps to finish: 20. Loss: 15.412206649780273 Reward: -16.0\nEpsilon = 0.2060721742566942\nEpsilon = 0.20605156703926852\nEpsilon = 0.2060309618825646\nEpsilon = 0.20601035878637633\nEpsilon = 0.2059897577504977\nEpsilon = 0.20596915877472263\nEpsilon = 0.20594856185884516\nEpsilon = 0.20592796700265928\nEpsilon = 0.205907374205959\nEpsilon = 0.20588678346853842\nEpsilon = 0.20586619479019158\nEpsilon = 0.20584560817071257\nEpsilon = 0.2058250236098955\nEpsilon = 0.2058044411075345\nEpsilon = 0.20578386066342377\nEpsilon = 0.20576328227735743\nEpsilon = 0.2057427059491297\nEpsilon = 0.2057221316785348\nEpsilon = 0.20570155946536695\nEpsilon = 0.20568098930942041\nAgent: ddqn_agent . Episode 861/2000. Number of steps to finish: 20. Loss: 15.569459915161133 Reward: -20.0\nEpsilon = 0.20566042121048947\nEpsilon = 0.2056398551683684\nEpsilon = 0.20561929118285158\nEpsilon = 0.2055987292537333\nEpsilon = 0.20557816938080792\nEpsilon = 0.20555761156386984\nEpsilon = 0.20553705580271345\nEpsilon = 0.20551650209713318\nEpsilon = 0.20549595044692348\nAgent: ddqn_agent . Episode 862/2000. Number of steps to finish: 9. Loss: 6.878945350646973 Reward: 3.0\nEpsilon = 0.2054754008518788\nEpsilon = 0.2054548533117936\nEpsilon = 0.20543430782646244\nEpsilon = 0.2054137643956798\nEpsilon = 0.20539322301924023\nEpsilon = 0.2053726836969383\nEpsilon = 0.2053521464285686\nEpsilon = 0.20533161121392576\nEpsilon = 0.20531107805280438\nEpsilon = 0.2052905469449991\nEpsilon = 0.2052700178903046\nEpsilon = 0.20524949088851557\nEpsilon = 0.20522896593942672\nEpsilon = 0.20520844304283278\nEpsilon = 0.2051879221985285\nEpsilon = 0.20516740340630865\nEpsilon = 0.20514688666596803\nEpsilon = 0.20512637197730144\nEpsilon = 0.2051058593401037\nEpsilon = 0.2050853487541697\nAgent: ddqn_agent . Episode 863/2000. Number of steps to finish: 20. Loss: 14.963964462280273 Reward: -12.0\nEpsilon = 0.20506484021929428\nEpsilon = 0.20504433373527234\nEpsilon = 0.2050238293018988\nEpsilon = 0.2050033269189686\nEpsilon = 0.2049828265862767\nEpsilon = 0.20496232830361807\nEpsilon = 0.20494183207078773\nEpsilon = 0.20492133788758066\nEpsilon = 0.2049008457537919\nEpsilon = 0.2048803556692165\nEpsilon = 0.20485986763364958\nEpsilon = 0.20483938164688623\nEpsilon = 0.20481889770872155\nEpsilon = 0.2047984158189507\nEpsilon = 0.2047779359773688\nEpsilon = 0.20475745818377106\nEpsilon = 0.20473698243795269\nEpsilon = 0.2047165087397089\nEpsilon = 0.20469603708883494\nEpsilon = 0.20467556748512605\nAgent: ddqn_agent . Episode 864/2000. Number of steps to finish: 20. Loss: 14.689760208129883 Reward: -16.0\nEpsilon = 0.20465509992837755\nEpsilon = 0.20463463441838473\nEpsilon = 0.2046141709549429\nEpsilon = 0.2045937095378474\nEpsilon = 0.2045732501668936\nEpsilon = 0.2045527928418769\nEpsilon = 0.20453233756259273\nEpsilon = 0.20451188432883646\nEpsilon = 0.20449143314040358\nAgent: ddqn_agent . Episode 865/2000. Number of steps to finish: 9. Loss: 6.463659286499023 Reward: 3.0\nEpsilon = 0.20447098399708954\nEpsilon = 0.20445053689868983\nEpsilon = 0.20443009184499997\nEpsilon = 0.20440964883581547\nEpsilon = 0.2043892078709319\nEpsilon = 0.2043687689501448\nEpsilon = 0.2043483320732498\nEpsilon = 0.20432789724004247\nEpsilon = 0.20430746445031847\nEpsilon = 0.20428703370387344\nEpsilon = 0.20426660500050306\nEpsilon = 0.204246178340003\nEpsilon = 0.204225753722169\nEpsilon = 0.2042053311467968\nEpsilon = 0.2041849106136821\nEpsilon = 0.20416449212262075\nEpsilon = 0.2041440756734085\nEpsilon = 0.20412366126584117\nEpsilon = 0.2041032488997146\nEpsilon = 0.20408283857482462\nAgent: ddqn_agent . Episode 866/2000. Number of steps to finish: 20. Loss: 14.21458625793457 Reward: -12.0\nEpsilon = 0.20406243029096713\nEpsilon = 0.20404202404793803\nEpsilon = 0.20402161984553324\nEpsilon = 0.2040012176835487\nEpsilon = 0.20398081756178035\nEpsilon = 0.2039604194800242\nEpsilon = 0.20394002343807618\nEpsilon = 0.20391962943573239\nEpsilon = 0.20389923747278882\nEpsilon = 0.20387884754904154\nEpsilon = 0.20385845966428665\nEpsilon = 0.20383807381832023\nEpsilon = 0.20381769001093838\nEpsilon = 0.20379730824193729\nEpsilon = 0.2037769285111131\nEpsilon = 0.203756550818262\nEpsilon = 0.20373617516318018\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.20371580154566388\nEpsilon = 0.20369542996550932\nEpsilon = 0.20367506042251277\nAgent: ddqn_agent . Episode 867/2000. Number of steps to finish: 20. Loss: 14.86491870880127 Reward: -12.0\nEpsilon = 0.2036546929164705\nEpsilon = 0.20363432744717885\nEpsilon = 0.20361396401443413\nEpsilon = 0.2035936026180327\nEpsilon = 0.2035732432577709\nEpsilon = 0.20355288593344512\nEpsilon = 0.20353253064485177\nEpsilon = 0.20351217739178729\nEpsilon = 0.2034918261740481\nEpsilon = 0.2034714769914307\nEpsilon = 0.20345112984373157\nEpsilon = 0.2034307847307472\nEpsilon = 0.20341044165227412\nEpsilon = 0.2033901006081089\nEpsilon = 0.20336976159804807\nEpsilon = 0.20334942462188826\nEpsilon = 0.2033290896794261\nEpsilon = 0.20330875677045815\nEpsilon = 0.2032884258947811\nEpsilon = 0.20326809705219162\nAgent: ddqn_agent . Episode 868/2000. Number of steps to finish: 20. Loss: 14.790390014648438 Reward: -10.0\nEpsilon = 0.20324777024248641\nEpsilon = 0.20322744546546218\nEpsilon = 0.20320712272091562\nEpsilon = 0.20318680200864353\nEpsilon = 0.20316648332844267\nEpsilon = 0.20314616668010982\nEpsilon = 0.2031258520634418\nEpsilon = 0.20310553947823545\nEpsilon = 0.20308522892428763\nEpsilon = 0.2030649204013952\nEpsilon = 0.20304461390935508\nEpsilon = 0.20302430944796415\nEpsilon = 0.20300400701701934\nEpsilon = 0.20298370661631765\nEpsilon = 0.202963408245656\nEpsilon = 0.20294311190483144\nEpsilon = 0.20292281759364095\nEpsilon = 0.20290252531188158\nEpsilon = 0.20288223505935038\nEpsilon = 0.20286194683584444\nAgent: ddqn_agent . Episode 869/2000. Number of steps to finish: 20. Loss: 14.30689525604248 Reward: -16.0\nEpsilon = 0.20284166064116085\nEpsilon = 0.20282137647509674\nEpsilon = 0.20280109433744922\nEpsilon = 0.20278081422801547\nEpsilon = 0.20276053614659267\nEpsilon = 0.20274026009297802\nEpsilon = 0.20271998606696873\nEpsilon = 0.20269971406836204\nEpsilon = 0.2026794440969552\nEpsilon = 0.20265917615254553\nEpsilon = 0.20263891023493028\nEpsilon = 0.20261864634390678\nEpsilon = 0.2025983844792724\nEpsilon = 0.20257812464082448\nEpsilon = 0.20255786682836038\nEpsilon = 0.20253761104167756\nEpsilon = 0.20251735728057338\nEpsilon = 0.20249710554484532\nEpsilon = 0.20247685583429084\nEpsilon = 0.20245660814870742\nAgent: ddqn_agent . Episode 870/2000. Number of steps to finish: 20. Loss: 14.689783096313477 Reward: -14.0\nEpsilon = 0.20243636248789254\nEpsilon = 0.20241611885164376\nEpsilon = 0.2023958772397586\nEpsilon = 0.20237563765203462\nEpsilon = 0.2023554000882694\nEpsilon = 0.2023351645482606\nEpsilon = 0.20231493103180576\nEpsilon = 0.20229469953870258\nEpsilon = 0.2022744700687487\nEpsilon = 0.20225424262174185\nEpsilon = 0.2022340171974797\nEpsilon = 0.20221379379575993\nEpsilon = 0.20219357241638036\nEpsilon = 0.20217335305913872\nEpsilon = 0.2021531357238328\nEpsilon = 0.20213292041026043\nEpsilon = 0.2021127071182194\nEpsilon = 0.2020924958475076\nEpsilon = 0.20207228659792287\nEpsilon = 0.2020520793692631\nAgent: ddqn_agent . Episode 871/2000. Number of steps to finish: 20. Loss: 14.750225067138672 Reward: -18.0\nEpsilon = 0.20203187416132617\nEpsilon = 0.20201167097391004\nEpsilon = 0.20199146980681265\nEpsilon = 0.20197127065983198\nEpsilon = 0.201951073532766\nEpsilon = 0.20193087842541274\nEpsilon = 0.2019106853375702\nEpsilon = 0.20189049426903644\nEpsilon = 0.20187030521960952\nEpsilon = 0.20185011818908757\nEpsilon = 0.20182993317726866\nEpsilon = 0.20180975018395095\nEpsilon = 0.20178956920893257\nEpsilon = 0.20176939025201168\nEpsilon = 0.20174921331298648\nEpsilon = 0.20172903839165518\nEpsilon = 0.20170886548781602\nEpsilon = 0.20168869460126723\nEpsilon = 0.2016685257318071\nEpsilon = 0.20164835887923394\nAgent: ddqn_agent . Episode 872/2000. Number of steps to finish: 20. Loss: 15.511829376220703 Reward: -14.0\nEpsilon = 0.201628194043346\nEpsilon = 0.20160803122394166\nEpsilon = 0.20158787042081927\nEpsilon = 0.20156771163377718\nEpsilon = 0.2015475548626138\nEpsilon = 0.20152740010712755\nEpsilon = 0.20150724736711684\nEpsilon = 0.20148709664238013\nEpsilon = 0.20146694793271588\nEpsilon = 0.2014468012379226\nEpsilon = 0.20142665655779882\nEpsilon = 0.20140651389214304\nEpsilon = 0.2013863732407538\nEpsilon = 0.20136623460342973\nEpsilon = 0.20134609797996939\nEpsilon = 0.2013259633701714\nEpsilon = 0.20130583077383438\nEpsilon = 0.201285700190757\nEpsilon = 0.20126557162073794\nEpsilon = 0.20124544506357586\nAgent: ddqn_agent . Episode 873/2000. Number of steps to finish: 20. Loss: 14.567360877990723 Reward: -18.0\nEpsilon = 0.2012253205190695\nEpsilon = 0.20120519798701758\nEpsilon = 0.2011850774672189\nEpsilon = 0.20116495895947217\nEpsilon = 0.20114484246357622\nEpsilon = 0.20112472797932987\nEpsilon = 0.20110461550653194\nEpsilon = 0.2010845050449813\nEpsilon = 0.2010643965944768\nEpsilon = 0.20104429015481737\nEpsilon = 0.20102418572580188\nEpsilon = 0.2010040833072293\nEpsilon = 0.20098398289889857\nEpsilon = 0.20096388450060867\nEpsilon = 0.20094378811215863\nEpsilon = 0.20092369373334742\nEpsilon = 0.2009036013639741\nEpsilon = 0.2008835110038377\nEpsilon = 0.20086342265273732\nEpsilon = 0.20084333631047205\nAgent: ddqn_agent . Episode 874/2000. Number of steps to finish: 20. Loss: 14.841300964355469 Reward: -12.0\nEpsilon = 0.200823251976841\nEpsilon = 0.20080316965164333\nEpsilon = 0.20078308933467817\nEpsilon = 0.2007630110257447\nEpsilon = 0.20074293472464214\nEpsilon = 0.2007228604311697\nEpsilon = 0.20070278814512657\nEpsilon = 0.20068271786631206\nEpsilon = 0.20066264959452543\nEpsilon = 0.20064258332956597\nEpsilon = 0.20062251907123302\nEpsilon = 0.2006024568193259\nEpsilon = 0.20058239657364396\nEpsilon = 0.2005623383339866\nEpsilon = 0.2005422821001532\nEpsilon = 0.20052222787194318\nEpsilon = 0.20050217564915598\nEpsilon = 0.20048212543159108\nEpsilon = 0.20046207721904793\nEpsilon = 0.20044203101132604\nAgent: ddqn_agent . Episode 875/2000. Number of steps to finish: 20. Loss: 15.015667915344238 Reward: -10.0\nEpsilon = 0.2004219868082249\nEpsilon = 0.20040194460954408\nEpsilon = 0.20038190441508313\nEpsilon = 0.20036186622464164\nEpsilon = 0.20034183003801917\nEpsilon = 0.20032179585501536\nEpsilon = 0.20030176367542987\nEpsilon = 0.20028173349906234\nEpsilon = 0.20026170532571244\nEpsilon = 0.20024167915517987\nEpsilon = 0.20022165498726435\nEpsilon = 0.20020163282176562\nEpsilon = 0.20018161265848344\nEpsilon = 0.2001615944972176\nEpsilon = 0.20014157833776788\nEpsilon = 0.2001215641799341\nEpsilon = 0.2001015520235161\nEpsilon = 0.20008154186831376\nEpsilon = 0.20006153371412694\nEpsilon = 0.20004152756075552\nAgent: ddqn_agent . Episode 876/2000. Number of steps to finish: 20. Loss: 14.715936660766602 Reward: -16.0\nEpsilon = 0.20002152340799945\nEpsilon = 0.20000152125565865\nEpsilon = 0.1999815211035331\nEpsilon = 0.19996152295142275\nEpsilon = 0.1999415267991276\nEpsilon = 0.19992153264644769\nEpsilon = 0.19990154049318304\nEpsilon = 0.1998815503391337\nEpsilon = 0.1998615621840998\nEpsilon = 0.1998415760278814\nEpsilon = 0.19982159187027862\nEpsilon = 0.1998016097110916\nEpsilon = 0.1997816295501205\nEpsilon = 0.19976165138716548\nEpsilon = 0.19974167522202677\nEpsilon = 0.19972170105450457\nEpsilon = 0.19970172888439913\nEpsilon = 0.1996817587115107\nEpsilon = 0.19966179053563954\nEpsilon = 0.19964182435658598\nAgent: ddqn_agent . Episode 877/2000. Number of steps to finish: 20. Loss: 14.672801971435547 Reward: -10.0\nEpsilon = 0.19962186017415032\nEpsilon = 0.1996018979881329\nEpsilon = 0.1995819377983341\nEpsilon = 0.19956197960455427\nEpsilon = 0.19954202340659383\nEpsilon = 0.19952206920425317\nEpsilon = 0.19950211699733275\nEpsilon = 0.19948216678563302\nEpsilon = 0.19946221856895446\nEpsilon = 0.19944227234709758\nEpsilon = 0.19942232811986288\nEpsilon = 0.1994023858870509\nEpsilon = 0.1993824456484622\nEpsilon = 0.19936250740389735\nEpsilon = 0.19934257115315696\nAgent: ddqn_agent . Episode 878/2000. Number of steps to finish: 15. Loss: 11.307649612426758 Reward: -3.0\nEpsilon = 0.19932263689604166\nEpsilon = 0.19930270463235206\nEpsilon = 0.19928277436188882\nEpsilon = 0.19926284608445263\nEpsilon = 0.19924291979984418\nEpsilon = 0.1992229955078642\nEpsilon = 0.19920307320831343\nEpsilon = 0.1991831529009926\nEpsilon = 0.19916323458570248\nEpsilon = 0.1991433182622439\nEpsilon = 0.1991234039304177\nEpsilon = 0.19910349159002466\nEpsilon = 0.19908358124086567\nEpsilon = 0.1990636728827416\nEpsilon = 0.19904376651545333\nEpsilon = 0.1990238621388018\nEpsilon = 0.19900395975258792\nEpsilon = 0.19898405935661265\nEpsilon = 0.19896416095067698\nEpsilon = 0.19894426453458192\nAgent: ddqn_agent . Episode 879/2000. Number of steps to finish: 20. Loss: 15.363327026367188 Reward: -14.0\nEpsilon = 0.19892437010812847\nEpsilon = 0.19890447767111766\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.19888458722335056\nEpsilon = 0.19886469876462823\nEpsilon = 0.19884481229475176\nEpsilon = 0.1988249278135223\nEpsilon = 0.19880504532074095\nEpsilon = 0.19878516481620886\nEpsilon = 0.19876528629972726\nEpsilon = 0.19874540977109728\nEpsilon = 0.19872553523012018\nEpsilon = 0.19870566267659717\nEpsilon = 0.19868579211032952\nEpsilon = 0.1986659235311185\nEpsilon = 0.1986460569387654\nEpsilon = 0.19862619233307152\nEpsilon = 0.1986063297138382\nEpsilon = 0.19858646908086683\nEpsilon = 0.19856661043395873\nEpsilon = 0.19854675377291534\nAgent: ddqn_agent . Episode 880/2000. Number of steps to finish: 20. Loss: 14.59823989868164 Reward: -14.0\nEpsilon = 0.19852689909753804\nEpsilon = 0.19850704640762828\nEpsilon = 0.19848719570298753\nEpsilon = 0.19846734698341723\nEpsilon = 0.19844750024871888\nEpsilon = 0.198427655498694\nEpsilon = 0.19840781273314415\nEpsilon = 0.19838797195187083\nEpsilon = 0.19836813315467564\nEpsilon = 0.19834829634136017\nEpsilon = 0.19832846151172603\nEpsilon = 0.19830862866557486\nEpsilon = 0.1982887978027083\nEpsilon = 0.19826896892292803\nEpsilon = 0.19824914202603575\nEpsilon = 0.19822931711183314\nEpsilon = 0.19820949418012196\nEpsilon = 0.19818967323070394\nEpsilon = 0.19816985426338088\nEpsilon = 0.19815003727795455\nAgent: ddqn_agent . Episode 881/2000. Number of steps to finish: 20. Loss: 14.848819732666016 Reward: -14.0\nEpsilon = 0.19813022227422675\nEpsilon = 0.19811040925199933\nEpsilon = 0.19809059821107414\nEpsilon = 0.19807078915125303\nEpsilon = 0.1980509820723379\nEpsilon = 0.19803117697413067\nEpsilon = 0.19801137385643325\nEpsilon = 0.1979915727190476\nEpsilon = 0.1979717735617757\nEpsilon = 0.19795197638441953\nEpsilon = 0.19793218118678108\nEpsilon = 0.1979123879686624\nEpsilon = 0.19789259672986553\nEpsilon = 0.19787280747019254\nEpsilon = 0.1978530201894455\nEpsilon = 0.19783323488742657\nEpsilon = 0.19781345156393784\nEpsilon = 0.19779367021878144\nAgent: ddqn_agent . Episode 882/2000. Number of steps to finish: 18. Loss: 13.57734489440918 Reward: -6.0\nEpsilon = 0.19777389085175956\nEpsilon = 0.19775411346267438\nEpsilon = 0.19773433805132812\nEpsilon = 0.197714564617523\nEpsilon = 0.19769479316106123\nEpsilon = 0.1976750236817451\nEpsilon = 0.19765525617937693\nEpsilon = 0.197635490653759\nEpsilon = 0.1976157271046936\nEpsilon = 0.19759596553198314\nEpsilon = 0.19757620593542993\nEpsilon = 0.1975564483148364\nEpsilon = 0.19753669267000493\nEpsilon = 0.19751693900073794\nEpsilon = 0.19749718730683788\nEpsilon = 0.1974774375881072\nEpsilon = 0.1974576898443484\nEpsilon = 0.19743794407536397\nEpsilon = 0.19741820028095644\nEpsilon = 0.19739845846092835\nAgent: ddqn_agent . Episode 883/2000. Number of steps to finish: 20. Loss: 14.687627792358398 Reward: -14.0\nEpsilon = 0.19737871861508227\nEpsilon = 0.19735898074322075\nEpsilon = 0.19733924484514642\nEpsilon = 0.1973195109206619\nEpsilon = 0.19729977896956985\nEpsilon = 0.1972800489916729\nEpsilon = 0.19726032098677374\nEpsilon = 0.19724059495467505\nEpsilon = 0.1972208708951796\nEpsilon = 0.1972011488080901\nEpsilon = 0.1971814286932093\nEpsilon = 0.19716171055033996\nEpsilon = 0.19714199437928492\nEpsilon = 0.197122280179847\nEpsilon = 0.19710256795182904\nEpsilon = 0.19708285769503386\nEpsilon = 0.19706314940926437\nEpsilon = 0.19704344309432345\nEpsilon = 0.19702373875001403\nEpsilon = 0.19700403637613903\nAgent: ddqn_agent . Episode 884/2000. Number of steps to finish: 20. Loss: 14.026020050048828 Reward: -10.0\nEpsilon = 0.19698433597250142\nEpsilon = 0.19696463753890417\nEpsilon = 0.1969449410751503\nEpsilon = 0.19692524658104277\nEpsilon = 0.19690555405638466\nEpsilon = 0.19688586350097903\nEpsilon = 0.19686617491462893\nEpsilon = 0.19684648829713747\nEpsilon = 0.19682680364830776\nEpsilon = 0.19680712096794292\nEpsilon = 0.19678744025584613\nEpsilon = 0.19676776151182054\nEpsilon = 0.19674808473566938\nEpsilon = 0.1967284099271958\nEpsilon = 0.1967087370862031\nEpsilon = 0.1966890662124945\nEpsilon = 0.19666939730587324\nEpsilon = 0.19664973036614267\nEpsilon = 0.19663006539310607\nEpsilon = 0.19661040238656677\nAgent: ddqn_agent . Episode 885/2000. Number of steps to finish: 20. Loss: 14.95177173614502 Reward: -20.0\nEpsilon = 0.19659074134632812\nEpsilon = 0.1965710822721935\nEpsilon = 0.1965514251639663\nEpsilon = 0.1965317700214499\nEpsilon = 0.19651211684444775\nEpsilon = 0.19649246563276332\nEpsilon = 0.19647281638620004\nEpsilon = 0.19645316910456143\nEpsilon = 0.19643352378765097\nEpsilon = 0.19641388043527222\nEpsilon = 0.1963942390472287\nEpsilon = 0.19637459962332396\nEpsilon = 0.19635496216336162\nEpsilon = 0.1963353266671453\nEpsilon = 0.19631569313447858\nEpsilon = 0.19629606156516513\nEpsilon = 0.19627643195900862\nEpsilon = 0.19625680431581272\nEpsilon = 0.19623717863538115\nEpsilon = 0.1962175549175176\nAgent: ddqn_agent . Episode 886/2000. Number of steps to finish: 20. Loss: 14.38216495513916 Reward: -16.0\nEpsilon = 0.19619793316202586\nEpsilon = 0.19617831336870967\nEpsilon = 0.1961586955373728\nEpsilon = 0.19613907966781907\nEpsilon = 0.19611946575985229\nEpsilon = 0.1960998538132763\nEpsilon = 0.19608024382789496\nEpsilon = 0.19606063580351218\nEpsilon = 0.19604102973993184\nEpsilon = 0.19602142563695785\nEpsilon = 0.19600182349439416\nEpsilon = 0.1959822233120447\nEpsilon = 0.19596262508971352\nEpsilon = 0.19594302882720455\nEpsilon = 0.19592343452432182\nEpsilon = 0.1959038421808694\nEpsilon = 0.1958842517966513\nEpsilon = 0.19586466337147165\nEpsilon = 0.1958450769051345\nEpsilon = 0.19582549239744398\nAgent: ddqn_agent . Episode 887/2000. Number of steps to finish: 20. Loss: 15.332365989685059 Reward: -12.0\nEpsilon = 0.19580590984820423\nEpsilon = 0.19578632925721942\nEpsilon = 0.1957667506242937\nEpsilon = 0.19574717394923127\nEpsilon = 0.19572759923183636\nEpsilon = 0.19570802647191318\nEpsilon = 0.195688455669266\nEpsilon = 0.19566888682369907\nEpsilon = 0.1956493199350167\nEpsilon = 0.19562975500302318\nEpsilon = 0.1956101920275229\nEpsilon = 0.19559063100832014\nEpsilon = 0.1955710719452193\nEpsilon = 0.1955515148380248\nEpsilon = 0.195531959686541\nEpsilon = 0.19551240649057233\nEpsilon = 0.19549285524992327\nEpsilon = 0.19547330596439827\nEpsilon = 0.19545375863380182\nEpsilon = 0.19543421325793844\nAgent: ddqn_agent . Episode 888/2000. Number of steps to finish: 20. Loss: 14.730005264282227 Reward: -18.0\nEpsilon = 0.19541466983661265\nEpsilon = 0.195395128369629\nEpsilon = 0.19537558885679204\nEpsilon = 0.19535605129790637\nEpsilon = 0.19533651569277657\nEpsilon = 0.1953169820412073\nEpsilon = 0.19529745034300316\nEpsilon = 0.19527792059796886\nEpsilon = 0.19525839280590906\nEpsilon = 0.19523886696662848\nEpsilon = 0.19521934307993183\nEpsilon = 0.19519982114562384\nEpsilon = 0.1951803011635093\nEpsilon = 0.19516078313339294\nEpsilon = 0.19514126705507961\nEpsilon = 0.1951217529283741\nEpsilon = 0.19510224075308127\nEpsilon = 0.19508273052900596\nEpsilon = 0.19506322225595307\nEpsilon = 0.1950437159337275\nAgent: ddqn_agent . Episode 889/2000. Number of steps to finish: 20. Loss: 15.327293395996094 Reward: -20.0\nEpsilon = 0.19502421156213412\nEpsilon = 0.19500470914097792\nEpsilon = 0.19498520867006383\nEpsilon = 0.19496571014919684\nEpsilon = 0.19494621357818193\nEpsilon = 0.19492671895682412\nEpsilon = 0.19490722628492843\nEpsilon = 0.19488773556229994\nEpsilon = 0.1948682467887437\nEpsilon = 0.19484875996406484\nEpsilon = 0.19482927508806844\nEpsilon = 0.19480979216055963\nEpsilon = 0.19479031118134357\nEpsilon = 0.19477083215022545\nEpsilon = 0.19475135506701044\nEpsilon = 0.19473187993150373\nEpsilon = 0.1947124067435106\nEpsilon = 0.19469293550283623\nEpsilon = 0.19467346620928594\nEpsilon = 0.19465399886266502\nAgent: ddqn_agent . Episode 890/2000. Number of steps to finish: 20. Loss: 14.844013214111328 Reward: -20.0\nEpsilon = 0.19463453346277876\nEpsilon = 0.19461507000943248\nEpsilon = 0.19459560850243154\nEpsilon = 0.1945761489415813\nEpsilon = 0.19455669132668713\nEpsilon = 0.19453723565755446\nEpsilon = 0.1945177819339887\nEpsilon = 0.1944983301557953\nEpsilon = 0.1944788803227797\nEpsilon = 0.19445943243474745\nEpsilon = 0.19443998649150399\nEpsilon = 0.19442054249285484\nEpsilon = 0.19440110043860556\nEpsilon = 0.19438166032856172\nEpsilon = 0.19436222216252885\nEpsilon = 0.1943427859403126\nEpsilon = 0.19432335166171857\nEpsilon = 0.19430391932655242\nEpsilon = 0.19428448893461978\nEpsilon = 0.1942650604857263\nAgent: ddqn_agent . Episode 891/2000. Number of steps to finish: 20. Loss: 14.410197257995605 Reward: -12.0\nEpsilon = 0.19424563397967773\nEpsilon = 0.19422620941627977\nEpsilon = 0.19420678679533815\nEpsilon = 0.19418736611665863\nEpsilon = 0.19416794738004697\nEpsilon = 0.19414853058530898\nEpsilon = 0.19412911573225045\nEpsilon = 0.19410970282067722\nEpsilon = 0.19409029185039514\nEpsilon = 0.1940708828212101\nEpsilon = 0.19405147573292797\nEpsilon = 0.19403207058535468\nEpsilon = 0.19401266737829614\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.19399326611155832\nEpsilon = 0.19397386678494716\nEpsilon = 0.19395446939826866\nAgent: ddqn_agent . Episode 892/2000. Number of steps to finish: 16. Loss: 12.157318115234375 Reward: -4.0\nEpsilon = 0.19393507395132883\nEpsilon = 0.1939156804439337\nEpsilon = 0.1938962888758893\nEpsilon = 0.19387689924700172\nEpsilon = 0.193857511557077\nEpsilon = 0.1938381258059213\nEpsilon = 0.1938187419933407\nEpsilon = 0.19379936011914137\nEpsilon = 0.19377998018312945\nEpsilon = 0.19376060218511112\nEpsilon = 0.1937412261248926\nEpsilon = 0.19372185200228012\nEpsilon = 0.1937024798170799\nEpsilon = 0.19368310956909818\nEpsilon = 0.19366374125814126\nEpsilon = 0.19364437488401545\nEpsilon = 0.19362501044652705\nEpsilon = 0.1936056479454824\nEpsilon = 0.19358628738068787\nEpsilon = 0.1935669287519498\nAgent: ddqn_agent . Episode 893/2000. Number of steps to finish: 20. Loss: 14.773483276367188 Reward: -8.0\nEpsilon = 0.1935475720590746\nEpsilon = 0.1935282173018687\nEpsilon = 0.19350886448013851\nEpsilon = 0.19348951359369052\nEpsilon = 0.19347016464233116\nEpsilon = 0.19345081762586694\nEpsilon = 0.19343147254410437\nEpsilon = 0.19341212939684996\nEpsilon = 0.19339278818391029\nEpsilon = 0.1933734489050919\nEpsilon = 0.19335411156020138\nEpsilon = 0.19333477614904537\nEpsilon = 0.19331544267143047\nEpsilon = 0.19329611112716333\nEpsilon = 0.19327678151605063\nEpsilon = 0.19325745383789902\nAgent: ddqn_agent . Episode 894/2000. Number of steps to finish: 16. Loss: 11.43418025970459 Reward: -4.0\nEpsilon = 0.19323812809251523\nEpsilon = 0.19321880427970597\nEpsilon = 0.193199482399278\nEpsilon = 0.19318016245103808\nEpsilon = 0.19316084443479298\nEpsilon = 0.1931415283503495\nEpsilon = 0.19312221419751446\nEpsilon = 0.19310290197609473\nEpsilon = 0.19308359168589712\nEpsilon = 0.19306428332672854\nEpsilon = 0.19304497689839586\nEpsilon = 0.19302567240070603\nEpsilon = 0.19300636983346597\nEpsilon = 0.19298706919648262\nEpsilon = 0.19296777048956298\nEpsilon = 0.19294847371251403\nEpsilon = 0.19292917886514277\nEpsilon = 0.19290988594725625\nEpsilon = 0.19289059495866154\nEpsilon = 0.19287130589916568\nAgent: ddqn_agent . Episode 895/2000. Number of steps to finish: 20. Loss: 14.678727149963379 Reward: -12.0\nEpsilon = 0.19285201876857577\nEpsilon = 0.19283273356669892\nEpsilon = 0.19281345029334224\nEpsilon = 0.19279416894831292\nEpsilon = 0.19277488953141808\nEpsilon = 0.19275561204246494\nEpsilon = 0.1927363364812607\nEpsilon = 0.19271706284761259\nEpsilon = 0.19269779114132782\nEpsilon = 0.1926785213622137\nEpsilon = 0.19265925351007748\nEpsilon = 0.19263998758472647\nEpsilon = 0.192620723585968\nEpsilon = 0.1926014615136094\nEpsilon = 0.19258220136745804\nEpsilon = 0.1925629431473213\nEpsilon = 0.19254368685300657\nEpsilon = 0.19252443248432127\nEpsilon = 0.19250518004107284\nEpsilon = 0.19248592952306873\nAgent: ddqn_agent . Episode 896/2000. Number of steps to finish: 20. Loss: 15.035263061523438 Reward: -14.0\nEpsilon = 0.19246668093011643\nEpsilon = 0.19244743426202343\nEpsilon = 0.19242818951859722\nEpsilon = 0.19240894669964537\nEpsilon = 0.1923897058049754\nEpsilon = 0.19237046683439493\nEpsilon = 0.1923512297877115\nEpsilon = 0.19233199466473272\nEpsilon = 0.19231276146526624\nEpsilon = 0.1922935301891197\nEpsilon = 0.1922743008361008\nEpsilon = 0.1922550734060172\nEpsilon = 0.1922358478986766\nEpsilon = 0.19221662431388672\nEpsilon = 0.19219740265145535\nEpsilon = 0.1921781829111902\nEpsilon = 0.1921589650928991\nEpsilon = 0.1921397491963898\nEpsilon = 0.19212053522147016\nEpsilon = 0.192101323167948\nAgent: ddqn_agent . Episode 897/2000. Number of steps to finish: 20. Loss: 14.95024585723877 Reward: -10.0\nEpsilon = 0.19208211303563122\nEpsilon = 0.19206290482432767\nEpsilon = 0.19204369853384523\nEpsilon = 0.19202449416399184\nEpsilon = 0.19200529171457545\nEpsilon = 0.191986091185404\nEpsilon = 0.19196689257628546\nEpsilon = 0.19194769588702784\nEpsilon = 0.19192850111743914\nEpsilon = 0.1919093082673274\nEpsilon = 0.1918901173365007\nEpsilon = 0.19187092832476704\nEpsilon = 0.19185174123193457\nEpsilon = 0.19183255605781138\nEpsilon = 0.1918133728022056\nEpsilon = 0.19179419146492538\nEpsilon = 0.19177501204577888\nEpsilon = 0.1917558345445743\nEpsilon = 0.19173665896111983\nEpsilon = 0.19171748529522373\nAgent: ddqn_agent . Episode 898/2000. Number of steps to finish: 20. Loss: 15.030462265014648 Reward: -20.0\nEpsilon = 0.1916983135466942\nEpsilon = 0.19167914371533953\nEpsilon = 0.191659975800968\nEpsilon = 0.1916408098033879\nEpsilon = 0.19162164572240756\nEpsilon = 0.19160248355783532\nEpsilon = 0.19158332330947955\nEpsilon = 0.1915641649771486\nEpsilon = 0.19154500856065088\nEpsilon = 0.1915258540597948\nEpsilon = 0.19150670147438884\nEpsilon = 0.1914875508042414\nEpsilon = 0.191468402049161\nEpsilon = 0.1914492552089561\nEpsilon = 0.1914301102834352\nEpsilon = 0.19141096727240686\nEpsilon = 0.19139182617567962\nEpsilon = 0.19137268699306206\nEpsilon = 0.19135354972436275\nEpsilon = 0.1913344143693903\nAgent: ddqn_agent . Episode 899/2000. Number of steps to finish: 20. Loss: 15.308837890625 Reward: -10.0\nEpsilon = 0.19131528092795336\nEpsilon = 0.19129614939986056\nEpsilon = 0.19127701978492057\nEpsilon = 0.19125789208294208\nEpsilon = 0.1912387662937338\nEpsilon = 0.19121964241710443\nEpsilon = 0.19120052045286273\nEpsilon = 0.19118140040081744\nEpsilon = 0.19116228226077736\nEpsilon = 0.1911431660325513\nEpsilon = 0.19112405171594804\nEpsilon = 0.19110493931077643\nEpsilon = 0.19108582881684535\nEpsilon = 0.19106672023396368\nEpsilon = 0.19104761356194028\nEpsilon = 0.19102850880058408\nEpsilon = 0.19100940594970403\nEpsilon = 0.19099030500910907\nEpsilon = 0.19097120597860817\nEpsilon = 0.1909521088580103\nAgent: ddqn_agent . Episode 900/2000. Number of steps to finish: 20. Loss: 15.001038551330566 Reward: -14.0\nEpsilon = 0.1909330136471245\nEpsilon = 0.1909139203457598\nEpsilon = 0.19089482895372523\nEpsilon = 0.19087573947082986\nEpsilon = 0.19085665189688278\nEpsilon = 0.1908375662316931\nEpsilon = 0.19081848247506994\nEpsilon = 0.19079940062682244\nEpsilon = 0.19078032068675976\nEpsilon = 0.19076124265469108\nEpsilon = 0.1907421665304256\nEpsilon = 0.19072309231377257\nEpsilon = 0.1907040200045412\nEpsilon = 0.19068494960254076\nEpsilon = 0.1906658811075805\nEpsilon = 0.19064681451946974\nEpsilon = 0.1906277498380178\nEpsilon = 0.190608687063034\nEpsilon = 0.1905896261943277\nEpsilon = 0.1905705672317083\nAgent: ddqn_agent . Episode 901/2000. Number of steps to finish: 20. Loss: 15.64962100982666 Reward: -10.0\nEpsilon = 0.19055151017498512\nEpsilon = 0.19053245502396762\nEpsilon = 0.19051340177846524\nEpsilon = 0.19049435043828739\nEpsilon = 0.19047530100324356\nEpsilon = 0.19045625347314324\nEpsilon = 0.19043720784779591\nEpsilon = 0.19041816412701112\nEpsilon = 0.1903991223105984\nEpsilon = 0.19038008239836735\nEpsilon = 0.19036104439012752\nEpsilon = 0.1903420082856885\nEpsilon = 0.19032297408485993\nEpsilon = 0.19030394178745144\nEpsilon = 0.1902849113932727\nEpsilon = 0.19026588290213337\nEpsilon = 0.19024685631384317\nEpsilon = 0.19022783162821177\nEpsilon = 0.19020880884504895\nEpsilon = 0.19018978796416444\nAgent: ddqn_agent . Episode 902/2000. Number of steps to finish: 20. Loss: 14.710225105285645 Reward: -20.0\nEpsilon = 0.19017076898536803\nEpsilon = 0.1901517519084695\nEpsilon = 0.19013273673327866\nEpsilon = 0.19011372345960534\nEpsilon = 0.1900947120872594\nEpsilon = 0.19007570261605067\nEpsilon = 0.19005669504578906\nEpsilon = 0.19003768937628449\nEpsilon = 0.19001868560734686\nEpsilon = 0.18999968373878612\nEpsilon = 0.18998068377041225\nEpsilon = 0.1899616857020352\nAgent: ddqn_agent . Episode 903/2000. Number of steps to finish: 12. Loss: 9.518473625183105 Reward: 0.0\nEpsilon = 0.189942689533465\nEpsilon = 0.18992369526451164\nEpsilon = 0.18990470289498518\nEpsilon = 0.18988571242469568\nEpsilon = 0.1898667238534532\nEpsilon = 0.18984773718106787\nEpsilon = 0.18982875240734975\nEpsilon = 0.189809769532109\nEpsilon = 0.1897907885551558\nEpsilon = 0.1897718094763003\nEpsilon = 0.18975283229535267\nEpsilon = 0.18973385701212314\nEpsilon = 0.18971488362642194\nEpsilon = 0.1896959121380593\nEpsilon = 0.1896769425468455\nEpsilon = 0.1896579748525908\nEpsilon = 0.18963900905510556\nEpsilon = 0.18962004515420006\nEpsilon = 0.18960108314968463\nEpsilon = 0.18958212304136968\nAgent: ddqn_agent . Episode 904/2000. Number of steps to finish: 20. Loss: 14.86773681640625 Reward: -18.0\nEpsilon = 0.18956316482906554\nEpsilon = 0.18954420851258263\nEpsilon = 0.18952525409173138\nEpsilon = 0.1895063015663222\nEpsilon = 0.18948735093616556\nEpsilon = 0.18946840220107194\nEpsilon = 0.18944945536085184\nEpsilon = 0.18943051041531575\nEpsilon = 0.18941156736427422\nEpsilon = 0.1893926262075378\nEpsilon = 0.18937368694491705\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.18935474957622256\nEpsilon = 0.18933581410126493\nEpsilon = 0.18931688051985482\nEpsilon = 0.18929794883180284\nEpsilon = 0.18927901903691965\nEpsilon = 0.18926009113501596\nEpsilon = 0.18924116512590247\nEpsilon = 0.1892222410093899\nEpsilon = 0.18920331878528895\nAgent: ddqn_agent . Episode 905/2000. Number of steps to finish: 20. Loss: 14.854460716247559 Reward: -14.0\nEpsilon = 0.18918439845341042\nEpsilon = 0.1891654800135651\nEpsilon = 0.18914656346556374\nEpsilon = 0.1891276488092172\nEpsilon = 0.18910873604433628\nEpsilon = 0.18908982517073186\nEpsilon = 0.18907091618821478\nEpsilon = 0.18905200909659597\nEpsilon = 0.1890331038956863\nEpsilon = 0.18901420058529675\nEpsilon = 0.18899529916523822\nEpsilon = 0.1889763996353217\nEpsilon = 0.18895750199535818\nEpsilon = 0.18893860624515865\nEpsilon = 0.18891971238453414\nEpsilon = 0.1889008204132957\nEpsilon = 0.18888193033125436\nEpsilon = 0.18886304213822125\nEpsilon = 0.18884415583400743\nEpsilon = 0.18882527141842403\nAgent: ddqn_agent . Episode 906/2000. Number of steps to finish: 20. Loss: 15.118865966796875 Reward: -18.0\nEpsilon = 0.1888063888912822\nEpsilon = 0.18878750825239307\nEpsilon = 0.18876862950156784\nEpsilon = 0.18874975263861768\nEpsilon = 0.1887308776633538\nEpsilon = 0.18871200457558748\nEpsilon = 0.18869313337512994\nEpsilon = 0.18867426406179244\nEpsilon = 0.18865539663538627\nEpsilon = 0.18863653109572273\nEpsilon = 0.18861766744261316\nEpsilon = 0.1885988056758689\nEpsilon = 0.18857994579530132\nEpsilon = 0.18856108780072178\nEpsilon = 0.1885422316919417\nEpsilon = 0.1885233774687725\nEpsilon = 0.18850452513102564\nEpsilon = 0.18848567467851254\nEpsilon = 0.1884668261110447\nEpsilon = 0.1884479794284336\nAgent: ddqn_agent . Episode 907/2000. Number of steps to finish: 20. Loss: 15.505974769592285 Reward: -10.0\nEpsilon = 0.18842913463049077\nEpsilon = 0.18841029171702772\nEpsilon = 0.188391450687856\nEpsilon = 0.18837261154278723\nEpsilon = 0.18835377428163294\nEpsilon = 0.18833493890420477\nEpsilon = 0.18831610541031435\nEpsilon = 0.18829727379977332\nEpsilon = 0.18827844407239333\nEpsilon = 0.1882596162279861\nEpsilon = 0.18824079026636328\nEpsilon = 0.18822196618733664\nEpsilon = 0.1882031439907179\nEpsilon = 0.18818432367631885\nEpsilon = 0.18816550524395123\nEpsilon = 0.18814668869342685\nEpsilon = 0.18812787402455752\nEpsilon = 0.18810906123715507\nEpsilon = 0.18809025033103136\nEpsilon = 0.18807144130599826\nAgent: ddqn_agent . Episode 908/2000. Number of steps to finish: 20. Loss: 15.167795181274414 Reward: -18.0\nEpsilon = 0.18805263416186765\nEpsilon = 0.18803382889845147\nEpsilon = 0.18801502551556162\nEpsilon = 0.18799622401301008\nEpsilon = 0.18797742439060877\nEpsilon = 0.1879586266481697\nEpsilon = 0.18793983078550489\nEpsilon = 0.18792103680242633\nEpsilon = 0.18790224469874608\nEpsilon = 0.1878834544742762\nEpsilon = 0.1878646661288288\nEpsilon = 0.18784587966221591\nEpsilon = 0.1878270950742497\nEpsilon = 0.18780831236474227\nEpsilon = 0.1877895315335058\nEpsilon = 0.18777075258035245\nEpsilon = 0.18775197550509443\nEpsilon = 0.18773320030754392\nEpsilon = 0.18771442698751317\nEpsilon = 0.18769565554481443\nAgent: ddqn_agent . Episode 909/2000. Number of steps to finish: 20. Loss: 15.363323211669922 Reward: -16.0\nEpsilon = 0.18767688597925997\nEpsilon = 0.18765811829066203\nEpsilon = 0.18763935247883298\nEpsilon = 0.1876205885435851\nEpsilon = 0.18760182648473075\nEpsilon = 0.1875830663020823\nEpsilon = 0.1875643079954521\nEpsilon = 0.18754555156465255\nEpsilon = 0.1875267970094961\nEpsilon = 0.18750804432979515\nEpsilon = 0.18748929352536217\nEpsilon = 0.18747054459600965\nEpsilon = 0.18745179754155006\nEpsilon = 0.18743305236179592\nEpsilon = 0.18741430905655973\nEpsilon = 0.18739556762565407\nEpsilon = 0.1873768280688915\nEpsilon = 0.1873580903860846\nEpsilon = 0.187339354577046\nEpsilon = 0.1873206206415883\nAgent: ddqn_agent . Episode 910/2000. Number of steps to finish: 20. Loss: 14.701371192932129 Reward: -16.0\nEpsilon = 0.18730188857952415\nEpsilon = 0.1872831583906662\nEpsilon = 0.18726443007482713\nEpsilon = 0.18724570363181964\nEpsilon = 0.18722697906145647\nEpsilon = 0.18720825636355032\nEpsilon = 0.18718953553791395\nEpsilon = 0.18717081658436016\nEpsilon = 0.18715209950270173\nEpsilon = 0.18713338429275145\nEpsilon = 0.1871146709543222\nEpsilon = 0.18709595948722677\nEpsilon = 0.18707724989127805\nEpsilon = 0.18705854216628892\nEpsilon = 0.1870398363120723\nEpsilon = 0.1870211323284411\nEpsilon = 0.18700243021520824\nEpsilon = 0.1869837299721867\nEpsilon = 0.18696503159918948\nEpsilon = 0.18694633509602956\nAgent: ddqn_agent . Episode 911/2000. Number of steps to finish: 20. Loss: 14.593235969543457 Reward: -14.0\nEpsilon = 0.18692764046251997\nEpsilon = 0.18690894769847372\nEpsilon = 0.18689025680370386\nEpsilon = 0.1868715677780235\nEpsilon = 0.1868528806212457\nEpsilon = 0.18683419533318357\nEpsilon = 0.18681551191365026\nEpsilon = 0.1867968303624589\nEpsilon = 0.18677815067942266\nEpsilon = 0.1867594728643547\nEpsilon = 0.18674079691706827\nEpsilon = 0.18672212283737658\nEpsilon = 0.18670345062509283\nEpsilon = 0.18668478028003033\nEpsilon = 0.18666611180200232\nEpsilon = 0.18664744519082213\nEpsilon = 0.18662878044630304\nEpsilon = 0.1866101175682584\nEpsilon = 0.18659145655650158\nEpsilon = 0.18657279741084593\nAgent: ddqn_agent . Episode 912/2000. Number of steps to finish: 20. Loss: 15.564327239990234 Reward: -12.0\nEpsilon = 0.18655414013110486\nEpsilon = 0.18653548471709175\nEpsilon = 0.18651683116862003\nEpsilon = 0.18649817948550318\nEpsilon = 0.18647952966755463\nEpsilon = 0.18646088171458788\nEpsilon = 0.18644223562641643\nEpsilon = 0.18642359140285378\nEpsilon = 0.1864049490437135\nEpsilon = 0.1863863085488091\nEpsilon = 0.18636766991795423\nEpsilon = 0.18634903315096243\nEpsilon = 0.18633039824764733\nEpsilon = 0.18631176520782256\nEpsilon = 0.18629313403130177\nEpsilon = 0.18627450471789864\nEpsilon = 0.18625587726742684\nEpsilon = 0.1862372516797001\nEpsilon = 0.18621862795453212\nEpsilon = 0.18620000609173668\nAgent: ddqn_agent . Episode 913/2000. Number of steps to finish: 20. Loss: 14.03939151763916 Reward: -16.0\nEpsilon = 0.1861813860911275\nEpsilon = 0.18616276795251838\nEpsilon = 0.18614415167572312\nEpsilon = 0.18612553726055556\nEpsilon = 0.1861069247068295\nEpsilon = 0.1860883140143588\nEpsilon = 0.18606970518295737\nEpsilon = 0.1860510982124391\nEpsilon = 0.18603249310261785\nEpsilon = 0.18601388985330758\nEpsilon = 0.18599528846432226\nEpsilon = 0.18597668893547584\nEpsilon = 0.1859580912665823\nEpsilon = 0.18593949545745564\nEpsilon = 0.1859209015079099\nEpsilon = 0.18590230941775912\nEpsilon = 0.18588371918681734\nEpsilon = 0.18586513081489867\nEpsilon = 0.18584654430181718\nEpsilon = 0.185827959647387\nAgent: ddqn_agent . Episode 914/2000. Number of steps to finish: 20. Loss: 14.809006690979004 Reward: -12.0\nEpsilon = 0.18580937685142226\nEpsilon = 0.18579079591373712\nEpsilon = 0.18577221683414574\nEpsilon = 0.18575363961246233\nEpsilon = 0.18573506424850109\nEpsilon = 0.18571649074207625\nEpsilon = 0.18569791909300204\nEpsilon = 0.18567934930109273\nEpsilon = 0.18566078136616262\nEpsilon = 0.18564221528802602\nEpsilon = 0.18562365106649723\nEpsilon = 0.18560508870139059\nEpsilon = 0.18558652819252044\nEpsilon = 0.1855679695397012\nEpsilon = 0.18554941274274722\nEpsilon = 0.18553085780147294\nEpsilon = 0.1855123047156928\nEpsilon = 0.18549375348522124\nEpsilon = 0.18547520410987273\nEpsilon = 0.18545665658946175\nAgent: ddqn_agent . Episode 915/2000. Number of steps to finish: 20. Loss: 15.265348434448242 Reward: -10.0\nEpsilon = 0.1854381109238028\nEpsilon = 0.18541956711271043\nEpsilon = 0.18540102515599916\nEpsilon = 0.18538248505348356\nEpsilon = 0.18536394680497822\nEpsilon = 0.18534541041029773\nEpsilon = 0.1853268758692567\nEpsilon = 0.18530834318166978\nEpsilon = 0.18528981234735162\nEpsilon = 0.1852712833661169\nEpsilon = 0.18525275623778029\nEpsilon = 0.18523423096215652\nEpsilon = 0.1852157075390603\nEpsilon = 0.1851971859683064\nEpsilon = 0.18517866624970958\nEpsilon = 0.18516014838308462\nEpsilon = 0.18514163236824632\nEpsilon = 0.1851231182050095\nEpsilon = 0.185104605893189\nEpsilon = 0.18508609543259968\nAgent: ddqn_agent . Episode 916/2000. Number of steps to finish: 20. Loss: 15.441791534423828 Reward: -16.0\nEpsilon = 0.18506758682305643\nEpsilon = 0.18504908006437412\nEpsilon = 0.1850305751563677\nEpsilon = 0.18501207209885207\nEpsilon = 0.18499357089164217\nEpsilon = 0.184975071534553\nEpsilon = 0.18495657402739954\nEpsilon = 0.1849380783699968\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.1849195845621598\nEpsilon = 0.18490109260370358\nEpsilon = 0.18488260249444322\nEpsilon = 0.18486411423419377\nEpsilon = 0.18484562782277036\nEpsilon = 0.18482714325998809\nEpsilon = 0.18480866054566208\nEpsilon = 0.1847901796796075\nEpsilon = 0.18477170066163956\nEpsilon = 0.1847532234915734\nEpsilon = 0.18473474816922422\nEpsilon = 0.1847162746944073\nAgent: ddqn_agent . Episode 917/2000. Number of steps to finish: 20. Loss: 15.289502143859863 Reward: -12.0\nEpsilon = 0.18469780306693787\nEpsilon = 0.18467933328663116\nEpsilon = 0.1846608653533025\nEpsilon = 0.18464239926676718\nEpsilon = 0.1846239350268405\nEpsilon = 0.18460547263333782\nEpsilon = 0.18458701208607448\nEpsilon = 0.18456855338486586\nEpsilon = 0.18455009652952736\nEpsilon = 0.18453164151987442\nEpsilon = 0.18451318835572242\nEpsilon = 0.18449473703688685\nEpsilon = 0.18447628756318316\nEpsilon = 0.18445783993442683\nEpsilon = 0.1844393941504334\nEpsilon = 0.18442095021101834\nEpsilon = 0.18440250811599723\nEpsilon = 0.18438406786518563\nEpsilon = 0.18436562945839913\nEpsilon = 0.1843471928954533\nAgent: ddqn_agent . Episode 918/2000. Number of steps to finish: 20. Loss: 14.510068893432617 Reward: -14.0\nEpsilon = 0.18432875817616373\nEpsilon = 0.1843103253003461\nEpsilon = 0.1842918942678161\nEpsilon = 0.18427346507838932\nEpsilon = 0.18425503773188148\nEpsilon = 0.1842366122281083\nEpsilon = 0.1842181885668855\nEpsilon = 0.1841997667480288\nEpsilon = 0.184181346771354\nEpsilon = 0.18416292863667688\nEpsilon = 0.1841445123438132\nEpsilon = 0.18412609789257883\nEpsilon = 0.1841076852827896\nEpsilon = 0.18408927451426133\nEpsilon = 0.1840708655868099\nEpsilon = 0.18405245850025123\nEpsilon = 0.1840340532544012\nEpsilon = 0.18401564984907576\nEpsilon = 0.18399724828409086\nEpsilon = 0.18397884855926247\nAgent: ddqn_agent . Episode 919/2000. Number of steps to finish: 20. Loss: 15.332476615905762 Reward: -10.0\nEpsilon = 0.18396045067440656\nEpsilon = 0.18394205462933913\nEpsilon = 0.1839236604238762\nEpsilon = 0.1839052680578338\nEpsilon = 0.18388687753102803\nEpsilon = 0.18386848884327492\nEpsilon = 0.18385010199439059\nEpsilon = 0.18383171698419115\nEpsilon = 0.18381333381249274\nEpsilon = 0.18379495247911148\nEpsilon = 0.18377657298386357\nEpsilon = 0.18375819532656518\nEpsilon = 0.18373981950703253\nEpsilon = 0.18372144552508182\nEpsilon = 0.18370307338052932\nEpsilon = 0.18368470307319126\nEpsilon = 0.18366633460288395\nEpsilon = 0.18364796796942368\nEpsilon = 0.18362960317262675\nEpsilon = 0.18361124021230948\nAgent: ddqn_agent . Episode 920/2000. Number of steps to finish: 20. Loss: 15.132355690002441 Reward: -12.0\nEpsilon = 0.18359287908828825\nEpsilon = 0.18357451980037942\nEpsilon = 0.1835561623483994\nEpsilon = 0.18353780673216455\nEpsilon = 0.18351945295149133\nEpsilon = 0.18350110100619618\nEpsilon = 0.18348275089609556\nEpsilon = 0.18346440262100594\nEpsilon = 0.18344605618074383\nEpsilon = 0.18342771157512577\nEpsilon = 0.18340936880396827\nEpsilon = 0.18339102786708789\nEpsilon = 0.18337268876430118\nEpsilon = 0.18335435149542476\nEpsilon = 0.18333601606027522\nEpsilon = 0.1833176824586692\nEpsilon = 0.18329935069042333\nAgent: ddqn_agent . Episode 921/2000. Number of steps to finish: 17. Loss: 12.616681098937988 Reward: -5.0\nEpsilon = 0.1832810207553543\nEpsilon = 0.18326269265327877\nEpsilon = 0.18324436638401345\nEpsilon = 0.18322604194737505\nEpsilon = 0.1832077193431803\nEpsilon = 0.183189398571246\nEpsilon = 0.18317107963138887\nEpsilon = 0.18315276252342574\nEpsilon = 0.1831344472471734\nEpsilon = 0.18311613380244868\nEpsilon = 0.18309782218906842\nEpsilon = 0.18307951240684953\nEpsilon = 0.18306120445560883\nEpsilon = 0.18304289833516327\nEpsilon = 0.18302459404532975\nEpsilon = 0.1830062915859252\nEpsilon = 0.18298799095676663\nEpsilon = 0.18296969215767095\nEpsilon = 0.18295139518845518\nEpsilon = 0.18293310004893634\nAgent: ddqn_agent . Episode 922/2000. Number of steps to finish: 20. Loss: 15.35649585723877 Reward: -14.0\nEpsilon = 0.18291480673893146\nEpsilon = 0.18289651525825756\nEpsilon = 0.18287822560673175\nEpsilon = 0.1828599377841711\nEpsilon = 0.18284165179039266\nEpsilon = 0.18282336762521362\nEpsilon = 0.1828050852884511\nEpsilon = 0.18278680477992226\nEpsilon = 0.18276852609944427\nEpsilon = 0.18275024924683433\nEpsilon = 0.18273197422190965\nEpsilon = 0.18271370102448745\nEpsilon = 0.182695429654385\nEpsilon = 0.18267716011141957\nEpsilon = 0.18265889239540845\nEpsilon = 0.1826406265061689\nEpsilon = 0.1826223624435183\nEpsilon = 0.18260410020727394\nEpsilon = 0.1825858397972532\nEpsilon = 0.1825675812132735\nAgent: ddqn_agent . Episode 923/2000. Number of steps to finish: 20. Loss: 15.455267906188965 Reward: -20.0\nEpsilon = 0.18254932445515215\nEpsilon = 0.18253106952270665\nEpsilon = 0.18251281641575437\nEpsilon = 0.1824945651341128\nEpsilon = 0.1824763156775994\nEpsilon = 0.18245806804603162\nEpsilon = 0.18243982223922703\nEpsilon = 0.18242157825700311\nEpsilon = 0.18240333609917742\nEpsilon = 0.1823850957655675\nEpsilon = 0.18236685725599094\nEpsilon = 0.18234862057026535\nEpsilon = 0.18233038570820834\nEpsilon = 0.18231215266963752\nEpsilon = 0.18229392145437057\nEpsilon = 0.18227569206222513\nEpsilon = 0.1822574644930189\nEpsilon = 0.1822392387465696\nEpsilon = 0.18222101482269495\nEpsilon = 0.18220279272121268\nAgent: ddqn_agent . Episode 924/2000. Number of steps to finish: 20. Loss: 15.710183143615723 Reward: -18.0\nEpsilon = 0.18218457244194056\nEpsilon = 0.18216635398469636\nEpsilon = 0.18214813734929788\nEpsilon = 0.18212992253556295\nEpsilon = 0.1821117095433094\nEpsilon = 0.18209349837235508\nEpsilon = 0.18207528902251785\nEpsilon = 0.1820570814936156\nEpsilon = 0.18203887578546624\nEpsilon = 0.1820206718978877\nEpsilon = 0.18200246983069793\nEpsilon = 0.18198426958371486\nEpsilon = 0.18196607115675648\nEpsilon = 0.18194787454964081\nEpsilon = 0.18192967976218585\nEpsilon = 0.18191148679420963\nEpsilon = 0.1818932956455302\nEpsilon = 0.18187510631596565\nEpsilon = 0.18185691880533406\nEpsilon = 0.18183873311345353\nAgent: ddqn_agent . Episode 925/2000. Number of steps to finish: 20. Loss: 14.277390480041504 Reward: -18.0\nEpsilon = 0.18182054924014218\nEpsilon = 0.18180236718521817\nEpsilon = 0.18178418694849965\nEpsilon = 0.18176600852980482\nEpsilon = 0.18174783192895183\nEpsilon = 0.18172965714575895\nEpsilon = 0.18171148418004437\nEpsilon = 0.18169331303162636\nEpsilon = 0.1816751437003232\nEpsilon = 0.18165697618595317\nEpsilon = 0.18163881048833458\nEpsilon = 0.18162064660728575\nEpsilon = 0.181602484542625\nEpsilon = 0.18158432429417073\nEpsilon = 0.18156616586174132\nEpsilon = 0.18154800924515516\nEpsilon = 0.18152985444423064\nEpsilon = 0.18151170145878623\nEpsilon = 0.18149355028864037\nEpsilon = 0.1814754009336115\nAgent: ddqn_agent . Episode 926/2000. Number of steps to finish: 20. Loss: 15.122072219848633 Reward: -12.0\nEpsilon = 0.18145725339351815\nEpsilon = 0.1814391076681788\nEpsilon = 0.18142096375741198\nEpsilon = 0.18140282166103625\nEpsilon = 0.18138468137887015\nEpsilon = 0.18136654291073226\nEpsilon = 0.1813484062564412\nEpsilon = 0.18133027141581554\nEpsilon = 0.18131213838867397\nEpsilon = 0.1812940071748351\nEpsilon = 0.1812758777741176\nEpsilon = 0.1812577501863402\nEpsilon = 0.18123962441132155\nEpsilon = 0.1812215004488804\nEpsilon = 0.18120337829883554\nEpsilon = 0.18118525796100565\nEpsilon = 0.18116713943520957\nEpsilon = 0.18114902272126604\nEpsilon = 0.1811309078189939\nEpsilon = 0.181112794728212\nAgent: ddqn_agent . Episode 927/2000. Number of steps to finish: 20. Loss: 15.13017463684082 Reward: -12.0\nEpsilon = 0.1810946834487392\nEpsilon = 0.1810765739803943\nEpsilon = 0.18105846632299627\nEpsilon = 0.18104036047636396\nEpsilon = 0.18102225644031633\nEpsilon = 0.1810041542146723\nEpsilon = 0.18098605379925084\nEpsilon = 0.1809679551938709\nEpsilon = 0.1809498583983515\nEpsilon = 0.1809317634125117\nEpsilon = 0.18091367023617044\nEpsilon = 0.18089557886914684\nEpsilon = 0.18087748931125994\nEpsilon = 0.18085940156232883\nEpsilon = 0.1808413156221726\nEpsilon = 0.1808232314906104\nEpsilon = 0.18080514916746132\nEpsilon = 0.18078706865254457\nEpsilon = 0.1807689899456793\nEpsilon = 0.18075091304668472\nAgent: ddqn_agent . Episode 928/2000. Number of steps to finish: 20. Loss: 14.755602836608887 Reward: -12.0\nEpsilon = 0.18073283795538006\nEpsilon = 0.18071476467158454\nEpsilon = 0.18069669319511739\nEpsilon = 0.1806786235257979\nEpsilon = 0.1806605556634453\nEpsilon = 0.18064248960787896\nEpsilon = 0.18062442535891818\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.1806063629163823\nEpsilon = 0.18058830228009065\nEpsilon = 0.18057024344986264\nEpsilon = 0.18055218642551765\nEpsilon = 0.18053413120687511\nEpsilon = 0.18051607779375442\nEpsilon = 0.18049802618597505\nEpsilon = 0.18047997638335644\nEpsilon = 0.18046192838571812\nEpsilon = 0.18044388219287955\nEpsilon = 0.18042583780466026\nEpsilon = 0.1804077952208798\nEpsilon = 0.1803897544413577\nAgent: ddqn_agent . Episode 929/2000. Number of steps to finish: 20. Loss: 15.27423095703125 Reward: -14.0\nEpsilon = 0.18037171546591357\nEpsilon = 0.180353678294367\nEpsilon = 0.18033564292653756\nEpsilon = 0.18031760936224492\nEpsilon = 0.18029957760130869\nEpsilon = 0.18028154764354856\nEpsilon = 0.18026351948878422\nEpsilon = 0.18024549313683536\nEpsilon = 0.18022746858752167\nEpsilon = 0.18020944584066292\nEpsilon = 0.18019142489607887\nEpsilon = 0.18017340575358926\nEpsilon = 0.1801553884130139\nEpsilon = 0.1801373728741726\nEpsilon = 0.1801193591368852\nEpsilon = 0.1801013472009715\nEpsilon = 0.1800833370662514\nEpsilon = 0.18006532873254477\nEpsilon = 0.1800473221996715\nEpsilon = 0.18002931746745154\nAgent: ddqn_agent . Episode 930/2000. Number of steps to finish: 20. Loss: 14.858776092529297 Reward: -16.0\nEpsilon = 0.1800113145357048\nEpsilon = 0.17999331340425123\nEpsilon = 0.1799753140729108\nEpsilon = 0.17995731654150351\nEpsilon = 0.17993932080984937\nEpsilon = 0.17992132687776838\nEpsilon = 0.1799033347450806\nEpsilon = 0.1798853444116061\nEpsilon = 0.17986735587716496\nEpsilon = 0.17984936914157723\nEpsilon = 0.17983138420466308\nEpsilon = 0.1798134010662426\nEpsilon = 0.179795419726136\nEpsilon = 0.1797774401841634\nEpsilon = 0.17975946244014498\nEpsilon = 0.17974148649390098\nEpsilon = 0.1797235123452516\nEpsilon = 0.1797055399940171\nEpsilon = 0.17968756944001768\nEpsilon = 0.1796696006830737\nAgent: ddqn_agent . Episode 931/2000. Number of steps to finish: 20. Loss: 14.93075942993164 Reward: -16.0\nEpsilon = 0.17965163372300538\nEpsilon = 0.1796336685596331\nEpsilon = 0.17961570519277711\nEpsilon = 0.17959774362225783\nEpsilon = 0.1795797838478956\nEpsilon = 0.17956182586951083\nEpsilon = 0.17954386968692387\nEpsilon = 0.17952591529995518\nEpsilon = 0.17950796270842517\nEpsilon = 0.17949001191215433\nEpsilon = 0.17947206291096313\nEpsilon = 0.17945411570467204\nEpsilon = 0.17943617029310158\nEpsilon = 0.17941822667607227\nEpsilon = 0.17940028485340467\nEpsilon = 0.17938234482491933\nEpsilon = 0.17936440659043684\nEpsilon = 0.1793464701497778\nEpsilon = 0.17932853550276284\nEpsilon = 0.17931060264921256\nAgent: ddqn_agent . Episode 932/2000. Number of steps to finish: 20. Loss: 14.855892181396484 Reward: -16.0\nEpsilon = 0.17929267158894763\nEpsilon = 0.17927474232178872\nEpsilon = 0.17925681484755654\nEpsilon = 0.17923888916607178\nEpsilon = 0.17922096527715517\nEpsilon = 0.17920304318062746\nEpsilon = 0.17918512287630942\nEpsilon = 0.17916720436402178\nEpsilon = 0.17914928764358537\nEpsilon = 0.17913137271482102\nEpsilon = 0.17911345957754954\nEpsilon = 0.1790955482315918\nEpsilon = 0.17907763867676862\nEpsilon = 0.17905973091290095\nEpsilon = 0.17904182493980966\nEpsilon = 0.17902392075731569\nEpsilon = 0.17900601836523997\nEpsilon = 0.17898811776340345\nEpsilon = 0.17897021895162712\nEpsilon = 0.17895232192973196\nAgent: ddqn_agent . Episode 933/2000. Number of steps to finish: 20. Loss: 14.30144214630127 Reward: -16.0\nEpsilon = 0.17893442669753898\nEpsilon = 0.17891653325486923\nEpsilon = 0.17889864160154373\nEpsilon = 0.17888075173738358\nEpsilon = 0.17886286366220983\nEpsilon = 0.1788449773758436\nEpsilon = 0.17882709287810603\nEpsilon = 0.1788092101688182\nEpsilon = 0.17879132924780133\nEpsilon = 0.17877345011487655\nEpsilon = 0.17875557276986506\nEpsilon = 0.1787376972125881\nEpsilon = 0.17871982344286683\nEpsilon = 0.17870195146052253\nEpsilon = 0.1786840812653765\nEpsilon = 0.17866621285724996\nEpsilon = 0.17864834623596423\nEpsilon = 0.17863048140134064\nEpsilon = 0.1786126183532005\nEpsilon = 0.17859475709136519\nAgent: ddqn_agent . Episode 934/2000. Number of steps to finish: 20. Loss: 15.188069343566895 Reward: -16.0\nEpsilon = 0.17857689761565604\nEpsilon = 0.17855903992589447\nEpsilon = 0.1785411840219019\nEpsilon = 0.1785233299034997\nEpsilon = 0.17850547757050936\nEpsilon = 0.1784876270227523\nEpsilon = 0.17846977826005003\nEpsilon = 0.17845193128222403\nEpsilon = 0.1784340860890958\nEpsilon = 0.1784162426804869\nEpsilon = 0.17839840105621885\nEpsilon = 0.17838056121611323\nEpsilon = 0.17836272315999163\nEpsilon = 0.17834488688767564\nEpsilon = 0.17832705239898686\nEpsilon = 0.17830921969374697\nEpsilon = 0.17829138877177758\nEpsilon = 0.1782735596329004\nEpsilon = 0.1782557322769371\nEpsilon = 0.17823790670370943\nAgent: ddqn_agent . Episode 935/2000. Number of steps to finish: 20. Loss: 15.083152770996094 Reward: -14.0\nEpsilon = 0.17822008291303906\nEpsilon = 0.17820226090474775\nEpsilon = 0.17818444067865727\nEpsilon = 0.1781666222345894\nEpsilon = 0.17814880557236595\nEpsilon = 0.1781309906918087\nEpsilon = 0.17811317759273954\nEpsilon = 0.17809536627498027\nEpsilon = 0.17807755673835277\nEpsilon = 0.17805974898267893\nEpsilon = 0.17804194300778067\nEpsilon = 0.1780241388134799\nEpsilon = 0.17800633639959854\nEpsilon = 0.17798853576595858\nEpsilon = 0.177970736912382\nEpsilon = 0.17795293983869076\nEpsilon = 0.17793514454470688\nEpsilon = 0.1779173510302524\nEpsilon = 0.1778995592951494\nEpsilon = 0.17788176933921987\nAgent: ddqn_agent . Episode 936/2000. Number of steps to finish: 20. Loss: 14.999013900756836 Reward: -20.0\nEpsilon = 0.17786398116228594\nEpsilon = 0.1778461947641697\nEpsilon = 0.1778284101446933\nEpsilon = 0.17781062730367883\nEpsilon = 0.17779284624094846\nEpsilon = 0.17777506695632436\nEpsilon = 0.17775728944962874\nEpsilon = 0.17773951372068378\nEpsilon = 0.17772173976931172\nEpsilon = 0.1777039675953348\nEpsilon = 0.17768619719857526\nEpsilon = 0.1776684285788554\nEpsilon = 0.1776506617359975\nEpsilon = 0.1776328966698239\nEpsilon = 0.17761513338015691\nEpsilon = 0.1775973718668189\nEpsilon = 0.17757961212963222\nEpsilon = 0.17756185416841927\nEpsilon = 0.17754409798300244\nEpsilon = 0.17752634357320415\nAgent: ddqn_agent . Episode 937/2000. Number of steps to finish: 20. Loss: 14.399947166442871 Reward: -12.0\nEpsilon = 0.17750859093884683\nEpsilon = 0.17749084007975294\nEpsilon = 0.17747309099574496\nEpsilon = 0.1774553436866454\nEpsilon = 0.17743759815227672\nEpsilon = 0.1774198543924615\nEpsilon = 0.17740211240702225\nEpsilon = 0.17738437219578154\nEpsilon = 0.17736663375856196\nEpsilon = 0.17734889709518611\nEpsilon = 0.1773311622054766\nEpsilon = 0.17731342908925604\nEpsilon = 0.17729569774634713\nEpsilon = 0.1772779681765725\nEpsilon = 0.17726024037975482\nEpsilon = 0.17724251435571686\nEpsilon = 0.1772247901042813\nEpsilon = 0.1772070676252709\nEpsilon = 0.17718934691850835\nEpsilon = 0.1771716279838165\nAgent: ddqn_agent . Episode 938/2000. Number of steps to finish: 20. Loss: 14.486984252929688 Reward: -12.0\nEpsilon = 0.1771539108210181\nEpsilon = 0.177136195429936\nEpsilon = 0.177118481810393\nEpsilon = 0.17710076996221197\nEpsilon = 0.17708305988521575\nEpsilon = 0.17706535157922723\nEpsilon = 0.1770476450440693\nEpsilon = 0.1770299402795649\nEpsilon = 0.17701223728553694\nEpsilon = 0.1769945360618084\nEpsilon = 0.1769768366082022\nEpsilon = 0.1769591389245414\nEpsilon = 0.17694144301064896\nEpsilon = 0.1769237488663479\nEpsilon = 0.17690605649146127\nEpsilon = 0.1768883658858121\nEpsilon = 0.17687067704922352\nEpsilon = 0.1768529899815186\nEpsilon = 0.17683530468252046\nEpsilon = 0.1768176211520522\nAgent: ddqn_agent . Episode 939/2000. Number of steps to finish: 20. Loss: 15.655912399291992 Reward: -12.0\nEpsilon = 0.17679993938993702\nEpsilon = 0.17678225939599804\nEpsilon = 0.17676458117005844\nEpsilon = 0.17674690471194143\nEpsilon = 0.17672923002147023\nEpsilon = 0.1767115570984681\nEpsilon = 0.17669388594275823\nEpsilon = 0.17667621655416396\nEpsilon = 0.17665854893250854\nEpsilon = 0.1766408830776153\nEpsilon = 0.17662321898930755\nEpsilon = 0.17660555666740863\nEpsilon = 0.1765878961117419\nEpsilon = 0.1765702373221307\nEpsilon = 0.1765525802983985\nEpsilon = 0.17653492504036866\nEpsilon = 0.17651727154786462\nEpsilon = 0.17649961982070983\nEpsilon = 0.17648196985872777\nEpsilon = 0.1764643216617419\nAgent: ddqn_agent . Episode 940/2000. Number of steps to finish: 20. Loss: 14.501843452453613 Reward: -14.0\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.17644667522957572\nEpsilon = 0.17642903056205275\nEpsilon = 0.17641138765899655\nEpsilon = 0.17639374652023065\nEpsilon = 0.17637610714557864\nEpsilon = 0.1763584695348641\nEpsilon = 0.1763408336879106\nEpsilon = 0.1763231996045418\nEpsilon = 0.17630556728458133\nEpsilon = 0.17628793672785287\nEpsilon = 0.17627030793418008\nEpsilon = 0.17625268090338667\nEpsilon = 0.17623505563529632\nEpsilon = 0.1762174321297328\nEpsilon = 0.17619981038651983\nEpsilon = 0.17618219040548116\nEpsilon = 0.17616457218644063\nEpsilon = 0.17614695572922198\nEpsilon = 0.17612934103364905\nEpsilon = 0.1761117280995457\nAgent: ddqn_agent . Episode 941/2000. Number of steps to finish: 20. Loss: 14.641206741333008 Reward: -12.0\nEpsilon = 0.17609411692673574\nEpsilon = 0.17607650751504306\nEpsilon = 0.17605889986429155\nEpsilon = 0.17604129397430512\nEpsilon = 0.17602368984490768\nEpsilon = 0.1760060874759232\nEpsilon = 0.1759884868671756\nEpsilon = 0.1759708880184889\nEpsilon = 0.17595329092968706\nEpsilon = 0.1759356956005941\nEpsilon = 0.17591810203103406\nEpsilon = 0.17590051022083095\nEpsilon = 0.17588292016980886\nEpsilon = 0.17586533187779188\nEpsilon = 0.1758477453446041\nEpsilon = 0.17583016057006964\nEpsilon = 0.17581257755401264\nEpsilon = 0.17579499629625725\nEpsilon = 0.17577741679662762\nEpsilon = 0.17575983905494796\nAgent: ddqn_agent . Episode 942/2000. Number of steps to finish: 20. Loss: 15.288875579833984 Reward: -14.0\nEpsilon = 0.17574226307104246\nEpsilon = 0.17572468884473535\nEpsilon = 0.17570711637585087\nEpsilon = 0.1756895456642133\nEpsilon = 0.17567197670964688\nEpsilon = 0.17565440951197592\nEpsilon = 0.17563684407102473\nEpsilon = 0.17561928038661762\nEpsilon = 0.17560171845857894\nEpsilon = 0.1755841582867331\nEpsilon = 0.17556659987090442\nEpsilon = 0.17554904321091733\nEpsilon = 0.17553148830659623\nEpsilon = 0.1755139351577656\nEpsilon = 0.1754963837642498\nEpsilon = 0.1754788341258734\nEpsilon = 0.17546128624246082\nEpsilon = 0.17544374011383657\nEpsilon = 0.17542619573982518\nEpsilon = 0.1754086531202512\nAgent: ddqn_agent . Episode 943/2000. Number of steps to finish: 20. Loss: 15.131535530090332 Reward: -18.0\nEpsilon = 0.17539111225493917\nEpsilon = 0.17537357314371368\nEpsilon = 0.17535603578639933\nEpsilon = 0.17533850018282068\nEpsilon = 0.1753209663328024\nEpsilon = 0.17530343423616912\nEpsilon = 0.1752859038927455\nEpsilon = 0.17526837530235623\nEpsilon = 0.175250848464826\nEpsilon = 0.17523332337997952\nEpsilon = 0.1752158000476415\nEpsilon = 0.17519827846763675\nEpsilon = 0.17518075863979\nEpsilon = 0.17516324056392601\nEpsilon = 0.17514572423986963\nAgent: ddqn_agent . Episode 944/2000. Number of steps to finish: 15. Loss: 11.344073295593262 Reward: -3.0\nEpsilon = 0.17512820966744563\nEpsilon = 0.1751106968464789\nEpsilon = 0.17509318577679425\nEpsilon = 0.17507567645821656\nEpsilon = 0.17505816889057074\nEpsilon = 0.17504066307368168\nEpsilon = 0.1750231590073743\nEpsilon = 0.17500565669147358\nEpsilon = 0.17498815612580443\nEpsilon = 0.17497065731019185\nEpsilon = 0.17495316024446084\nEpsilon = 0.1749356649284364\nEpsilon = 0.17491817136194357\nEpsilon = 0.17490067954480737\nEpsilon = 0.17488318947685288\nEpsilon = 0.1748657011579052\nEpsilon = 0.1748482145877894\nEpsilon = 0.17483072976633063\nEpsilon = 0.174813246693354\nEpsilon = 0.17479576536868466\nAgent: ddqn_agent . Episode 945/2000. Number of steps to finish: 20. Loss: 14.531268119812012 Reward: -14.0\nEpsilon = 0.1747782857921478\nEpsilon = 0.17476080796356858\nEpsilon = 0.17474333188277222\nEpsilon = 0.17472585754958395\nEpsilon = 0.17470838496382898\nEpsilon = 0.1746909141253326\nEpsilon = 0.17467344503392007\nEpsilon = 0.17465597768941668\nEpsilon = 0.17463851209164774\nEpsilon = 0.1746210482404386\nEpsilon = 0.17460358613561455\nEpsilon = 0.174586125777001\nEpsilon = 0.1745686671644233\nEpsilon = 0.17455121029770684\nEpsilon = 0.17453375517667707\nEpsilon = 0.1745163018011594\nEpsilon = 0.1744988501709793\nEpsilon = 0.1744814002859622\nEpsilon = 0.1744639521459336\nEpsilon = 0.17444650575071902\nAgent: ddqn_agent . Episode 946/2000. Number of steps to finish: 20. Loss: 14.849318504333496 Reward: -16.0\nEpsilon = 0.17442906110014395\nEpsilon = 0.17441161819403395\nEpsilon = 0.17439417703221455\nEpsilon = 0.17437673761451133\nEpsilon = 0.1743592999407499\nEpsilon = 0.17434186401075583\nEpsilon = 0.17432442982435475\nEpsilon = 0.17430699738137231\nEpsilon = 0.17428956668163417\nEpsilon = 0.174272137724966\nEpsilon = 0.1742547105111935\nEpsilon = 0.17423728504014238\nEpsilon = 0.17421986131163836\nEpsilon = 0.1742024393255072\nEpsilon = 0.17418501908157463\nEpsilon = 0.1741676005796665\nEpsilon = 0.17415018381960853\nEpsilon = 0.17413276880122658\nEpsilon = 0.17411535552434645\nEpsilon = 0.17409794398879402\nAgent: ddqn_agent . Episode 947/2000. Number of steps to finish: 20. Loss: 15.331632614135742 Reward: -16.0\nEpsilon = 0.17408053419439515\nEpsilon = 0.17406312614097572\nEpsilon = 0.17404571982836162\nEpsilon = 0.1740283152563788\nEpsilon = 0.17401091242485317\nEpsilon = 0.17399351133361068\nEpsilon = 0.17397611198247734\nEpsilon = 0.1739587143712791\nEpsilon = 0.17394131849984198\nEpsilon = 0.173923924367992\nEpsilon = 0.1739065319755552\nEpsilon = 0.17388914132235764\nEpsilon = 0.17387175240822542\nEpsilon = 0.1738543652329846\nEpsilon = 0.1738369797964613\nEpsilon = 0.17381959609848163\nEpsilon = 0.17380221413887179\nEpsilon = 0.1737848339174579\nEpsilon = 0.17376745543406613\nEpsilon = 0.1737500786885227\nAgent: ddqn_agent . Episode 948/2000. Number of steps to finish: 20. Loss: 14.53805160522461 Reward: -12.0\nEpsilon = 0.17373270368065385\nEpsilon = 0.17371533041028578\nEpsilon = 0.17369795887724476\nEpsilon = 0.17368058908135703\nEpsilon = 0.17366322102244888\nEpsilon = 0.17364585470034663\nEpsilon = 0.1736284901148766\nEpsilon = 0.1736111272658651\nEpsilon = 0.17359376615313854\nEpsilon = 0.17357640677652322\nEpsilon = 0.17355904913584558\nAgent: ddqn_agent . Episode 949/2000. Number of steps to finish: 11. Loss: 8.137304306030273 Reward: 1.0\nEpsilon = 0.173541693230932\nEpsilon = 0.1735243390616089\nEpsilon = 0.17350698662770275\nEpsilon = 0.17348963592904\nEpsilon = 0.1734722869654471\nEpsilon = 0.17345493973675055\nEpsilon = 0.17343759424277688\nEpsilon = 0.1734202504833526\nEpsilon = 0.17340290845830428\nEpsilon = 0.17338556816745845\nEpsilon = 0.17336822961064172\nEpsilon = 0.17335089278768065\nEpsilon = 0.1733335576984019\nEpsilon = 0.17331622434263205\nEpsilon = 0.1732988927201978\nEpsilon = 0.17328156283092577\nEpsilon = 0.1732642346746427\nEpsilon = 0.17324690825117522\nEpsilon = 0.17322958356035012\nEpsilon = 0.1732122606019941\nAgent: ddqn_agent . Episode 950/2000. Number of steps to finish: 20. Loss: 14.605779647827148 Reward: -16.0\nEpsilon = 0.17319493937593392\nEpsilon = 0.17317761988199631\nEpsilon = 0.1731603021200081\nEpsilon = 0.1731429860897961\nEpsilon = 0.17312567179118712\nEpsilon = 0.173108359224008\nEpsilon = 0.1730910483880856\nEpsilon = 0.1730737392832468\nEpsilon = 0.17305643190931846\nEpsilon = 0.17303912626612752\nEpsilon = 0.17302182235350091\nEpsilon = 0.17300452017126558\nEpsilon = 0.17298721971924846\nEpsilon = 0.17296992099727654\nEpsilon = 0.1729526240051768\nEpsilon = 0.17293532874277628\nEpsilon = 0.172918035209902\nEpsilon = 0.17290074340638104\nEpsilon = 0.1728834533320404\nEpsilon = 0.17286616498670718\nAgent: ddqn_agent . Episode 951/2000. Number of steps to finish: 20. Loss: 15.368361473083496 Reward: -16.0\nEpsilon = 0.17284887837020851\nEpsilon = 0.17283159348237148\nEpsilon = 0.17281431032302325\nEpsilon = 0.17279702889199094\nEpsilon = 0.17277974918910174\nEpsilon = 0.17276247121418284\nEpsilon = 0.17274519496706142\nEpsilon = 0.17272792044756471\nEpsilon = 0.17271064765551997\nEpsilon = 0.17269337659075443\nEpsilon = 0.17267610725309535\nEpsilon = 0.17265883964237003\nEpsilon = 0.1726415737584058\nEpsilon = 0.17262430960102995\nEpsilon = 0.17260704717006986\nEpsilon = 0.17258978646535286\nEpsilon = 0.17257252748670632\nEpsilon = 0.17255527023395764\nEpsilon = 0.17253801470693425\nEpsilon = 0.17252076090546356\nAgent: ddqn_agent . Episode 952/2000. Number of steps to finish: 20. Loss: 15.407905578613281 Reward: -16.0\nEpsilon = 0.17250350882937301\nEpsilon = 0.17248625847849008\nEpsilon = 0.17246900985264224\nEpsilon = 0.17245176295165698\nEpsilon = 0.1724345177753618\nEpsilon = 0.17241727432358428\nEpsilon = 0.17240003259615191\nEpsilon = 0.1723827925928923\nEpsilon = 0.172365554313633\nEpsilon = 0.17234831775820164\nEpsilon = 0.17233108292642582\nEpsilon = 0.1723138498181332\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.1722966184331514\nEpsilon = 0.17227938877130808\nEpsilon = 0.17226216083243096\nEpsilon = 0.1722449346163477\nEpsilon = 0.17222771012288607\nEpsilon = 0.17221048735187378\nEpsilon = 0.17219326630313858\nEpsilon = 0.17217604697650826\nAgent: ddqn_agent . Episode 953/2000. Number of steps to finish: 20. Loss: 15.434830665588379 Reward: -12.0\nEpsilon = 0.17215882937181062\nEpsilon = 0.17214161348887344\nEpsilon = 0.17212439932752455\nEpsilon = 0.1721071868875918\nEpsilon = 0.17208997616890304\nEpsilon = 0.17207276717128614\nEpsilon = 0.172055559894569\nEpsilon = 0.17203835433857956\nEpsilon = 0.1720211505031457\nEpsilon = 0.17200394838809538\nEpsilon = 0.17198674799325658\nEpsilon = 0.17196954931845726\nEpsilon = 0.1719523523635254\nEpsilon = 0.17193515712828905\nEpsilon = 0.17191796361257622\nEpsilon = 0.17190077181621496\nEpsilon = 0.17188358173903334\nEpsilon = 0.17186639338085943\nEpsilon = 0.17184920674152135\nEpsilon = 0.1718320218208472\nAgent: ddqn_agent . Episode 954/2000. Number of steps to finish: 20. Loss: 15.503028869628906 Reward: -18.0\nEpsilon = 0.17181483861866512\nEpsilon = 0.17179765713480324\nEpsilon = 0.17178047736908977\nEpsilon = 0.17176329932135287\nEpsilon = 0.17174612299142072\nEpsilon = 0.1717289483791216\nEpsilon = 0.17171177548428368\nEpsilon = 0.17169460430673525\nEpsilon = 0.17167743484630457\nEpsilon = 0.17166026710281995\nEpsilon = 0.17164310107610967\nEpsilon = 0.17162593676600205\nEpsilon = 0.17160877417232545\nEpsilon = 0.1715916132949082\nEpsilon = 0.17157445413357872\nEpsilon = 0.17155729668816536\nEpsilon = 0.17154014095849654\nEpsilon = 0.17152298694440068\nEpsilon = 0.17150583464570623\nEpsilon = 0.17148868406224166\nAgent: ddqn_agent . Episode 955/2000. Number of steps to finish: 20. Loss: 15.058711051940918 Reward: -20.0\nEpsilon = 0.17147153519383543\nEpsilon = 0.17145438804031604\nEpsilon = 0.17143724260151202\nEpsilon = 0.17142009887725188\nEpsilon = 0.17140295686736415\nEpsilon = 0.17138581657167742\nEpsilon = 0.17136867799002026\nEpsilon = 0.17135154112222126\nEpsilon = 0.17133440596810903\nEpsilon = 0.17131727252751222\nEpsilon = 0.17130014080025946\nEpsilon = 0.17128301078617944\nEpsilon = 0.17126588248510083\nEpsilon = 0.17124875589685232\nEpsilon = 0.17123163102126263\nEpsilon = 0.1712145078581605\nEpsilon = 0.1711973864073747\nEpsilon = 0.17118026666873395\nEpsilon = 0.17116314864206708\nEpsilon = 0.17114603232720288\nAgent: ddqn_agent . Episode 956/2000. Number of steps to finish: 20. Loss: 15.58291244506836 Reward: -12.0\nEpsilon = 0.17112891772397015\nEpsilon = 0.17111180483219776\nEpsilon = 0.17109469365171454\nEpsilon = 0.17107758418234936\nEpsilon = 0.17106047642393113\nEpsilon = 0.17104337037628875\nEpsilon = 0.1710262660392511\nEpsilon = 0.17100916341264719\nEpsilon = 0.1709920624963059\nEpsilon = 0.1709749632900563\nEpsilon = 0.1709578657937273\nEpsilon = 0.1709407700071479\nEpsilon = 0.1709236759301472\nEpsilon = 0.1709065835625542\nEpsilon = 0.17088949290419794\nEpsilon = 0.17087240395490752\nEpsilon = 0.17085531671451204\nEpsilon = 0.17083823118284058\nEpsilon = 0.17082114735972231\nEpsilon = 0.17080406524498634\nAgent: ddqn_agent . Episode 957/2000. Number of steps to finish: 20. Loss: 14.415335655212402 Reward: -12.0\nEpsilon = 0.17078698483846186\nEpsilon = 0.170769906139978\nEpsilon = 0.17075282914936402\nEpsilon = 0.17073575386644907\nEpsilon = 0.17071868029106244\nEpsilon = 0.17070160842303334\nEpsilon = 0.17068453826219104\nEpsilon = 0.17066746980836484\nEpsilon = 0.170650403061384\nEpsilon = 0.17063333802107786\nEpsilon = 0.17061627468727575\nEpsilon = 0.17059921305980702\nEpsilon = 0.17058215313850103\nEpsilon = 0.1705650949231872\nEpsilon = 0.17054803841369487\nEpsilon = 0.1705309836098535\nEpsilon = 0.17051393051149252\nEpsilon = 0.17049687911844139\nEpsilon = 0.17047982943052956\nEpsilon = 0.1704627814475865\nAgent: ddqn_agent . Episode 958/2000. Number of steps to finish: 20. Loss: 15.416135787963867 Reward: -16.0\nEpsilon = 0.17044573516944175\nEpsilon = 0.1704286905959248\nEpsilon = 0.1704116477268652\nEpsilon = 0.17039460656209252\nEpsilon = 0.1703775671014363\nEpsilon = 0.17036052934472617\nEpsilon = 0.17034349329179171\nEpsilon = 0.17032645894246254\nEpsilon = 0.17030942629656828\nEpsilon = 0.17029239535393864\nEpsilon = 0.17027536611440325\nEpsilon = 0.17025833857779182\nEpsilon = 0.17024131274393406\nEpsilon = 0.17022428861265967\nEpsilon = 0.17020726618379842\nEpsilon = 0.17019024545718003\nEpsilon = 0.17017322643263433\nEpsilon = 0.17015620910999107\nEpsilon = 0.17013919348908008\nEpsilon = 0.17012217956973116\nAgent: ddqn_agent . Episode 959/2000. Number of steps to finish: 20. Loss: 15.347092628479004 Reward: -10.0\nEpsilon = 0.1701051673517742\nEpsilon = 0.17008815683503903\nEpsilon = 0.17007114801935552\nEpsilon = 0.1700541409045536\nEpsilon = 0.17003713549046315\nEpsilon = 0.1700201317769141\nEpsilon = 0.1700031297637364\nEpsilon = 0.16998612945076003\nEpsilon = 0.16996913083781495\nEpsilon = 0.16995213392473119\nEpsilon = 0.16993513871133872\nEpsilon = 0.1699181451974676\nEpsilon = 0.16990115338294784\nEpsilon = 0.16988416326760955\nEpsilon = 0.1698671748512828\nEpsilon = 0.16985018813379768\nEpsilon = 0.16983320311498432\nEpsilon = 0.16981621979467282\nEpsilon = 0.16979923817269335\nEpsilon = 0.16978225824887608\nAgent: ddqn_agent . Episode 960/2000. Number of steps to finish: 20. Loss: 14.93545150756836 Reward: -12.0\nEpsilon = 0.1697652800230512\nEpsilon = 0.16974830349504888\nEpsilon = 0.16973132866469937\nEpsilon = 0.1697143555318329\nEpsilon = 0.1696973840962797\nEpsilon = 0.1696804143578701\nEpsilon = 0.16966344631643432\nEpsilon = 0.16964647997180268\nEpsilon = 0.1696295153238055\nEpsilon = 0.1696125523722731\nEpsilon = 0.16959559111703587\nEpsilon = 0.16957863155792416\nEpsilon = 0.16956167369476838\nEpsilon = 0.1695447175273989\nEpsilon = 0.16952776305564618\nEpsilon = 0.16951081027934062\nEpsilon = 0.1694938591983127\nEpsilon = 0.16947690981239286\nEpsilon = 0.16945996212141162\nEpsilon = 0.1694430161251995\nAgent: ddqn_agent . Episode 961/2000. Number of steps to finish: 20. Loss: 15.480064392089844 Reward: -18.0\nEpsilon = 0.16942607182358696\nEpsilon = 0.1694091292164046\nEpsilon = 0.16939218830348296\nEpsilon = 0.16937524908465262\nEpsilon = 0.16935831155974415\nEpsilon = 0.16934137572858818\nEpsilon = 0.1693244415910153\nEpsilon = 0.16930750914685622\nAgent: ddqn_agent . Episode 962/2000. Number of steps to finish: 8. Loss: 6.2760725021362305 Reward: 4.0\nEpsilon = 0.16929057839594153\nEpsilon = 0.16927364933810193\nEpsilon = 0.16925672197316813\nEpsilon = 0.1692397963009708\nEpsilon = 0.1692228723213407\nEpsilon = 0.16920595003410857\nEpsilon = 0.16918902943910516\nEpsilon = 0.16917211053616124\nEpsilon = 0.16915519332510762\nEpsilon = 0.16913827780577512\nEpsilon = 0.16912136397799454\nEpsilon = 0.16910445184159675\nEpsilon = 0.1690875413964126\nEpsilon = 0.16907063264227296\nEpsilon = 0.16905372557900872\nEpsilon = 0.1690368202064508\nEpsilon = 0.16901991652443016\nEpsilon = 0.16900301453277772\nEpsilon = 0.16898611423132445\nEpsilon = 0.16896921561990133\nAgent: ddqn_agent . Episode 963/2000. Number of steps to finish: 20. Loss: 15.096521377563477 Reward: -12.0\nEpsilon = 0.16895231869833935\nEpsilon = 0.1689354234664695\nEpsilon = 0.16891852992412285\nEpsilon = 0.16890163807113043\nEpsilon = 0.16888474790732333\nEpsilon = 0.1688678594325326\nEpsilon = 0.16885097264658935\nEpsilon = 0.1688340875493247\nEpsilon = 0.16881720414056975\nEpsilon = 0.1688003224201557\nEpsilon = 0.16878344238791368\nEpsilon = 0.16876656404367488\nEpsilon = 0.1687496873872705\nEpsilon = 0.16873281241853177\nEpsilon = 0.1687159391372899\nEpsilon = 0.16869906754337619\nEpsilon = 0.16868219763662184\nEpsilon = 0.1686653294168582\nEpsilon = 0.1686484628839165\nEpsilon = 0.16863159803762812\nAgent: ddqn_agent . Episode 964/2000. Number of steps to finish: 20. Loss: 16.32952880859375 Reward: -16.0\nEpsilon = 0.16861473487782436\nEpsilon = 0.16859787340433657\nEpsilon = 0.16858101361699615\nEpsilon = 0.16856415551563445\nEpsilon = 0.16854729910008287\nEpsilon = 0.16853044437017287\nEpsilon = 0.16851359132573585\nEpsilon = 0.16849673996660328\nEpsilon = 0.16847989029260663\nEpsilon = 0.16846304230357736\nEpsilon = 0.16844619599934701\nEpsilon = 0.16842935137974707\nEpsilon = 0.1684125084446091\nAgent: ddqn_agent . Episode 965/2000. Number of steps to finish: 13. Loss: 9.435946464538574 Reward: -1.0\nEpsilon = 0.16839566719376464\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.16837882762704526\nEpsilon = 0.16836198974428254\nEpsilon = 0.16834515354530813\nEpsilon = 0.1683283190299536\nEpsilon = 0.1683114861980506\nEpsilon = 0.1682946550494308\nEpsilon = 0.16827782558392587\nEpsilon = 0.16826099780136747\nEpsilon = 0.16824417170158734\nEpsilon = 0.16822734728441718\nEpsilon = 0.16821052454968874\nEpsilon = 0.16819370349723378\nEpsilon = 0.16817688412688406\nEpsilon = 0.16816006643847137\nEpsilon = 0.1681432504318275\nEpsilon = 0.16812643610678432\nEpsilon = 0.16810962346317365\nEpsilon = 0.16809281250082733\nEpsilon = 0.16807600321957725\nAgent: ddqn_agent . Episode 966/2000. Number of steps to finish: 20. Loss: 15.444990158081055 Reward: -12.0\nEpsilon = 0.16805919561925528\nEpsilon = 0.16804238969969335\nEpsilon = 0.16802558546072338\nEpsilon = 0.1680087829021773\nEpsilon = 0.16799198202388707\nEpsilon = 0.1679751828256847\nEpsilon = 0.16795838530740212\nEpsilon = 0.1679415894688714\nEpsilon = 0.1679247953099245\nEpsilon = 0.1679080028303935\nEpsilon = 0.16789121203011045\nEpsilon = 0.16787442290890744\nEpsilon = 0.16785763546661656\nEpsilon = 0.1678408497030699\nEpsilon = 0.1678240656180996\nEpsilon = 0.1678072832115378\nEpsilon = 0.16779050248321664\nEpsilon = 0.1677737234329683\nEpsilon = 0.16775694606062502\nEpsilon = 0.16774017036601896\nAgent: ddqn_agent . Episode 967/2000. Number of steps to finish: 20. Loss: 15.081579208374023 Reward: -12.0\nEpsilon = 0.16772339634898237\nEpsilon = 0.16770662400934747\nEpsilon = 0.16768985334694653\nEpsilon = 0.16767308436161182\nEpsilon = 0.16765631705317566\nEpsilon = 0.16763955142147033\nEpsilon = 0.1676227874663282\nEpsilon = 0.16760602518758155\nEpsilon = 0.1675892645850628\nEpsilon = 0.1675725056586043\nEpsilon = 0.16755574840803844\nEpsilon = 0.16753899283319765\nEpsilon = 0.16752223893391432\nEpsilon = 0.16750548671002094\nEpsilon = 0.16748873616134993\nEpsilon = 0.1674719872877338\nEpsilon = 0.16745524008900503\nEpsilon = 0.16743849456499613\nEpsilon = 0.16742175071553964\nEpsilon = 0.16740500854046808\nAgent: ddqn_agent . Episode 968/2000. Number of steps to finish: 20. Loss: 15.605290412902832 Reward: -12.0\nEpsilon = 0.16738826803961404\nEpsilon = 0.1673715292128101\nEpsilon = 0.16735479205988882\nEpsilon = 0.16733805658068282\nEpsilon = 0.16732132277502476\nEpsilon = 0.16730459064274725\nEpsilon = 0.16728786018368297\nEpsilon = 0.1672711313976646\nEpsilon = 0.16725440428452484\nEpsilon = 0.1672376788440964\nEpsilon = 0.16722095507621199\nEpsilon = 0.16720423298070436\nEpsilon = 0.1671875125574063\nEpsilon = 0.16717079380615055\nEpsilon = 0.16715407672676993\nEpsilon = 0.16713736131909726\nEpsilon = 0.16712064758296535\nEpsilon = 0.16710393551820707\nEpsilon = 0.16708722512465526\nEpsilon = 0.1670705164021428\nAgent: ddqn_agent . Episode 969/2000. Number of steps to finish: 20. Loss: 15.682207107543945 Reward: -12.0\nEpsilon = 0.16705380935050257\nEpsilon = 0.16703710396956753\nEpsilon = 0.1670204002591706\nEpsilon = 0.16700369821914468\nEpsilon = 0.16698699784932278\nEpsilon = 0.16697029914953784\nEpsilon = 0.1669536021196229\nEpsilon = 0.16693690675941095\nEpsilon = 0.166920213068735\nEpsilon = 0.16690352104742812\nEpsilon = 0.16688683069532337\nEpsilon = 0.16687014201225384\nEpsilon = 0.16685345499805262\nEpsilon = 0.1668367696525528\nEpsilon = 0.16682008597558756\nEpsilon = 0.16680340396699\nEpsilon = 0.1667867236265933\nEpsilon = 0.16677004495423065\nEpsilon = 0.16675336794973522\nEpsilon = 0.16673669261294025\nAgent: ddqn_agent . Episode 970/2000. Number of steps to finish: 20. Loss: 16.164058685302734 Reward: -18.0\nEpsilon = 0.16672001894367897\nEpsilon = 0.1667033469417846\nEpsilon = 0.1666866766070904\nEpsilon = 0.1666700079394297\nEpsilon = 0.16665334093863576\nEpsilon = 0.1666366756045419\nEpsilon = 0.16662001193698145\nEpsilon = 0.16660334993578776\nEpsilon = 0.16658668960079417\nEpsilon = 0.1665700309318341\nEpsilon = 0.16655337392874092\nEpsilon = 0.16653671859134805\nEpsilon = 0.16652006491948892\nEpsilon = 0.16650341291299697\nEpsilon = 0.16648676257170567\nEpsilon = 0.1664701138954485\nEpsilon = 0.16645346688405896\nEpsilon = 0.16643682153737055\nEpsilon = 0.16642017785521682\nEpsilon = 0.1664035358374313\nAgent: ddqn_agent . Episode 971/2000. Number of steps to finish: 20. Loss: 14.943143844604492 Reward: -14.0\nEpsilon = 0.16638689548384758\nEpsilon = 0.1663702567942992\nEpsilon = 0.1663536197686198\nEpsilon = 0.16633698440664293\nEpsilon = 0.16632035070820228\nEpsilon = 0.16630371867313146\nEpsilon = 0.16628708830126415\nEpsilon = 0.16627045959243403\nEpsilon = 0.1662538325464748\nEpsilon = 0.16623720716322016\nEpsilon = 0.16622058344250384\nEpsilon = 0.1662039613841596\nEpsilon = 0.16618734098802118\nEpsilon = 0.16617072225392238\nEpsilon = 0.166154105181697\nEpsilon = 0.16613748977117881\nEpsilon = 0.16612087602220169\nEpsilon = 0.16610426393459946\nEpsilon = 0.166087653508206\nEpsilon = 0.1660710447428552\nAgent: ddqn_agent . Episode 972/2000. Number of steps to finish: 20. Loss: 14.386807441711426 Reward: -14.0\nEpsilon = 0.1660544376383809\nEpsilon = 0.16603783219461707\nEpsilon = 0.1660212284113976\nEpsilon = 0.16600462628855647\nEpsilon = 0.16598802582592762\nEpsilon = 0.16597142702334503\nEpsilon = 0.1659548298806427\nEpsilon = 0.16593823439765462\nEpsilon = 0.16592164057421485\nEpsilon = 0.16590504841015744\nEpsilon = 0.16588845790531642\nEpsilon = 0.1658718690595259\nEpsilon = 0.16585528187261994\nEpsilon = 0.16583869634443268\nEpsilon = 0.16582211247479825\nEpsilon = 0.16580553026355077\nEpsilon = 0.1657889497105244\nEpsilon = 0.16577237081555335\nEpsilon = 0.16575579357847178\nEpsilon = 0.16573921799911392\nAgent: ddqn_agent . Episode 973/2000. Number of steps to finish: 20. Loss: 15.675804138183594 Reward: -20.0\nEpsilon = 0.16572264407731402\nEpsilon = 0.16570607181290628\nEpsilon = 0.16568950120572498\nEpsilon = 0.16567293225560442\nEpsilon = 0.16565636496237887\nEpsilon = 0.16563979932588263\nEpsilon = 0.16562323534595005\nEpsilon = 0.16560667302241547\nEpsilon = 0.16559011235511323\nEpsilon = 0.16557355334387772\nEpsilon = 0.16555699598854334\nEpsilon = 0.1655404402889445\nEpsilon = 0.1655238862449156\nEpsilon = 0.1655073338562911\nEpsilon = 0.16549078312290547\nEpsilon = 0.1654742340445932\nEpsilon = 0.16545768662118873\nEpsilon = 0.16544114085252662\nEpsilon = 0.16542459673844137\nEpsilon = 0.16540805427876754\nAgent: ddqn_agent . Episode 974/2000. Number of steps to finish: 20. Loss: 14.50989055633545 Reward: -16.0\nEpsilon = 0.16539151347333966\nEpsilon = 0.16537497432199233\nEpsilon = 0.16535843682456014\nEpsilon = 0.1653419009808777\nEpsilon = 0.1653253667907796\nEpsilon = 0.16530883425410053\nEpsilon = 0.16529230337067513\nEpsilon = 0.16527577414033806\nEpsilon = 0.16525924656292404\nEpsilon = 0.16524272063826775\nEpsilon = 0.16522619636620392\nEpsilon = 0.1652096737465673\nEpsilon = 0.16519315277919266\nEpsilon = 0.16517663346391476\nEpsilon = 0.16516011580056836\nEpsilon = 0.1651435997889883\nEpsilon = 0.1651270854290094\nEpsilon = 0.1651105727204665\nEpsilon = 0.16509406166319446\nEpsilon = 0.16507755225702814\nAgent: ddqn_agent . Episode 975/2000. Number of steps to finish: 20. Loss: 15.695466995239258 Reward: -16.0\nEpsilon = 0.16506104450180242\nEpsilon = 0.16504453839735225\nEpsilon = 0.1650280339435125\nEpsilon = 0.16501153114011816\nEpsilon = 0.16499502998700413\nEpsilon = 0.16497853048400543\nEpsilon = 0.16496203263095705\nEpsilon = 0.16494553642769394\nEpsilon = 0.16492904187405116\nEpsilon = 0.16491254896986376\nEpsilon = 0.1648960577149668\nEpsilon = 0.1648795681091953\nEpsilon = 0.16486308015238438\nEpsilon = 0.16484659384436914\nEpsilon = 0.1648301091849847\nEpsilon = 0.1648136261740662\nEpsilon = 0.1647971448114488\nEpsilon = 0.16478066509696765\nEpsilon = 0.16476418703045795\nEpsilon = 0.16474771061175492\nAgent: ddqn_agent . Episode 976/2000. Number of steps to finish: 20. Loss: 14.851991653442383 Reward: -18.0\nEpsilon = 0.16473123584069374\nEpsilon = 0.16471476271710966\nEpsilon = 0.16469829124083796\nEpsilon = 0.16468182141171386\nEpsilon = 0.16466535322957268\nEpsilon = 0.16464888669424974\nEpsilon = 0.16463242180558033\nEpsilon = 0.16461595856339978\nEpsilon = 0.16459949696754345\nEpsilon = 0.16458303701784668\nEpsilon = 0.1645665787141449\nEpsilon = 0.16455012205627348\nEpsilon = 0.16453366704406786\nEpsilon = 0.16451721367736344\nEpsilon = 0.1645007619559957\nEpsilon = 0.1644843118798001\nEpsilon = 0.1644678634486121\nEpsilon = 0.16445141666226726\nEpsilon = 0.16443497152060102\nEpsilon = 0.16441852802344897\nAgent: ddqn_agent . Episode 977/2000. Number of steps to finish: 20. Loss: 15.074460983276367 Reward: -20.0\nEpsilon = 0.16440208617064664\nEpsilon = 0.16438564596202956\nEpsilon = 0.16436920739743335\nEpsilon = 0.1643527704766936\nEpsilon = 0.16433633519964594\nEpsilon = 0.16431990156612597\nEpsilon = 0.16430346957596936\nEpsilon = 0.16428703922901178\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.16427061052508887\nEpsilon = 0.16425418346403636\nEpsilon = 0.16423775804568996\nEpsilon = 0.1642213342698854\nEpsilon = 0.1642049121364584\nEpsilon = 0.16418849164524477\nEpsilon = 0.16417207279608023\nEpsilon = 0.16415565558880063\nEpsilon = 0.16413924002324176\nEpsilon = 0.16412282609923945\nEpsilon = 0.16410641381662952\nEpsilon = 0.16409000317524786\nAgent: ddqn_agent . Episode 978/2000. Number of steps to finish: 20. Loss: 15.179502487182617 Reward: -20.0\nEpsilon = 0.16407359417493034\nEpsilon = 0.16405718681551285\nEpsilon = 0.1640407810968313\nEpsilon = 0.1640243770187216\nEpsilon = 0.16400797458101973\nEpsilon = 0.16399157378356163\nEpsilon = 0.16397517462618327\nEpsilon = 0.16395877710872064\nEpsilon = 0.16394238123100977\nEpsilon = 0.16392598699288666\nEpsilon = 0.16390959439418737\nEpsilon = 0.16389320343474795\nEpsilon = 0.16387681411440447\nEpsilon = 0.16386042643299303\nEpsilon = 0.16384404039034975\nEpsilon = 0.16382765598631072\nEpsilon = 0.1638112732207121\nEpsilon = 0.16379489209339002\nEpsilon = 0.16377851260418066\nEpsilon = 0.16376213475292026\nAgent: ddqn_agent . Episode 979/2000. Number of steps to finish: 20. Loss: 16.009763717651367 Reward: -14.0\nEpsilon = 0.16374575853944498\nEpsilon = 0.16372938396359105\nEpsilon = 0.1637130110251947\nEpsilon = 0.16369663972409218\nEpsilon = 0.16368027006011976\nEpsilon = 0.16366390203311376\nEpsilon = 0.16364753564291046\nEpsilon = 0.16363117088934617\nEpsilon = 0.16361480777225723\nEpsilon = 0.16359844629148002\nEpsilon = 0.16358208644685088\nEpsilon = 0.1635657282382062\nEpsilon = 0.1635493716653824\nEpsilon = 0.16353301672821585\nEpsilon = 0.16351666342654303\nEpsilon = 0.16350031176020038\nEpsilon = 0.16348396172902438\nEpsilon = 0.16346761333285148\nEpsilon = 0.1634512665715182\nEpsilon = 0.16343492144486105\nAgent: ddqn_agent . Episode 980/2000. Number of steps to finish: 20. Loss: 14.748603820800781 Reward: -14.0\nEpsilon = 0.16341857795271655\nEpsilon = 0.16340223609492127\nEpsilon = 0.16338589587131178\nEpsilon = 0.16336955728172464\nEpsilon = 0.16335322032599647\nEpsilon = 0.16333688500396387\nEpsilon = 0.16332055131546347\nEpsilon = 0.16330421926033192\nEpsilon = 0.1632878888384059\nEpsilon = 0.16327156004952206\nEpsilon = 0.16325523289351712\nEpsilon = 0.16323890737022778\nEpsilon = 0.16322258347949076\nEpsilon = 0.16320626122114282\nEpsilon = 0.1631899405950207\nEpsilon = 0.16317362160096122\nEpsilon = 0.16315730423880112\nEpsilon = 0.16314098850837724\nEpsilon = 0.1631246744095264\nEpsilon = 0.16310836194208547\nAgent: ddqn_agent . Episode 981/2000. Number of steps to finish: 20. Loss: 14.797865867614746 Reward: -12.0\nEpsilon = 0.16309205110589126\nEpsilon = 0.16307574190078067\nEpsilon = 0.1630594343265906\nEpsilon = 0.16304312838315793\nEpsilon = 0.16302682407031963\nEpsilon = 0.16301052138791258\nEpsilon = 0.1629942203357738\nEpsilon = 0.16297792091374022\nEpsilon = 0.16296162312164886\nEpsilon = 0.1629453269593367\nEpsilon = 0.16292903242664075\nEpsilon = 0.1629127395233981\nEpsilon = 0.16289644824944577\nEpsilon = 0.16288015860462082\nEpsilon = 0.16286387058876037\nEpsilon = 0.1628475842017015\nEpsilon = 0.16283129944328134\nEpsilon = 0.162815016313337\nEpsilon = 0.16279873481170568\nEpsilon = 0.16278245493822452\nAgent: ddqn_agent . Episode 982/2000. Number of steps to finish: 20. Loss: 14.741007804870605 Reward: -14.0\nEpsilon = 0.1627661766927307\nEpsilon = 0.16274990007506143\nEpsilon = 0.16273362508505393\nEpsilon = 0.16271735172254542\nEpsilon = 0.16270107998737318\nEpsilon = 0.16268480987937445\nEpsilon = 0.16266854139838652\nEpsilon = 0.1626522745442467\nEpsilon = 0.16263600931679226\nEpsilon = 0.16261974571586058\nEpsilon = 0.162603483741289\nEpsilon = 0.16258722339291487\nEpsilon = 0.16257096467057558\nEpsilon = 0.16255470757410853\nEpsilon = 0.16253845210335113\nEpsilon = 0.1625221982581408\nEpsilon = 0.16250594603831497\nEpsilon = 0.16248969544371114\nEpsilon = 0.16247344647416678\nEpsilon = 0.16245719912951936\nAgent: ddqn_agent . Episode 983/2000. Number of steps to finish: 20. Loss: 14.964764595031738 Reward: -12.0\nEpsilon = 0.1624409534096064\nEpsilon = 0.16242470931426545\nEpsilon = 0.16240846684333402\nEpsilon = 0.16239222599664968\nEpsilon = 0.16237598677405002\nEpsilon = 0.1623597491753726\nEpsilon = 0.16234351320045506\nEpsilon = 0.16232727884913503\nEpsilon = 0.1623110461212501\nEpsilon = 0.16229481501663798\nEpsilon = 0.16227858553513633\nEpsilon = 0.1622623576765828\nEpsilon = 0.16224613144081515\nEpsilon = 0.16222990682767108\nEpsilon = 0.1622136838369883\nEpsilon = 0.16219746246860461\nEpsilon = 0.16218124272235776\nEpsilon = 0.16216502459808552\nEpsilon = 0.1621488080956257\nEpsilon = 0.16213259321481616\nAgent: ddqn_agent . Episode 984/2000. Number of steps to finish: 20. Loss: 15.700479507446289 Reward: -10.0\nEpsilon = 0.1621163799554947\nEpsilon = 0.16210016831749913\nEpsilon = 0.16208395830066738\nEpsilon = 0.16206774990483733\nEpsilon = 0.16205154312984685\nEpsilon = 0.16203533797553388\nEpsilon = 0.16201913444173632\nEpsilon = 0.16200293252829215\nEpsilon = 0.16198673223503932\nEpsilon = 0.16197053356181582\nEpsilon = 0.16195433650845964\nEpsilon = 0.1619381410748088\nEpsilon = 0.16192194726070133\nEpsilon = 0.16190575506597527\nEpsilon = 0.16188956449046868\nEpsilon = 0.16187337553401965\nEpsilon = 0.16185718819646625\nEpsilon = 0.1618410024776466\nEpsilon = 0.16182481837739884\nEpsilon = 0.1618086358955611\nAgent: ddqn_agent . Episode 985/2000. Number of steps to finish: 20. Loss: 15.717479705810547 Reward: -10.0\nEpsilon = 0.16179245503197154\nEpsilon = 0.16177627578646833\nEpsilon = 0.1617600981588897\nEpsilon = 0.16174392214907382\nEpsilon = 0.16172774775685891\nEpsilon = 0.16171157498208322\nEpsilon = 0.161695403824585\nEpsilon = 0.16167923428420256\nEpsilon = 0.16166306636077413\nEpsilon = 0.16164690005413807\nEpsilon = 0.16163073536413267\nEpsilon = 0.16161457229059625\nEpsilon = 0.1615984108333672\nEpsilon = 0.16158225099228388\nEpsilon = 0.16156609276718464\nEpsilon = 0.16154993615790791\nEpsilon = 0.16153378116429212\nEpsilon = 0.1615176277861757\nEpsilon = 0.1615014760233971\nEpsilon = 0.16148532587579476\nAgent: ddqn_agent . Episode 986/2000. Number of steps to finish: 20. Loss: 15.024700164794922 Reward: -10.0\nEpsilon = 0.16146917734320718\nEpsilon = 0.16145303042547285\nEpsilon = 0.1614368851224303\nEpsilon = 0.16142074143391805\nEpsilon = 0.16140459935977466\nEpsilon = 0.1613884588998387\nEpsilon = 0.1613723200539487\nEpsilon = 0.1613561828219433\nEpsilon = 0.1613400472036611\nEpsilon = 0.16132391319894074\nEpsilon = 0.16130778080762084\nEpsilon = 0.16129165002954007\nEpsilon = 0.16127552086453711\nEpsilon = 0.16125939331245065\nEpsilon = 0.16124326737311942\nEpsilon = 0.1612271430463821\nEpsilon = 0.1612110203320775\nEpsilon = 0.1611948992300443\nEpsilon = 0.1611787797401213\nEpsilon = 0.16116266186214728\nAgent: ddqn_agent . Episode 987/2000. Number of steps to finish: 20. Loss: 16.636606216430664 Reward: -14.0\nEpsilon = 0.16114654559596106\nEpsilon = 0.16113043094140148\nEpsilon = 0.16111431789830732\nEpsilon = 0.16109820646651749\nEpsilon = 0.16108209664587084\nEpsilon = 0.16106598843620626\nEpsilon = 0.16104988183736263\nEpsilon = 0.16103377684917888\nEpsilon = 0.16101767347149396\nEpsilon = 0.1610015717041468\nEpsilon = 0.1609854715469764\nEpsilon = 0.1609693729998217\nEpsilon = 0.16095327606252172\nEpsilon = 0.16093718073491547\nEpsilon = 0.160921087016842\nEpsilon = 0.1609049949081403\nEpsilon = 0.1608889044086495\nEpsilon = 0.16087281551820865\nEpsilon = 0.16085672823665684\nEpsilon = 0.16084064256383318\nAgent: ddqn_agent . Episode 988/2000. Number of steps to finish: 20. Loss: 14.754561424255371 Reward: -14.0\nEpsilon = 0.1608245584995768\nEpsilon = 0.16080847604372683\nEpsilon = 0.16079239519612246\nEpsilon = 0.16077631595660286\nEpsilon = 0.1607602383250072\nEpsilon = 0.1607441623011747\nEpsilon = 0.16072808788494458\nEpsilon = 0.1607120150761561\nEpsilon = 0.16069594387464847\nEpsilon = 0.160679874280261\nEpsilon = 0.16066380629283297\nEpsilon = 0.16064773991220369\nEpsilon = 0.16063167513821247\nEpsilon = 0.16061561197069865\nEpsilon = 0.16059955040950158\nEpsilon = 0.16058349045446063\nEpsilon = 0.1605674321054152\nEpsilon = 0.16055137536220465\nEpsilon = 0.16053532022466843\nEpsilon = 0.16051926669264596\nAgent: ddqn_agent . Episode 989/2000. Number of steps to finish: 20. Loss: 16.008384704589844 Reward: -10.0\nEpsilon = 0.1605032147659767\nEpsilon = 0.16048716444450012\nEpsilon = 0.16047111572805567\nEpsilon = 0.16045506861648287\nEpsilon = 0.16043902310962121\nEpsilon = 0.16042297920731025\nEpsilon = 0.1604069369093895\nEpsilon = 0.16039089621569858\nEpsilon = 0.16037485712607702\nEpsilon = 0.16035881964036441\nEpsilon = 0.1603427837584004\nEpsilon = 0.16032674948002454\nEpsilon = 0.16031071680507653\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.16029468573339603\nEpsilon = 0.16027865626482268\nEpsilon = 0.1602626283991962\nEpsilon = 0.16024660213635628\nEpsilon = 0.16023057747614264\nEpsilon = 0.16021455441839502\nEpsilon = 0.1601985329629532\nAgent: ddqn_agent . Episode 990/2000. Number of steps to finish: 20. Loss: 14.966935157775879 Reward: -18.0\nEpsilon = 0.1601825131096569\nEpsilon = 0.16016649485834594\nEpsilon = 0.1601504782088601\nEpsilon = 0.16013446316103921\nEpsilon = 0.1601184497147231\nEpsilon = 0.16010243786975165\nEpsilon = 0.16008642762596467\nEpsilon = 0.16007041898320207\nEpsilon = 0.16005441194130374\nEpsilon = 0.16003840650010961\nEpsilon = 0.1600224026594596\nEpsilon = 0.16000640041919367\nEpsilon = 0.15999039977915175\nEpsilon = 0.15997440073917382\nEpsilon = 0.1599584032990999\nEpsilon = 0.15994240745877\nEpsilon = 0.15992641321802412\nEpsilon = 0.15991042057670232\nEpsilon = 0.15989442953464467\nEpsilon = 0.1598784400916912\nAgent: ddqn_agent . Episode 991/2000. Number of steps to finish: 20. Loss: 15.718061447143555 Reward: -12.0\nEpsilon = 0.15986245224768203\nEpsilon = 0.15984646600245728\nEpsilon = 0.15983048135585703\nEpsilon = 0.15981449830772146\nEpsilon = 0.15979851685789068\nEpsilon = 0.1597825370062049\nEpsilon = 0.15976655875250428\nEpsilon = 0.15975058209662904\nEpsilon = 0.15973460703841938\nEpsilon = 0.15971863357771554\nEpsilon = 0.15970266171435776\nEpsilon = 0.15968669144818634\nEpsilon = 0.1596707227790415\nEpsilon = 0.1596547557067636\nEpsilon = 0.15963879023119293\nEpsilon = 0.15962282635216982\nEpsilon = 0.1596068640695346\nEpsilon = 0.15959090338312765\nEpsilon = 0.15957494429278934\nEpsilon = 0.15955898679836006\nAgent: ddqn_agent . Episode 992/2000. Number of steps to finish: 20. Loss: 15.171487808227539 Reward: -14.0\nEpsilon = 0.15954303089968022\nEpsilon = 0.15952707659659027\nEpsilon = 0.1595111238889306\nEpsilon = 0.1594951727765417\nEpsilon = 0.15947922325926406\nEpsilon = 0.15946327533693813\nEpsilon = 0.15944732900940445\nEpsilon = 0.1594313842765035\nEpsilon = 0.15941544113807585\nEpsilon = 0.15939949959396205\nEpsilon = 0.15938355964400266\nEpsilon = 0.15936762128803827\nEpsilon = 0.15935168452590948\nEpsilon = 0.1593357493574569\nEpsilon = 0.15931981578252116\nEpsilon = 0.1593038838009429\nEpsilon = 0.1592879534125628\nAgent: ddqn_agent . Episode 993/2000. Number of steps to finish: 17. Loss: 12.576664924621582 Reward: -5.0\nEpsilon = 0.15927202461722154\nEpsilon = 0.15925609741475982\nEpsilon = 0.15924017180501834\nEpsilon = 0.15922424778783784\nEpsilon = 0.15920832536305907\nEpsilon = 0.15919240453052277\nEpsilon = 0.15917648529006972\nEpsilon = 0.1591605676415407\nEpsilon = 0.15914465158477656\nEpsilon = 0.15912873711961809\nEpsilon = 0.15911282424590611\nEpsilon = 0.15909691296348152\nEpsilon = 0.1590810032721852\nEpsilon = 0.15906509517185796\nEpsilon = 0.15904918866234077\nEpsilon = 0.15903328374347453\nEpsilon = 0.1590173804151002\nEpsilon = 0.1590014786770587\nEpsilon = 0.15898557852919099\nEpsilon = 0.15896967997133807\nAgent: ddqn_agent . Episode 994/2000. Number of steps to finish: 20. Loss: 15.280540466308594 Reward: -16.0\nEpsilon = 0.15895378300334093\nEpsilon = 0.1589378876250406\nEpsilon = 0.1589219938362781\nEpsilon = 0.15890610163689448\nEpsilon = 0.1588902110267308\nEpsilon = 0.15887432200562812\nEpsilon = 0.15885843457342755\nEpsilon = 0.1588425487299702\nEpsilon = 0.1588266644750972\nEpsilon = 0.1588107818086497\nEpsilon = 0.1587949007304688\nEpsilon = 0.15877902124039578\nEpsilon = 0.15876314333827174\nEpsilon = 0.1587472670239379\nEpsilon = 0.1587313922972355\nEpsilon = 0.1587155191580058\nEpsilon = 0.15869964760609\nEpsilon = 0.1586837776413294\nEpsilon = 0.15866790926356528\nEpsilon = 0.15865204247263892\nAgent: ddqn_agent . Episode 995/2000. Number of steps to finish: 20. Loss: 14.992941856384277 Reward: -16.0\nEpsilon = 0.15863617726839166\nEpsilon = 0.1586203136506648\nEpsilon = 0.15860445161929976\nEpsilon = 0.15858859117413782\nEpsilon = 0.1585727323150204\nEpsilon = 0.1585568750417889\nEpsilon = 0.15854101935428472\nEpsilon = 0.1585251652523493\nEpsilon = 0.15850931273582408\nEpsilon = 0.1584934618045505\nEpsilon = 0.15847761245837005\nEpsilon = 0.15846176469712422\nEpsilon = 0.1584459185206545\nEpsilon = 0.15843007392880246\nEpsilon = 0.15841423092140958\nEpsilon = 0.15839838949831744\nEpsilon = 0.1583825496593676\nEpsilon = 0.15836671140440167\nEpsilon = 0.15835087473326123\nEpsilon = 0.1583350396457879\nAgent: ddqn_agent . Episode 996/2000. Number of steps to finish: 20. Loss: 14.743292808532715 Reward: -14.0\nEpsilon = 0.15831920614182332\nEpsilon = 0.15830337422120913\nEpsilon = 0.15828754388378702\nEpsilon = 0.15827171512939864\nEpsilon = 0.1582558879578857\nEpsilon = 0.1582400623690899\nEpsilon = 0.15822423836285301\nEpsilon = 0.15820841593901674\nEpsilon = 0.15819259509742284\nEpsilon = 0.1581767758379131\nEpsilon = 0.1581609581603293\nEpsilon = 0.15814514206451327\nEpsilon = 0.15812932755030681\nEpsilon = 0.1581135146175518\nEpsilon = 0.15809770326609004\nEpsilon = 0.15808189349576343\nEpsilon = 0.15806608530641386\nEpsilon = 0.1580502786978832\nEpsilon = 0.15803447367001341\nEpsilon = 0.1580186702226464\nAgent: ddqn_agent . Episode 997/2000. Number of steps to finish: 20. Loss: 15.228343963623047 Reward: -14.0\nEpsilon = 0.15800286835562413\nEpsilon = 0.15798706806878857\nEpsilon = 0.1579712693619817\nEpsilon = 0.1579554722350455\nEpsilon = 0.157939676687822\nEpsilon = 0.1579238827201532\nEpsilon = 0.1579080903318812\nEpsilon = 0.157892299522848\nEpsilon = 0.1578765102928957\nEpsilon = 0.15786072264186643\nEpsilon = 0.15784493656960225\nEpsilon = 0.15782915207594528\nEpsilon = 0.15781336916073768\nEpsilon = 0.1577975878238216\nEpsilon = 0.15778180806503922\nEpsilon = 0.15776602988423272\nEpsilon = 0.15775025328124428\nEpsilon = 0.15773447825591616\nEpsilon = 0.15771870480809058\nEpsilon = 0.15770293293760979\nAgent: ddqn_agent . Episode 998/2000. Number of steps to finish: 20. Loss: 15.02096176147461 Reward: -10.0\nEpsilon = 0.15768716264431604\nEpsilon = 0.1576713939280516\nEpsilon = 0.1576556267886588\nEpsilon = 0.15763986122597992\nEpsilon = 0.15762409723985732\nEpsilon = 0.15760833483013334\nEpsilon = 0.15759257399665033\nEpsilon = 0.15757681473925067\nEpsilon = 0.15756105705777676\nEpsilon = 0.15754530095207098\nEpsilon = 0.15752954642197578\nEpsilon = 0.15751379346733357\nEpsilon = 0.15749804208798684\nEpsilon = 0.15748229228377805\nEpsilon = 0.15746654405454968\nEpsilon = 0.15745079740014423\nEpsilon = 0.15743505232040422\nEpsilon = 0.1574193088151722\nEpsilon = 0.15740356688429066\nEpsilon = 0.15738782652760225\nAgent: ddqn_agent . Episode 999/2000. Number of steps to finish: 20. Loss: 14.545248031616211 Reward: -10.0\nEpsilon = 0.15737208774494948\nEpsilon = 0.157356350536175\nEpsilon = 0.1573406149011214\nEpsilon = 0.15732488083963128\nEpsilon = 0.15730914835154733\nEpsilon = 0.15729341743671219\nEpsilon = 0.1572776880949685\nEpsilon = 0.15726196032615902\nEpsilon = 0.15724623413012642\nEpsilon = 0.1572305095067134\nEpsilon = 0.15721478645576273\nEpsilon = 0.15719906497711716\nEpsilon = 0.15718334507061946\nEpsilon = 0.1571676267361124\nEpsilon = 0.15715190997343878\nEpsilon = 0.15713619478244145\nEpsilon = 0.1571204811629632\nEpsilon = 0.15710476911484691\nEpsilon = 0.15708905863793543\nEpsilon = 0.15707334973207163\nAgent: ddqn_agent . Episode 1000/2000. Number of steps to finish: 20. Loss: 15.419886589050293 Reward: -16.0\nEpsilon = 0.15705764239709843\nEpsilon = 0.15704193663285873\nEpsilon = 0.15702623243919545\nEpsilon = 0.15701052981595154\nEpsilon = 0.15699482876296994\nEpsilon = 0.15697912928009364\nEpsilon = 0.15696343136716562\nEpsilon = 0.1569477350240289\nEpsilon = 0.15693204025052648\nEpsilon = 0.15691634704650143\nEpsilon = 0.15690065541179676\nEpsilon = 0.1568849653462556\nEpsilon = 0.15686927684972096\nEpsilon = 0.156853589922036\nEpsilon = 0.15683790456304378\nEpsilon = 0.15682222077258748\nEpsilon = 0.15680653855051022\nEpsilon = 0.15679085789665517\nEpsilon = 0.1567751788108655\nEpsilon = 0.1567595012929844\nAgent: ddqn_agent . Episode 1001/2000. Number of steps to finish: 20. Loss: 14.957934379577637 Reward: -14.0\nEpsilon = 0.1567438253428551\nEpsilon = 0.15672815096032083\nEpsilon = 0.1567124781452248\nEpsilon = 0.15669680689741028\nEpsilon = 0.15668113721672053\nEpsilon = 0.15666546910299886\nEpsilon = 0.15664980255608857\nEpsilon = 0.15663413757583297\nEpsilon = 0.1566184741620754\nEpsilon = 0.1566028123146592\nEpsilon = 0.15658715203342774\nEpsilon = 0.1565714933182244\nEpsilon = 0.15655583616889257\nEpsilon = 0.1565401805852757\nEpsilon = 0.15652452656721716\nEpsilon = 0.15650887411456044\nEpsilon = 0.15649322322714898\nEpsilon = 0.15647757390482628\nEpsilon = 0.1564619261474358\nEpsilon = 0.15644627995482105\nAgent: ddqn_agent . Episode 1002/2000. Number of steps to finish: 20. Loss: 14.659811019897461 Reward: -14.0\nEpsilon = 0.15643063532682558\nEpsilon = 0.1564149922632929\nEpsilon = 0.15639935076406655\nEpsilon = 0.15638371082899014\nEpsilon = 0.15636807245790724\nEpsilon = 0.15635243565066145\nEpsilon = 0.15633680040709638\nEpsilon = 0.15632116672705568\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.15630553461038296\nEpsilon = 0.15628990405692192\nEpsilon = 0.15627427506651623\nEpsilon = 0.15625864763900957\nEpsilon = 0.15624302177424568\nEpsilon = 0.15622739747206826\nEpsilon = 0.15621177473232106\nEpsilon = 0.15619615355484784\nEpsilon = 0.15618053393949236\nEpsilon = 0.1561649158860984\nEpsilon = 0.1561492993945098\nEpsilon = 0.15613368446457035\nAgent: ddqn_agent . Episode 1003/2000. Number of steps to finish: 20. Loss: 16.025205612182617 Reward: -18.0\nEpsilon = 0.15611807109612388\nEpsilon = 0.15610245928901428\nEpsilon = 0.1560868490430854\nEpsilon = 0.1560712403581811\nEpsilon = 0.15605563323414529\nEpsilon = 0.15604002767082187\nEpsilon = 0.1560244236680548\nEpsilon = 0.156008821225688\nEpsilon = 0.15599322034356541\nEpsilon = 0.15597762102153107\nEpsilon = 0.1559620232594289\nEpsilon = 0.15594642705710296\nEpsilon = 0.15593083241439726\nEpsilon = 0.1559152393311558\nEpsilon = 0.1558996478072227\nEpsilon = 0.155884057842442\nEpsilon = 0.15586846943665775\nEpsilon = 0.15585288258971408\nEpsilon = 0.15583729730145512\nEpsilon = 0.15582171357172497\nAgent: ddqn_agent . Episode 1004/2000. Number of steps to finish: 20. Loss: 15.377913475036621 Reward: -20.0\nEpsilon = 0.1558061314003678\nEpsilon = 0.15579055078722778\nEpsilon = 0.15577497173214905\nEpsilon = 0.15575939423497584\nEpsilon = 0.15574381829555234\nEpsilon = 0.15572824391372278\nEpsilon = 0.1557126710893314\nEpsilon = 0.15569709982222246\nEpsilon = 0.15568153011224023\nEpsilon = 0.155665961959229\nEpsilon = 0.15565039536303307\nEpsilon = 0.15563483032349676\nEpsilon = 0.1556192668404644\nEpsilon = 0.15560370491378037\nEpsilon = 0.15558814454328898\nEpsilon = 0.15557258572883464\nEpsilon = 0.15555702847026176\nEpsilon = 0.15554147276741473\nEpsilon = 0.15552591862013798\nEpsilon = 0.15551036602827598\nAgent: ddqn_agent . Episode 1005/2000. Number of steps to finish: 20. Loss: 16.09004783630371 Reward: -10.0\nEpsilon = 0.15549481499167314\nEpsilon = 0.15547926551017396\nEpsilon = 0.15546371758362296\nEpsilon = 0.1554481712118646\nEpsilon = 0.15543262639474342\nEpsilon = 0.15541708313210395\nEpsilon = 0.15540154142379073\nEpsilon = 0.15538600126964836\nEpsilon = 0.1553704626695214\nEpsilon = 0.15535492562325445\nEpsilon = 0.15533939013069212\nEpsilon = 0.15532385619167904\nEpsilon = 0.15530832380605988\nEpsilon = 0.1552927929736793\nEpsilon = 0.1552772636943819\nEpsilon = 0.1552617359680125\nEpsilon = 0.1552462097944157\nEpsilon = 0.15523068517343624\nEpsilon = 0.1552151621049189\nEpsilon = 0.15519964058870842\nAgent: ddqn_agent . Episode 1006/2000. Number of steps to finish: 20. Loss: 15.180109977722168 Reward: -14.0\nEpsilon = 0.15518412062464954\nEpsilon = 0.15516860221258708\nEpsilon = 0.15515308535236583\nEpsilon = 0.15513757004383058\nEpsilon = 0.1551220562868262\nEpsilon = 0.15510654408119753\nEpsilon = 0.15509103342678943\nEpsilon = 0.15507552432344676\nEpsilon = 0.15506001677101441\nEpsilon = 0.1550445107693373\nEpsilon = 0.1550290063182604\nEpsilon = 0.15501350341762857\nEpsilon = 0.1549980020672868\nEpsilon = 0.15498250226708007\nEpsilon = 0.15496700401685337\nEpsilon = 0.1549515073164517\nEpsilon = 0.15493601216572003\nEpsilon = 0.15492051856450345\nEpsilon = 0.154905026512647\nEpsilon = 0.15488953600999575\nAgent: ddqn_agent . Episode 1007/2000. Number of steps to finish: 20. Loss: 15.804938316345215 Reward: -16.0\nEpsilon = 0.15487404705639474\nEpsilon = 0.1548585596516891\nEpsilon = 0.15484307379572393\nEpsilon = 0.15482758948834435\nEpsilon = 0.15481210672939552\nEpsilon = 0.15479662551872259\nEpsilon = 0.15478114585617073\nEpsilon = 0.15476566774158512\nEpsilon = 0.15475019117481095\nEpsilon = 0.15473471615569348\nEpsilon = 0.1547192426840779\nEpsilon = 0.1547037707598095\nEpsilon = 0.15468830038273354\nEpsilon = 0.15467283155269526\nEpsilon = 0.15465736426954\nEpsilon = 0.15464189853311305\nEpsilon = 0.15462643434325973\nEpsilon = 0.1546109716998254\nEpsilon = 0.15459551060265542\nEpsilon = 0.15458005105159514\nAgent: ddqn_agent . Episode 1008/2000. Number of steps to finish: 20. Loss: 15.880887985229492 Reward: -14.0\nEpsilon = 0.15456459304648998\nEpsilon = 0.15454913658718533\nEpsilon = 0.1545336816735266\nEpsilon = 0.15451822830535925\nEpsilon = 0.15450277648252872\nEpsilon = 0.15448732620488048\nEpsilon = 0.15447187747226\nEpsilon = 0.15445643028451275\nEpsilon = 0.1544409846414843\nEpsilon = 0.15442554054302016\nEpsilon = 0.15441009798896585\nEpsilon = 0.15439465697916696\nEpsilon = 0.15437921751346906\nEpsilon = 0.1543637795917177\nEpsilon = 0.15434834321375854\nEpsilon = 0.15433290837943717\nEpsilon = 0.15431747508859922\nEpsilon = 0.15430204334109035\nEpsilon = 0.15428661313675623\nEpsilon = 0.15427118447544255\nAgent: ddqn_agent . Episode 1009/2000. Number of steps to finish: 20. Loss: 14.8958740234375 Reward: -14.0\nEpsilon = 0.154255757356995\nEpsilon = 0.15424033178125932\nEpsilon = 0.1542249077480812\nEpsilon = 0.1542094852573064\nEpsilon = 0.1541940643087807\nEpsilon = 0.1541786449023498\nEpsilon = 0.15416322703785956\nEpsilon = 0.1541478107151558\nEpsilon = 0.1541323959340843\nEpsilon = 0.15411698269449087\nEpsilon = 0.15410157099622143\nEpsilon = 0.1540861608391218\nEpsilon = 0.1540707522230379\nEpsilon = 0.1540553451478156\nEpsilon = 0.1540399396133008\nEpsilon = 0.1540245356193395\nEpsilon = 0.15400913316577755\nEpsilon = 0.15399373225246096\nEpsilon = 0.1539783328792357\nEpsilon = 0.1539629350459478\nAgent: ddqn_agent . Episode 1010/2000. Number of steps to finish: 20. Loss: 16.48593521118164 Reward: -18.0\nEpsilon = 0.1539475387524432\nEpsilon = 0.15393214399856797\nEpsilon = 0.1539167507841681\nEpsilon = 0.1539013591090897\nEpsilon = 0.1538859689731788\nEpsilon = 0.15387058037628148\nEpsilon = 0.15385519331824385\nEpsilon = 0.15383980779891201\nEpsilon = 0.15382442381813213\nEpsilon = 0.15380904137575033\nEpsilon = 0.15379366047161275\nEpsilon = 0.1537782811055656\nEpsilon = 0.15376290327745504\nEpsilon = 0.1537475269871273\nEpsilon = 0.15373215223442857\nEpsilon = 0.15371677901920514\nEpsilon = 0.15370140734130322\nEpsilon = 0.1536860372005691\nEpsilon = 0.15367066859684902\nEpsilon = 0.15365530152998935\nAgent: ddqn_agent . Episode 1011/2000. Number of steps to finish: 20. Loss: 15.362495422363281 Reward: -12.0\nEpsilon = 0.15363993599983636\nEpsilon = 0.1536245720062364\nEpsilon = 0.15360920954903576\nEpsilon = 0.15359384862808084\nEpsilon = 0.15357848924321804\nEpsilon = 0.15356313139429373\nEpsilon = 0.1535477750811543\nEpsilon = 0.1535324203036462\nEpsilon = 0.15351706706161583\nEpsilon = 0.15350171535490967\nEpsilon = 0.1534863651833742\nEpsilon = 0.15347101654685585\nEpsilon = 0.15345566944520117\nEpsilon = 0.15344032387825665\nEpsilon = 0.15342497984586884\nEpsilon = 0.15340963734788426\nEpsilon = 0.15339429638414948\nEpsilon = 0.15337895695451106\nEpsilon = 0.15336361905881563\nEpsilon = 0.15334828269690975\nAgent: ddqn_agent . Episode 1012/2000. Number of steps to finish: 20. Loss: 16.67255401611328 Reward: -16.0\nEpsilon = 0.15333294786864007\nEpsilon = 0.1533176145738532\nEpsilon = 0.15330228281239583\nEpsilon = 0.1532869525841146\nEpsilon = 0.15327162388885618\nEpsilon = 0.1532562967264673\nEpsilon = 0.15324097109679463\nEpsilon = 0.15322564699968497\nEpsilon = 0.153210324434985\nEpsilon = 0.1531950034025415\nEpsilon = 0.15317968390220124\nEpsilon = 0.15316436593381103\nEpsilon = 0.15314904949721764\nEpsilon = 0.15313373459226792\nAgent: ddqn_agent . Episode 1013/2000. Number of steps to finish: 14. Loss: 10.488513946533203 Reward: -2.0\nEpsilon = 0.1531184212188087\nEpsilon = 0.1531031093766868\nEpsilon = 0.15308779906574915\nEpsilon = 0.15307249028584258\nEpsilon = 0.15305718303681398\nEpsilon = 0.1530418773185103\nEpsilon = 0.15302657313077844\nEpsilon = 0.15301127047346536\nEpsilon = 0.15299596934641801\nEpsilon = 0.15298066974948338\nEpsilon = 0.15296537168250843\nEpsilon = 0.15295007514534018\nEpsilon = 0.15293478013782563\nEpsilon = 0.15291948665981187\nEpsilon = 0.15290419471114589\nEpsilon = 0.15288890429167476\nEpsilon = 0.15287361540124558\nEpsilon = 0.15285832803970545\nEpsilon = 0.15284304220690148\nEpsilon = 0.1528277579026808\nAgent: ddqn_agent . Episode 1014/2000. Number of steps to finish: 20. Loss: 15.654583930969238 Reward: -12.0\nEpsilon = 0.15281247512689053\nEpsilon = 0.15279719387937785\nEpsilon = 0.1527819141599899\nEpsilon = 0.1527666359685739\nEpsilon = 0.15275135930497705\nEpsilon = 0.15273608416904655\nEpsilon = 0.15272081056062964\nEpsilon = 0.15270553847957358\nEpsilon = 0.15269026792572563\nEpsilon = 0.15267499889893305\nEpsilon = 0.15265973139904315\nEpsilon = 0.15264446542590324\nEpsilon = 0.15262920097936064\nEpsilon = 0.1526139380592627\nEpsilon = 0.1525986766654568\nEpsilon = 0.15258341679779025\nEpsilon = 0.15256815845611046\nEpsilon = 0.15255290164026486\nEpsilon = 0.15253764635010084\nEpsilon = 0.15252239258546582\nAgent: ddqn_agent . Episode 1015/2000. Number of steps to finish: 20. Loss: 15.957953453063965 Reward: -14.0\nEpsilon = 0.15250714034620727\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.15249188963217264\nEpsilon = 0.1524766404432094\nEpsilon = 0.1524613927791651\nEpsilon = 0.15244614663988718\nEpsilon = 0.1524309020252232\nEpsilon = 0.15241565893502068\nEpsilon = 0.15240041736912718\nEpsilon = 0.15238517732739026\nEpsilon = 0.15236993880965752\nEpsilon = 0.15235470181577657\nEpsilon = 0.152339466345595\nEpsilon = 0.15232423239896045\nEpsilon = 0.15230899997572056\nEpsilon = 0.15229376907572298\nEpsilon = 0.1522785396988154\nEpsilon = 0.15226331184484554\nEpsilon = 0.15224808551366106\nEpsilon = 0.1522328607051097\nEpsilon = 0.15221763741903918\nAgent: ddqn_agent . Episode 1016/2000. Number of steps to finish: 20. Loss: 14.831598281860352 Reward: -18.0\nEpsilon = 0.15220241565529727\nEpsilon = 0.15218719541373174\nEpsilon = 0.15217197669419036\nEpsilon = 0.15215675949652094\nEpsilon = 0.1521415438205713\nEpsilon = 0.15212632966618925\nEpsilon = 0.15211111703322264\nEpsilon = 0.1520959059215193\nEpsilon = 0.15208069633092716\nEpsilon = 0.15206548826129407\nEpsilon = 0.15205028171246793\nEpsilon = 0.1520350766842967\nEpsilon = 0.15201987317662827\nEpsilon = 0.1520046711893106\nEpsilon = 0.15198947072219168\nEpsilon = 0.15197427177511946\nEpsilon = 0.15195907434794195\nEpsilon = 0.15194387844050716\nEpsilon = 0.1519286840526631\nEpsilon = 0.15191349118425784\nAgent: ddqn_agent . Episode 1017/2000. Number of steps to finish: 20. Loss: 14.77694034576416 Reward: -16.0\nEpsilon = 0.15189829983513942\nEpsilon = 0.1518831100051559\nEpsilon = 0.1518679216941554\nEpsilon = 0.15185273490198598\nEpsilon = 0.15183754962849577\nEpsilon = 0.15182236587353293\nEpsilon = 0.15180718363694556\nEpsilon = 0.15179200291858186\nEpsilon = 0.15177682371829002\nEpsilon = 0.15176164603591819\nEpsilon = 0.1517464698713146\nEpsilon = 0.15173129522432746\nEpsilon = 0.15171612209480503\nEpsilon = 0.15170095048259555\nEpsilon = 0.1516857803875473\nEpsilon = 0.15167061180950853\nEpsilon = 0.15165544474832757\nEpsilon = 0.15164027920385273\nEpsilon = 0.15162511517593236\nEpsilon = 0.15160995266441477\nAgent: ddqn_agent . Episode 1018/2000. Number of steps to finish: 20. Loss: 15.378962516784668 Reward: -16.0\nEpsilon = 0.15159479166914833\nEpsilon = 0.1515796321899814\nEpsilon = 0.15156447422676242\nEpsilon = 0.15154931777933975\nEpsilon = 0.15153416284756183\nEpsilon = 0.15151900943127708\nEpsilon = 0.15150385753033396\nEpsilon = 0.15148870714458093\nEpsilon = 0.15147355827386647\nEpsilon = 0.15145841091803908\nEpsilon = 0.15144326507694728\nEpsilon = 0.15142812075043957\nEpsilon = 0.15141297793836453\nEpsilon = 0.1513978366405707\nEpsilon = 0.15138269685690664\nEpsilon = 0.15136755858722095\nEpsilon = 0.15135242183136224\nEpsilon = 0.1513372865891791\nEpsilon = 0.1513221528605202\nEpsilon = 0.15130702064523416\nAgent: ddqn_agent . Episode 1019/2000. Number of steps to finish: 20. Loss: 16.54054069519043 Reward: -10.0\nEpsilon = 0.15129188994316964\nEpsilon = 0.15127676075417532\nEpsilon = 0.1512616330780999\nEpsilon = 0.15124650691479208\nEpsilon = 0.1512313822641006\nEpsilon = 0.1512162591258742\nEpsilon = 0.1512011374999616\nAgent: ddqn_agent . Episode 1020/2000. Number of steps to finish: 7. Loss: 5.061800479888916 Reward: 5.0\nEpsilon = 0.15118601738621162\nEpsilon = 0.151170898784473\nEpsilon = 0.15115578169459457\nEpsilon = 0.1511406661164251\nEpsilon = 0.15112555204981346\nEpsilon = 0.15111043949460848\nEpsilon = 0.151095328450659\nEpsilon = 0.15108021891781395\nEpsilon = 0.15106511089592217\nEpsilon = 0.15105000438483257\nEpsilon = 0.1510348993843941\nEpsilon = 0.15101979589445566\nEpsilon = 0.1510046939148662\nEpsilon = 0.15098959344547472\nEpsilon = 0.15097449448613018\nEpsilon = 0.15095939703668157\nEpsilon = 0.1509443010969779\nEpsilon = 0.1509292066668682\nEpsilon = 0.15091411374620153\nEpsilon = 0.1508990223348269\nAgent: ddqn_agent . Episode 1021/2000. Number of steps to finish: 20. Loss: 15.985618591308594 Reward: -12.0\nEpsilon = 0.15088393243259343\nEpsilon = 0.15086884403935016\nEpsilon = 0.15085375715494623\nEpsilon = 0.15083867177923074\nEpsilon = 0.15082358791205283\nEpsilon = 0.15080850555326164\nEpsilon = 0.15079342470270632\nEpsilon = 0.15077834536023604\nEpsilon = 0.15076326752570002\nEpsilon = 0.15074819119894745\nEpsilon = 0.15073311637982756\nEpsilon = 0.1507180430681896\nEpsilon = 0.15070297126388277\nEpsilon = 0.15068790096675638\nEpsilon = 0.15067283217665972\nEpsilon = 0.15065776489344207\nEpsilon = 0.15064269911695272\nEpsilon = 0.15062763484704103\nEpsilon = 0.15061257208355633\nEpsilon = 0.15059751082634798\nAgent: ddqn_agent . Episode 1022/2000. Number of steps to finish: 20. Loss: 15.619318962097168 Reward: -16.0\nEpsilon = 0.15058245107526536\nEpsilon = 0.15056739283015783\nEpsilon = 0.15055233609087482\nEpsilon = 0.15053728085726573\nEpsilon = 0.15052222712918\nEpsilon = 0.1505071749064671\nEpsilon = 0.15049212418897645\nEpsilon = 0.15047707497655755\nEpsilon = 0.15046202726905988\nEpsilon = 0.15044698106633297\nEpsilon = 0.15043193636822635\nEpsilon = 0.15041689317458953\nEpsilon = 0.15040185148527208\nEpsilon = 0.15038681130012355\nEpsilon = 0.15037177261899354\nEpsilon = 0.15035673544173164\nEpsilon = 0.15034169976818748\nEpsilon = 0.15032666559821067\nEpsilon = 0.15031163293165084\nEpsilon = 0.15029660176835768\nAgent: ddqn_agent . Episode 1023/2000. Number of steps to finish: 20. Loss: 16.419633865356445 Reward: -10.0\nEpsilon = 0.15028157210818086\nEpsilon = 0.15026654395097003\nEpsilon = 0.15025151729657493\nEpsilon = 0.15023649214484527\nEpsilon = 0.15022146849563078\nEpsilon = 0.15020644634878122\nEpsilon = 0.15019142570414634\nEpsilon = 0.15017640656157594\nEpsilon = 0.1501613889209198\nEpsilon = 0.1501463727820277\nEpsilon = 0.15013135814474948\nEpsilon = 0.150116345008935\nEpsilon = 0.1501013333744341\nEpsilon = 0.15008632324109666\nEpsilon = 0.15007131460877254\nEpsilon = 0.15005630747731166\nEpsilon = 0.15004130184656395\nEpsilon = 0.1500262977163793\nEpsilon = 0.15001129508660765\nEpsilon = 0.149996293957099\nAgent: ddqn_agent . Episode 1024/2000. Number of steps to finish: 20. Loss: 15.396759033203125 Reward: -12.0\nEpsilon = 0.14998129432770327\nEpsilon = 0.1499662961982705\nEpsilon = 0.1499512995686507\nEpsilon = 0.14993630443869382\nEpsilon = 0.14992131080824997\nEpsilon = 0.14990631867716914\nEpsilon = 0.14989132804530142\nEpsilon = 0.14987633891249688\nEpsilon = 0.14986135127860564\nEpsilon = 0.1498463651434778\nEpsilon = 0.14983138050696346\nEpsilon = 0.14981639736891275\nEpsilon = 0.14980141572917585\nEpsilon = 0.14978643558760293\nEpsilon = 0.14977145694404417\nEpsilon = 0.14975647979834977\nEpsilon = 0.14974150415036994\nEpsilon = 0.1497265299999549\nEpsilon = 0.1497115573469549\nEpsilon = 0.1496965861912202\nAgent: ddqn_agent . Episode 1025/2000. Number of steps to finish: 20. Loss: 14.716864585876465 Reward: -16.0\nEpsilon = 0.1496816165326011\nEpsilon = 0.14966664837094784\nEpsilon = 0.14965168170611076\nEpsilon = 0.14963671653794014\nEpsilon = 0.14962175286628634\nEpsilon = 0.14960679069099972\nEpsilon = 0.14959183001193063\nEpsilon = 0.14957687082892945\nEpsilon = 0.14956191314184655\nEpsilon = 0.14954695695053238\nEpsilon = 0.14953200225483734\nEpsilon = 0.14951704905461186\nEpsilon = 0.1495020973497064\nEpsilon = 0.14948714713997144\nEpsilon = 0.14947219842525744\nEpsilon = 0.14945725120541492\nEpsilon = 0.14944230548029439\nEpsilon = 0.14942736124974637\nEpsilon = 0.1494124185136214\nEpsilon = 0.14939747727177002\nAgent: ddqn_agent . Episode 1026/2000. Number of steps to finish: 20. Loss: 15.8018798828125 Reward: -16.0\nEpsilon = 0.14938253752404285\nEpsilon = 0.14936759927029045\nEpsilon = 0.14935266251036342\nEpsilon = 0.14933772724411237\nEpsilon = 0.14932279347138797\nEpsilon = 0.14930786119204084\nEpsilon = 0.14929293040592165\nEpsilon = 0.14927800111288106\nEpsilon = 0.14926307331276978\nEpsilon = 0.1492481470054385\nEpsilon = 0.14923322219073798\nEpsilon = 0.1492182988685189\nEpsilon = 0.14920337703863204\nEpsilon = 0.14918845670092817\nEpsilon = 0.14917353785525808\nEpsilon = 0.14915862050147255\nEpsilon = 0.1491437046394224\nEpsilon = 0.14912879026895845\nEpsilon = 0.14911387738993157\nEpsilon = 0.14909896600219258\nAgent: ddqn_agent . Episode 1027/2000. Number of steps to finish: 20. Loss: 15.812484741210938 Reward: -18.0\nEpsilon = 0.14908405610559236\nEpsilon = 0.1490691476999818\nEpsilon = 0.14905424078521182\nEpsilon = 0.14903933536113328\nEpsilon = 0.14902443142759716\nEpsilon = 0.1490095289844544\nEpsilon = 0.14899462803155597\nEpsilon = 0.14897972856875283\nEpsilon = 0.14896483059589596\nEpsilon = 0.14894993411283639\nEpsilon = 0.1489350391194251\nEpsilon = 0.14892014561551317\nEpsilon = 0.1489052536009516\nEpsilon = 0.1488903630755915\nEpsilon = 0.14887547403928394\nEpsilon = 0.14886058649188003\nEpsilon = 0.14884570043323084\nEpsilon = 0.14883081586318753\nEpsilon = 0.1488159327816012\nEpsilon = 0.14880105118832304\nAgent: ddqn_agent . Episode 1028/2000. Number of steps to finish: 20. Loss: 15.564339637756348 Reward: -14.0\nEpsilon = 0.14878617108320422\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.14877129246609588\nEpsilon = 0.14875641533684927\nEpsilon = 0.1487415396953156\nEpsilon = 0.14872666554134606\nEpsilon = 0.14871179287479191\nEpsilon = 0.14869692169550444\nEpsilon = 0.1486820520033349\nEpsilon = 0.14866718379813457\nEpsilon = 0.14865231707975476\nAgent: ddqn_agent . Episode 1029/2000. Number of steps to finish: 10. Loss: 7.870867729187012 Reward: 2.0\nEpsilon = 0.1486374518480468\nEpsilon = 0.148622588102862\nEpsilon = 0.1486077258440517\nEpsilon = 0.14859286507146732\nEpsilon = 0.14857800578496017\nEpsilon = 0.14856314798438167\nEpsilon = 0.14854829166958322\nEpsilon = 0.14853343684041626\nEpsilon = 0.14851858349673222\nEpsilon = 0.14850373163838254\nEpsilon = 0.1484888812652187\nEpsilon = 0.14847403237709217\nEpsilon = 0.14845918497385446\nEpsilon = 0.1484443390553571\nEpsilon = 0.14842949462145155\nEpsilon = 0.14841465167198942\nEpsilon = 0.14839981020682222\nEpsilon = 0.14838497022580155\nEpsilon = 0.14837013172877897\nEpsilon = 0.1483552947156061\nAgent: ddqn_agent . Episode 1030/2000. Number of steps to finish: 20. Loss: 15.439632415771484 Reward: -14.0\nEpsilon = 0.14834045918613453\nEpsilon = 0.1483256251402159\nEpsilon = 0.14831079257770188\nEpsilon = 0.14829596149844412\nEpsilon = 0.14828113190229428\nEpsilon = 0.14826630378910405\nEpsilon = 0.14825147715872514\nEpsilon = 0.14823665201100927\nEpsilon = 0.1482218283458082\nEpsilon = 0.1482070061629736\nEpsilon = 0.14819218546235732\nEpsilon = 0.1481773662438111\nEpsilon = 0.14816254850718671\nEpsilon = 0.148147732252336\nEpsilon = 0.14813291747911075\nEpsilon = 0.14811810418736285\nEpsilon = 0.1481032923769441\nEpsilon = 0.14808848204770642\nEpsilon = 0.14807367319950165\nEpsilon = 0.1480588658321817\nAgent: ddqn_agent . Episode 1031/2000. Number of steps to finish: 20. Loss: 15.926880836486816 Reward: -10.0\nEpsilon = 0.14804405994559847\nEpsilon = 0.14802925553960392\nEpsilon = 0.14801445261404997\nEpsilon = 0.14799965116878858\nEpsilon = 0.1479848512036717\nEpsilon = 0.14797005271855132\nEpsilon = 0.14795525571327947\nEpsilon = 0.14794046018770815\nEpsilon = 0.1479256661416894\nEpsilon = 0.14791087357507524\nEpsilon = 0.14789608248771774\nEpsilon = 0.14788129287946897\nEpsilon = 0.147866504750181\nEpsilon = 0.147851718099706\nEpsilon = 0.14783693292789601\nEpsilon = 0.14782214923460324\nEpsilon = 0.14780736701967978\nEpsilon = 0.1477925862829778\nEpsilon = 0.1477778070243495\nEpsilon = 0.14776302924364706\nAgent: ddqn_agent . Episode 1032/2000. Number of steps to finish: 20. Loss: 15.520246505737305 Reward: -14.0\nEpsilon = 0.1477482529407227\nEpsilon = 0.1477334781154286\nEpsilon = 0.14771870476761706\nEpsilon = 0.1477039328971403\nEpsilon = 0.14768916250385059\nEpsilon = 0.1476743935876002\nEpsilon = 0.14765962614824144\nEpsilon = 0.14764486018562661\nEpsilon = 0.14763009569960805\nEpsilon = 0.1476153326900381\nEpsilon = 0.14760057115676908\nEpsilon = 0.1475858110996534\nEpsilon = 0.14757105251854344\nEpsilon = 0.1475562954132916\nEpsilon = 0.14754153978375026\nAgent: ddqn_agent . Episode 1033/2000. Number of steps to finish: 15. Loss: 11.306178092956543 Reward: -3.0\nEpsilon = 0.1475267856297719\nEpsilon = 0.14751203295120893\nEpsilon = 0.14749728174791382\nEpsilon = 0.14748253201973904\nEpsilon = 0.14746778376653707\nEpsilon = 0.14745303698816042\nEpsilon = 0.1474382916844616\nEpsilon = 0.14742354785529316\nEpsilon = 0.14740880550050764\nEpsilon = 0.14739406461995758\nEpsilon = 0.1473793252134956\nEpsilon = 0.14736458728097424\nEpsilon = 0.14734985082224614\nEpsilon = 0.1473351158371639\nEpsilon = 0.1473203823255802\nEpsilon = 0.14730565028734766\nEpsilon = 0.14729091972231892\nEpsilon = 0.1472761906303467\nEpsilon = 0.14726146301128365\nEpsilon = 0.14724673686498252\nAgent: ddqn_agent . Episode 1034/2000. Number of steps to finish: 20. Loss: 15.949568748474121 Reward: -14.0\nEpsilon = 0.14723201219129603\nEpsilon = 0.1472172889900769\nEpsilon = 0.1472025672611779\nEpsilon = 0.14718784700445178\nEpsilon = 0.14717312821975134\nEpsilon = 0.14715841090692938\nEpsilon = 0.1471436950658387\nEpsilon = 0.1471289806963321\nEpsilon = 0.14711426779826248\nEpsilon = 0.14709955637148264\nEpsilon = 0.1470848464158455\nEpsilon = 0.1470701379312039\nEpsilon = 0.1470554309174108\nEpsilon = 0.14704072537431906\nEpsilon = 0.14702602130178163\nEpsilon = 0.14701131869965145\nEpsilon = 0.14699661756778148\nEpsilon = 0.14698191790602472\nEpsilon = 0.14696721971423413\nEpsilon = 0.1469525229922627\nAgent: ddqn_agent . Episode 1035/2000. Number of steps to finish: 20. Loss: 15.654613494873047 Reward: -14.0\nEpsilon = 0.14693782773996347\nEpsilon = 0.14692313395718948\nEpsilon = 0.14690844164379377\nEpsilon = 0.14689375079962938\nEpsilon = 0.14687906142454943\nEpsilon = 0.14686437351840698\nEpsilon = 0.14684968708105514\nEpsilon = 0.14683500211234704\nEpsilon = 0.14682031861213582\nEpsilon = 0.1468056365802746\nEpsilon = 0.14679095601661657\nEpsilon = 0.14677627692101491\nEpsilon = 0.1467615992933228\nEpsilon = 0.14674692313339346\nEpsilon = 0.14673224844108013\nEpsilon = 0.14671757521623602\nEpsilon = 0.1467029034587144\nEpsilon = 0.14668823316836851\nEpsilon = 0.1466735643450517\nEpsilon = 0.14665889698861717\nAgent: ddqn_agent . Episode 1036/2000. Number of steps to finish: 20. Loss: 15.35806941986084 Reward: -14.0\nEpsilon = 0.1466442310989183\nEpsilon = 0.14662956667580843\nEpsilon = 0.14661490371914085\nEpsilon = 0.14660024222876894\nEpsilon = 0.14658558220454607\nEpsilon = 0.14657092364632562\nEpsilon = 0.146556266553961\nEpsilon = 0.1465416109273056\nEpsilon = 0.14652695676621288\nEpsilon = 0.14651230407053625\nEpsilon = 0.1464976528401292\nEpsilon = 0.14648300307484519\nEpsilon = 0.1464683547745377\nEpsilon = 0.14645370793906026\nEpsilon = 0.14643906256826636\nEpsilon = 0.14642441866200953\nEpsilon = 0.14640977622014334\nEpsilon = 0.14639513524252132\nEpsilon = 0.14638049572899706\nEpsilon = 0.14636585767942417\nAgent: ddqn_agent . Episode 1037/2000. Number of steps to finish: 20. Loss: 15.83824634552002 Reward: -10.0\nEpsilon = 0.14635122109365623\nEpsilon = 0.14633658597154686\nEpsilon = 0.1463219523129497\nEpsilon = 0.1463073201177184\nEpsilon = 0.14629268938570664\nEpsilon = 0.14627806011676805\nEpsilon = 0.1462634323107564\nEpsilon = 0.14624880596752532\nEpsilon = 0.14623418108692857\nEpsilon = 0.1462195576688199\nEpsilon = 0.146204935713053\nEpsilon = 0.1461903152194817\nEpsilon = 0.14617569618795975\nEpsilon = 0.14616107861834096\nEpsilon = 0.14614646251047914\nEpsilon = 0.1461318478642281\nEpsilon = 0.14611723467944165\nEpsilon = 0.1461026229559737\nEpsilon = 0.1460880126936781\nEpsilon = 0.14607340389240875\nAgent: ddqn_agent . Episode 1038/2000. Number of steps to finish: 20. Loss: 14.518038749694824 Reward: -16.0\nEpsilon = 0.1460587965520195\nEpsilon = 0.1460441906723643\nEpsilon = 0.14602958625329707\nEpsilon = 0.14601498329467175\nEpsilon = 0.14600038179634228\nEpsilon = 0.14598578175816265\nEpsilon = 0.14597118317998684\nEpsilon = 0.14595658606166884\nEpsilon = 0.14594199040306266\nEpsilon = 0.14592739620402237\nEpsilon = 0.14591280346440197\nEpsilon = 0.14589821218405552\nEpsilon = 0.1458836223628371\nEpsilon = 0.14586903400060083\nEpsilon = 0.14585444709720077\nAgent: ddqn_agent . Episode 1039/2000. Number of steps to finish: 15. Loss: 11.444772720336914 Reward: -3.0\nEpsilon = 0.14583986165249105\nEpsilon = 0.1458252776663258\nEpsilon = 0.14581069513855918\nEpsilon = 0.14579611406904533\nEpsilon = 0.14578153445763842\nEpsilon = 0.14576695630419265\nEpsilon = 0.14575237960856222\nEpsilon = 0.14573780437060135\nEpsilon = 0.1457232305901643\nEpsilon = 0.14570865826710527\nEpsilon = 0.14569408740127857\nEpsilon = 0.14567951799253845\nEpsilon = 0.1456649500407392\nEpsilon = 0.14565038354573515\nEpsilon = 0.14563581850738058\nAgent: ddqn_agent . Episode 1040/2000. Number of steps to finish: 15. Loss: 11.691000938415527 Reward: -3.0\nEpsilon = 0.14562125492552985\nEpsilon = 0.1456066928000373\nEpsilon = 0.1455921321307573\nEpsilon = 0.14557757291754422\nEpsilon = 0.14556301516025247\nEpsilon = 0.14554845885873643\nEpsilon = 0.14553390401285057\nEpsilon = 0.14551935062244928\nEpsilon = 0.14550479868738703\nEpsilon = 0.1454902482075183\nEpsilon = 0.14547569918269754\nEpsilon = 0.14546115161277928\nEpsilon = 0.145446605497618\nEpsilon = 0.14543206083706825\nEpsilon = 0.14541751763098454\nEpsilon = 0.14540297587922144\nEpsilon = 0.1453884355816335\nEpsilon = 0.14537389673807535\nEpsilon = 0.14535935934840155\nAgent: ddqn_agent . Episode 1041/2000. Number of steps to finish: 19. Loss: 15.201030731201172 Reward: -7.0\nEpsilon = 0.1453448234124667\nEpsilon = 0.14533028893012548\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.14531575590123247\nEpsilon = 0.14530122432564235\nEpsilon = 0.1452866942032098\nEpsilon = 0.1452721655337895\nEpsilon = 0.14525763831723612\nEpsilon = 0.14524311255340439\nEpsilon = 0.14522858824214904\nEpsilon = 0.14521406538332482\nEpsilon = 0.1451995439767865\nEpsilon = 0.1451850240223888\nEpsilon = 0.14517050551998656\nEpsilon = 0.14515598846943456\nEpsilon = 0.14514147287058762\nEpsilon = 0.14512695872330056\nEpsilon = 0.14511244602742823\nEpsilon = 0.14509793478282548\nEpsilon = 0.1450834249893472\nEpsilon = 0.14506891664684826\nAgent: ddqn_agent . Episode 1042/2000. Number of steps to finish: 20. Loss: 16.282960891723633 Reward: -10.0\nEpsilon = 0.14505440975518358\nEpsilon = 0.14503990431420807\nEpsilon = 0.14502540032377664\nEpsilon = 0.14501089778374426\nEpsilon = 0.14499639669396588\nEpsilon = 0.14498189705429648\nEpsilon = 0.14496739886459106\nEpsilon = 0.1449529021247046\nEpsilon = 0.14493840683449213\nEpsilon = 0.1449239129938087\nEpsilon = 0.1449094206025093\nEpsilon = 0.14489492966044906\nEpsilon = 0.14488044016748303\nEpsilon = 0.14486595212346629\nEpsilon = 0.14485146552825395\nEpsilon = 0.14483698038170112\nEpsilon = 0.14482249668366295\nEpsilon = 0.1448080144339946\nEpsilon = 0.1447935336325512\nEpsilon = 0.14477905427918794\nAgent: ddqn_agent . Episode 1043/2000. Number of steps to finish: 20. Loss: 15.564279556274414 Reward: -14.0\nEpsilon = 0.14476457637376003\nEpsilon = 0.14475009991612264\nEpsilon = 0.14473562490613104\nEpsilon = 0.14472115134364041\nEpsilon = 0.14470667922850605\nEpsilon = 0.1446922085605832\nEpsilon = 0.14467773933972716\nEpsilon = 0.1446632715657932\nEpsilon = 0.14464880523863663\nEpsilon = 0.14463434035811276\nEpsilon = 0.14461987692407696\nEpsilon = 0.14460541493638457\nEpsilon = 0.14459095439489092\nEpsilon = 0.14457649529945144\nEpsilon = 0.14456203764992148\nEpsilon = 0.14454758144615648\nEpsilon = 0.14453312668801185\nEpsilon = 0.14451867337534305\nEpsilon = 0.14450422150800551\nEpsilon = 0.14448977108585473\nAgent: ddqn_agent . Episode 1044/2000. Number of steps to finish: 20. Loss: 15.318649291992188 Reward: -14.0\nEpsilon = 0.14447532210874614\nEpsilon = 0.14446087457653528\nEpsilon = 0.14444642848907763\nEpsilon = 0.1444319838462287\nEpsilon = 0.1444175406478441\nEpsilon = 0.1444030988937793\nEpsilon = 0.14438865858388994\nEpsilon = 0.14437421971803155\nEpsilon = 0.14435978229605975\nEpsilon = 0.14434534631783014\nEpsilon = 0.14433091178319835\nEpsilon = 0.14431647869202002\nEpsilon = 0.1443020470441508\nEpsilon = 0.1442876168394464\nEpsilon = 0.14427318807776246\nEpsilon = 0.1442587607589547\nEpsilon = 0.1442443348828788\nEpsilon = 0.1442299104493905\nEpsilon = 0.1442154874583456\nEpsilon = 0.14420106590959975\nAgent: ddqn_agent . Episode 1045/2000. Number of steps to finish: 20. Loss: 15.26133918762207 Reward: -12.0\nEpsilon = 0.1441866458030088\nEpsilon = 0.1441722271384285\nEpsilon = 0.14415780991571467\nEpsilon = 0.1441433941347231\nEpsilon = 0.14412897979530964\nEpsilon = 0.14411456689733013\nEpsilon = 0.1441001554406404\nEpsilon = 0.14408574542509633\nEpsilon = 0.14407133685055382\nEpsilon = 0.14405692971686876\nEpsilon = 0.14404252402389708\nEpsilon = 0.14402811977149468\nEpsilon = 0.14401371695951753\nEpsilon = 0.14399931558782159\nEpsilon = 0.1439849156562628\nEpsilon = 0.1439705171646972\nEpsilon = 0.14395612011298073\nEpsilon = 0.14394172450096943\nEpsilon = 0.14392733032851934\nEpsilon = 0.14391293759548648\nAgent: ddqn_agent . Episode 1046/2000. Number of steps to finish: 20. Loss: 16.138965606689453 Reward: -18.0\nEpsilon = 0.14389854630172694\nEpsilon = 0.14388415644709676\nEpsilon = 0.14386976803145204\nEpsilon = 0.1438553810546489\nEpsilon = 0.14384099551654345\nEpsilon = 0.1438266114169918\nEpsilon = 0.1438122287558501\nEpsilon = 0.1437978475329745\nEpsilon = 0.1437834677482212\nEpsilon = 0.1437690894014464\nEpsilon = 0.14375471249250624\nEpsilon = 0.143740337021257\nEpsilon = 0.14372596298755486\nEpsilon = 0.1437115903912561\nEpsilon = 0.14369721923221698\nEpsilon = 0.14368284951029375\nEpsilon = 0.14366848122534273\nEpsilon = 0.1436541143772202\nEpsilon = 0.14363974896578247\nEpsilon = 0.1436253849908859\nAgent: ddqn_agent . Episode 1047/2000. Number of steps to finish: 20. Loss: 15.60937786102295 Reward: -10.0\nEpsilon = 0.1436110224523868\nEpsilon = 0.14359666135014157\nEpsilon = 0.14358230168400657\nEpsilon = 0.14356794345383816\nEpsilon = 0.14355358665949278\nEpsilon = 0.14353923130082683\nEpsilon = 0.14352487737769673\nEpsilon = 0.14351052488995897\nEpsilon = 0.14349617383747\nEpsilon = 0.14348182422008623\nEpsilon = 0.14346747603766422\nEpsilon = 0.14345312929006046\nEpsilon = 0.14343878397713145\nEpsilon = 0.14342444009873373\nEpsilon = 0.14341009765472384\nEpsilon = 0.14339575664495838\nEpsilon = 0.1433814170692939\nAgent: ddqn_agent . Episode 1048/2000. Number of steps to finish: 17. Loss: 13.346878051757812 Reward: -5.0\nEpsilon = 0.14336707892758696\nEpsilon = 0.1433527422196942\nEpsilon = 0.14333840694547226\nEpsilon = 0.1433240731047777\nEpsilon = 0.14330974069746724\nEpsilon = 0.1432954097233975\nEpsilon = 0.14328108018242516\nEpsilon = 0.14326675207440692\nEpsilon = 0.1432524253991995\nEpsilon = 0.14323810015665958\nEpsilon = 0.1432237763466439\nEpsilon = 0.14320945396900925\nEpsilon = 0.14319513302361234\nEpsilon = 0.14318081351030998\nEpsilon = 0.14316649542895896\nEpsilon = 0.14315217877941605\nEpsilon = 0.1431378635615381\nEpsilon = 0.14312354977518194\nEpsilon = 0.14310923742020443\nEpsilon = 0.1430949264964624\nAgent: ddqn_agent . Episode 1049/2000. Number of steps to finish: 20. Loss: 15.248188972473145 Reward: -14.0\nEpsilon = 0.14308061700381278\nEpsilon = 0.1430663089421124\nEpsilon = 0.14305200231121817\nEpsilon = 0.14303769711098704\nEpsilon = 0.14302339334127595\nEpsilon = 0.1430090910019418\nEpsilon = 0.14299479009284163\nEpsilon = 0.14298049061383236\nEpsilon = 0.14296619256477097\nEpsilon = 0.1429518959455145\nEpsilon = 0.14293760075591994\nEpsilon = 0.14292330699584435\nEpsilon = 0.14290901466514477\nEpsilon = 0.14289472376367826\nEpsilon = 0.1428804342913019\nEpsilon = 0.14286614624787278\nEpsilon = 0.14285185963324798\nEpsilon = 0.14283757444728465\nEpsilon = 0.14282329068983993\nEpsilon = 0.14280900836077096\nAgent: ddqn_agent . Episode 1050/2000. Number of steps to finish: 20. Loss: 15.900769233703613 Reward: -18.0\nEpsilon = 0.14279472745993488\nEpsilon = 0.1427804479871889\nEpsilon = 0.14276616994239016\nEpsilon = 0.14275189332539592\nEpsilon = 0.1427376181360634\nEpsilon = 0.1427233443742498\nEpsilon = 0.14270907203981237\nEpsilon = 0.14269480113260838\nEpsilon = 0.1426805316524951\nEpsilon = 0.14266626359932985\nEpsilon = 0.14265199697296993\nEpsilon = 0.14263773177327263\nEpsilon = 0.14262346800009532\nEpsilon = 0.1426092056532953\nEpsilon = 0.14259494473272996\nEpsilon = 0.14258068523825668\nEpsilon = 0.14256642716973286\nEpsilon = 0.14255217052701588\nEpsilon = 0.14253791530996318\nEpsilon = 0.14252366151843218\nAgent: ddqn_agent . Episode 1051/2000. Number of steps to finish: 20. Loss: 15.706238746643066 Reward: -10.0\nEpsilon = 0.14250940915228033\nEpsilon = 0.1424951582113651\nEpsilon = 0.14248090869554397\nEpsilon = 0.14246666060467442\nEpsilon = 0.14245241393861396\nEpsilon = 0.1424381686972201\nEpsilon = 0.14242392488035038\nEpsilon = 0.14240968248786234\nEpsilon = 0.14239544151961356\nEpsilon = 0.14238120197546159\nEpsilon = 0.14236696385526404\nEpsilon = 0.1423527271588785\nEpsilon = 0.14233849188616263\nEpsilon = 0.14232425803697402\nEpsilon = 0.14231002561117032\nEpsilon = 0.1422957946086092\nEpsilon = 0.14228156502914832\nEpsilon = 0.1422673368726454\nEpsilon = 0.14225311013895814\nEpsilon = 0.14223888482794425\nAgent: ddqn_agent . Episode 1052/2000. Number of steps to finish: 20. Loss: 14.738397598266602 Reward: -10.0\nEpsilon = 0.14222466093946146\nEpsilon = 0.1422104384733675\nEpsilon = 0.14219621742952018\nEpsilon = 0.14218199780777724\nEpsilon = 0.14216777960799645\nEpsilon = 0.14215356283003566\nEpsilon = 0.14213934747375265\nEpsilon = 0.14212513353900527\nEpsilon = 0.14211092102565137\nEpsilon = 0.1420967099335488\nEpsilon = 0.14208250026255545\nEpsilon = 0.14206829201252918\nEpsilon = 0.14205408518332793\nEpsilon = 0.1420398797748096\nEpsilon = 0.14202567578683215\nEpsilon = 0.14201147321925348\nEpsilon = 0.14199727207193155\nEpsilon = 0.14198307234472435\nEpsilon = 0.14196887403748987\nEpsilon = 0.1419546771500861\nAgent: ddqn_agent . Episode 1053/2000. Number of steps to finish: 20. Loss: 16.492340087890625 Reward: -18.0\nEpsilon = 0.1419404816823711\nEpsilon = 0.14192628763420287\nEpsilon = 0.14191209500543944\nEpsilon = 0.1418979037959389\nEpsilon = 0.1418837140055593\nEpsilon = 0.14186952563415875\nEpsilon = 0.14185533868159533\nEpsilon = 0.14184115314772716\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.1418269690324124\nEpsilon = 0.14181278633550917\nEpsilon = 0.1417986050568756\nEpsilon = 0.1417844251963699\nEpsilon = 0.14177024675385028\nEpsilon = 0.1417560697291749\nEpsilon = 0.14174189412220198\nEpsilon = 0.14172771993278976\nEpsilon = 0.14171354716079648\nEpsilon = 0.1416993758060804\nEpsilon = 0.1416852058684998\nEpsilon = 0.14167103734791295\nAgent: ddqn_agent . Episode 1054/2000. Number of steps to finish: 20. Loss: 15.113290786743164 Reward: -14.0\nEpsilon = 0.14165687024417817\nEpsilon = 0.14164270455715375\nEpsilon = 0.14162854028669802\nEpsilon = 0.14161437743266936\nEpsilon = 0.1416002159949261\nEpsilon = 0.1415860559733266\nEpsilon = 0.14157189736772927\nEpsilon = 0.1415577401779925\nEpsilon = 0.1415435844039747\nEpsilon = 0.14152943004553428\nEpsilon = 0.14151527710252973\nEpsilon = 0.14150112557481948\nEpsilon = 0.141486975462262\nEpsilon = 0.14147282676471576\nEpsilon = 0.1414586794820393\nEpsilon = 0.1414445336140911\nEpsilon = 0.1414303891607297\nEpsilon = 0.14141624612181364\nEpsilon = 0.14140210449720145\nEpsilon = 0.14138796428675174\nAgent: ddqn_agent . Episode 1055/2000. Number of steps to finish: 20. Loss: 16.44673728942871 Reward: -14.0\nEpsilon = 0.14137382549032307\nEpsilon = 0.14135968810777405\nEpsilon = 0.14134555213896327\nEpsilon = 0.14133141758374937\nEpsilon = 0.141317284441991\nEpsilon = 0.1413031527135468\nEpsilon = 0.14128902239827543\nEpsilon = 0.14127489349603561\nEpsilon = 0.14126076600668602\nEpsilon = 0.14124663993008535\nEpsilon = 0.14123251526609235\nEpsilon = 0.14121839201456574\nEpsilon = 0.14120427017536427\nEpsilon = 0.14119014974834673\nEpsilon = 0.1411760307333719\nEpsilon = 0.14116191313029855\nEpsilon = 0.14114779693898552\nEpsilon = 0.14113368215929162\nEpsilon = 0.14111956879107568\nAgent: ddqn_agent . Episode 1056/2000. Number of steps to finish: 19. Loss: 15.621406555175781 Reward: -7.0\nEpsilon = 0.14110545683419656\nEpsilon = 0.14109134628851314\nEpsilon = 0.1410772371538843\nEpsilon = 0.1410631294301689\nEpsilon = 0.1410490231172259\nEpsilon = 0.14103491821491418\nEpsilon = 0.1410208147230927\nEpsilon = 0.14100671264162037\nEpsilon = 0.1409926119703562\nEpsilon = 0.14097851270915918\nEpsilon = 0.14096441485788827\nEpsilon = 0.1409503184164025\nEpsilon = 0.14093622338456085\nEpsilon = 0.1409221297622224\nEpsilon = 0.1409080375492462\nEpsilon = 0.14089394674549127\nEpsilon = 0.14087985735081673\nEpsilon = 0.14086576936508166\nEpsilon = 0.14085168278814517\nEpsilon = 0.14083759761986636\nAgent: ddqn_agent . Episode 1057/2000. Number of steps to finish: 20. Loss: 15.511860847473145 Reward: -10.0\nEpsilon = 0.1408235138601044\nEpsilon = 0.14080943150871839\nEpsilon = 0.14079535056556752\nEpsilon = 0.14078127103051097\nEpsilon = 0.14076719290340792\nEpsilon = 0.14075311618411757\nEpsilon = 0.14073904087249917\nEpsilon = 0.1407249669684119\nEpsilon = 0.1407108944717151\nEpsilon = 0.1406968233822679\nEpsilon = 0.1406827536999297\nEpsilon = 0.1406686854245597\nEpsilon = 0.14065461855601724\nEpsilon = 0.14064055309416165\nEpsilon = 0.14062648903885225\nEpsilon = 0.14061242638994836\nEpsilon = 0.14059836514730936\nEpsilon = 0.14058430531079463\nEpsilon = 0.14057024688026357\nEpsilon = 0.14055618985557555\nAgent: ddqn_agent . Episode 1058/2000. Number of steps to finish: 20. Loss: 15.79261302947998 Reward: -20.0\nEpsilon = 0.14054213423659\nEpsilon = 0.14052808002316636\nEpsilon = 0.14051402721516404\nEpsilon = 0.14049997581244253\nEpsilon = 0.14048592581486127\nEpsilon = 0.1404718772222798\nEpsilon = 0.14045783003455758\nEpsilon = 0.14044378425155413\nEpsilon = 0.14042973987312898\nEpsilon = 0.14041569689914168\nEpsilon = 0.14040165532945179\nEpsilon = 0.14038761516391884\nEpsilon = 0.14037357640240244\nEpsilon = 0.1403595390447622\nEpsilon = 0.14034550309085772\nEpsilon = 0.14033146854054865\nEpsilon = 0.1403174353936946\nEpsilon = 0.14030340365015523\nEpsilon = 0.14028937330979022\nEpsilon = 0.14027534437245923\nAgent: ddqn_agent . Episode 1059/2000. Number of steps to finish: 20. Loss: 15.72945785522461 Reward: -18.0\nEpsilon = 0.14026131683802198\nEpsilon = 0.14024729070633818\nEpsilon = 0.14023326597726754\nEpsilon = 0.14021924265066982\nEpsilon = 0.14020522072640476\nEpsilon = 0.14019120020433212\nEpsilon = 0.14017718108431168\nEpsilon = 0.14016316336620324\nEpsilon = 0.1401491470498666\nEpsilon = 0.14013513213516163\nEpsilon = 0.14012111862194812\nEpsilon = 0.14010710651008593\nEpsilon = 0.1400930957994349\nEpsilon = 0.14007908648985495\nEpsilon = 0.14006507858120598\nEpsilon = 0.14005107207334785\nEpsilon = 0.14003706696614052\nEpsilon = 0.14002306325944391\nEpsilon = 0.14000906095311796\nEpsilon = 0.13999506004702264\nAgent: ddqn_agent . Episode 1060/2000. Number of steps to finish: 20. Loss: 15.026899337768555 Reward: -18.0\nEpsilon = 0.13998106054101794\nEpsilon = 0.13996706243496385\nEpsilon = 0.13995306572872035\nEpsilon = 0.13993907042214748\nEpsilon = 0.13992507651510527\nEpsilon = 0.13991108400745375\nEpsilon = 0.139897092899053\nEpsilon = 0.1398831031897631\nEpsilon = 0.13986911487944412\nEpsilon = 0.13985512796795618\nEpsilon = 0.13984114245515938\nEpsilon = 0.13982715834091386\nEpsilon = 0.13981317562507978\nEpsilon = 0.13979919430751728\nEpsilon = 0.13978521438808653\nEpsilon = 0.13977123586664772\nEpsilon = 0.13975725874306105\nEpsilon = 0.13974328301718675\nEpsilon = 0.13972930868888503\nEpsilon = 0.13971533575801615\nAgent: ddqn_agent . Episode 1061/2000. Number of steps to finish: 20. Loss: 15.683772087097168 Reward: -14.0\nEpsilon = 0.13970136422444035\nEpsilon = 0.13968739408801792\nEpsilon = 0.13967342534860913\nEpsilon = 0.13965945800607427\nEpsilon = 0.13964549206027366\nEpsilon = 0.13963152751106764\nEpsilon = 0.13961756435831654\nEpsilon = 0.1396036026018807\nAgent: ddqn_agent . Episode 1062/2000. Number of steps to finish: 8. Loss: 6.079386234283447 Reward: 4.0\nEpsilon = 0.13958964224162052\nEpsilon = 0.13957568327739636\nEpsilon = 0.13956172570906863\nEpsilon = 0.13954776953649772\nEpsilon = 0.13953381475954407\nEpsilon = 0.13951986137806813\nEpsilon = 0.13950590939193033\nEpsilon = 0.13949195880099113\nEpsilon = 0.13947800960511103\nEpsilon = 0.13946406180415052\nEpsilon = 0.13945011539797011\nEpsilon = 0.13943617038643033\nEpsilon = 0.1394222267693917\nEpsilon = 0.13940828454671475\nEpsilon = 0.13939434371826007\nEpsilon = 0.13938040428388823\nEpsilon = 0.13936646624345986\nEpsilon = 0.1393525295968355\nEpsilon = 0.13933859434387583\nEpsilon = 0.13932466048444145\nAgent: ddqn_agent . Episode 1063/2000. Number of steps to finish: 20. Loss: 15.43895149230957 Reward: -18.0\nEpsilon = 0.139310728018393\nEpsilon = 0.13929679694559116\nEpsilon = 0.13928286726589661\nEpsilon = 0.13926893897917003\nEpsilon = 0.1392550120852721\nEpsilon = 0.1392410865840636\nEpsilon = 0.13922716247540518\nEpsilon = 0.13921323975915764\nEpsilon = 0.13919931843518174\nEpsilon = 0.13918539850333822\nAgent: ddqn_agent . Episode 1064/2000. Number of steps to finish: 10. Loss: 7.666057586669922 Reward: 2.0\nEpsilon = 0.1391714799634879\nEpsilon = 0.13915756281549155\nEpsilon = 0.13914364705921\nEpsilon = 0.1391297326945041\nEpsilon = 0.13911581972123466\nEpsilon = 0.13910190813926254\nEpsilon = 0.13908799794844862\nEpsilon = 0.13907408914865377\nEpsilon = 0.13906018173973891\nEpsilon = 0.13904627572156494\nEpsilon = 0.1390323710939928\nEpsilon = 0.1390184678568834\nEpsilon = 0.13900456601009772\nEpsilon = 0.1389906655534967\nEpsilon = 0.13897676648694135\nEpsilon = 0.13896286881029266\nEpsilon = 0.13894897252341162\nEpsilon = 0.13893507762615928\nEpsilon = 0.13892118411839666\nEpsilon = 0.1389072919999848\nAgent: ddqn_agent . Episode 1065/2000. Number of steps to finish: 20. Loss: 16.29914093017578 Reward: -12.0\nEpsilon = 0.1388934012707848\nEpsilon = 0.13887951193065773\nEpsilon = 0.13886562397946467\nEpsilon = 0.13885173741706672\nEpsilon = 0.138837852243325\nEpsilon = 0.1388239684581007\nEpsilon = 0.1388100860612549\nEpsilon = 0.13879620505264875\nEpsilon = 0.1387823254321435\nEpsilon = 0.1387684471996003\nEpsilon = 0.13875457035488034\nEpsilon = 0.13874069489784485\nEpsilon = 0.13872682082835508\nEpsilon = 0.13871294814627225\nEpsilon = 0.13869907685145763\nEpsilon = 0.13868520694377248\nEpsilon = 0.1386713384230781\nEpsilon = 0.1386574712892358\nEpsilon = 0.1386436055421069\nEpsilon = 0.1386297411815527\nAgent: ddqn_agent . Episode 1066/2000. Number of steps to finish: 20. Loss: 16.165428161621094 Reward: -10.0\nEpsilon = 0.13861587820743454\nEpsilon = 0.1386020166196138\nEpsilon = 0.13858815641795183\nEpsilon = 0.13857429760231005\nEpsilon = 0.13856044017254981\nEpsilon = 0.13854658412853257\nEpsilon = 0.13853272947011971\nEpsilon = 0.1385188761971727\nEpsilon = 0.138505024309553\nEpsilon = 0.13849117380712203\nEpsilon = 0.13847732468974133\nEpsilon = 0.13846347695727235\nEpsilon = 0.13844963060957663\nEpsilon = 0.13843578564651568\nEpsilon = 0.13842194206795103\nEpsilon = 0.13840809987374422\nEpsilon = 0.13839425906375685\nEpsilon = 0.13838041963785047\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.1383665815958867\nEpsilon = 0.1383527449377271\nAgent: ddqn_agent . Episode 1067/2000. Number of steps to finish: 20. Loss: 14.825789451599121 Reward: -14.0\nEpsilon = 0.1383389096632333\nEpsilon = 0.138325075772267\nEpsilon = 0.13831124326468977\nEpsilon = 0.1382974121403633\nEpsilon = 0.13828358239914929\nEpsilon = 0.13826975404090938\nEpsilon = 0.1382559270655053\nEpsilon = 0.13824210147279875\nEpsilon = 0.13822827726265147\nEpsilon = 0.1382144544349252\nEpsilon = 0.1382006329894817\nEpsilon = 0.13818681292618276\nEpsilon = 0.13817299424489016\nEpsilon = 0.13815917694546567\nEpsilon = 0.13814536102777114\nEpsilon = 0.13813154649166837\nEpsilon = 0.1381177333370192\nEpsilon = 0.1381039215636855\nEpsilon = 0.13809011117152914\nEpsilon = 0.138076302160412\nAgent: ddqn_agent . Episode 1068/2000. Number of steps to finish: 20. Loss: 16.1334171295166 Reward: -12.0\nEpsilon = 0.13806249453019595\nEpsilon = 0.13804868828074293\nEpsilon = 0.13803488341191486\nEpsilon = 0.13802107992357368\nEpsilon = 0.13800727781558134\nEpsilon = 0.13799347708779977\nEpsilon = 0.13797967774009098\nEpsilon = 0.13796587977231697\nEpsilon = 0.13795208318433974\nEpsilon = 0.1379382879760213\nEpsilon = 0.13792449414722371\nEpsilon = 0.137910701697809\nEpsilon = 0.13789691062763923\nEpsilon = 0.13788312093657648\nEpsilon = 0.13786933262448284\nEpsilon = 0.13785554569122038\nEpsilon = 0.13784176013665125\nEpsilon = 0.1378279759606376\nEpsilon = 0.13781419316304153\nEpsilon = 0.13780041174372523\nAgent: ddqn_agent . Episode 1069/2000. Number of steps to finish: 20. Loss: 15.344182968139648 Reward: -14.0\nEpsilon = 0.13778663170255087\nEpsilon = 0.1377728530393806\nEpsilon = 0.13775907575407667\nEpsilon = 0.13774529984650127\nEpsilon = 0.1377315253165166\nEpsilon = 0.13771775216398496\nEpsilon = 0.13770398038876858\nEpsilon = 0.1376902099907297\nEpsilon = 0.13767644096973064\nEpsilon = 0.13766267332563367\nEpsilon = 0.1376489070583011\nEpsilon = 0.13763514216759526\nEpsilon = 0.1376213786533785\nEpsilon = 0.13760761651551318\nEpsilon = 0.13759385575386163\nEpsilon = 0.13758009636828625\nEpsilon = 0.13756633835864943\nEpsilon = 0.13755258172481355\nEpsilon = 0.13753882646664106\nEpsilon = 0.1375250725839944\nAgent: ddqn_agent . Episode 1070/2000. Number of steps to finish: 20. Loss: 15.255727767944336 Reward: -14.0\nEpsilon = 0.137511320076736\nEpsilon = 0.13749756894472834\nEpsilon = 0.13748381918783387\nEpsilon = 0.13747007080591508\nEpsilon = 0.1374563237988345\nEpsilon = 0.1374425781664546\nEpsilon = 0.13742883390863797\nEpsilon = 0.1374150910252471\nEpsilon = 0.13740134951614458\nEpsilon = 0.13738760938119296\nEpsilon = 0.13737387062025483\nEpsilon = 0.1373601332331928\nEpsilon = 0.13734639721986946\nEpsilon = 0.13733266258014748\nEpsilon = 0.13731892931388948\nEpsilon = 0.1373051974209581\nEpsilon = 0.137291466901216\nEpsilon = 0.13727773775452587\nEpsilon = 0.13726400998075042\nEpsilon = 0.13725028357975236\nAgent: ddqn_agent . Episode 1071/2000. Number of steps to finish: 20. Loss: 16.204559326171875 Reward: -10.0\nEpsilon = 0.13723655855139438\nEpsilon = 0.13722283489553924\nEpsilon = 0.13720911261204968\nEpsilon = 0.13719539170078848\nEpsilon = 0.13718167216161842\nEpsilon = 0.13716795399440226\nEpsilon = 0.13715423719900283\nEpsilon = 0.13714052177528294\nEpsilon = 0.1371268077231054\nEpsilon = 0.1371130950423331\nEpsilon = 0.13709938373282887\nEpsilon = 0.1370856737944556\nEpsilon = 0.13707196522707615\nEpsilon = 0.13705825803055344\nEpsilon = 0.13704455220475037\nEpsilon = 0.1370308477495299\nEpsilon = 0.13701714466475495\nEpsilon = 0.1370034429502885\nEpsilon = 0.13698974260599348\nEpsilon = 0.13697604363173288\nAgent: ddqn_agent . Episode 1072/2000. Number of steps to finish: 20. Loss: 15.89413070678711 Reward: -10.0\nEpsilon = 0.1369623460273697\nEpsilon = 0.13694864979276697\nEpsilon = 0.1369349549277877\nEpsilon = 0.13692126143229494\nEpsilon = 0.13690756930615172\nEpsilon = 0.1368938785492211\nEpsilon = 0.13688018916136618\nEpsilon = 0.13686650114245005\nEpsilon = 0.1368528144923358\nEpsilon = 0.13683912921088656\nEpsilon = 0.13682544529796548\nEpsilon = 0.13681176275343568\nEpsilon = 0.13679808157716034\nEpsilon = 0.1367844017690026\nEpsilon = 0.1367707233288257\nEpsilon = 0.13675704625649282\nEpsilon = 0.13674337055186717\nEpsilon = 0.13672969621481199\nEpsilon = 0.1367160232451905\nEpsilon = 0.13670235164286598\nAgent: ddqn_agent . Episode 1073/2000. Number of steps to finish: 20. Loss: 16.081594467163086 Reward: -18.0\nEpsilon = 0.13668868140770168\nEpsilon = 0.1366750125395609\nEpsilon = 0.13666134503830696\nEpsilon = 0.13664767890380314\nEpsilon = 0.13663401413591275\nEpsilon = 0.13662035073449916\nEpsilon = 0.13660668869942572\nEpsilon = 0.13659302803055579\nEpsilon = 0.13657936872775273\nEpsilon = 0.13656571079087995\nEpsilon = 0.13655205421980088\nEpsilon = 0.1365383990143789\nEpsilon = 0.13652474517447746\nEpsilon = 0.13651109269996\nEpsilon = 0.13649744159069002\nEpsilon = 0.13648379184653095\nEpsilon = 0.1364701434673463\nEpsilon = 0.13645649645299956\nEpsilon = 0.13644285080335428\nEpsilon = 0.13642920651827395\nAgent: ddqn_agent . Episode 1074/2000. Number of steps to finish: 20. Loss: 15.99306869506836 Reward: -14.0\nEpsilon = 0.13641556359762214\nEpsilon = 0.13640192204126236\nEpsilon = 0.13638828184905824\nEpsilon = 0.13637464302087335\nEpsilon = 0.13636100555657127\nEpsilon = 0.1363473694560156\nEpsilon = 0.13633373471907\nEpsilon = 0.1363201013455981\nEpsilon = 0.13630646933546356\nEpsilon = 0.13629283868853\nEpsilon = 0.13627920940466115\nEpsilon = 0.1362655814837207\nEpsilon = 0.13625195492557232\nEpsilon = 0.13623832973007977\nEpsilon = 0.13622470589710675\nEpsilon = 0.13621108342651705\nEpsilon = 0.1361974623181744\nEpsilon = 0.13618384257194258\nEpsilon = 0.13617022418768537\nEpsilon = 0.13615660716526662\nAgent: ddqn_agent . Episode 1075/2000. Number of steps to finish: 20. Loss: 15.209970474243164 Reward: -18.0\nEpsilon = 0.1361429915045501\nEpsilon = 0.13612937720539964\nEpsilon = 0.1361157642676791\nEpsilon = 0.13610215269125234\nEpsilon = 0.1360885424759832\nEpsilon = 0.13607493362173562\nEpsilon = 0.13606132612837343\nEpsilon = 0.1360477199957606\nEpsilon = 0.13603411522376102\nEpsilon = 0.13602051181223865\nEpsilon = 0.13600690976105742\nEpsilon = 0.1359933090700813\nEpsilon = 0.1359797097391743\nEpsilon = 0.1359661117682004\nEpsilon = 0.13595251515702358\nEpsilon = 0.13593891990550788\nEpsilon = 0.13592532601351734\nEpsilon = 0.13591173348091598\nEpsilon = 0.1358981423075679\nEpsilon = 0.13588455249333714\nAgent: ddqn_agent . Episode 1076/2000. Number of steps to finish: 20. Loss: 15.554381370544434 Reward: -20.0\nEpsilon = 0.13587096403808782\nEpsilon = 0.135857376941684\nEpsilon = 0.13584379120398984\nEpsilon = 0.13583020682486946\nEpsilon = 0.13581662380418696\nEpsilon = 0.13580304214180655\nEpsilon = 0.13578946183759238\nEpsilon = 0.13577588289140863\nEpsilon = 0.1357623053031195\nEpsilon = 0.1357487290725892\nEpsilon = 0.13573515419968193\nEpsilon = 0.13572158068426196\nEpsilon = 0.13570800852619352\nEpsilon = 0.1356944377253409\nEpsilon = 0.13568086828156836\nEpsilon = 0.13566730019474021\nEpsilon = 0.13565373346472073\nEpsilon = 0.13564016809137427\nEpsilon = 0.13562660407456514\nEpsilon = 0.1356130414141577\nAgent: ddqn_agent . Episode 1077/2000. Number of steps to finish: 20. Loss: 16.029808044433594 Reward: -20.0\nEpsilon = 0.1355994801100163\nEpsilon = 0.1355859201620053\nEpsilon = 0.13557236156998909\nEpsilon = 0.13555880433383208\nEpsilon = 0.1355452484533987\nEpsilon = 0.13553169392855335\nEpsilon = 0.1355181407591605\nEpsilon = 0.13550458894508458\nEpsilon = 0.13549103848619007\nEpsilon = 0.13547748938234144\nEpsilon = 0.13546394163340322\nEpsilon = 0.13545039523923988\nEpsilon = 0.13543685019971596\nEpsilon = 0.135423306514696\nEpsilon = 0.13540976418404455\nEpsilon = 0.13539622320762615\nEpsilon = 0.1353826835853054\nEpsilon = 0.13536914531694685\nEpsilon = 0.13535560840241517\nEpsilon = 0.13534207284157493\nAgent: ddqn_agent . Episode 1078/2000. Number of steps to finish: 20. Loss: 16.06791114807129 Reward: -18.0\nEpsilon = 0.13532853863429078\nEpsilon = 0.13531500578042735\nEpsilon = 0.13530147427984932\nEpsilon = 0.13528794413242134\nEpsilon = 0.1352744153380081\nEpsilon = 0.1352608878964743\nEpsilon = 0.13524736180768465\nEpsilon = 0.13523383707150388\nEpsilon = 0.13522031368779674\nEpsilon = 0.13520679165642796\nEpsilon = 0.13519327097726233\nEpsilon = 0.1351797516501646\nEpsilon = 0.13516623367499958\nEpsilon = 0.1351527170516321\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.13513920177992694\nEpsilon = 0.13512568785974896\nEpsilon = 0.13511217529096298\nEpsilon = 0.13509866407343388\nEpsilon = 0.13508515420702655\nEpsilon = 0.13507164569160585\nAgent: ddqn_agent . Episode 1079/2000. Number of steps to finish: 20. Loss: 15.163700103759766 Reward: -14.0\nEpsilon = 0.1350581385270367\nEpsilon = 0.13504463271318398\nEpsilon = 0.13503112824991267\nEpsilon = 0.13501762513708768\nEpsilon = 0.13500412337457396\nEpsilon = 0.1349906229622365\nEpsilon = 0.13497712389994027\nEpsilon = 0.13496362618755028\nEpsilon = 0.13495012982493154\nEpsilon = 0.13493663481194906\nEpsilon = 0.13492314114846787\nEpsilon = 0.13490964883435302\nEpsilon = 0.1348961578694696\nEpsilon = 0.13488266825368264\nEpsilon = 0.13486917998685727\nEpsilon = 0.13485569306885858\nEpsilon = 0.1348422074995517\nEpsilon = 0.13482872327880174\nEpsilon = 0.13481524040647386\nEpsilon = 0.1348017588824332\nAgent: ddqn_agent . Episode 1080/2000. Number of steps to finish: 20. Loss: 15.205100059509277 Reward: -14.0\nEpsilon = 0.13478827870654497\nEpsilon = 0.13477479987867433\nEpsilon = 0.13476132239868646\nEpsilon = 0.1347478462664466\nEpsilon = 0.13473437148181994\nEpsilon = 0.13472089804467174\nEpsilon = 0.1347074259548673\nEpsilon = 0.13469395521227182\nEpsilon = 0.1346804858167506\nEpsilon = 0.13466701776816892\nEpsilon = 0.1346535510663921\nEpsilon = 0.13464008571128547\nEpsilon = 0.13462662170271433\nEpsilon = 0.13461315904054405\nEpsilon = 0.13459969772464\nEpsilon = 0.13458623775486755\nEpsilon = 0.13457277913109206\nEpsilon = 0.13455932185317895\nEpsilon = 0.13454586592099363\nEpsilon = 0.13453241133440152\nAgent: ddqn_agent . Episode 1081/2000. Number of steps to finish: 20. Loss: 15.335031509399414 Reward: -14.0\nEpsilon = 0.13451895809326808\nEpsilon = 0.13450550619745877\nEpsilon = 0.13449205564683903\nEpsilon = 0.13447860644127435\nEpsilon = 0.1344651585806302\nEpsilon = 0.13445171206477216\nEpsilon = 0.1344382668935657\nEpsilon = 0.13442482306687634\nEpsilon = 0.13441138058456964\nEpsilon = 0.13439793944651118\nEpsilon = 0.13438449965256655\nEpsilon = 0.13437106120260128\nEpsilon = 0.13435762409648103\nEpsilon = 0.13434418833407138\nEpsilon = 0.13433075391523797\nEpsilon = 0.13431732083984643\nEpsilon = 0.13430388910776245\nEpsilon = 0.13429045871885167\nEpsilon = 0.1342770296729798\nEpsilon = 0.1342636019700125\nAgent: ddqn_agent . Episode 1082/2000. Number of steps to finish: 20. Loss: 15.608711242675781 Reward: -16.0\nEpsilon = 0.1342501756098155\nEpsilon = 0.1342367505922545\nEpsilon = 0.1342233269171953\nEpsilon = 0.13420990458450358\nEpsilon = 0.13419648359404512\nEpsilon = 0.13418306394568572\nEpsilon = 0.13416964563929115\nEpsilon = 0.13415622867472723\nEpsilon = 0.13414281305185977\nEpsilon = 0.13412939877055458\nEpsilon = 0.13411598583067752\nEpsilon = 0.13410257423209446\nEpsilon = 0.13408916397467124\nEpsilon = 0.13407575505827377\nEpsilon = 0.13406234748276794\nEpsilon = 0.13404894124801967\nEpsilon = 0.13403553635389487\nEpsilon = 0.13402213280025949\nEpsilon = 0.13400873058697946\nEpsilon = 0.13399532971392075\nAgent: ddqn_agent . Episode 1083/2000. Number of steps to finish: 20. Loss: 15.2111177444458 Reward: -20.0\nEpsilon = 0.13398193018094937\nEpsilon = 0.13396853198793127\nEpsilon = 0.13395513513473248\nEpsilon = 0.133941739621219\nEpsilon = 0.13392834544725687\nEpsilon = 0.13391495261271216\nEpsilon = 0.13390156111745088\nEpsilon = 0.13388817096133915\nEpsilon = 0.133874782144243\nEpsilon = 0.1338613946660286\nEpsilon = 0.13384800852656198\nEpsilon = 0.1338346237257093\nEpsilon = 0.13382124026333675\nEpsilon = 0.13380785813931043\nEpsilon = 0.1337944773534965\nEpsilon = 0.13378109790576115\nEpsilon = 0.13376771979597057\nEpsilon = 0.13375434302399097\nEpsilon = 0.13374096758968856\nEpsilon = 0.1337275934929296\nAgent: ddqn_agent . Episode 1084/2000. Number of steps to finish: 20. Loss: 15.72077465057373 Reward: -10.0\nEpsilon = 0.1337142207335803\nEpsilon = 0.13370084931150694\nEpsilon = 0.13368747922657578\nEpsilon = 0.13367411047865313\nEpsilon = 0.13366074306760525\nEpsilon = 0.13364737699329848\nEpsilon = 0.13363401225559915\nEpsilon = 0.13362064885437358\nEpsilon = 0.13360728678948813\nEpsilon = 0.1335939260608092\nEpsilon = 0.13358056666820312\nEpsilon = 0.1335672086115363\nEpsilon = 0.13355385189067515\nEpsilon = 0.13354049650548608\nEpsilon = 0.13352714245583552\nEpsilon = 0.13351378974158995\nEpsilon = 0.1335004383626158\nEpsilon = 0.13348708831877953\nEpsilon = 0.13347373960994766\nEpsilon = 0.13346039223598666\nAgent: ddqn_agent . Episode 1085/2000. Number of steps to finish: 20. Loss: 15.807442665100098 Reward: -12.0\nEpsilon = 0.13344704619676306\nEpsilon = 0.1334337014921434\nEpsilon = 0.13342035812199418\nEpsilon = 0.133407016086182\nEpsilon = 0.1333936753845734\nEpsilon = 0.13338033601703495\nEpsilon = 0.13336699798343324\nEpsilon = 0.1333536612836349\nEpsilon = 0.13334032591750655\nEpsilon = 0.13332699188491481\nEpsilon = 0.13331365918572632\nEpsilon = 0.13330032781980775\nEpsilon = 0.13328699778702577\nEpsilon = 0.13327366908724705\nEpsilon = 0.13326034172033832\nEpsilon = 0.1332470156861663\nEpsilon = 0.13323369098459767\nEpsilon = 0.1332203676154992\nEpsilon = 0.13320704557873767\nEpsilon = 0.1331937248741798\nAgent: ddqn_agent . Episode 1086/2000. Number of steps to finish: 20. Loss: 16.03436279296875 Reward: -14.0\nEpsilon = 0.13318040550169238\nEpsilon = 0.13316708746114223\nEpsilon = 0.13315377075239612\nEpsilon = 0.13314045537532088\nEpsilon = 0.13312714132978334\nEpsilon = 0.13311382861565035\nEpsilon = 0.1331005172327888\nEpsilon = 0.1330872071810655\nEpsilon = 0.1330738984603474\nEpsilon = 0.13306059107050136\nEpsilon = 0.13304728501139432\nEpsilon = 0.13303398028289318\nEpsilon = 0.13302067688486488\nEpsilon = 0.1330073748171764\nEpsilon = 0.13299407407969469\nEpsilon = 0.13298077467228672\nEpsilon = 0.1329674765948195\nEpsilon = 0.13295417984716001\nEpsilon = 0.1329408844291753\nEpsilon = 0.13292759034073237\nAgent: ddqn_agent . Episode 1087/2000. Number of steps to finish: 20. Loss: 16.652040481567383 Reward: -14.0\nEpsilon = 0.1329142975816983\nEpsilon = 0.13290100615194012\nEpsilon = 0.13288771605132493\nEpsilon = 0.1328744272797198\nEpsilon = 0.13286113983699183\nEpsilon = 0.13284785372300814\nEpsilon = 0.13283456893763584\nEpsilon = 0.13282128548074207\nEpsilon = 0.132808003352194\nEpsilon = 0.13279472255185878\nEpsilon = 0.1327814430796036\nEpsilon = 0.13276816493529564\nEpsilon = 0.13275488811880212\nEpsilon = 0.13274161262999024\nEpsilon = 0.13272833846872722\nEpsilon = 0.13271506563488036\nEpsilon = 0.13270179412831687\nEpsilon = 0.13268852394890404\nEpsilon = 0.13267525509650915\nEpsilon = 0.1326619875709995\nAgent: ddqn_agent . Episode 1088/2000. Number of steps to finish: 20. Loss: 15.58520793914795 Reward: -14.0\nEpsilon = 0.1326487213722424\nEpsilon = 0.13263545650010516\nEpsilon = 0.13262219295445515\nEpsilon = 0.1326089307351597\nEpsilon = 0.13259566984208618\nEpsilon = 0.13258241027510198\nEpsilon = 0.13256915203407446\nEpsilon = 0.13255589511887106\nEpsilon = 0.13254263952935919\nEpsilon = 0.13252938526540625\nEpsilon = 0.13251613232687973\nEpsilon = 0.13250288071364705\nEpsilon = 0.13248963042557568\nEpsilon = 0.13247638146253313\nEpsilon = 0.13246313382438688\nEpsilon = 0.13244988751100445\nEpsilon = 0.13243664252225334\nEpsilon = 0.1324233988580011\nEpsilon = 0.1324101565181153\nEpsilon = 0.1323969155024635\nAgent: ddqn_agent . Episode 1089/2000. Number of steps to finish: 20. Loss: 15.256839752197266 Reward: -12.0\nEpsilon = 0.13238367581091326\nEpsilon = 0.13237043744333216\nEpsilon = 0.13235720039958782\nEpsilon = 0.13234396467954787\nEpsilon = 0.13233073028307993\nEpsilon = 0.13231749721005162\nEpsilon = 0.1323042654603306\nEpsilon = 0.1322910350337846\nEpsilon = 0.1322778059302812\nEpsilon = 0.13226457814968817\nEpsilon = 0.13225135169187321\nEpsilon = 0.13223812655670403\nEpsilon = 0.13222490274404836\nEpsilon = 0.13221168025377394\nEpsilon = 0.13219845908574857\nEpsilon = 0.13218523923984\nEpsilon = 0.13217202071591602\nEpsilon = 0.13215880351384443\nEpsilon = 0.13214558763349304\nEpsilon = 0.1321323730747297\nAgent: ddqn_agent . Episode 1090/2000. Number of steps to finish: 20. Loss: 15.4434814453125 Reward: -12.0\nEpsilon = 0.13211915983742223\nEpsilon = 0.13210594792143848\nEpsilon = 0.13209273732664634\nEpsilon = 0.13207952805291368\nEpsilon = 0.13206632010010838\nEpsilon = 0.13205311346809837\nEpsilon = 0.13203990815675157\nEpsilon = 0.1320267041659359\nEpsilon = 0.1320135014955193\nEpsilon = 0.13200030014536976\nEpsilon = 0.13198710011535522\nEpsilon = 0.13197390140534368\nEpsilon = 0.13196070401520316\nEpsilon = 0.13194750794480165\nEpsilon = 0.13193431319400717\nEpsilon = 0.13192111976268778\nEpsilon = 0.13190792765071152\nEpsilon = 0.13189473685794645\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.13188154738426067\nEpsilon = 0.13186835922952225\nAgent: ddqn_agent . Episode 1091/2000. Number of steps to finish: 20. Loss: 15.971372604370117 Reward: -20.0\nEpsilon = 0.1318551723935993\nEpsilon = 0.13184198687635995\nEpsilon = 0.13182880267767233\nEpsilon = 0.13181561979740455\nEpsilon = 0.1318024382354248\nEpsilon = 0.13178925799160127\nEpsilon = 0.1317760790658021\nEpsilon = 0.13176290145789551\nEpsilon = 0.13174972516774971\nEpsilon = 0.13173655019523295\nEpsilon = 0.13172337654021343\nEpsilon = 0.1317102042025594\nEpsilon = 0.13169703318213916\nEpsilon = 0.13168386347882094\nEpsilon = 0.13167069509247306\nEpsilon = 0.13165752802296382\nEpsilon = 0.13164436227016152\nEpsilon = 0.1316311978339345\nEpsilon = 0.13161803471415112\nEpsilon = 0.1316048729106797\nAgent: ddqn_agent . Episode 1092/2000. Number of steps to finish: 20. Loss: 15.521013259887695 Reward: -18.0\nEpsilon = 0.13159171242338866\nEpsilon = 0.13157855325214632\nEpsilon = 0.1315653953968211\nEpsilon = 0.13155223885728143\nEpsilon = 0.1315390836333957\nEpsilon = 0.13152592972503235\nEpsilon = 0.13151277713205986\nEpsilon = 0.13149962585434666\nEpsilon = 0.1314864758917612\nEpsilon = 0.13147332724417204\nEpsilon = 0.13146017991144762\nEpsilon = 0.13144703389345647\nEpsilon = 0.13143388919006713\nEpsilon = 0.13142074580114813\nEpsilon = 0.131407603726568\nEpsilon = 0.13139446296619536\nEpsilon = 0.13138132351989876\nEpsilon = 0.13136818538754677\nEpsilon = 0.131355048569008\nEpsilon = 0.1313419130641511\nAgent: ddqn_agent . Episode 1093/2000. Number of steps to finish: 20. Loss: 16.150651931762695 Reward: -10.0\nEpsilon = 0.1313287788728447\nEpsilon = 0.1313156459949574\nEpsilon = 0.1313025144303579\nEpsilon = 0.13128938417891486\nEpsilon = 0.13127625524049696\nEpsilon = 0.1312631276149729\nEpsilon = 0.13125000130221143\nEpsilon = 0.1312368763020812\nEpsilon = 0.131223752614451\nEpsilon = 0.13121063023918955\nEpsilon = 0.13119750917616563\nEpsilon = 0.13118438942524802\nEpsilon = 0.1311712709863055\nEpsilon = 0.13115815385920687\nEpsilon = 0.13114503804382097\nEpsilon = 0.1311319235400166\nEpsilon = 0.1311188103476626\nEpsilon = 0.13110569846662784\nEpsilon = 0.1310925878967812\nEpsilon = 0.13107947863799152\nAgent: ddqn_agent . Episode 1094/2000. Number of steps to finish: 20. Loss: 16.526792526245117 Reward: -18.0\nEpsilon = 0.13106637069012772\nEpsilon = 0.1310532640530587\nEpsilon = 0.1310401587266534\nEpsilon = 0.13102705471078074\nEpsilon = 0.13101395200530966\nEpsilon = 0.13100085061010913\nEpsilon = 0.13098775052504813\nEpsilon = 0.13097465174999562\nEpsilon = 0.13096155428482062\nEpsilon = 0.13094845812939213\nEpsilon = 0.1309353632835792\nEpsilon = 0.13092226974725082\nEpsilon = 0.1309091775202761\nEpsilon = 0.13089608660252405\nEpsilon = 0.1308829969938638\nEpsilon = 0.1308699086941644\nEpsilon = 0.130856821703295\nEpsilon = 0.13084373602112467\nEpsilon = 0.13083065164752256\nEpsilon = 0.1308175685823578\nAgent: ddqn_agent . Episode 1095/2000. Number of steps to finish: 20. Loss: 16.48747444152832 Reward: -16.0\nEpsilon = 0.13080448682549956\nEpsilon = 0.130791406376817\nEpsilon = 0.13077832723617933\nEpsilon = 0.1307652494034557\nEpsilon = 0.13075217287851537\nEpsilon = 0.13073909766122752\nEpsilon = 0.1307260237514614\nEpsilon = 0.13071295114908624\nEpsilon = 0.13069987985397133\nEpsilon = 0.13068680986598594\nEpsilon = 0.13067374118499933\nEpsilon = 0.13066067381088084\nEpsilon = 0.13064760774349976\nEpsilon = 0.13063454298272542\nEpsilon = 0.13062147952842715\nEpsilon = 0.1306084173804743\nEpsilon = 0.13059535653873625\nEpsilon = 0.13058229700308238\nEpsilon = 0.13056923877338208\nEpsilon = 0.13055618184950474\nAgent: ddqn_agent . Episode 1096/2000. Number of steps to finish: 20. Loss: 16.128931045532227 Reward: -14.0\nEpsilon = 0.13054312623131978\nEpsilon = 0.13053007191869664\nEpsilon = 0.13051701891150477\nEpsilon = 0.13050396720961363\nEpsilon = 0.13049091681289268\nEpsilon = 0.1304778677212114\nEpsilon = 0.13046481993443929\nEpsilon = 0.13045177345244585\nAgent: ddqn_agent . Episode 1097/2000. Number of steps to finish: 8. Loss: 6.320895195007324 Reward: 4.0\nEpsilon = 0.1304387282751006\nEpsilon = 0.13042568440227312\nEpsilon = 0.1304126418338329\nEpsilon = 0.1303996005696495\nEpsilon = 0.13038656060959253\nEpsilon = 0.13037352195353158\nEpsilon = 0.13036048460133623\nEpsilon = 0.1303474485528761\nEpsilon = 0.13033441380802083\nEpsilon = 0.13032138036664004\nEpsilon = 0.13030834822860338\nEpsilon = 0.13029531739378053\nEpsilon = 0.13028228786204116\nEpsilon = 0.13026925963325495\nEpsilon = 0.1302562327072916\nEpsilon = 0.13024320708402087\nEpsilon = 0.13023018276331247\nEpsilon = 0.13021715974503614\nEpsilon = 0.13020413802906164\nEpsilon = 0.13019111761525873\nAgent: ddqn_agent . Episode 1098/2000. Number of steps to finish: 20. Loss: 16.66252326965332 Reward: -18.0\nEpsilon = 0.1301780985034972\nEpsilon = 0.13016508069364685\nEpsilon = 0.13015206418557748\nEpsilon = 0.13013904897915893\nEpsilon = 0.13012603507426102\nEpsilon = 0.1301130224707536\nEpsilon = 0.13010001116850653\nEpsilon = 0.1300870011673897\nEpsilon = 0.13007399246727294\nEpsilon = 0.1300609850680262\nEpsilon = 0.13004797896951942\nEpsilon = 0.13003497417162246\nEpsilon = 0.1300219706742053\nEpsilon = 0.13000896847713786\nEpsilon = 0.12999596758029014\nEpsilon = 0.12998296798353212\nEpsilon = 0.12996996968673377\nEpsilon = 0.1299569726897651\nEpsilon = 0.12994397699249613\nEpsilon = 0.12993098259479688\nAgent: ddqn_agent . Episode 1099/2000. Number of steps to finish: 20. Loss: 15.0676851272583 Reward: -18.0\nEpsilon = 0.1299179894965374\nEpsilon = 0.12990499769758776\nEpsilon = 0.12989200719781802\nEpsilon = 0.12987901799709822\nEpsilon = 0.1298660300952985\nEpsilon = 0.129853043492289\nEpsilon = 0.12984005818793976\nEpsilon = 0.12982707418212097\nEpsilon = 0.12981409147470277\nEpsilon = 0.1298011100655553\nEpsilon = 0.12978812995454875\nEpsilon = 0.1297751511415533\nEpsilon = 0.12976217362643916\nEpsilon = 0.1297491974090765\nEpsilon = 0.1297362224893356\nEpsilon = 0.1297232488670867\nEpsilon = 0.1297102765422\nEpsilon = 0.12969730551454578\nEpsilon = 0.12968433578399433\nEpsilon = 0.12967136735041593\nAgent: ddqn_agent . Episode 1100/2000. Number of steps to finish: 20. Loss: 16.763507843017578 Reward: -14.0\nEpsilon = 0.12965840021368089\nEpsilon = 0.12964543437365952\nEpsilon = 0.12963246983022217\nEpsilon = 0.12961950658323915\nEpsilon = 0.1296065446325808\nEpsilon = 0.12959358397811754\nEpsilon = 0.12958062461971973\nEpsilon = 0.12956766655725777\nEpsilon = 0.12955470979060205\nEpsilon = 0.12954175431962298\nEpsilon = 0.12952880014419102\nEpsilon = 0.12951584726417661\nEpsilon = 0.1295028956794502\nEpsilon = 0.12948994538988226\nEpsilon = 0.12947699639534327\nEpsilon = 0.12946404869570374\nEpsilon = 0.12945110229083417\nEpsilon = 0.12943815718060508\nEpsilon = 0.12942521336488702\nEpsilon = 0.12941227084355053\nAgent: ddqn_agent . Episode 1101/2000. Number of steps to finish: 20. Loss: 15.082782745361328 Reward: -10.0\nEpsilon = 0.1293993296164662\nEpsilon = 0.12938638968350455\nEpsilon = 0.1293734510445362\nEpsilon = 0.12936051369943175\nEpsilon = 0.1293475776480618\nEpsilon = 0.12933464289029697\nEpsilon = 0.12932170942600796\nEpsilon = 0.12930877725506537\nEpsilon = 0.12929584637733987\nEpsilon = 0.12928291679270215\nEpsilon = 0.1292699885010229\nEpsilon = 0.12925706150217278\nEpsilon = 0.12924413579602256\nEpsilon = 0.12923121138244295\nEpsilon = 0.12921828826130471\nEpsilon = 0.1292053664324786\nEpsilon = 0.12919244589583534\nEpsilon = 0.12917952665124577\nEpsilon = 0.12916660869858065\nEpsilon = 0.1291536920377108\nAgent: ddqn_agent . Episode 1102/2000. Number of steps to finish: 20. Loss: 15.635185241699219 Reward: -12.0\nEpsilon = 0.129140776668507\nEpsilon = 0.12912786259084016\nEpsilon = 0.12911494980458108\nEpsilon = 0.12910203830960063\nEpsilon = 0.12908912810576967\nEpsilon = 0.12907621919295909\nEpsilon = 0.12906331157103978\nEpsilon = 0.12905040523988268\nEpsilon = 0.1290375001993587\nEpsilon = 0.12902459644933875\nEpsilon = 0.1290116939896938\nEpsilon = 0.12899879282029483\nEpsilon = 0.12898589294101281\nEpsilon = 0.12897299435171872\nEpsilon = 0.12896009705228353\nEpsilon = 0.1289472010425783\nEpsilon = 0.12893430632247405\nEpsilon = 0.1289214128918418\nEpsilon = 0.12890852075055262\nEpsilon = 0.12889562989847755\nAgent: ddqn_agent . Episode 1103/2000. Number of steps to finish: 20. Loss: 16.244800567626953 Reward: -16.0\nEpsilon = 0.12888274033548772\nEpsilon = 0.12886985206145418\nEpsilon = 0.12885696507624803\nEpsilon = 0.1288440793797404\nEpsilon = 0.12883119497180243\nEpsilon = 0.12881831185230525\nEpsilon = 0.12880543002112002\nEpsilon = 0.1287925494781179\nEpsilon = 0.1287796702231701\nEpsilon = 0.1287667922561478\nEpsilon = 0.12875391557692217\nEpsilon = 0.12874104018536447\nEpsilon = 0.12872816608134593\nEpsilon = 0.1287152932647378\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.12870242173541133\nEpsilon = 0.1286895514932378\nEpsilon = 0.12867668253808848\nEpsilon = 0.12866381486983466\nEpsilon = 0.12865094848834768\nEpsilon = 0.12863808339349886\nAgent: ddqn_agent . Episode 1104/2000. Number of steps to finish: 20. Loss: 15.83254337310791 Reward: -14.0\nEpsilon = 0.12862521958515952\nEpsilon = 0.128612357063201\nEpsilon = 0.1285994958274947\nEpsilon = 0.12858663587791194\nEpsilon = 0.12857377721432414\nEpsilon = 0.1285609198366027\nEpsilon = 0.12854806374461905\nEpsilon = 0.12853520893824458\nEpsilon = 0.12852235541735074\nEpsilon = 0.128509503181809\nEpsilon = 0.12849665223149084\nEpsilon = 0.1284838025662677\nEpsilon = 0.1284709541860111\nEpsilon = 0.1284581070905925\nEpsilon = 0.12844526127988343\nEpsilon = 0.12843241675375544\nEpsilon = 0.12841957351208005\nEpsilon = 0.12840673155472884\nEpsilon = 0.12839389088157338\nEpsilon = 0.12838105149248522\nAgent: ddqn_agent . Episode 1105/2000. Number of steps to finish: 20. Loss: 15.939223289489746 Reward: -14.0\nEpsilon = 0.12836821338733598\nEpsilon = 0.12835537656599724\nEpsilon = 0.12834254102834064\nEpsilon = 0.1283297067742378\nEpsilon = 0.12831687380356038\nEpsilon = 0.12830404211618002\nEpsilon = 0.1282912117119684\nEpsilon = 0.12827838259079719\nEpsilon = 0.12826555475253812\nEpsilon = 0.12825272819706288\nEpsilon = 0.12823990292424317\nEpsilon = 0.12822707893395074\nEpsilon = 0.12821425622605734\nEpsilon = 0.12820143480043472\nEpsilon = 0.1281886146569547\nEpsilon = 0.128175795795489\nEpsilon = 0.12816297821590944\nEpsilon = 0.12815016191808784\nEpsilon = 0.12813734690189604\nEpsilon = 0.12812453316720585\nAgent: ddqn_agent . Episode 1106/2000. Number of steps to finish: 20. Loss: 15.248394012451172 Reward: -16.0\nEpsilon = 0.12811172071388913\nEpsilon = 0.12809890954181774\nEpsilon = 0.12808609965086357\nEpsilon = 0.12807329104089848\nEpsilon = 0.1280604837117944\nEpsilon = 0.12804767766342323\nEpsilon = 0.1280348728956569\nEpsilon = 0.12802206940836733\nEpsilon = 0.1280092672014265\nEpsilon = 0.12799646627470634\nEpsilon = 0.12798366662807886\nEpsilon = 0.12797086826141604\nEpsilon = 0.1279580711745899\nEpsilon = 0.12794527536747244\nEpsilon = 0.1279324808399357\nEpsilon = 0.1279196875918517\nEpsilon = 0.12790689562309251\nEpsilon = 0.1278941049335302\nEpsilon = 0.12788131552303686\nEpsilon = 0.12786852739148455\nAgent: ddqn_agent . Episode 1107/2000. Number of steps to finish: 20. Loss: 15.036116600036621 Reward: -18.0\nEpsilon = 0.1278557405387454\nEpsilon = 0.12784295496469153\nEpsilon = 0.12783017066919505\nEpsilon = 0.12781738765212813\nEpsilon = 0.12780460591336293\nEpsilon = 0.1277918254527716\nEpsilon = 0.12777904627022632\nEpsilon = 0.1277662683655993\nEpsilon = 0.12775349173876274\nEpsilon = 0.12774071638958887\nEpsilon = 0.1277279423179499\nEpsilon = 0.12771516952371811\nEpsilon = 0.12770239800676575\nEpsilon = 0.12768962776696507\nEpsilon = 0.1276768588041884\nEpsilon = 0.12766409111830798\nEpsilon = 0.12765132470919616\nEpsilon = 0.12763855957672524\nEpsilon = 0.12762579572076757\nEpsilon = 0.1276130331411955\nAgent: ddqn_agent . Episode 1108/2000. Number of steps to finish: 20. Loss: 15.460576057434082 Reward: -14.0\nEpsilon = 0.12760027183788136\nEpsilon = 0.12758751181069758\nEpsilon = 0.12757475305951652\nEpsilon = 0.12756199558421055\nEpsilon = 0.12754923938465212\nEpsilon = 0.12753648446071367\nEpsilon = 0.1275237308122676\nEpsilon = 0.12751097843918635\nEpsilon = 0.12749822734134245\nEpsilon = 0.1274854775186083\nEpsilon = 0.12747272897085646\nEpsilon = 0.12745998169795938\nEpsilon = 0.12744723569978958\nEpsilon = 0.1274344909762196\nEpsilon = 0.12742174752712196\nEpsilon = 0.12740900535236924\nEpsilon = 0.127396264451834\nEpsilon = 0.1273835248253888\nEpsilon = 0.12737078647290628\nEpsilon = 0.127358049394259\nAgent: ddqn_agent . Episode 1109/2000. Number of steps to finish: 20. Loss: 16.576807022094727 Reward: -12.0\nEpsilon = 0.12734531358931955\nEpsilon = 0.1273325790579606\nEpsilon = 0.12731984580005481\nEpsilon = 0.1273071138154748\nEpsilon = 0.12729438310409324\nEpsilon = 0.12728165366578284\nEpsilon = 0.12726892550041627\nEpsilon = 0.12725619860786622\nEpsilon = 0.12724347298800542\nEpsilon = 0.12723074864070663\nEpsilon = 0.12721802556584255\nEpsilon = 0.12720530376328598\nEpsilon = 0.12719258323290966\nEpsilon = 0.12717986397458636\nEpsilon = 0.1271671459881889\nEpsilon = 0.12715442927359008\nEpsilon = 0.12714171383066272\nEpsilon = 0.12712899965927965\nEpsilon = 0.12711628675931372\nEpsilon = 0.12710357513063777\nAgent: ddqn_agent . Episode 1110/2000. Number of steps to finish: 20. Loss: 16.73104476928711 Reward: -20.0\nEpsilon = 0.1270908647731247\nEpsilon = 0.12707815568664738\nEpsilon = 0.1270654478710787\nEpsilon = 0.1270527413262916\nEpsilon = 0.12704003605215897\nEpsilon = 0.12702733204855376\nEpsilon = 0.1270146293153489\nEpsilon = 0.12700192785241737\nEpsilon = 0.12698922765963214\nEpsilon = 0.12697652873686618\nEpsilon = 0.1269638310839925\nEpsilon = 0.12695113470088412\nEpsilon = 0.12693843958741405\nEpsilon = 0.1269257457434553\nEpsilon = 0.12691305316888096\nEpsilon = 0.12690036186356407\nEpsilon = 0.12688767182737773\nEpsilon = 0.12687498306019498\nEpsilon = 0.12686229556188897\nEpsilon = 0.12684960933233277\nAgent: ddqn_agent . Episode 1111/2000. Number of steps to finish: 20. Loss: 15.774178504943848 Reward: -18.0\nEpsilon = 0.12683692437139954\nEpsilon = 0.1268242406789624\nEpsilon = 0.12681155825489449\nEpsilon = 0.126798877099069\nEpsilon = 0.12678619721135911\nEpsilon = 0.126773518591638\nEpsilon = 0.12676084123977882\nEpsilon = 0.12674816515565485\nEpsilon = 0.12673549033913928\nEpsilon = 0.12672281679010536\nEpsilon = 0.12671014450842635\nEpsilon = 0.1266974734939755\nEpsilon = 0.1266848037466261\nEpsilon = 0.12667213526625146\nEpsilon = 0.12665946805272482\nEpsilon = 0.12664680210591955\nEpsilon = 0.12663413742570898\nEpsilon = 0.1266214740119664\nEpsilon = 0.1266088118645652\nEpsilon = 0.12659615098337876\nAgent: ddqn_agent . Episode 1112/2000. Number of steps to finish: 20. Loss: 15.281225204467773 Reward: -10.0\nEpsilon = 0.12658349136828043\nEpsilon = 0.1265708330191436\nEpsilon = 0.12655817593584168\nEpsilon = 0.1265455201182481\nEpsilon = 0.12653286556623627\nEpsilon = 0.12652021227967966\nEpsilon = 0.1265075602584517\nEpsilon = 0.12649490950242584\nEpsilon = 0.1264822600114756\nEpsilon = 0.12646961178547447\nEpsilon = 0.12645696482429591\nEpsilon = 0.12644431912781348\nEpsilon = 0.1264316746959007\nEpsilon = 0.1264190315284311\nEpsilon = 0.12640638962527825\nEpsilon = 0.12639374898631572\nEpsilon = 0.1263811096114171\nEpsilon = 0.12636847150045596\nEpsilon = 0.1263558346533059\nEpsilon = 0.12634319906984057\nAgent: ddqn_agent . Episode 1113/2000. Number of steps to finish: 20. Loss: 16.6320858001709 Reward: -18.0\nEpsilon = 0.1263305647499336\nEpsilon = 0.1263179316934586\nEpsilon = 0.12630529990028924\nEpsilon = 0.1262926693702992\nEpsilon = 0.12628004010336216\nEpsilon = 0.12626741209935183\nEpsilon = 0.12625478535814189\nEpsilon = 0.12624215987960608\nEpsilon = 0.1262295356636181\nEpsilon = 0.12621691271005175\nEpsilon = 0.12620429101878075\nEpsilon = 0.12619167058967887\nEpsilon = 0.1261790514226199\nEpsilon = 0.12616643351747764\nEpsilon = 0.12615381687412588\nEpsilon = 0.1261412014924385\nEpsilon = 0.12612858737228924\nEpsilon = 0.12611597451355203\nEpsilon = 0.12610336291610066\nEpsilon = 0.12609075257980906\nAgent: ddqn_agent . Episode 1114/2000. Number of steps to finish: 20. Loss: 16.064044952392578 Reward: -12.0\nEpsilon = 0.12607814350455107\nEpsilon = 0.12606553569020063\nEpsilon = 0.1260529291366316\nEpsilon = 0.12604032384371794\nEpsilon = 0.12602771981133357\nEpsilon = 0.12601511703935245\nEpsilon = 0.12600251552764852\nEpsilon = 0.12598991527609577\nEpsilon = 0.12597731628456815\nEpsilon = 0.1259647185529397\nEpsilon = 0.12595212208108442\nEpsilon = 0.1259395268688763\nEpsilon = 0.12592693291618942\nEpsilon = 0.1259143402228978\nEpsilon = 0.1259017487888755\nEpsilon = 0.1258891586139966\nEpsilon = 0.1258765696981352\nEpsilon = 0.1258639820411654\nEpsilon = 0.12585139564296127\nEpsilon = 0.125838810503397\nAgent: ddqn_agent . Episode 1115/2000. Number of steps to finish: 20. Loss: 14.983695983886719 Reward: -10.0\nEpsilon = 0.12582622662234666\nEpsilon = 0.12581364399968442\nEpsilon = 0.12580106263528446\nEpsilon = 0.12578848252902092\nEpsilon = 0.12577590368076802\nEpsilon = 0.12576332609039995\nEpsilon = 0.1257507497577909\nEpsilon = 0.12573817468281512\nEpsilon = 0.12572560086534684\nEpsilon = 0.12571302830526032\nEpsilon = 0.1257004570024298\nEpsilon = 0.12568788695672956\nEpsilon = 0.12567531816803387\nEpsilon = 0.12566275063621707\nEpsilon = 0.12565018436115344\nEpsilon = 0.12563761934271733\nEpsilon = 0.12562505558078307\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.12561249307522498\nEpsilon = 0.12559993182591747\nEpsilon = 0.12558737183273488\nAgent: ddqn_agent . Episode 1116/2000. Number of steps to finish: 20. Loss: 15.695589065551758 Reward: -14.0\nEpsilon = 0.1255748130955516\nEpsilon = 0.12556225561424206\nEpsilon = 0.12554969938868063\nEpsilon = 0.12553714441874175\nEpsilon = 0.1255245907042999\nEpsilon = 0.12551203824522947\nEpsilon = 0.12549948704140496\nEpsilon = 0.1254869370927008\nEpsilon = 0.12547438839899155\nEpsilon = 0.12546184096015164\nEpsilon = 0.12544929477605563\nEpsilon = 0.12543674984657802\nEpsilon = 0.12542420617159336\nEpsilon = 0.1254116637509762\nEpsilon = 0.12539912258460112\nEpsilon = 0.12538658267234265\nEpsilon = 0.12537404401407543\nEpsilon = 0.12536150660967402\nEpsilon = 0.12534897045901305\nEpsilon = 0.12533643556196716\nAgent: ddqn_agent . Episode 1117/2000. Number of steps to finish: 20. Loss: 16.082256317138672 Reward: -14.0\nEpsilon = 0.12532390191841097\nEpsilon = 0.12531136952821914\nEpsilon = 0.12529883839126632\nEpsilon = 0.1252863085074272\nEpsilon = 0.12527377987657645\nEpsilon = 0.1252612524985888\nEpsilon = 0.12524872637333895\nEpsilon = 0.1252362015007016\nEpsilon = 0.12522367788055155\nEpsilon = 0.1252111555127635\nEpsilon = 0.1251986343972122\nEpsilon = 0.12518611453377249\nEpsilon = 0.1251735959223191\nEpsilon = 0.12516107856272687\nEpsilon = 0.1251485624548706\nEpsilon = 0.12513604759862512\nEpsilon = 0.12512353399386525\nEpsilon = 0.12511102164046586\nEpsilon = 0.12509851053830182\nEpsilon = 0.12508600068724798\nAgent: ddqn_agent . Episode 1118/2000. Number of steps to finish: 20. Loss: 16.906850814819336 Reward: -12.0\nEpsilon = 0.12507349208717927\nEpsilon = 0.12506098473797056\nEpsilon = 0.12504847863949675\nEpsilon = 0.1250359737916328\nEpsilon = 0.12502347019425364\nEpsilon = 0.12501096784723423\nEpsilon = 0.12499846675044951\nEpsilon = 0.12498596690377446\nEpsilon = 0.12497346830708408\nEpsilon = 0.12496097096025338\nEpsilon = 0.12494847486315735\nEpsilon = 0.12493598001567104\nEpsilon = 0.12492348641766947\nEpsilon = 0.1249109940690277\nEpsilon = 0.1248985029696208\nEpsilon = 0.12488601311932383\nEpsilon = 0.1248735245180119\nEpsilon = 0.1248610371655601\nEpsilon = 0.12484855106184355\nEpsilon = 0.12483606620673736\nAgent: ddqn_agent . Episode 1119/2000. Number of steps to finish: 20. Loss: 15.941436767578125 Reward: -16.0\nEpsilon = 0.12482358260011668\nEpsilon = 0.12481110024185667\nEpsilon = 0.12479861913183249\nEpsilon = 0.12478613926991931\nEpsilon = 0.12477366065599232\nEpsilon = 0.12476118328992673\nEpsilon = 0.12474870717159774\nEpsilon = 0.12473623230088057\nEpsilon = 0.12472375867765048\nEpsilon = 0.12471128630178271\nEpsilon = 0.12469881517315254\nEpsilon = 0.12468634529163522\nEpsilon = 0.12467387665710605\nEpsilon = 0.12466140926944035\nEpsilon = 0.1246489431285134\nEpsilon = 0.12463647823420056\nEpsilon = 0.12462401458637713\nEpsilon = 0.1246115521849185\nEpsilon = 0.1245990910297\nEpsilon = 0.12458663112059704\nAgent: ddqn_agent . Episode 1120/2000. Number of steps to finish: 20. Loss: 15.594610214233398 Reward: -12.0\nEpsilon = 0.12457417245748498\nEpsilon = 0.12456171504023923\nEpsilon = 0.1245492588687352\nEpsilon = 0.12453680394284833\nEpsilon = 0.12452435026245405\nEpsilon = 0.1245118978274278\nEpsilon = 0.12449944663764506\nEpsilon = 0.1244869966929813\nEpsilon = 0.124474547993312\nEpsilon = 0.12446210053851267\nEpsilon = 0.12444965432845882\nEpsilon = 0.12443720936302598\nEpsilon = 0.12442476564208968\nEpsilon = 0.12441232316552547\nEpsilon = 0.12439988193320892\nEpsilon = 0.1243874419450156\nEpsilon = 0.1243750032008211\nEpsilon = 0.12436256570050101\nEpsilon = 0.12435012944393097\nEpsilon = 0.12433769443098658\nAgent: ddqn_agent . Episode 1121/2000. Number of steps to finish: 20. Loss: 16.0094051361084 Reward: -14.0\nEpsilon = 0.12432526066154348\nEpsilon = 0.12431282813547732\nEpsilon = 0.12430039685266378\nEpsilon = 0.12428796681297852\nEpsilon = 0.12427553801629722\nEpsilon = 0.1242631104624956\nEpsilon = 0.12425068415144934\nEpsilon = 0.1242382590830342\nEpsilon = 0.1242258352571259\nEpsilon = 0.12421341267360018\nEpsilon = 0.12420099133233282\nEpsilon = 0.1241885712331996\nAgent: ddqn_agent . Episode 1122/2000. Number of steps to finish: 12. Loss: 9.808785438537598 Reward: 0.0\nEpsilon = 0.12417615237607628\nEpsilon = 0.12416373476083867\nEpsilon = 0.12415131838736258\nEpsilon = 0.12413890325552385\nEpsilon = 0.1241264893651983\nEpsilon = 0.12411407671626178\nEpsilon = 0.12410166530859015\nEpsilon = 0.12408925514205929\nEpsilon = 0.12407684621654509\nEpsilon = 0.12406443853192343\nEpsilon = 0.12405203208807024\nEpsilon = 0.12403962688486143\nEpsilon = 0.12402722292217296\nEpsilon = 0.12401482019988073\nEpsilon = 0.12400241871786075\nEpsilon = 0.12399001847598896\nEpsilon = 0.12397761947414136\nEpsilon = 0.12396522171219394\nEpsilon = 0.12395282519002272\nEpsilon = 0.12394042990750372\nAgent: ddqn_agent . Episode 1123/2000. Number of steps to finish: 20. Loss: 15.730246543884277 Reward: -10.0\nEpsilon = 0.12392803586451297\nEpsilon = 0.12391564306092652\nEpsilon = 0.12390325149662043\nEpsilon = 0.12389086117147077\nEpsilon = 0.12387847208535363\nEpsilon = 0.1238660842381451\nEpsilon = 0.12385369762972129\nEpsilon = 0.12384131225995831\nEpsilon = 0.12382892812873232\nEpsilon = 0.12381654523591945\nEpsilon = 0.12380416358139586\nEpsilon = 0.12379178316503772\nEpsilon = 0.12377940398672121\nEpsilon = 0.12376702604632255\nEpsilon = 0.12375464934371791\nEpsilon = 0.12374227387878355\nEpsilon = 0.12372989965139566\nEpsilon = 0.12371752666143053\nEpsilon = 0.1237051549087644\nEpsilon = 0.12369278439327352\nAgent: ddqn_agent . Episode 1124/2000. Number of steps to finish: 20. Loss: 14.828313827514648 Reward: -18.0\nEpsilon = 0.1236804151148342\nEpsilon = 0.12366804707332271\nEpsilon = 0.12365568026861538\nEpsilon = 0.12364331470058852\nEpsilon = 0.12363095036911846\nEpsilon = 0.12361858727408155\nEpsilon = 0.12360622541535414\nEpsilon = 0.1235938647928126\nEpsilon = 0.12358150540633332\nEpsilon = 0.1235691472557927\nEpsilon = 0.12355679034106712\nEpsilon = 0.123544434662033\nEpsilon = 0.1235320802185668\nEpsilon = 0.12351972701054495\nEpsilon = 0.1235073750378439\nEpsilon = 0.12349502430034011\nEpsilon = 0.12348267479791007\nEpsilon = 0.12347032653043029\nEpsilon = 0.12345797949777725\nEpsilon = 0.12344563369982747\nAgent: ddqn_agent . Episode 1125/2000. Number of steps to finish: 20. Loss: 16.37498664855957 Reward: -14.0\nEpsilon = 0.12343328913645749\nEpsilon = 0.12342094580754384\nEpsilon = 0.12340860371296308\nEpsilon = 0.12339626285259178\nEpsilon = 0.12338392322630652\nEpsilon = 0.12337158483398389\nEpsilon = 0.1233592476755005\nEpsilon = 0.12334691175073295\nEpsilon = 0.12333457705955787\nEpsilon = 0.12332224360185191\nEpsilon = 0.12330991137749173\nEpsilon = 0.12329758038635398\nEpsilon = 0.12328525062831534\nEpsilon = 0.12327292210325251\nEpsilon = 0.12326059481104219\nEpsilon = 0.12324826875156109\nEpsilon = 0.12323594392468593\nEpsilon = 0.12322362033029347\nEpsilon = 0.12321129796826044\nEpsilon = 0.12319897683846362\nAgent: ddqn_agent . Episode 1126/2000. Number of steps to finish: 20. Loss: 15.927407264709473 Reward: -10.0\nEpsilon = 0.12318665694077978\nEpsilon = 0.1231743382750857\nEpsilon = 0.12316202084125819\nEpsilon = 0.12314970463917406\nEpsilon = 0.12313738966871014\nEpsilon = 0.12312507592974327\nEpsilon = 0.1231127634221503\nEpsilon = 0.12310045214580809\nEpsilon = 0.1230881421005935\nEpsilon = 0.12307583328638344\nEpsilon = 0.1230635257030548\nEpsilon = 0.12305121935048449\nEpsilon = 0.12303891422854944\nEpsilon = 0.12302661033712659\nEpsilon = 0.12301430767609288\nEpsilon = 0.12300200624532527\nEpsilon = 0.12298970604470073\nEpsilon = 0.12297740707409627\nEpsilon = 0.12296510933338886\nEpsilon = 0.12295281282245553\nAgent: ddqn_agent . Episode 1127/2000. Number of steps to finish: 20. Loss: 15.156013488769531 Reward: -20.0\nEpsilon = 0.12294051754117329\nEpsilon = 0.12292822348941917\nEpsilon = 0.12291593066707024\nEpsilon = 0.12290363907400353\nEpsilon = 0.12289134871009613\nEpsilon = 0.12287905957522513\nEpsilon = 0.12286677166926761\nEpsilon = 0.1228544849921007\nEpsilon = 0.12284219954360148\nEpsilon = 0.12282991532364712\nEpsilon = 0.12281763233211476\nEpsilon = 0.12280535056888155\nEpsilon = 0.12279307003382466\nEpsilon = 0.12278079072682127\nEpsilon = 0.12276851264774859\nEpsilon = 0.12275623579648381\nEpsilon = 0.12274396017290416\nEpsilon = 0.12273168577688687\nEpsilon = 0.12271941260830918\nEpsilon = 0.12270714066704835\nAgent: ddqn_agent . Episode 1128/2000. Number of steps to finish: 20. Loss: 16.40254783630371 Reward: -12.0\nEpsilon = 0.12269486995298165\nEpsilon = 0.12268260046598635\nEpsilon = 0.12267033220593976\nEpsilon = 0.12265806517271917\nEpsilon = 0.1226457993662019\nEpsilon = 0.12263353478626528\nEpsilon = 0.12262127143278666\nEpsilon = 0.12260900930564338\nEpsilon = 0.12259674840471281\nEpsilon = 0.12258448872987233\nEpsilon = 0.12257223028099935\nEpsilon = 0.12255997305797126\nEpsilon = 0.12254771706066546\nEpsilon = 0.1225354622889594\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.1225232087427305\nEpsilon = 0.12251095642185622\nEpsilon = 0.12249870532621404\nEpsilon = 0.12248645545568142\nEpsilon = 0.12247420681013585\nEpsilon = 0.12246195938945484\nAgent: ddqn_agent . Episode 1129/2000. Number of steps to finish: 20. Loss: 16.366880416870117 Reward: -10.0\nEpsilon = 0.1224497131935159\nEpsilon = 0.12243746822219655\nEpsilon = 0.12242522447537432\nEpsilon = 0.12241298195292678\nEpsilon = 0.12240074065473149\nEpsilon = 0.12238850058066601\nEpsilon = 0.12237626173060795\nEpsilon = 0.1223640241044349\nEpsilon = 0.12235178770202446\nEpsilon = 0.12233955252325425\nEpsilon = 0.12232731856800193\nEpsilon = 0.12231508583614513\nEpsilon = 0.12230285432756152\nEpsilon = 0.12229062404212877\nEpsilon = 0.12227839497972456\nAgent: ddqn_agent . Episode 1130/2000. Number of steps to finish: 15. Loss: 12.22793960571289 Reward: -3.0\nEpsilon = 0.12226616714022659\nEpsilon = 0.12225394052351257\nEpsilon = 0.12224171512946022\nEpsilon = 0.12222949095794727\nEpsilon = 0.12221726800885148\nEpsilon = 0.12220504628205059\nEpsilon = 0.12219282577742238\nEpsilon = 0.12218060649484463\nEpsilon = 0.12216838843419515\nEpsilon = 0.12215617159535173\nEpsilon = 0.1221439559781922\nEpsilon = 0.12213174158259439\nEpsilon = 0.12211952840843614\nEpsilon = 0.12210731645559529\nEpsilon = 0.12209510572394973\nEpsilon = 0.12208289621337734\nEpsilon = 0.122070687923756\nEpsilon = 0.12205848085496362\nEpsilon = 0.12204627500687813\nEpsilon = 0.12203407037937744\nAgent: ddqn_agent . Episode 1131/2000. Number of steps to finish: 20. Loss: 16.828153610229492 Reward: -10.0\nEpsilon = 0.12202186697233951\nEpsilon = 0.12200966478564228\nEpsilon = 0.12199746381916371\nEpsilon = 0.12198526407278179\nEpsilon = 0.12197306554637452\nEpsilon = 0.12196086823981989\nEpsilon = 0.1219486721529959\nEpsilon = 0.12193647728578061\nEpsilon = 0.12192428363805204\nAgent: ddqn_agent . Episode 1132/2000. Number of steps to finish: 9. Loss: 7.395042896270752 Reward: 3.0\nEpsilon = 0.12191209120968824\nEpsilon = 0.12189990000056727\nEpsilon = 0.12188771001056722\nEpsilon = 0.12187552123956617\nEpsilon = 0.12186333368744222\nEpsilon = 0.12185114735407347\nEpsilon = 0.12183896223933807\nEpsilon = 0.12182677834311414\nEpsilon = 0.12181459566527983\nEpsilon = 0.1218024142057133\nEpsilon = 0.12179023396429274\nEpsilon = 0.12177805494089632\nEpsilon = 0.12176587713540223\nEpsilon = 0.12175370054768869\nEpsilon = 0.12174152517763392\nEpsilon = 0.12172935102511616\nEpsilon = 0.12171717809001364\nEpsilon = 0.12170500637220465\nEpsilon = 0.12169283587156743\nEpsilon = 0.12168066658798027\nAgent: ddqn_agent . Episode 1133/2000. Number of steps to finish: 20. Loss: 16.94141960144043 Reward: -18.0\nEpsilon = 0.12166849852132147\nEpsilon = 0.12165633167146934\nEpsilon = 0.12164416603830219\nEpsilon = 0.12163200162169836\nEpsilon = 0.12161983842153619\nEpsilon = 0.12160767643769403\nEpsilon = 0.12159551567005027\nEpsilon = 0.12158335611848327\nEpsilon = 0.12157119778287143\nEpsilon = 0.12155904066309314\nEpsilon = 0.12154688475902684\nEpsilon = 0.12153473007055093\nEpsilon = 0.12152257659754388\nEpsilon = 0.12151042433988413\nEpsilon = 0.12149827329745014\nEpsilon = 0.12148612347012039\nEpsilon = 0.12147397485777338\nEpsilon = 0.12146182746028761\nEpsilon = 0.12144968127754158\nEpsilon = 0.12143753630941383\nAgent: ddqn_agent . Episode 1134/2000. Number of steps to finish: 20. Loss: 16.306373596191406 Reward: -10.0\nEpsilon = 0.12142539255578289\nEpsilon = 0.12141325001652731\nEpsilon = 0.12140110869152565\nEpsilon = 0.1213889685806565\nEpsilon = 0.12137682968379843\nEpsilon = 0.12136469200083005\nEpsilon = 0.12135255553162998\nEpsilon = 0.12134042027607682\nEpsilon = 0.12132828623404922\nEpsilon = 0.12131615340542581\nEpsilon = 0.12130402179008527\nEpsilon = 0.12129189138790626\nEpsilon = 0.12127976219876747\nEpsilon = 0.1212676342225476\nEpsilon = 0.12125550745912535\nEpsilon = 0.12124338190837944\nEpsilon = 0.1212312575701886\nEpsilon = 0.12121913444443158\nEpsilon = 0.12120701253098715\nEpsilon = 0.12119489182973404\nAgent: ddqn_agent . Episode 1135/2000. Number of steps to finish: 20. Loss: 15.507686614990234 Reward: -14.0\nEpsilon = 0.12118277234055107\nEpsilon = 0.12117065406331701\nEpsilon = 0.12115853699791068\nEpsilon = 0.1211464211442109\nEpsilon = 0.12113430650209647\nEpsilon = 0.12112219307144627\nEpsilon = 0.12111008085213913\nEpsilon = 0.12109796984405392\nEpsilon = 0.12108586004706952\nEpsilon = 0.12107375146106482\nEpsilon = 0.12106164408591871\nEpsilon = 0.12104953792151012\nEpsilon = 0.12103743296771796\nEpsilon = 0.12102532922442119\nEpsilon = 0.12101322669149875\nEpsilon = 0.1210011253688296\nEpsilon = 0.12098902525629272\nEpsilon = 0.1209769263537671\nEpsilon = 0.12096482866113172\nEpsilon = 0.1209527321782656\nAgent: ddqn_agent . Episode 1136/2000. Number of steps to finish: 20. Loss: 15.29860782623291 Reward: -14.0\nEpsilon = 0.12094063690504778\nEpsilon = 0.12092854284135728\nEpsilon = 0.12091644998707314\nEpsilon = 0.12090435834207444\nEpsilon = 0.12089226790624023\nEpsilon = 0.12088017867944961\nEpsilon = 0.12086809066158166\nEpsilon = 0.1208560038525155\nEpsilon = 0.12084391825213026\nEpsilon = 0.12083183386030505\nEpsilon = 0.12081975067691902\nEpsilon = 0.12080766870185133\nEpsilon = 0.12079558793498114\nEpsilon = 0.12078350837618765\nEpsilon = 0.12077143002535003\nEpsilon = 0.12075935288234749\nEpsilon = 0.12074727694705925\nEpsilon = 0.12073520221936454\nEpsilon = 0.1207231286991426\nEpsilon = 0.12071105638627269\nAgent: ddqn_agent . Episode 1137/2000. Number of steps to finish: 20. Loss: 15.747227668762207 Reward: -14.0\nEpsilon = 0.12069898528063407\nEpsilon = 0.120686915382106\nEpsilon = 0.1206748466905678\nEpsilon = 0.12066277920589874\nEpsilon = 0.12065071292797815\nEpsilon = 0.12063864785668535\nEpsilon = 0.12062658399189968\nEpsilon = 0.1206145213335005\nEpsilon = 0.12060245988136714\nEpsilon = 0.120590399635379\nEpsilon = 0.12057834059541546\nEpsilon = 0.12056628276135592\nEpsilon = 0.12055422613307978\nEpsilon = 0.12054217071046648\nEpsilon = 0.12053011649339544\nEpsilon = 0.12051806348174611\nEpsilon = 0.12050601167539794\nEpsilon = 0.12049396107423041\nEpsilon = 0.12048191167812299\nEpsilon = 0.12046986348695518\nAgent: ddqn_agent . Episode 1138/2000. Number of steps to finish: 20. Loss: 16.2994384765625 Reward: -14.0\nEpsilon = 0.12045781650060648\nEpsilon = 0.12044577071895643\nEpsilon = 0.12043372614188454\nEpsilon = 0.12042168276927034\nEpsilon = 0.12040964060099342\nEpsilon = 0.12039759963693332\nEpsilon = 0.12038555987696963\nEpsilon = 0.12037352132098193\nEpsilon = 0.12036148396884984\nEpsilon = 0.12034944782045295\nEpsilon = 0.12033741287567092\nEpsilon = 0.12032537913438335\nEpsilon = 0.12031334659646992\nEpsilon = 0.12030131526181027\nEpsilon = 0.1202892851302841\nEpsilon = 0.12027725620177107\nEpsilon = 0.1202652284761509\nEpsilon = 0.12025320195330329\nEpsilon = 0.12024117663310796\nEpsilon = 0.12022915251544465\nAgent: ddqn_agent . Episode 1139/2000. Number of steps to finish: 20. Loss: 16.04161834716797 Reward: -16.0\nEpsilon = 0.12021712960019311\nEpsilon = 0.1202051078872331\nEpsilon = 0.12019308737644438\nEpsilon = 0.12018106806770673\nEpsilon = 0.12016904996089996\nEpsilon = 0.12015703305590386\nEpsilon = 0.12014501735259828\nEpsilon = 0.12013300285086302\nEpsilon = 0.12012098955057794\nEpsilon = 0.12010897745162288\nEpsilon = 0.12009696655387772\nEpsilon = 0.12008495685722234\nEpsilon = 0.12007294836153662\nEpsilon = 0.12006094106670047\nEpsilon = 0.12004893497259381\nEpsilon = 0.12003693007909655\nEpsilon = 0.12002492638608864\nEpsilon = 0.12001292389345003\nEpsilon = 0.12000092260106068\nEpsilon = 0.11998892250880058\nAgent: ddqn_agent . Episode 1140/2000. Number of steps to finish: 20. Loss: 17.000783920288086 Reward: -14.0\nEpsilon = 0.1199769236165497\nEpsilon = 0.11996492592418805\nEpsilon = 0.11995292943159563\nEpsilon = 0.11994093413865248\nEpsilon = 0.11992894004523862\nEpsilon = 0.11991694715123409\nEpsilon = 0.11990495545651897\nEpsilon = 0.11989296496097332\nEpsilon = 0.11988097566447722\nEpsilon = 0.11986898756691078\nEpsilon = 0.11985700066815409\nEpsilon = 0.11984501496808728\nEpsilon = 0.11983303046659047\nEpsilon = 0.1198210471635438\nEpsilon = 0.11980906505882745\nEpsilon = 0.11979708415232157\nEpsilon = 0.11978510444390635\nEpsilon = 0.11977312593346195\nEpsilon = 0.1197611486208686\nEpsilon = 0.11974917250600652\nAgent: ddqn_agent . Episode 1141/2000. Number of steps to finish: 20. Loss: 16.70067024230957 Reward: -18.0\nEpsilon = 0.11973719758875592\nEpsilon = 0.11972522386899705\nEpsilon = 0.11971325134661015\nEpsilon = 0.1197012800214755\nEpsilon = 0.11968930989347334\nEpsilon = 0.119677340962484\nEpsilon = 0.11966537322838776\nEpsilon = 0.11965340669106492\nEpsilon = 0.11964144135039581\nEpsilon = 0.11962947720626077\nEpsilon = 0.11961751425854014\nEpsilon = 0.11960555250711428\nEpsilon = 0.11959359195186357\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.1195816325926684\nEpsilon = 0.11956967442940913\nEpsilon = 0.1195577174619662\nEpsilon = 0.11954576169022\nEpsilon = 0.11953380711405098\nEpsilon = 0.11952185373333958\nEpsilon = 0.11950990154796624\nAgent: ddqn_agent . Episode 1142/2000. Number of steps to finish: 20. Loss: 16.489240646362305 Reward: -14.0\nEpsilon = 0.11949795055781144\nEpsilon = 0.11948600076275566\nEpsilon = 0.11947405216267938\nEpsilon = 0.11946210475746312\nEpsilon = 0.11945015854698737\nEpsilon = 0.11943821353113268\nEpsilon = 0.11942626970977957\nEpsilon = 0.1194143270828086\nEpsilon = 0.11940238565010032\nEpsilon = 0.11939044541153532\nEpsilon = 0.11937850636699417\nEpsilon = 0.11936656851635746\nEpsilon = 0.11935463185950583\nEpsilon = 0.11934269639631988\nEpsilon = 0.11933076212668024\nEpsilon = 0.11931882905046758\nEpsilon = 0.11930689716756253\nEpsilon = 0.11929496647784578\nEpsilon = 0.11928303698119799\nEpsilon = 0.11927110867749988\nAgent: ddqn_agent . Episode 1143/2000. Number of steps to finish: 20. Loss: 16.737667083740234 Reward: -14.0\nEpsilon = 0.11925918156663212\nEpsilon = 0.11924725564847546\nEpsilon = 0.11923533092291062\nEpsilon = 0.11922340738981833\nEpsilon = 0.11921148504907936\nEpsilon = 0.11919956390057446\nEpsilon = 0.11918764394418441\nEpsilon = 0.11917572517978998\nEpsilon = 0.11916380760727201\nEpsilon = 0.11915189122651128\nEpsilon = 0.11913997603738863\nEpsilon = 0.11912806203978489\nEpsilon = 0.11911614923358091\nEpsilon = 0.11910423761865756\nEpsilon = 0.1190923271948957\nEpsilon = 0.11908041796217621\nEpsilon = 0.11906850992038\nEpsilon = 0.11905660306938796\nEpsilon = 0.11904469740908102\nEpsilon = 0.11903279293934012\nAgent: ddqn_agent . Episode 1144/2000. Number of steps to finish: 20. Loss: 14.975388526916504 Reward: -10.0\nEpsilon = 0.11902088966004619\nEpsilon = 0.11900898757108018\nEpsilon = 0.11899708667232307\nEpsilon = 0.11898518696365584\nEpsilon = 0.11897328844495948\nEpsilon = 0.11896139111611499\nEpsilon = 0.11894949497700338\nEpsilon = 0.11893760002750568\nEpsilon = 0.11892570626750293\nEpsilon = 0.11891381369687617\nEpsilon = 0.11890192231550649\nEpsilon = 0.11889003212327494\nEpsilon = 0.11887814312006262\nEpsilon = 0.11886625530575061\nEpsilon = 0.11885436868022003\nEpsilon = 0.11884248324335202\nEpsilon = 0.11883059899502768\nEpsilon = 0.11881871593512817\nEpsilon = 0.11880683406353466\nEpsilon = 0.11879495338012831\nAgent: ddqn_agent . Episode 1145/2000. Number of steps to finish: 20. Loss: 15.742815017700195 Reward: -12.0\nEpsilon = 0.1187830738847903\nEpsilon = 0.11877119557740182\nEpsilon = 0.11875931845784408\nEpsilon = 0.1187474425259983\nEpsilon = 0.1187355677817457\nEpsilon = 0.11872369422496752\nEpsilon = 0.11871182185554503\nEpsilon = 0.11869995067335948\nEpsilon = 0.11868808067829215\nEpsilon = 0.11867621187022431\nEpsilon = 0.1186643442490373\nEpsilon = 0.1186524778146124\nEpsilon = 0.11864061256683094\nEpsilon = 0.11862874850557426\nEpsilon = 0.1186168856307237\nEpsilon = 0.11860502394216063\nEpsilon = 0.11859316343976642\nEpsilon = 0.11858130412342244\nEpsilon = 0.1185694459930101\nEpsilon = 0.1185575890484108\nAgent: ddqn_agent . Episode 1146/2000. Number of steps to finish: 20. Loss: 17.549909591674805 Reward: -14.0\nEpsilon = 0.11854573328950596\nEpsilon = 0.11853387871617702\nEpsilon = 0.1185220253283054\nEpsilon = 0.11851017312577257\nEpsilon = 0.11849832210846\nEpsilon = 0.11848647227624914\nEpsilon = 0.11847462362902152\nEpsilon = 0.11846277616665861\nEpsilon = 0.11845092988904195\nEpsilon = 0.11843908479605304\nEpsilon = 0.11842724088757343\nEpsilon = 0.11841539816348468\nEpsilon = 0.11840355662366833\nEpsilon = 0.11839171626800597\nEpsilon = 0.11837987709637918\nEpsilon = 0.11836803910866954\nEpsilon = 0.11835620230475867\nEpsilon = 0.11834436668452819\nEpsilon = 0.11833253224785974\nEpsilon = 0.11832069899463495\nAgent: ddqn_agent . Episode 1147/2000. Number of steps to finish: 20. Loss: 16.149213790893555 Reward: -14.0\nEpsilon = 0.11830886692473548\nEpsilon = 0.11829703603804301\nEpsilon = 0.1182852063344392\nEpsilon = 0.11827337781380576\nEpsilon = 0.11826155047602438\nEpsilon = 0.11824972432097677\nEpsilon = 0.11823789934854467\nEpsilon = 0.11822607555860982\nEpsilon = 0.11821425295105396\nEpsilon = 0.11820243152575886\nEpsilon = 0.11819061128260629\nEpsilon = 0.11817879222147802\nEpsilon = 0.11816697434225587\nEpsilon = 0.11815515764482165\nEpsilon = 0.11814334212905717\nEpsilon = 0.11813152779484426\nEpsilon = 0.11811971464206478\nEpsilon = 0.11810790267060058\nEpsilon = 0.11809609188033351\nEpsilon = 0.11808428227114548\nAgent: ddqn_agent . Episode 1148/2000. Number of steps to finish: 20. Loss: 17.607393264770508 Reward: -16.0\nEpsilon = 0.11807247384291837\nEpsilon = 0.11806066659553408\nEpsilon = 0.11804886052887452\nEpsilon = 0.11803705564282163\nEpsilon = 0.11802525193725735\nEpsilon = 0.11801344941206363\nEpsilon = 0.11800164806712242\nEpsilon = 0.1179898479023157\nEpsilon = 0.11797804891752547\nEpsilon = 0.11796625111263373\nEpsilon = 0.11795445448752247\nEpsilon = 0.11794265904207371\nEpsilon = 0.1179308647761695\nEpsilon = 0.11791907168969189\nEpsilon = 0.11790727978252293\nEpsilon = 0.11789548905454468\nEpsilon = 0.11788369950563923\nEpsilon = 0.11787191113568866\nEpsilon = 0.1178601239445751\nEpsilon = 0.11784833793218064\nAgent: ddqn_agent . Episode 1149/2000. Number of steps to finish: 20. Loss: 15.72557544708252 Reward: -10.0\nEpsilon = 0.11783655309838742\nEpsilon = 0.11782476944307758\nEpsilon = 0.11781298696613328\nEpsilon = 0.11780120566743667\nEpsilon = 0.11778942554686993\nEpsilon = 0.11777764660431524\nEpsilon = 0.1177658688396548\nEpsilon = 0.11775409225277084\nEpsilon = 0.11774231684354557\nEpsilon = 0.11773054261186121\nEpsilon = 0.11771876955760002\nEpsilon = 0.11770699768064427\nEpsilon = 0.11769522698087621\nEpsilon = 0.11768345745817813\nEpsilon = 0.11767168911243231\nEpsilon = 0.11765992194352107\nEpsilon = 0.11764815595132672\nEpsilon = 0.11763639113573159\nEpsilon = 0.11762462749661802\nEpsilon = 0.11761286503386836\nAgent: ddqn_agent . Episode 1150/2000. Number of steps to finish: 20. Loss: 16.34485626220703 Reward: -16.0\nEpsilon = 0.11760110374736497\nEpsilon = 0.11758934363699024\nEpsilon = 0.11757758470262654\nEpsilon = 0.11756582694415628\nEpsilon = 0.11755407036146187\nEpsilon = 0.11754231495442573\nEpsilon = 0.11753056072293029\nEpsilon = 0.117518807666858\nEpsilon = 0.11750705578609132\nEpsilon = 0.1174953050805127\nEpsilon = 0.11748355555000466\nEpsilon = 0.11747180719444966\nEpsilon = 0.11746006001373022\nEpsilon = 0.11744831400772884\nEpsilon = 0.11743656917632807\nEpsilon = 0.11742482551941044\nEpsilon = 0.1174130830368585\nEpsilon = 0.11740134172855482\nEpsilon = 0.11738960159438197\nEpsilon = 0.11737786263422254\nAgent: ddqn_agent . Episode 1151/2000. Number of steps to finish: 20. Loss: 15.821775436401367 Reward: -10.0\nEpsilon = 0.11736612484795911\nEpsilon = 0.11735438823547431\nEpsilon = 0.11734265279665076\nEpsilon = 0.1173309185313711\nEpsilon = 0.11731918543951797\nEpsilon = 0.11730745352097402\nEpsilon = 0.11729572277562192\nEpsilon = 0.11728399320334436\nEpsilon = 0.11727226480402403\nEpsilon = 0.11726053757754362\nEpsilon = 0.11724881152378587\nEpsilon = 0.11723708664263349\nEpsilon = 0.11722536293396923\nEpsilon = 0.11721364039767584\nEpsilon = 0.11720191903363608\nEpsilon = 0.11719019884173272\nEpsilon = 0.11717847982184855\nEpsilon = 0.11716676197386637\nEpsilon = 0.11715504529766899\nEpsilon = 0.11714332979313923\nAgent: ddqn_agent . Episode 1152/2000. Number of steps to finish: 20. Loss: 15.296711921691895 Reward: -10.0\nEpsilon = 0.11713161546015992\nEpsilon = 0.11711990229861391\nEpsilon = 0.11710819030838406\nEpsilon = 0.11709647948935321\nEpsilon = 0.11708476984140428\nEpsilon = 0.11707306136442014\nEpsilon = 0.1170613540582837\nEpsilon = 0.11704964792287788\nEpsilon = 0.11703794295808559\nEpsilon = 0.11702623916378978\nEpsilon = 0.1170145365398734\nEpsilon = 0.1170028350862194\nEpsilon = 0.11699113480271078\nEpsilon = 0.11697943568923051\nEpsilon = 0.11696773774566159\nEpsilon = 0.11695604097188703\nEpsilon = 0.11694434536778985\nEpsilon = 0.11693265093325307\nEpsilon = 0.11692095766815974\nEpsilon = 0.11690926557239292\nAgent: ddqn_agent . Episode 1153/2000. Number of steps to finish: 20. Loss: 16.530628204345703 Reward: -18.0\nEpsilon = 0.11689757464583568\nEpsilon = 0.1168858848883711\nEpsilon = 0.11687419629988226\nEpsilon = 0.11686250888025226\nEpsilon = 0.11685082262936423\nEpsilon = 0.1168391375471013\nEpsilon = 0.1168274536333466\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.11681577088798326\nEpsilon = 0.11680408931089446\nEpsilon = 0.11679240890196338\nEpsilon = 0.11678072966107318\nEpsilon = 0.11676905158810708\nEpsilon = 0.11675737468294826\nEpsilon = 0.11674569894547997\nEpsilon = 0.11673402437558542\nEpsilon = 0.11672235097314786\nEpsilon = 0.11671067873805055\nEpsilon = 0.11669900767017674\nEpsilon = 0.11668733776940972\nEpsilon = 0.11667566903563278\nAgent: ddqn_agent . Episode 1154/2000. Number of steps to finish: 20. Loss: 15.934995651245117 Reward: -12.0\nEpsilon = 0.11666400146872923\nEpsilon = 0.11665233506858236\nEpsilon = 0.1166406698350755\nEpsilon = 0.11662900576809199\nEpsilon = 0.11661734286751518\nEpsilon = 0.11660568113322843\nEpsilon = 0.11659402056511511\nEpsilon = 0.1165823611630586\nEpsilon = 0.1165707029269423\nEpsilon = 0.11655904585664961\nEpsilon = 0.11654738995206394\nEpsilon = 0.11653573521306874\nEpsilon = 0.11652408163954744\nEpsilon = 0.1165124292313835\nEpsilon = 0.11650077798846036\nEpsilon = 0.11648912791066152\nEpsilon = 0.11647747899787045\nEpsilon = 0.11646583124997066\nEpsilon = 0.11645418466684566\nEpsilon = 0.11644253924837898\nAgent: ddqn_agent . Episode 1155/2000. Number of steps to finish: 20. Loss: 16.25835418701172 Reward: -14.0\nEpsilon = 0.11643089499445415\nEpsilon = 0.11641925190495471\nEpsilon = 0.11640760997976421\nEpsilon = 0.11639596921876623\nEpsilon = 0.11638432962184435\nEpsilon = 0.11637269118888216\nEpsilon = 0.11636105391976327\nEpsilon = 0.1163494178143713\nEpsilon = 0.11633778287258986\nEpsilon = 0.11632614909430261\nEpsilon = 0.11631451647939318\nEpsilon = 0.11630288502774525\nAgent: ddqn_agent . Episode 1156/2000. Number of steps to finish: 12. Loss: 9.066422462463379 Reward: 0.0\nEpsilon = 0.11629125473924247\nEpsilon = 0.11627962561376855\nEpsilon = 0.11626799765120717\nEpsilon = 0.11625637085144205\nEpsilon = 0.1162447452143569\nEpsilon = 0.11623312073983547\nEpsilon = 0.11622149742776149\nEpsilon = 0.11620987527801871\nAgent: ddqn_agent . Episode 1157/2000. Number of steps to finish: 8. Loss: 6.188426971435547 Reward: 4.0\nEpsilon = 0.11619825429049091\nEpsilon = 0.11618663446506186\nEpsilon = 0.11617501580161535\nEpsilon = 0.11616339830003519\nEpsilon = 0.11615178196020519\nEpsilon = 0.11614016678200917\nEpsilon = 0.11612855276533098\nEpsilon = 0.11611693991005444\nEpsilon = 0.11610532821606344\nEpsilon = 0.11609371768324184\nEpsilon = 0.11608210831147352\nAgent: ddqn_agent . Episode 1158/2000. Number of steps to finish: 11. Loss: 9.994119644165039 Reward: 1.0\nEpsilon = 0.11607050010064238\nEpsilon = 0.11605889305063231\nEpsilon = 0.11604728716132726\nEpsilon = 0.11603568243261113\nEpsilon = 0.11602407886436787\nEpsilon = 0.11601247645648144\nEpsilon = 0.1160008752088358\nEpsilon = 0.11598927512131492\nEpsilon = 0.1159776761938028\nEpsilon = 0.11596607842618342\nEpsilon = 0.11595448181834081\nEpsilon = 0.11594288637015898\nEpsilon = 0.11593129208152196\nEpsilon = 0.11591969895231381\nEpsilon = 0.11590810698241859\nEpsilon = 0.11589651617172035\nEpsilon = 0.11588492652010318\nEpsilon = 0.11587333802745117\nEpsilon = 0.11586175069364843\nEpsilon = 0.11585016451857906\nAgent: ddqn_agent . Episode 1159/2000. Number of steps to finish: 20. Loss: 15.269366264343262 Reward: -12.0\nEpsilon = 0.1158385795021272\nEpsilon = 0.11582699564417699\nEpsilon = 0.11581541294461258\nEpsilon = 0.11580383140331811\nEpsilon = 0.11579225102017777\nEpsilon = 0.11578067179507576\nEpsilon = 0.11576909372789625\nEpsilon = 0.11575751681852346\nEpsilon = 0.1157459410668416\nEpsilon = 0.11573436647273493\nEpsilon = 0.11572279303608765\nEpsilon = 0.11571122075678404\nEpsilon = 0.11569964963470837\nEpsilon = 0.1156880796697449\nEpsilon = 0.11567651086177792\nEpsilon = 0.11566494321069175\nEpsilon = 0.11565337671637067\nEpsilon = 0.11564181137869904\nEpsilon = 0.11563024719756117\nEpsilon = 0.11561868417284141\nAgent: ddqn_agent . Episode 1160/2000. Number of steps to finish: 20. Loss: 17.032695770263672 Reward: -16.0\nEpsilon = 0.11560712230442412\nEpsilon = 0.11559556159219368\nEpsilon = 0.11558400203603446\nEpsilon = 0.11557244363583086\nEpsilon = 0.11556088639146728\nEpsilon = 0.11554933030282813\nEpsilon = 0.11553777536979785\nEpsilon = 0.11552622159226088\nEpsilon = 0.11551466897010165\nEpsilon = 0.11550311750320465\nEpsilon = 0.11549156719145433\nEpsilon = 0.11548001803473519\nEpsilon = 0.11546847003293172\nEpsilon = 0.11545692318592843\nEpsilon = 0.11544537749360984\nEpsilon = 0.11543383295586047\nEpsilon = 0.1154222895725649\nEpsilon = 0.11541074734360764\nEpsilon = 0.11539920626887328\nEpsilon = 0.11538766634824639\nAgent: ddqn_agent . Episode 1161/2000. Number of steps to finish: 20. Loss: 16.16387939453125 Reward: -14.0\nEpsilon = 0.11537612758161157\nEpsilon = 0.11536458996885342\nEpsilon = 0.11535305350985653\nEpsilon = 0.11534151820450554\nEpsilon = 0.11532998405268509\nEpsilon = 0.11531845105427982\nEpsilon = 0.1153069192091744\nEpsilon = 0.11529538851725347\nEpsilon = 0.11528385897840175\nEpsilon = 0.11527233059250391\nEpsilon = 0.11526080335944466\nEpsilon = 0.11524927727910872\nEpsilon = 0.11523775235138081\nEpsilon = 0.11522622857614567\nEpsilon = 0.11521470595328806\nEpsilon = 0.11520318448269273\nEpsilon = 0.11519166416424446\nEpsilon = 0.11518014499782804\nEpsilon = 0.11516862698332826\nEpsilon = 0.11515711012062993\nAgent: ddqn_agent . Episode 1162/2000. Number of steps to finish: 20. Loss: 15.990070343017578 Reward: -14.0\nEpsilon = 0.11514559440961787\nEpsilon = 0.11513407985017692\nEpsilon = 0.1151225664421919\nEpsilon = 0.11511105418554769\nEpsilon = 0.11509954308012914\nEpsilon = 0.11508803312582112\nEpsilon = 0.11507652432250855\nEpsilon = 0.11506501667007629\nEpsilon = 0.11505351016840928\nEpsilon = 0.11504200481739245\nEpsilon = 0.11503050061691071\nEpsilon = 0.11501899756684902\nEpsilon = 0.11500749566709234\nEpsilon = 0.11499599491752563\nEpsilon = 0.11498449531803388\nEpsilon = 0.11497299686850208\nEpsilon = 0.11496149956881524\nEpsilon = 0.11495000341885836\nEpsilon = 0.11493850841851648\nEpsilon = 0.11492701456767462\nAgent: ddqn_agent . Episode 1163/2000. Number of steps to finish: 20. Loss: 15.926607131958008 Reward: -20.0\nEpsilon = 0.11491552186621785\nEpsilon = 0.11490403031403124\nEpsilon = 0.11489253991099983\nEpsilon = 0.11488105065700874\nEpsilon = 0.11486956255194304\nEpsilon = 0.11485807559568785\nEpsilon = 0.11484658978812828\nEpsilon = 0.11483510512914948\nEpsilon = 0.11482362161863656\nEpsilon = 0.1148121392564747\nEpsilon = 0.11480065804254905\nEpsilon = 0.11478917797674479\nEpsilon = 0.11477769905894712\nEpsilon = 0.11476622128904122\nEpsilon = 0.11475474466691232\nEpsilon = 0.11474326919244564\nEpsilon = 0.1147317948655264\nEpsilon = 0.11472032168603985\nEpsilon = 0.11470884965387125\nEpsilon = 0.11469737876890586\nAgent: ddqn_agent . Episode 1164/2000. Number of steps to finish: 20. Loss: 15.714371681213379 Reward: -16.0\nEpsilon = 0.11468590903102897\nEpsilon = 0.11467444044012587\nEpsilon = 0.11466297299608186\nEpsilon = 0.11465150669878225\nEpsilon = 0.11464004154811237\nEpsilon = 0.11462857754395757\nEpsilon = 0.11461711468620317\nEpsilon = 0.11460565297473455\nEpsilon = 0.11459419240943708\nEpsilon = 0.11458273299019614\nEpsilon = 0.11457127471689713\nEpsilon = 0.11455981758942543\nEpsilon = 0.11454836160766649\nEpsilon = 0.11453690677150573\nEpsilon = 0.11452545308082858\nEpsilon = 0.1145140005355205\nEpsilon = 0.11450254913546694\nEpsilon = 0.1144910988805534\nEpsilon = 0.11447964977066534\nEpsilon = 0.11446820180568827\nAgent: ddqn_agent . Episode 1165/2000. Number of steps to finish: 20. Loss: 16.261079788208008 Reward: -12.0\nEpsilon = 0.11445675498550771\nEpsilon = 0.11444530931000917\nEpsilon = 0.11443386477907817\nEpsilon = 0.11442242139260027\nEpsilon = 0.11441097915046101\nEpsilon = 0.11439953805254598\nEpsilon = 0.11438809809874072\nEpsilon = 0.11437665928893084\nEpsilon = 0.11436522162300194\nEpsilon = 0.11435378510083964\nEpsilon = 0.11434234972232955\nEpsilon = 0.11433091548735733\nEpsilon = 0.1143194823958086\nEpsilon = 0.11430805044756902\nEpsilon = 0.11429661964252426\nEpsilon = 0.11428518998056\nEpsilon = 0.11427376146156194\nEpsilon = 0.11426233408541579\nEpsilon = 0.11425090785200726\nEpsilon = 0.11423948276122206\nAgent: ddqn_agent . Episode 1166/2000. Number of steps to finish: 20. Loss: 16.60000991821289 Reward: -10.0\nEpsilon = 0.11422805881294594\nEpsilon = 0.11421663600706465\nEpsilon = 0.11420521434346395\nEpsilon = 0.11419379382202961\nEpsilon = 0.11418237444264741\nEpsilon = 0.11417095620520315\nEpsilon = 0.11415953910958263\nEpsilon = 0.11414812315567167\nEpsilon = 0.11413670834335611\nEpsilon = 0.11412529467252178\nEpsilon = 0.11411388214305453\nEpsilon = 0.11410247075484022\nEpsilon = 0.11409106050776473\nEpsilon = 0.11407965140171396\nEpsilon = 0.11406824343657379\nEpsilon = 0.11405683661223014\nEpsilon = 0.11404543092856892\nEpsilon = 0.11403402638547606\nEpsilon = 0.11402262298283752\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.11401122072053924\nAgent: ddqn_agent . Episode 1167/2000. Number of steps to finish: 20. Loss: 16.994943618774414 Reward: -10.0\nEpsilon = 0.11399981959846718\nEpsilon = 0.11398841961650734\nEpsilon = 0.11397702077454569\nEpsilon = 0.11396562307246824\nEpsilon = 0.113954226510161\nEpsilon = 0.11394283108751\nEpsilon = 0.11393143680440124\nEpsilon = 0.1139200436607208\nEpsilon = 0.11390865165635472\nEpsilon = 0.11389726079118909\nEpsilon = 0.11388587106510997\nEpsilon = 0.11387448247800347\nEpsilon = 0.11386309502975567\nEpsilon = 0.11385170872025269\nEpsilon = 0.11384032354938066\nEpsilon = 0.11382893951702572\nEpsilon = 0.11381755662307402\nEpsilon = 0.11380617486741172\nEpsilon = 0.11379479424992499\nEpsilon = 0.1137834147705\nAgent: ddqn_agent . Episode 1168/2000. Number of steps to finish: 20. Loss: 16.375728607177734 Reward: -12.0\nEpsilon = 0.11377203642902295\nEpsilon = 0.11376065922538005\nEpsilon = 0.11374928315945751\nEpsilon = 0.11373790823114156\nEpsilon = 0.11372653444031845\nEpsilon = 0.11371516178687442\nEpsilon = 0.11370379027069574\nEpsilon = 0.11369241989166867\nEpsilon = 0.11368105064967951\nEpsilon = 0.11366968254461454\nEpsilon = 0.11365831557636008\nEpsilon = 0.11364694974480244\nEpsilon = 0.11363558504982796\nEpsilon = 0.11362422149132298\nEpsilon = 0.11361285906917384\nEpsilon = 0.11360149778326693\nEpsilon = 0.1135901376334886\nEpsilon = 0.11357877861972525\nEpsilon = 0.11356742074186328\nEpsilon = 0.1135560639997891\nAgent: ddqn_agent . Episode 1169/2000. Number of steps to finish: 20. Loss: 15.998385429382324 Reward: -14.0\nEpsilon = 0.11354470839338912\nEpsilon = 0.11353335392254979\nEpsilon = 0.11352200058715754\nEpsilon = 0.11351064838709883\nEpsilon = 0.11349929732226012\nEpsilon = 0.11348794739252789\nEpsilon = 0.11347659859778864\nEpsilon = 0.11346525093792886\nEpsilon = 0.11345390441283507\nEpsilon = 0.11344255902239378\nEpsilon = 0.11343121476649154\nEpsilon = 0.1134198716450149\nEpsilon = 0.1134085296578504\nEpsilon = 0.11339718880488461\nEpsilon = 0.11338584908600413\nEpsilon = 0.11337451050109554\nEpsilon = 0.11336317305004542\nEpsilon = 0.11335183673274042\nEpsilon = 0.11334050154906715\nEpsilon = 0.11332916749891224\nAgent: ddqn_agent . Episode 1170/2000. Number of steps to finish: 20. Loss: 17.49288558959961 Reward: -20.0\nEpsilon = 0.11331783458216235\nEpsilon = 0.11330650279870413\nEpsilon = 0.11329517214842426\nEpsilon = 0.11328384263120941\nEpsilon = 0.11327251424694629\nEpsilon = 0.1132611869955216\nEpsilon = 0.11324986087682204\nEpsilon = 0.11323853589073436\nEpsilon = 0.1132272120371453\nEpsilon = 0.11321588931594158\nEpsilon = 0.11320456772700999\nEpsilon = 0.11319324727023729\nEpsilon = 0.11318192794551027\nEpsilon = 0.11317060975271571\nEpsilon = 0.11315929269174044\nEpsilon = 0.11314797676247126\nEpsilon = 0.11313666196479502\nEpsilon = 0.11312534829859855\nEpsilon = 0.11311403576376869\nEpsilon = 0.11310272436019231\nAgent: ddqn_agent . Episode 1171/2000. Number of steps to finish: 20. Loss: 16.220367431640625 Reward: -14.0\nEpsilon = 0.11309141408775629\nEpsilon = 0.11308010494634751\nEpsilon = 0.11306879693585288\nEpsilon = 0.1130574900561593\nEpsilon = 0.11304618430715369\nEpsilon = 0.11303487968872297\nEpsilon = 0.1130235762007541\nEpsilon = 0.11301227384313402\nEpsilon = 0.1130009726157497\nEpsilon = 0.11298967251848813\nEpsilon = 0.11297837355123629\nEpsilon = 0.11296707571388116\nEpsilon = 0.11295577900630978\nEpsilon = 0.11294448342840915\nEpsilon = 0.11293318898006631\nEpsilon = 0.11292189566116831\nEpsilon = 0.1129106034716022\nEpsilon = 0.11289931241125505\nEpsilon = 0.11288802248001392\nEpsilon = 0.11287673367776592\nAgent: ddqn_agent . Episode 1172/2000. Number of steps to finish: 20. Loss: 17.0090389251709 Reward: -18.0\nEpsilon = 0.11286544600439814\nEpsilon = 0.11285415945979771\nEpsilon = 0.11284287404385174\nEpsilon = 0.11283158975644736\nEpsilon = 0.11282030659747172\nEpsilon = 0.11280902456681198\nEpsilon = 0.1127977436643553\nEpsilon = 0.11278646388998886\nEpsilon = 0.11277518524359986\nEpsilon = 0.11276390772507551\nEpsilon = 0.11275263133430301\nEpsilon = 0.11274135607116959\nEpsilon = 0.11273008193556247\nEpsilon = 0.11271880892736892\nEpsilon = 0.11270753704647618\nEpsilon = 0.11269626629277153\nEpsilon = 0.11268499666614226\nEpsilon = 0.11267372816647565\nEpsilon = 0.112662460793659\nEpsilon = 0.11265119454757964\nAgent: ddqn_agent . Episode 1173/2000. Number of steps to finish: 20. Loss: 17.303300857543945 Reward: -12.0\nEpsilon = 0.11263992942812488\nEpsilon = 0.11262866543518207\nEpsilon = 0.11261740256863856\nEpsilon = 0.11260614082838169\nEpsilon = 0.11259488021429885\nEpsilon = 0.11258362072627742\nEpsilon = 0.1125723623642048\nEpsilon = 0.11256110512796838\nEpsilon = 0.11254984901745559\nEpsilon = 0.11253859403255384\nEpsilon = 0.11252734017315058\nEpsilon = 0.11251608743913327\nEpsilon = 0.11250483583038937\nEpsilon = 0.11249358534680633\nEpsilon = 0.11248233598827165\nEpsilon = 0.11247108775467282\nEpsilon = 0.11245984064589735\nEpsilon = 0.11244859466183277\nEpsilon = 0.11243734980236658\nEpsilon = 0.11242610606738634\nAgent: ddqn_agent . Episode 1174/2000. Number of steps to finish: 20. Loss: 15.930179595947266 Reward: -14.0\nEpsilon = 0.1124148634567796\nEpsilon = 0.11240362197043392\nEpsilon = 0.11239238160823688\nEpsilon = 0.11238114237007606\nEpsilon = 0.11236990425583905\nEpsilon = 0.11235866726541346\nEpsilon = 0.11234743139868693\nEpsilon = 0.11233619665554706\nEpsilon = 0.1123249630358815\nEpsilon = 0.11231373053957792\nEpsilon = 0.11230249916652396\nEpsilon = 0.1122912689166073\nEpsilon = 0.11228003978971564\nEpsilon = 0.11226881178573667\nEpsilon = 0.11225758490455809\nEpsilon = 0.11224635914606763\nEpsilon = 0.11223513451015303\nEpsilon = 0.11222391099670202\nEpsilon = 0.11221268860560235\nEpsilon = 0.11220146733674179\nAgent: ddqn_agent . Episode 1175/2000. Number of steps to finish: 20. Loss: 15.514496803283691 Reward: -14.0\nEpsilon = 0.11219024719000811\nEpsilon = 0.11217902816528912\nEpsilon = 0.11216781026247259\nEpsilon = 0.11215659348144634\nEpsilon = 0.1121453778220982\nEpsilon = 0.112134163284316\nEpsilon = 0.11212294986798757\nEpsilon = 0.11211173757300077\nEpsilon = 0.11210052639924348\nEpsilon = 0.11208931634660356\nEpsilon = 0.1120781074149689\nEpsilon = 0.1120668996042274\nEpsilon = 0.11205569291426698\nEpsilon = 0.11204448734497556\nEpsilon = 0.11203328289624107\nEpsilon = 0.11202207956795145\nEpsilon = 0.11201087735999465\nEpsilon = 0.11199967627225865\nEpsilon = 0.11198847630463143\nEpsilon = 0.11197727745700098\nAgent: ddqn_agent . Episode 1176/2000. Number of steps to finish: 20. Loss: 15.726086616516113 Reward: -18.0\nEpsilon = 0.11196607972925528\nEpsilon = 0.11195488312128235\nEpsilon = 0.11194368763297022\nEpsilon = 0.11193249326420693\nEpsilon = 0.11192130001488051\nEpsilon = 0.11191010788487903\nEpsilon = 0.11189891687409054\nEpsilon = 0.11188772698240312\nEpsilon = 0.11187653820970489\nEpsilon = 0.11186535055588392\nEpsilon = 0.11185416402082833\nEpsilon = 0.11184297860442625\nEpsilon = 0.11183179430656581\nEpsilon = 0.11182061112713516\nEpsilon = 0.11180942906602245\nEpsilon = 0.11179824812311585\nEpsilon = 0.11178706829830354\nEpsilon = 0.1117758895914737\nEpsilon = 0.11176471200251456\nEpsilon = 0.11175353553131431\nAgent: ddqn_agent . Episode 1177/2000. Number of steps to finish: 20. Loss: 17.76154136657715 Reward: -18.0\nEpsilon = 0.11174236017776118\nEpsilon = 0.1117311859417434\nEpsilon = 0.11172001282314922\nEpsilon = 0.11170884082186691\nEpsilon = 0.11169766993778472\nEpsilon = 0.11168650017079094\nEpsilon = 0.11167533152077387\nAgent: ddqn_agent . Episode 1178/2000. Number of steps to finish: 7. Loss: 5.531336307525635 Reward: 5.0\nEpsilon = 0.1116641639876218\nEpsilon = 0.11165299757122304\nEpsilon = 0.11164183227146593\nEpsilon = 0.11163066808823878\nEpsilon = 0.11161950502142996\nEpsilon = 0.11160834307092782\nEpsilon = 0.11159718223662073\nEpsilon = 0.11158602251839707\nEpsilon = 0.11157486391614523\nEpsilon = 0.1115637064297536\nEpsilon = 0.11155255005911063\nEpsilon = 0.11154139480410473\nEpsilon = 0.11153024066462432\nEpsilon = 0.11151908764055786\nEpsilon = 0.11150793573179381\nEpsilon = 0.11149678493822063\nEpsilon = 0.11148563525972681\nEpsilon = 0.11147448669620083\nEpsilon = 0.11146333924753121\nEpsilon = 0.11145219291360646\nAgent: ddqn_agent . Episode 1179/2000. Number of steps to finish: 20. Loss: 16.730331420898438 Reward: -10.0\nEpsilon = 0.1114410476943151\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.11142990358954567\nEpsilon = 0.11141876059918672\nEpsilon = 0.11140761872312681\nEpsilon = 0.1113964779612545\nEpsilon = 0.11138533831345837\nEpsilon = 0.11137419977962702\nEpsilon = 0.11136306235964906\nEpsilon = 0.1113519260534131\nEpsilon = 0.11134079086080775\nEpsilon = 0.11132965678172167\nEpsilon = 0.1113185238160435\nEpsilon = 0.11130739196366189\nEpsilon = 0.11129626122446552\nEpsilon = 0.11128513159834308\nEpsilon = 0.11127400308518325\nEpsilon = 0.11126287568487472\nEpsilon = 0.11125174939730624\nEpsilon = 0.11124062422236651\nEpsilon = 0.11122950015994428\nAgent: ddqn_agent . Episode 1180/2000. Number of steps to finish: 20. Loss: 16.91303825378418 Reward: -20.0\nEpsilon = 0.11121837720992828\nEpsilon = 0.11120725537220728\nEpsilon = 0.11119613464667007\nEpsilon = 0.1111850150332054\nEpsilon = 0.11117389653170208\nEpsilon = 0.1111627791420489\nEpsilon = 0.1111516628641347\nEpsilon = 0.1111405476978483\nEpsilon = 0.11112943364307852\nEpsilon = 0.11111832069971421\nEpsilon = 0.11110720886764423\nEpsilon = 0.11109609814675747\nEpsilon = 0.11108498853694279\nEpsilon = 0.1110738800380891\nEpsilon = 0.1110627726500853\nEpsilon = 0.11105166637282028\nEpsilon = 0.111040561206183\nEpsilon = 0.11102945715006238\nEpsilon = 0.11101835420434737\nEpsilon = 0.11100725236892695\nAgent: ddqn_agent . Episode 1181/2000. Number of steps to finish: 20. Loss: 16.3939208984375 Reward: -18.0\nEpsilon = 0.11099615164369005\nEpsilon = 0.11098505202852568\nEpsilon = 0.11097395352332283\nEpsilon = 0.1109628561279705\nEpsilon = 0.1109517598423577\nEpsilon = 0.11094066466637346\nEpsilon = 0.11092957059990682\nEpsilon = 0.11091847764284683\nEpsilon = 0.11090738579508255\nEpsilon = 0.11089629505650304\nEpsilon = 0.1108852054269974\nEpsilon = 0.1108741169064547\nEpsilon = 0.11086302949476405\nEpsilon = 0.11085194319181457\nEpsilon = 0.11084085799749539\nEpsilon = 0.11082977391169564\nEpsilon = 0.11081869093430448\nEpsilon = 0.11080760906521105\nEpsilon = 0.11079652830430453\nEpsilon = 0.1107854486514741\nAgent: ddqn_agent . Episode 1182/2000. Number of steps to finish: 20. Loss: 16.24118423461914 Reward: -20.0\nEpsilon = 0.11077437010660895\nEpsilon = 0.11076329266959829\nEpsilon = 0.11075221634033133\nEpsilon = 0.1107411411186973\nEpsilon = 0.11073006700458543\nEpsilon = 0.11071899399788497\nEpsilon = 0.11070792209848518\nEpsilon = 0.11069685130627534\nEpsilon = 0.11068578162114472\nEpsilon = 0.1106747130429826\nEpsilon = 0.1106636455716783\nEpsilon = 0.11065257920712114\nEpsilon = 0.11064151394920042\nEpsilon = 0.11063044979780551\nEpsilon = 0.11061938675282573\nEpsilon = 0.11060832481415045\nEpsilon = 0.11059726398166904\nEpsilon = 0.11058620425527087\nEpsilon = 0.11057514563484534\nEpsilon = 0.11056408812028186\nAgent: ddqn_agent . Episode 1183/2000. Number of steps to finish: 20. Loss: 16.369421005249023 Reward: -14.0\nEpsilon = 0.11055303171146984\nEpsilon = 0.11054197640829869\nEpsilon = 0.11053092221065786\nEpsilon = 0.1105198691184368\nEpsilon = 0.11050881713152495\nEpsilon = 0.11049776624981181\nEpsilon = 0.11048671647318682\nEpsilon = 0.1104756678015395\nEpsilon = 0.11046462023475935\nEpsilon = 0.11045357377273587\nEpsilon = 0.1104425284153586\nEpsilon = 0.11043148416251707\nEpsilon = 0.11042044101410081\nEpsilon = 0.1104093989699994\nEpsilon = 0.11039835803010241\nEpsilon = 0.1103873181942994\nEpsilon = 0.11037627946247996\nEpsilon = 0.11036524183453372\nEpsilon = 0.11035420531035027\nEpsilon = 0.11034316988981924\nAgent: ddqn_agent . Episode 1184/2000. Number of steps to finish: 20. Loss: 15.67513656616211 Reward: -14.0\nEpsilon = 0.11033213557283025\nEpsilon = 0.11032110235927298\nEpsilon = 0.11031007024903705\nEpsilon = 0.11029903924201215\nEpsilon = 0.11028800933808795\nEpsilon = 0.11027698053715415\nEpsilon = 0.11026595283910044\nEpsilon = 0.11025492624381654\nEpsilon = 0.11024390075119216\nEpsilon = 0.11023287636111703\nEpsilon = 0.11022185307348092\nEpsilon = 0.11021083088817357\nEpsilon = 0.11019980980508476\nEpsilon = 0.11018878982410425\nEpsilon = 0.11017777094512185\nEpsilon = 0.11016675316802733\nEpsilon = 0.11015573649271053\nEpsilon = 0.11014472091906126\nEpsilon = 0.11013370644696935\nEpsilon = 0.11012269307632465\nAgent: ddqn_agent . Episode 1185/2000. Number of steps to finish: 20. Loss: 17.121171951293945 Reward: -14.0\nEpsilon = 0.11011168080701701\nEpsilon = 0.1101006696389363\nEpsilon = 0.1100896595719724\nEpsilon = 0.11007865060601521\nEpsilon = 0.11006764274095461\nEpsilon = 0.11005663597668051\nEpsilon = 0.11004563031308284\nEpsilon = 0.11003462575005153\nEpsilon = 0.11002362228747653\nEpsilon = 0.11001261992524779\nEpsilon = 0.11000161866325527\nEpsilon = 0.10999061850138894\nEpsilon = 0.10997961943953881\nEpsilon = 0.10996862147759486\nEpsilon = 0.1099576246154471\nEpsilon = 0.10994662885298556\nEpsilon = 0.10993563419010026\nEpsilon = 0.10992464062668125\nEpsilon = 0.10991364816261859\nEpsilon = 0.10990265679780233\nAgent: ddqn_agent . Episode 1186/2000. Number of steps to finish: 20. Loss: 15.987723350524902 Reward: -18.0\nEpsilon = 0.10989166653212255\nEpsilon = 0.10988067736546935\nEpsilon = 0.1098696892977328\nEpsilon = 0.10985870232880303\nEpsilon = 0.10984771645857015\nEpsilon = 0.1098367316869243\nEpsilon = 0.10982574801375561\nEpsilon = 0.10981476543895423\nEpsilon = 0.10980378396241033\nEpsilon = 0.1097928035840141\nEpsilon = 0.1097818243036557\nEpsilon = 0.10977084612122533\nEpsilon = 0.10975986903661321\nEpsilon = 0.10974889304970954\nEpsilon = 0.10973791816040457\nEpsilon = 0.10972694436858853\nEpsilon = 0.10971597167415167\nEpsilon = 0.10970500007698426\nEpsilon = 0.10969402957697656\nEpsilon = 0.10968306017401887\nAgent: ddqn_agent . Episode 1187/2000. Number of steps to finish: 20. Loss: 17.618051528930664 Reward: -20.0\nEpsilon = 0.10967209186800146\nEpsilon = 0.10966112465881467\nEpsilon = 0.10965015854634878\nEpsilon = 0.10963919353049414\nEpsilon = 0.10962822961114109\nEpsilon = 0.10961726678817997\nEpsilon = 0.10960630506150115\nEpsilon = 0.109595344430995\nEpsilon = 0.10958438489655191\nEpsilon = 0.10957342645806226\nEpsilon = 0.10956246911541645\nEpsilon = 0.10955151286850491\nEpsilon = 0.10954055771721806\nEpsilon = 0.10952960366144635\nEpsilon = 0.1095186507010802\nEpsilon = 0.1095076988360101\nEpsilon = 0.1094967480661265\nEpsilon = 0.10948579839131989\nEpsilon = 0.10947484981148076\nEpsilon = 0.10946390232649961\nAgent: ddqn_agent . Episode 1188/2000. Number of steps to finish: 20. Loss: 15.748198509216309 Reward: -16.0\nEpsilon = 0.10945295593626696\nEpsilon = 0.10944201064067333\nEpsilon = 0.10943106643960926\nEpsilon = 0.1094201233329653\nEpsilon = 0.109409181320632\nEpsilon = 0.10939824040249994\nEpsilon = 0.10938730057845969\nEpsilon = 0.10937636184840184\nEpsilon = 0.109365424212217\nEpsilon = 0.10935448766979577\nEpsilon = 0.1093435522210288\nEpsilon = 0.1093326178658067\nEpsilon = 0.10932168460402011\nEpsilon = 0.10931075243555971\nEpsilon = 0.10929982136031616\nEpsilon = 0.10928889137818013\nEpsilon = 0.10927796248904231\nEpsilon = 0.1092670346927934\nEpsilon = 0.10925610798932413\nEpsilon = 0.1092451823785252\nAgent: ddqn_agent . Episode 1189/2000. Number of steps to finish: 20. Loss: 16.329740524291992 Reward: -18.0\nEpsilon = 0.10923425786028734\nEpsilon = 0.10922333443450132\nEpsilon = 0.10921241210105787\nEpsilon = 0.10920149085984776\nEpsilon = 0.10919057071076178\nEpsilon = 0.10917965165369072\nEpsilon = 0.10916873368852535\nEpsilon = 0.1091578168151565\nEpsilon = 0.10914690103347499\nEpsilon = 0.10913598634337164\nEpsilon = 0.1091250727447373\nEpsilon = 0.10911416023746283\nEpsilon = 0.10910324882143908\nEpsilon = 0.10909233849655695\nEpsilon = 0.1090814292627073\nEpsilon = 0.10907052111978102\nEpsilon = 0.10905961406766905\nEpsilon = 0.10904870810626228\nEpsilon = 0.10903780323545166\nEpsilon = 0.10902689945512811\nAgent: ddqn_agent . Episode 1190/2000. Number of steps to finish: 20. Loss: 16.722923278808594 Reward: -10.0\nEpsilon = 0.1090159967651826\nEpsilon = 0.10900509516550609\nEpsilon = 0.10899419465598954\nEpsilon = 0.10898329523652395\nEpsilon = 0.1089723969070003\nEpsilon = 0.1089614996673096\nEpsilon = 0.10895060351734287\nEpsilon = 0.10893970845699114\nEpsilon = 0.10892881448614544\nEpsilon = 0.10891792160469684\nEpsilon = 0.10890702981253636\nEpsilon = 0.1088961391095551\nEpsilon = 0.10888524949564415\nEpsilon = 0.1088743609706946\nEpsilon = 0.10886347353459754\nEpsilon = 0.10885258718724408\nEpsilon = 0.10884170192852535\nEpsilon = 0.1088308177583325\nEpsilon = 0.10881993467655668\nEpsilon = 0.10880905268308902\nAgent: ddqn_agent . Episode 1191/2000. Number of steps to finish: 20. Loss: 16.138824462890625 Reward: -10.0\nEpsilon = 0.10879817177782071\nEpsilon = 0.10878729196064293\nEpsilon = 0.10877641323144688\nEpsilon = 0.10876553559012374\nEpsilon = 0.10875465903656473\nEpsilon = 0.10874378357066107\nAgent: ddqn_agent . Episode 1192/2000. Number of steps to finish: 6. Loss: 4.18428373336792 Reward: 6.0\nEpsilon = 0.10873290919230401\nEpsilon = 0.10872203590138478\nEpsilon = 0.10871116369779464\nEpsilon = 0.10870029258142486\nEpsilon = 0.10868942255216671\nEpsilon = 0.1086785536099115\nEpsilon = 0.1086676857545505\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.10865681898597505\nEpsilon = 0.10864595330407646\nEpsilon = 0.10863508870874605\nAgent: ddqn_agent . Episode 1193/2000. Number of steps to finish: 10. Loss: 7.629100799560547 Reward: 2.0\nEpsilon = 0.10862422519987518\nEpsilon = 0.10861336277735519\nEpsilon = 0.10860250144107746\nEpsilon = 0.10859164119093336\nEpsilon = 0.10858078202681426\nEpsilon = 0.10856992394861158\nEpsilon = 0.10855906695621671\nEpsilon = 0.10854821104952109\nEpsilon = 0.10853735622841613\nEpsilon = 0.10852650249279329\nAgent: ddqn_agent . Episode 1194/2000. Number of steps to finish: 10. Loss: 7.738603591918945 Reward: 2.0\nEpsilon = 0.10851564984254401\nEpsilon = 0.10850479827755975\nEpsilon = 0.108493947797732\nEpsilon = 0.10848309840295223\nEpsilon = 0.10847225009311193\nEpsilon = 0.10846140286810262\nEpsilon = 0.10845055672781581\nEpsilon = 0.10843971167214303\nEpsilon = 0.10842886770097582\nEpsilon = 0.10841802481420572\nEpsilon = 0.10840718301172429\nEpsilon = 0.10839634229342313\nEpsilon = 0.10838550265919379\nEpsilon = 0.10837466410892788\nEpsilon = 0.10836382664251698\nEpsilon = 0.10835299025985273\nEpsilon = 0.10834215496082675\nEpsilon = 0.10833132074533067\nEpsilon = 0.10832048761325613\nEpsilon = 0.1083096555644948\nAgent: ddqn_agent . Episode 1195/2000. Number of steps to finish: 20. Loss: 18.203540802001953 Reward: -14.0\nEpsilon = 0.10829882459893836\nEpsilon = 0.10828799471647847\nEpsilon = 0.10827716591700683\nEpsilon = 0.10826633820041513\nEpsilon = 0.10825551156659509\nEpsilon = 0.10824468601543843\nEpsilon = 0.1082338615468369\nEpsilon = 0.10822303816068221\nEpsilon = 0.10821221585686615\nEpsilon = 0.10820139463528046\nEpsilon = 0.10819057449581693\nEpsilon = 0.10817975543836734\nEpsilon = 0.10816893746282351\nEpsilon = 0.10815812056907723\nEpsilon = 0.10814730475702032\nEpsilon = 0.10813649002654462\nEpsilon = 0.10812567637754196\nEpsilon = 0.1081148638099042\nEpsilon = 0.10810405232352321\nEpsilon = 0.10809324191829087\nAgent: ddqn_agent . Episode 1196/2000. Number of steps to finish: 20. Loss: 16.678646087646484 Reward: -20.0\nEpsilon = 0.10808243259409904\nEpsilon = 0.10807162435083963\nEpsilon = 0.10806081718840455\nEpsilon = 0.1080500111066857\nEpsilon = 0.10803920610557503\nEpsilon = 0.10802840218496448\nEpsilon = 0.10801759934474599\nEpsilon = 0.10800679758481152\nEpsilon = 0.10799599690505304\nEpsilon = 0.10798519730536255\nEpsilon = 0.10797439878563202\nEpsilon = 0.10796360134575346\nEpsilon = 0.10795280498561889\nEpsilon = 0.10794200970512033\nEpsilon = 0.10793121550414982\nEpsilon = 0.1079204223825994\nEpsilon = 0.10790963034036113\nEpsilon = 0.1078988393773271\nEpsilon = 0.10788804949338937\nEpsilon = 0.10787726068844003\nAgent: ddqn_agent . Episode 1197/2000. Number of steps to finish: 20. Loss: 16.249174118041992 Reward: -8.0\nEpsilon = 0.10786647296237119\nEpsilon = 0.10785568631507496\nEpsilon = 0.10784490074644346\nEpsilon = 0.10783411625636881\nEpsilon = 0.10782333284474317\nEpsilon = 0.1078125505114587\nEpsilon = 0.10780176925640755\nEpsilon = 0.10779098907948191\nAgent: ddqn_agent . Episode 1198/2000. Number of steps to finish: 8. Loss: 6.37816047668457 Reward: 4.0\nEpsilon = 0.10778020998057396\nEpsilon = 0.1077694319595759\nEpsilon = 0.10775865501637995\nEpsilon = 0.10774787915087831\nEpsilon = 0.10773710436296323\nEpsilon = 0.10772633065252693\nEpsilon = 0.10771555801946169\nEpsilon = 0.10770478646365975\nEpsilon = 0.10769401598501338\nEpsilon = 0.10768324658341488\nEpsilon = 0.10767247825875655\nEpsilon = 0.10766171101093067\nEpsilon = 0.10765094483982958\nEpsilon = 0.1076401797453456\nEpsilon = 0.10762941572737106\nEpsilon = 0.10761865278579832\nEpsilon = 0.10760789092051974\nEpsilon = 0.10759713013142769\nEpsilon = 0.10758637041841454\nEpsilon = 0.1075756117813727\nAgent: ddqn_agent . Episode 1199/2000. Number of steps to finish: 20. Loss: 15.973563194274902 Reward: -12.0\nEpsilon = 0.10756485422019457\nEpsilon = 0.10755409773477255\nEpsilon = 0.10754334232499907\nEpsilon = 0.10753258799076656\nEpsilon = 0.10752183473196748\nEpsilon = 0.10751108254849429\nEpsilon = 0.10750033144023943\nEpsilon = 0.10748958140709541\nEpsilon = 0.1074788324489547\nEpsilon = 0.1074680845657098\nEpsilon = 0.10745733775725323\nEpsilon = 0.1074465920234775\nEpsilon = 0.10743584736427515\nEpsilon = 0.10742510377953873\nEpsilon = 0.10741436126916078\nEpsilon = 0.10740361983303386\nEpsilon = 0.10739287947105056\nEpsilon = 0.10738214018310345\nEpsilon = 0.10737140196908514\nEpsilon = 0.10736066482888823\nAgent: ddqn_agent . Episode 1200/2000. Number of steps to finish: 20. Loss: 16.1989803314209 Reward: -18.0\nEpsilon = 0.10734992876240534\nEpsilon = 0.1073391937695291\nEpsilon = 0.10732845985015216\nEpsilon = 0.10731772700416714\nEpsilon = 0.10730699523146672\nEpsilon = 0.10729626453194357\nEpsilon = 0.10728553490549038\nEpsilon = 0.10727480635199983\nEpsilon = 0.10726407887136463\nEpsilon = 0.1072533524634775\nEpsilon = 0.10724262712823115\nEpsilon = 0.10723190286551833\nEpsilon = 0.10722117967523177\nEpsilon = 0.10721045755726426\nEpsilon = 0.10719973651150853\nEpsilon = 0.10718901653785738\nEpsilon = 0.1071782976362036\nEpsilon = 0.10716757980643998\nEpsilon = 0.10715686304845934\nEpsilon = 0.1071461473621545\nAgent: ddqn_agent . Episode 1201/2000. Number of steps to finish: 20. Loss: 16.84174156188965 Reward: -16.0\nEpsilon = 0.10713543274741828\nEpsilon = 0.10712471920414354\nEpsilon = 0.10711400673222313\nEpsilon = 0.1071032953315499\nEpsilon = 0.10709258500201675\nEpsilon = 0.10708187574351655\nEpsilon = 0.10707116755594219\nEpsilon = 0.1070604604391866\nEpsilon = 0.10704975439314268\nEpsilon = 0.10703904941770337\nEpsilon = 0.1070283455127616\nEpsilon = 0.10701764267821032\nEpsilon = 0.1070069409139425\nEpsilon = 0.1069962402198511\nEpsilon = 0.10698554059582911\nEpsilon = 0.10697484204176953\nEpsilon = 0.10696414455756535\nEpsilon = 0.1069534481431096\nEpsilon = 0.10694275279829529\nEpsilon = 0.10693205852301546\nAgent: ddqn_agent . Episode 1202/2000. Number of steps to finish: 20. Loss: 16.457653045654297 Reward: -14.0\nEpsilon = 0.10692136531716316\nEpsilon = 0.10691067318063145\nEpsilon = 0.10689998211331339\nEpsilon = 0.10688929211510206\nEpsilon = 0.10687860318589056\nEpsilon = 0.10686791532557197\nEpsilon = 0.10685722853403941\nEpsilon = 0.10684654281118601\nEpsilon = 0.1068358581569049\nEpsilon = 0.10682517457108921\nEpsilon = 0.10681449205363211\nEpsilon = 0.10680381060442674\nEpsilon = 0.1067931302233663\nEpsilon = 0.10678245091034397\nEpsilon = 0.10677177266525294\nEpsilon = 0.10676109548798642\nEpsilon = 0.10675041937843761\nEpsilon = 0.10673974433649977\nEpsilon = 0.10672907036206612\nEpsilon = 0.10671839745502992\nAgent: ddqn_agent . Episode 1203/2000. Number of steps to finish: 20. Loss: 16.84388542175293 Reward: -16.0\nEpsilon = 0.10670772561528442\nEpsilon = 0.10669705484272289\nEpsilon = 0.10668638513723862\nEpsilon = 0.1066757164987249\nEpsilon = 0.10666504892707503\nEpsilon = 0.10665438242218232\nEpsilon = 0.10664371698394011\nEpsilon = 0.10663305261224172\nEpsilon = 0.1066223893069805\nEpsilon = 0.10661172706804979\nEpsilon = 0.10660106589534299\nEpsilon = 0.10659040578875345\nEpsilon = 0.10657974674817458\nEpsilon = 0.10656908877349977\nEpsilon = 0.10655843186462242\nEpsilon = 0.10654777602143596\nEpsilon = 0.10653712124383381\nEpsilon = 0.10652646753170943\nEpsilon = 0.10651581488495626\nEpsilon = 0.10650516330346776\nAgent: ddqn_agent . Episode 1204/2000. Number of steps to finish: 20. Loss: 17.366512298583984 Reward: -16.0\nEpsilon = 0.10649451278713742\nEpsilon = 0.10648386333585871\nEpsilon = 0.10647321494952514\nEpsilon = 0.10646256762803018\nEpsilon = 0.10645192137126737\nEpsilon = 0.10644127617913024\nEpsilon = 0.10643063205151233\nEpsilon = 0.10641998898830718\nEpsilon = 0.10640934698940835\nEpsilon = 0.10639870605470941\nEpsilon = 0.10638806618410394\nEpsilon = 0.10637742737748554\nEpsilon = 0.10636678963474779\nEpsilon = 0.10635615295578431\nEpsilon = 0.10634551734048873\nEpsilon = 0.10633488278875469\nEpsilon = 0.10632424930047582\nEpsilon = 0.10631361687554577\nEpsilon = 0.10630298551385822\nEpsilon = 0.10629235521530683\nAgent: ddqn_agent . Episode 1205/2000. Number of steps to finish: 20. Loss: 17.467967987060547 Reward: -14.0\nEpsilon = 0.1062817259797853\nEpsilon = 0.10627109780718733\nEpsilon = 0.10626047069740661\nEpsilon = 0.10624984465033688\nEpsilon = 0.10623921966587184\nEpsilon = 0.10622859574390525\nEpsilon = 0.10621797288433087\nEpsilon = 0.10620735108704243\nEpsilon = 0.10619673035193374\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.10618611067889855\nEpsilon = 0.10617549206783065\nEpsilon = 0.10616487451862387\nEpsilon = 0.10615425803117201\nEpsilon = 0.10614364260536889\nAgent: ddqn_agent . Episode 1206/2000. Number of steps to finish: 14. Loss: 12.868435859680176 Reward: -2.0\nEpsilon = 0.10613302824110836\nEpsilon = 0.10612241493828425\nEpsilon = 0.10611180269679042\nEpsilon = 0.10610119151652074\nEpsilon = 0.10609058139736909\nEpsilon = 0.10607997233922935\nEpsilon = 0.10606936434199543\nEpsilon = 0.10605875740556123\nEpsilon = 0.10604815152982067\nEpsilon = 0.10603754671466768\nEpsilon = 0.10602694295999622\nEpsilon = 0.10601634026570023\nEpsilon = 0.10600573863167366\nEpsilon = 0.1059951380578105\nEpsilon = 0.10598453854400472\nEpsilon = 0.10597394009015032\nEpsilon = 0.10596334269614131\nEpsilon = 0.1059527463618717\nEpsilon = 0.10594215108723551\nEpsilon = 0.10593155687212678\nAgent: ddqn_agent . Episode 1207/2000. Number of steps to finish: 20. Loss: 16.90662384033203 Reward: -12.0\nEpsilon = 0.10592096371643957\nEpsilon = 0.10591037162006793\nEpsilon = 0.10589978058290593\nEpsilon = 0.10588919060484764\nEpsilon = 0.10587860168578715\nEpsilon = 0.10586801382561857\nEpsilon = 0.105857427024236\nEpsilon = 0.10584684128153359\nEpsilon = 0.10583625659740543\nEpsilon = 0.1058256729717457\nAgent: ddqn_agent . Episode 1208/2000. Number of steps to finish: 10. Loss: 8.067411422729492 Reward: 2.0\nEpsilon = 0.10581509040444852\nEpsilon = 0.10580450889540807\nEpsilon = 0.10579392844451853\nEpsilon = 0.10578334905167408\nEpsilon = 0.10577277071676891\nEpsilon = 0.10576219343969724\nEpsilon = 0.10575161722035327\nEpsilon = 0.10574104205863123\nEpsilon = 0.10573046795442537\nEpsilon = 0.10571989490762992\nEpsilon = 0.10570932291813916\nEpsilon = 0.10569875198584734\nEpsilon = 0.10568818211064876\nEpsilon = 0.1056776132924377\nEpsilon = 0.10566704553110846\nEpsilon = 0.10565647882655535\nEpsilon = 0.1056459131786727\nEpsilon = 0.10563534858735484\nEpsilon = 0.1056247850524961\nEpsilon = 0.10561422257399085\nAgent: ddqn_agent . Episode 1209/2000. Number of steps to finish: 20. Loss: 17.128103256225586 Reward: -10.0\nEpsilon = 0.10560366115173345\nEpsilon = 0.10559310078561827\nEpsilon = 0.1055825414755397\nEpsilon = 0.10557198322139215\nEpsilon = 0.10556142602307\nEpsilon = 0.1055508698804677\nEpsilon = 0.10554031479347965\nEpsilon = 0.10552976076200031\nEpsilon = 0.10551920778592411\nEpsilon = 0.10550865586514552\nEpsilon = 0.105498104999559\nEpsilon = 0.10548755518905904\nEpsilon = 0.10547700643354013\nEpsilon = 0.10546645873289678\nEpsilon = 0.1054559120870235\nEpsilon = 0.10544536649581479\nEpsilon = 0.1054348219591652\nEpsilon = 0.1054242784769693\nEpsilon = 0.1054137360491216\nEpsilon = 0.10540319467551669\nAgent: ddqn_agent . Episode 1210/2000. Number of steps to finish: 20. Loss: 16.688514709472656 Reward: -10.0\nEpsilon = 0.10539265435604914\nEpsilon = 0.10538211509061353\nEpsilon = 0.10537157687910446\nEpsilon = 0.10536103972141656\nEpsilon = 0.10535050361744441\nEpsilon = 0.10533996856708266\nEpsilon = 0.10532943457022596\nEpsilon = 0.10531890162676895\nEpsilon = 0.10530836973660627\nEpsilon = 0.10529783889963261\nEpsilon = 0.10528730911574265\nEpsilon = 0.10527678038483107\nEpsilon = 0.10526625270679259\nEpsilon = 0.10525572608152191\nEpsilon = 0.10524520050891376\nEpsilon = 0.10523467598886287\nEpsilon = 0.10522415252126398\nEpsilon = 0.10521363010601185\nEpsilon = 0.10520310874300126\nEpsilon = 0.10519258843212696\nAgent: ddqn_agent . Episode 1211/2000. Number of steps to finish: 20. Loss: 17.572969436645508 Reward: -20.0\nEpsilon = 0.10518206917328375\nEpsilon = 0.10517155096636642\nEpsilon = 0.10516103381126979\nEpsilon = 0.10515051770788866\nEpsilon = 0.10514000265611788\nEpsilon = 0.10512948865585227\nEpsilon = 0.10511897570698668\nEpsilon = 0.10510846380941598\nEpsilon = 0.10509795296303504\nEpsilon = 0.10508744316773873\nEpsilon = 0.10507693442342196\nEpsilon = 0.10506642672997962\nEpsilon = 0.10505592008730663\nEpsilon = 0.1050454144952979\nEpsilon = 0.10503490995384837\nEpsilon = 0.10502440646285299\nEpsilon = 0.10501390402220671\nEpsilon = 0.1050034026318045\nEpsilon = 0.10499290229154132\nEpsilon = 0.10498240300131216\nAgent: ddqn_agent . Episode 1212/2000. Number of steps to finish: 20. Loss: 17.019004821777344 Reward: -14.0\nEpsilon = 0.10497190476101204\nEpsilon = 0.10496140757053593\nEpsilon = 0.10495091142977887\nEpsilon = 0.10494041633863589\nEpsilon = 0.10492992229700203\nEpsilon = 0.10491942930477233\nEpsilon = 0.10490893736184186\nEpsilon = 0.10489844646810567\nEpsilon = 0.10488795662345886\nEpsilon = 0.10487746782779651\nEpsilon = 0.10486698008101374\nEpsilon = 0.10485649338300564\nEpsilon = 0.10484600773366734\nEpsilon = 0.10483552313289397\nEpsilon = 0.10482503958058069\nEpsilon = 0.10481455707662263\nEpsilon = 0.10480407562091497\nEpsilon = 0.10479359521335288\nEpsilon = 0.10478311585383154\nEpsilon = 0.10477263754224617\nAgent: ddqn_agent . Episode 1213/2000. Number of steps to finish: 20. Loss: 19.103342056274414 Reward: -10.0\nEpsilon = 0.10476216027849194\nEpsilon = 0.1047516840624641\nEpsilon = 0.10474120889405786\nEpsilon = 0.10473073477316845\nEpsilon = 0.10472026169969113\nEpsilon = 0.10470978967352117\nEpsilon = 0.10469931869455382\nEpsilon = 0.10468884876268436\nEpsilon = 0.10467837987780809\nEpsilon = 0.10466791203982032\nEpsilon = 0.10465744524861634\nEpsilon = 0.10464697950409148\nEpsilon = 0.10463651480614107\nEpsilon = 0.10462605115466046\nEpsilon = 0.104615588549545\nEpsilon = 0.10460512699069005\nEpsilon = 0.10459466647799098\nEpsilon = 0.10458420701134319\nEpsilon = 0.10457374859064206\nEpsilon = 0.10456329121578299\nAgent: ddqn_agent . Episode 1214/2000. Number of steps to finish: 20. Loss: 17.120708465576172 Reward: -12.0\nEpsilon = 0.10455283488666141\nEpsilon = 0.10454237960317274\nEpsilon = 0.10453192536521243\nEpsilon = 0.1045214721726759\nEpsilon = 0.10451102002545865\nEpsilon = 0.1045005689234561\nEpsilon = 0.10449011886656376\nEpsilon = 0.1044796698546771\nEpsilon = 0.10446922188769164\nEpsilon = 0.10445877496550288\nEpsilon = 0.10444832908800633\nEpsilon = 0.10443788425509754\nEpsilon = 0.10442744046667203\nEpsilon = 0.10441699772262536\nEpsilon = 0.10440655602285309\nEpsilon = 0.10439611536725081\nEpsilon = 0.10438567575571409\nEpsilon = 0.10437523718813851\nEpsilon = 0.10436479966441971\nEpsilon = 0.10435436318445326\nAgent: ddqn_agent . Episode 1215/2000. Number of steps to finish: 20. Loss: 15.387338638305664 Reward: -20.0\nEpsilon = 0.10434392774813482\nEpsilon = 0.10433349335536\nEpsilon = 0.10432306000602447\nEpsilon = 0.10431262770002386\nEpsilon = 0.10430219643725386\nEpsilon = 0.10429176621761013\nEpsilon = 0.10428133704098837\nEpsilon = 0.10427090890728427\nEpsilon = 0.10426048181639354\nEpsilon = 0.1042500557682119\nEpsilon = 0.10423963076263508\nEpsilon = 0.10422920679955881\nEpsilon = 0.10421878387887885\nEpsilon = 0.10420836200049097\nEpsilon = 0.10419794116429092\nEpsilon = 0.1041875213701745\nEpsilon = 0.10417710261803748\nEpsilon = 0.10416668490777567\nEpsilon = 0.1041562682392849\nEpsilon = 0.10414585261246097\nAgent: ddqn_agent . Episode 1216/2000. Number of steps to finish: 20. Loss: 17.34364891052246 Reward: -14.0\nEpsilon = 0.10413543802719973\nEpsilon = 0.10412502448339701\nEpsilon = 0.10411461198094868\nEpsilon = 0.10410420051975058\nEpsilon = 0.10409379009969862\nEpsilon = 0.10408338072068865\nEpsilon = 0.10407297238261658\nEpsilon = 0.10406256508537831\nEpsilon = 0.10405215882886977\nEpsilon = 0.10404175361298688\nEpsilon = 0.10403134943762558\nEpsilon = 0.10402094630268183\nEpsilon = 0.10401054420805156\nEpsilon = 0.10400014315363075\nEpsilon = 0.10398974313931539\nEpsilon = 0.10397934416500146\nEpsilon = 0.10396894623058496\nEpsilon = 0.10395854933596191\nEpsilon = 0.10394815348102832\nEpsilon = 0.10393775866568021\nAgent: ddqn_agent . Episode 1217/2000. Number of steps to finish: 20. Loss: 17.683961868286133 Reward: -16.0\nEpsilon = 0.10392736488981365\nEpsilon = 0.10391697215332467\nEpsilon = 0.10390658045610934\nEpsilon = 0.10389618979806374\nEpsilon = 0.10388580017908393\nEpsilon = 0.10387541159906602\nEpsilon = 0.10386502405790611\nEpsilon = 0.10385463755550033\nEpsilon = 0.10384425209174478\nEpsilon = 0.1038338676665356\nAgent: ddqn_agent . Episode 1218/2000. Number of steps to finish: 10. Loss: 8.188949584960938 Reward: 2.0\nEpsilon = 0.10382348427976895\nEpsilon = 0.10381310193134098\nEpsilon = 0.10380272062114784\nEpsilon = 0.10379234034908573\nEpsilon = 0.10378196111505082\nEpsilon = 0.10377158291893931\nEpsilon = 0.10376120576064742\nEpsilon = 0.10375082964007135\nEpsilon = 0.10374045455710734\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.10373008051165163\nEpsilon = 0.10371970750360046\nEpsilon = 0.1037093355328501\nEpsilon = 0.10369896459929681\nEpsilon = 0.10368859470283688\nEpsilon = 0.1036782258433666\nEpsilon = 0.10366785802078227\nEpsilon = 0.10365749123498019\nEpsilon = 0.1036471254858567\nEpsilon = 0.1036367607733081\nEpsilon = 0.10362639709723077\nAgent: ddqn_agent . Episode 1219/2000. Number of steps to finish: 20. Loss: 16.59807777404785 Reward: -16.0\nEpsilon = 0.10361603445752106\nEpsilon = 0.1036056728540753\nEpsilon = 0.1035953122867899\nEpsilon = 0.10358495275556123\nEpsilon = 0.10357459426028567\nEpsilon = 0.10356423680085965\nEpsilon = 0.10355388037717957\nEpsilon = 0.10354352498914185\nEpsilon = 0.10353317063664294\nEpsilon = 0.10352281731957928\nEpsilon = 0.10351246503784732\nEpsilon = 0.10350211379134354\nEpsilon = 0.10349176357996441\nEpsilon = 0.10348141440360642\nEpsilon = 0.10347106626216607\nEpsilon = 0.10346071915553985\nEpsilon = 0.1034503730836243\nEpsilon = 0.10344002804631594\nEpsilon = 0.10342968404351131\nEpsilon = 0.10341934107510696\nAgent: ddqn_agent . Episode 1220/2000. Number of steps to finish: 20. Loss: 16.299131393432617 Reward: -18.0\nEpsilon = 0.10340899914099945\nEpsilon = 0.10339865824108535\nEpsilon = 0.10338831837526125\nEpsilon = 0.10337797954342372\nEpsilon = 0.10336764174546938\nEpsilon = 0.10335730498129483\nEpsilon = 0.1033469692507967\nEpsilon = 0.10333663455387163\nEpsilon = 0.10332630089041624\nEpsilon = 0.1033159682603272\nEpsilon = 0.10330563666350116\nEpsilon = 0.10329530609983481\nEpsilon = 0.10328497656922483\nEpsilon = 0.10327464807156791\nEpsilon = 0.10326432060676076\nEpsilon = 0.10325399417470009\nEpsilon = 0.10324366877528263\nEpsilon = 0.1032333444084051\nEpsilon = 0.10322302107396426\nEpsilon = 0.10321269877185686\nAgent: ddqn_agent . Episode 1221/2000. Number of steps to finish: 20. Loss: 16.15546417236328 Reward: -16.0\nEpsilon = 0.10320237750197968\nEpsilon = 0.10319205726422948\nEpsilon = 0.10318173805850306\nEpsilon = 0.1031714198846972\nEpsilon = 0.10316110274270873\nEpsilon = 0.10315078663243446\nEpsilon = 0.10314047155377122\nEpsilon = 0.10313015750661585\nEpsilon = 0.10311984449086518\nEpsilon = 0.1031095325064161\nEpsilon = 0.10309922155316545\nEpsilon = 0.10308891163101014\nEpsilon = 0.10307860273984704\nEpsilon = 0.10306829487957306\nEpsilon = 0.1030579880500851\nEpsilon = 0.1030476822512801\nEpsilon = 0.10303737748305497\nEpsilon = 0.10302707374530666\nEpsilon = 0.10301677103793214\nEpsilon = 0.10300646936082834\nAgent: ddqn_agent . Episode 1222/2000. Number of steps to finish: 20. Loss: 15.371524810791016 Reward: -14.0\nEpsilon = 0.10299616871389226\nEpsilon = 0.10298586909702087\nEpsilon = 0.10297557051011116\nEpsilon = 0.10296527295306016\nEpsilon = 0.10295497642576484\nEpsilon = 0.10294468092812227\nEpsilon = 0.10293438646002946\nEpsilon = 0.10292409302138346\nEpsilon = 0.10291380061208132\nEpsilon = 0.10290350923202012\nEpsilon = 0.10289321888109691\nEpsilon = 0.1028829295592088\nEpsilon = 0.10287264126625288\nEpsilon = 0.10286235400212626\nAgent: ddqn_agent . Episode 1223/2000. Number of steps to finish: 14. Loss: 11.99912166595459 Reward: -2.0\nEpsilon = 0.10285206776672605\nEpsilon = 0.10284178255994937\nEpsilon = 0.10283149838169338\nEpsilon = 0.10282121523185521\nEpsilon = 0.10281093311033203\nEpsilon = 0.102800652017021\nEpsilon = 0.1027903719518193\nEpsilon = 0.10278009291462412\nEpsilon = 0.10276981490533266\nEpsilon = 0.10275953792384213\nEpsilon = 0.10274926197004974\nEpsilon = 0.10273898704385274\nEpsilon = 0.10272871314514836\nEpsilon = 0.10271844027383385\nEpsilon = 0.10270816842980647\nEpsilon = 0.1026978976129635\nAgent: ddqn_agent . Episode 1224/2000. Number of steps to finish: 16. Loss: 14.182069778442383 Reward: -4.0\nEpsilon = 0.1026876278232022\nEpsilon = 0.10267735906041989\nEpsilon = 0.10266709132451385\nEpsilon = 0.10265682461538139\nEpsilon = 0.10264655893291985\nEpsilon = 0.10263629427702656\nEpsilon = 0.10262603064759886\nEpsilon = 0.1026157680445341\nEpsilon = 0.10260550646772965\nEpsilon = 0.10259524591708287\nEpsilon = 0.10258498639249117\nEpsilon = 0.10257472789385191\nEpsilon = 0.10256447042106252\nEpsilon = 0.10255421397402041\nEpsilon = 0.10254395855262301\nEpsilon = 0.10253370415676774\nEpsilon = 0.10252345078635207\nEpsilon = 0.10251319844127343\nEpsilon = 0.10250294712142931\nEpsilon = 0.10249269682671716\nAgent: ddqn_agent . Episode 1225/2000. Number of steps to finish: 20. Loss: 17.029640197753906 Reward: -16.0\nEpsilon = 0.10248244755703449\nEpsilon = 0.10247219931227879\nEpsilon = 0.10246195209234756\nEpsilon = 0.10245170589713833\nEpsilon = 0.10244146072654861\nEpsilon = 0.10243121658047595\nEpsilon = 0.1024209734588179\nEpsilon = 0.10241073136147202\nEpsilon = 0.10240049028833588\nEpsilon = 0.10239025023930705\nEpsilon = 0.10238001121428311\nEpsilon = 0.10236977321316168\nEpsilon = 0.10235953623584036\nEpsilon = 0.10234930028221678\nEpsilon = 0.10233906535218856\nEpsilon = 0.10232883144565334\nEpsilon = 0.10231859856250877\nEpsilon = 0.10230836670265252\nEpsilon = 0.10229813586598226\nEpsilon = 0.10228790605239566\nAgent: ddqn_agent . Episode 1226/2000. Number of steps to finish: 20. Loss: 17.260040283203125 Reward: -14.0\nEpsilon = 0.10227767726179042\nEpsilon = 0.10226744949406424\nEpsilon = 0.10225722274911483\nEpsilon = 0.10224699702683993\nEpsilon = 0.10223677232713724\nEpsilon = 0.10222654864990452\nEpsilon = 0.10221632599503953\nEpsilon = 0.10220610436244003\nEpsilon = 0.10219588375200378\nEpsilon = 0.10218566416362858\nEpsilon = 0.10217544559721221\nEpsilon = 0.10216522805265249\nEpsilon = 0.10215501152984723\nEpsilon = 0.10214479602869424\nEpsilon = 0.10213458154909137\nEpsilon = 0.10212436809093646\nEpsilon = 0.10211415565412736\nEpsilon = 0.10210394423856195\nEpsilon = 0.1020937338441381\nEpsilon = 0.10208352447075368\nAgent: ddqn_agent . Episode 1227/2000. Number of steps to finish: 20. Loss: 17.587194442749023 Reward: -14.0\nEpsilon = 0.10207331611830661\nEpsilon = 0.10206310878669478\nEpsilon = 0.10205290247581611\nEpsilon = 0.10204269718556853\nEpsilon = 0.10203249291584998\nEpsilon = 0.1020222896665584\nEpsilon = 0.10201208743759174\nEpsilon = 0.10200188622884798\nEpsilon = 0.1019916860402251\nEpsilon = 0.10198148687162108\nEpsilon = 0.10197128872293391\nEpsilon = 0.10196109159406162\nEpsilon = 0.10195089548490222\nEpsilon = 0.10194070039535373\nEpsilon = 0.1019305063253142\nEpsilon = 0.10192031327468166\nEpsilon = 0.1019101212433542\nEpsilon = 0.10189993023122987\nEpsilon = 0.10188974023820675\nEpsilon = 0.10187955126418294\nAgent: ddqn_agent . Episode 1228/2000. Number of steps to finish: 20. Loss: 17.77961540222168 Reward: -20.0\nEpsilon = 0.10186936330905652\nEpsilon = 0.10185917637272561\nEpsilon = 0.10184899045508834\nEpsilon = 0.10183880555604284\nEpsilon = 0.10182862167548724\nEpsilon = 0.1018184388133197\nEpsilon = 0.10180825696943838\nEpsilon = 0.10179807614374144\nEpsilon = 0.10178789633612706\nEpsilon = 0.10177771754649345\nEpsilon = 0.1017675397747388\nEpsilon = 0.10175736302076133\nEpsilon = 0.10174718728445925\nEpsilon = 0.10173701256573081\nEpsilon = 0.10172683886447424\nEpsilon = 0.10171666618058779\nEpsilon = 0.10170649451396974\nEpsilon = 0.10169632386451834\nEpsilon = 0.10168615423213188\nEpsilon = 0.10167598561670867\nAgent: ddqn_agent . Episode 1229/2000. Number of steps to finish: 20. Loss: 17.100543975830078 Reward: -16.0\nEpsilon = 0.101665818018147\nEpsilon = 0.1016556514363452\nEpsilon = 0.10164548587120156\nEpsilon = 0.10163532132261445\nEpsilon = 0.10162515779048219\nEpsilon = 0.10161499527470313\nEpsilon = 0.10160483377517567\nEpsilon = 0.10159467329179815\nEpsilon = 0.10158451382446897\nEpsilon = 0.10157435537308653\nEpsilon = 0.10156419793754921\nEpsilon = 0.10155404151775546\nEpsilon = 0.10154388611360368\nEpsilon = 0.10153373172499232\nEpsilon = 0.10152357835181983\nEpsilon = 0.10151342599398465\nEpsilon = 0.10150327465138526\nEpsilon = 0.10149312432392012\nEpsilon = 0.10148297501148773\nEpsilon = 0.10147282671398658\nAgent: ddqn_agent . Episode 1230/2000. Number of steps to finish: 20. Loss: 16.333879470825195 Reward: -10.0\nEpsilon = 0.10146267943131518\nEpsilon = 0.10145253316337205\nEpsilon = 0.10144238791005572\nEpsilon = 0.10143224367126472\nEpsilon = 0.10142210044689759\nEpsilon = 0.1014119582368529\nEpsilon = 0.10140181704102921\nEpsilon = 0.10139167685932511\nEpsilon = 0.10138153769163918\nEpsilon = 0.10137139953787003\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.10136126239791624\nEpsilon = 0.10135112627167644\nEpsilon = 0.10134099115904928\nEpsilon = 0.10133085705993337\nEpsilon = 0.10132072397422738\nEpsilon = 0.10131059190182996\nEpsilon = 0.10130046084263979\nEpsilon = 0.10129033079655553\nEpsilon = 0.10128020176347588\nEpsilon = 0.10127007374329954\nAgent: ddqn_agent . Episode 1231/2000. Number of steps to finish: 20. Loss: 16.479703903198242 Reward: -12.0\nEpsilon = 0.10125994673592521\nEpsilon = 0.10124982074125162\nEpsilon = 0.10123969575917749\nEpsilon = 0.10122957178960157\nEpsilon = 0.1012194488324226\nEpsilon = 0.10120932688753936\nEpsilon = 0.10119920595485062\nEpsilon = 0.10118908603425514\nEpsilon = 0.10117896712565172\nEpsilon = 0.10116884922893915\nEpsilon = 0.10115873234401626\nEpsilon = 0.10114861647078185\nEpsilon = 0.10113850160913478\nEpsilon = 0.10112838775897387\nEpsilon = 0.10111827492019797\nEpsilon = 0.10110816309270595\nEpsilon = 0.10109805227639669\nEpsilon = 0.10108794247116905\nEpsilon = 0.10107783367692193\nEpsilon = 0.10106772589355424\nAgent: ddqn_agent . Episode 1232/2000. Number of steps to finish: 20. Loss: 15.840929985046387 Reward: -16.0\nEpsilon = 0.10105761912096489\nEpsilon = 0.10104751335905279\nEpsilon = 0.10103740860771689\nEpsilon = 0.10102730486685611\nEpsilon = 0.10101720213636943\nEpsilon = 0.10100710041615579\nEpsilon = 0.10099699970611418\nEpsilon = 0.10098690000614358\nEpsilon = 0.10097680131614296\nEpsilon = 0.10096670363601135\nEpsilon = 0.10095660696564775\nEpsilon = 0.10094651130495118\nEpsilon = 0.10093641665382068\nEpsilon = 0.1009263230121553\nEpsilon = 0.10091623037985409\nEpsilon = 0.1009061387568161\nEpsilon = 0.10089604814294043\nEpsilon = 0.10088595853812614\nEpsilon = 0.10087586994227232\nEpsilon = 0.1008657823552781\nAgent: ddqn_agent . Episode 1233/2000. Number of steps to finish: 20. Loss: 16.225879669189453 Reward: -14.0\nEpsilon = 0.10085569577704258\nEpsilon = 0.10084561020746488\nEpsilon = 0.10083552564644413\nEpsilon = 0.10082544209387949\nEpsilon = 0.10081535954967011\nEpsilon = 0.10080527801371514\nEpsilon = 0.10079519748591377\nEpsilon = 0.10078511796616518\nEpsilon = 0.10077503945436857\nEpsilon = 0.10076496195042313\nEpsilon = 0.10075488545422809\nEpsilon = 0.10074480996568266\nEpsilon = 0.1007347354846861\nEpsilon = 0.10072466201113764\nEpsilon = 0.10071458954493652\nEpsilon = 0.10070451808598203\nEpsilon = 0.10069444763417344\nEpsilon = 0.10068437818941002\nEpsilon = 0.10067430975159108\nEpsilon = 0.10066424232061592\nAgent: ddqn_agent . Episode 1234/2000. Number of steps to finish: 20. Loss: 17.21411895751953 Reward: -12.0\nEpsilon = 0.10065417589638387\nEpsilon = 0.10064411047879424\nEpsilon = 0.10063404606774635\nEpsilon = 0.10062398266313959\nEpsilon = 0.10061392026487327\nEpsilon = 0.10060385887284679\nEpsilon = 0.1005937984869595\nEpsilon = 0.1005837391071108\nEpsilon = 0.10057368073320008\nEpsilon = 0.10056362336512677\nEpsilon = 0.10055356700279025\nEpsilon = 0.10054351164608998\nEpsilon = 0.10053345729492537\nEpsilon = 0.10052340394919587\nEpsilon = 0.10051335160880095\nEpsilon = 0.10050330027364007\nEpsilon = 0.10049324994361271\nEpsilon = 0.10048320061861835\nEpsilon = 0.10047315229855648\nEpsilon = 0.10046310498332663\nAgent: ddqn_agent . Episode 1235/2000. Number of steps to finish: 20. Loss: 16.46122932434082 Reward: -10.0\nEpsilon = 0.10045305867282829\nEpsilon = 0.10044301336696101\nEpsilon = 0.10043296906562432\nEpsilon = 0.10042292576871777\nEpsilon = 0.1004128834761409\nEpsilon = 0.10040284218779329\nEpsilon = 0.10039280190357451\nEpsilon = 0.10038276262338415\nEpsilon = 0.10037272434712181\nEpsilon = 0.10036268707468711\nEpsilon = 0.10035265080597965\nEpsilon = 0.10034261554089904\nEpsilon = 0.10033258127934495\nEpsilon = 0.10032254802121703\nEpsilon = 0.10031251576641491\nEpsilon = 0.10030248451483827\nEpsilon = 0.10029245426638679\nEpsilon = 0.10028242502096016\nEpsilon = 0.10027239677845806\nEpsilon = 0.10026236953878022\nAgent: ddqn_agent . Episode 1236/2000. Number of steps to finish: 20. Loss: 16.736053466796875 Reward: -20.0\nEpsilon = 0.10025234330182634\nEpsilon = 0.10024231806749616\nEpsilon = 0.1002322938356894\nEpsilon = 0.10022227060630584\nEpsilon = 0.10021224837924521\nEpsilon = 0.10020222715440728\nEpsilon = 0.10019220693169184\nEpsilon = 0.10018218771099867\nEpsilon = 0.10017216949222757\nEpsilon = 0.10016215227527835\nAgent: ddqn_agent . Episode 1237/2000. Number of steps to finish: 10. Loss: 8.492595672607422 Reward: 2.0\nEpsilon = 0.10015213606005081\nEpsilon = 0.1001421208464448\nEpsilon = 0.10013210663436016\nEpsilon = 0.10012209342369673\nEpsilon = 0.10011208121435436\nEpsilon = 0.10010207000623292\nEpsilon = 0.1000920597992323\nEpsilon = 0.10008205059325238\nEpsilon = 0.10007204238819306\nEpsilon = 0.10006203518395425\nEpsilon = 0.10005202898043586\nEpsilon = 0.10004202377753782\nEpsilon = 0.10003201957516007\nEpsilon = 0.10002201637320256\nEpsilon = 0.10001201417156524\nEpsilon = 0.10000201297014809\nEpsilon = 0.09999201276885107\nEpsilon = 0.09998201356757419\nEpsilon = 0.09997201536621743\nEpsilon = 0.09996201816468081\nAgent: ddqn_agent . Episode 1238/2000. Number of steps to finish: 20. Loss: 17.768089294433594 Reward: -16.0\nEpsilon = 0.09995202196286435\nEpsilon = 0.09994202676066806\nEpsilon = 0.09993203255799199\nEpsilon = 0.09992203935473619\nEpsilon = 0.09991204715080072\nEpsilon = 0.09990205594608564\nEpsilon = 0.09989206574049103\nEpsilon = 0.09988207653391698\nEpsilon = 0.09987208832626358\nEpsilon = 0.09986210111743096\nEpsilon = 0.09985211490731923\nEpsilon = 0.0998421296958285\nEpsilon = 0.09983214548285892\nEpsilon = 0.09982216226831063\nEpsilon = 0.0998121800520838\nEpsilon = 0.09980219883407859\nEpsilon = 0.09979221861419518\nEpsilon = 0.09978223939233376\nEpsilon = 0.09977226116839452\nEpsilon = 0.09976228394227768\nAgent: ddqn_agent . Episode 1239/2000. Number of steps to finish: 20. Loss: 16.740747451782227 Reward: -18.0\nEpsilon = 0.09975230771388345\nEpsilon = 0.09974233248311207\nEpsilon = 0.09973235824986376\nEpsilon = 0.09972238501403878\nEpsilon = 0.09971241277553738\nEpsilon = 0.09970244153425983\nEpsilon = 0.0996924712901064\nEpsilon = 0.09968250204297739\nEpsilon = 0.09967253379277309\nEpsilon = 0.09966256653939382\nEpsilon = 0.09965260028273988\nEpsilon = 0.0996426350227116\nEpsilon = 0.09963267075920933\nEpsilon = 0.09962270749213341\nEpsilon = 0.09961274522138419\nEpsilon = 0.09960278394686205\nEpsilon = 0.09959282366846736\nEpsilon = 0.09958286438610052\nAgent: ddqn_agent . Episode 1240/2000. Number of steps to finish: 18. Loss: 14.899826049804688 Reward: -6.0\nEpsilon = 0.0995729060996619\nEpsilon = 0.09956294880905193\nEpsilon = 0.09955299251417103\nEpsilon = 0.09954303721491961\nEpsilon = 0.09953308291119813\nEpsilon = 0.099523129602907\nEpsilon = 0.09951317728994671\nEpsilon = 0.09950322597221772\nEpsilon = 0.0994932756496205\nEpsilon = 0.09948332632205555\nEpsilon = 0.09947337798942334\nEpsilon = 0.0994634306516244\nEpsilon = 0.09945348430855924\nEpsilon = 0.09944353896012839\nEpsilon = 0.09943359460623238\nEpsilon = 0.09942365124677176\nEpsilon = 0.09941370888164708\nEpsilon = 0.09940376751075891\nEpsilon = 0.09939382713400784\nEpsilon = 0.09938388775129443\nAgent: ddqn_agent . Episode 1241/2000. Number of steps to finish: 20. Loss: 16.742687225341797 Reward: -10.0\nEpsilon = 0.09937394936251931\nEpsilon = 0.09936401196758306\nEpsilon = 0.0993540755663863\nEpsilon = 0.09934414015882966\nEpsilon = 0.09933420574481377\nEpsilon = 0.0993242723242393\nEpsilon = 0.09931433989700687\nEpsilon = 0.09930440846301718\nEpsilon = 0.09929447802217088\nEpsilon = 0.09928454857436866\nEpsilon = 0.09927462011951123\nEpsilon = 0.09926469265749928\nEpsilon = 0.09925476618823353\nEpsilon = 0.09924484071161471\nEpsilon = 0.09923491622754355\nEpsilon = 0.0992249927359208\nEpsilon = 0.0992150702366472\nEpsilon = 0.09920514872962354\nEpsilon = 0.09919522821475057\nEpsilon = 0.0991853086919291\nAgent: ddqn_agent . Episode 1242/2000. Number of steps to finish: 20. Loss: 17.251373291015625 Reward: -14.0\nEpsilon = 0.0991753901610599\nEpsilon = 0.0991654726220438\nEpsilon = 0.09915555607478159\nEpsilon = 0.09914564051917411\nEpsilon = 0.0991357259551222\nEpsilon = 0.09912581238252668\nEpsilon = 0.09911589980128843\nEpsilon = 0.09910598821130831\nEpsilon = 0.09909607761248718\nEpsilon = 0.09908616800472593\nEpsilon = 0.09907625938792546\nEpsilon = 0.09906635176198667\nEpsilon = 0.09905644512681047\nEpsilon = 0.09904653948229779\nEpsilon = 0.09903663482834955\nEpsilon = 0.09902673116486672\nEpsilon = 0.09901682849175024\nEpsilon = 0.09900692680890107\nEpsilon = 0.09899702611622017\nEpsilon = 0.09898712641360856\nAgent: ddqn_agent . Episode 1243/2000. Number of steps to finish: 20. Loss: 16.946352005004883 Reward: -14.0\nEpsilon = 0.0989772277009672\nEpsilon = 0.0989673299781971\nEpsilon = 0.09895743324519927\nEpsilon = 0.09894753750187475\nEpsilon = 0.09893764274812457\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.09892774898384976\nEpsilon = 0.09891785620895138\nEpsilon = 0.09890796442333048\nEpsilon = 0.09889807362688816\nEpsilon = 0.09888818381952547\nEpsilon = 0.09887829500114352\nEpsilon = 0.09886840717164341\nEpsilon = 0.09885852033092625\nEpsilon = 0.09884863447889315\nEpsilon = 0.09883874961544527\nEpsilon = 0.09882886574048372\nEpsilon = 0.09881898285390966\nEpsilon = 0.09880910095562427\nEpsilon = 0.09879922004552871\nEpsilon = 0.09878934012352417\nAgent: ddqn_agent . Episode 1244/2000. Number of steps to finish: 20. Loss: 15.809826850891113 Reward: -18.0\nEpsilon = 0.09877946118951182\nEpsilon = 0.09876958324339287\nEpsilon = 0.09875970628506853\nEpsilon = 0.09874983031444003\nEpsilon = 0.09873995533140859\nEpsilon = 0.09873008133587545\nEpsilon = 0.09872020832774187\nEpsilon = 0.09871033630690909\nEpsilon = 0.0987004652732784\nEpsilon = 0.09869059522675107\nEpsilon = 0.0986807261672284\nEpsilon = 0.09867085809461167\nEpsilon = 0.09866099100880221\nEpsilon = 0.09865112490970133\nEpsilon = 0.09864125979721036\nEpsilon = 0.09863139567123064\nEpsilon = 0.09862153253166352\nEpsilon = 0.09861167037841036\nEpsilon = 0.09860180921137252\nEpsilon = 0.09859194903045138\nAgent: ddqn_agent . Episode 1245/2000. Number of steps to finish: 20. Loss: 16.642126083374023 Reward: -10.0\nEpsilon = 0.09858208983554834\nEpsilon = 0.09857223162656478\nEpsilon = 0.09856237440340213\nEpsilon = 0.09855251816596179\nEpsilon = 0.0985426629141452\nEpsilon = 0.09853280864785378\nEpsilon = 0.098522955366989\nEpsilon = 0.09851310307145231\nEpsilon = 0.09850325176114516\nEpsilon = 0.09849340143596905\nEpsilon = 0.09848355209582545\nEpsilon = 0.09847370374061587\nEpsilon = 0.09846385637024181\nEpsilon = 0.0984540099846048\nEpsilon = 0.09844416458360633\nEpsilon = 0.09843432016714797\nEpsilon = 0.09842447673513126\nEpsilon = 0.09841463428745775\nEpsilon = 0.098404792824029\nEpsilon = 0.0983949523447466\nAgent: ddqn_agent . Episode 1246/2000. Number of steps to finish: 20. Loss: 16.295515060424805 Reward: -12.0\nEpsilon = 0.09838511284951212\nEpsilon = 0.09837527433822717\nEpsilon = 0.09836543681079335\nEpsilon = 0.09835560026711228\nEpsilon = 0.09834576470708557\nEpsilon = 0.09833593013061487\nEpsilon = 0.09832609653760181\nEpsilon = 0.09831626392794805\nEpsilon = 0.09830643230155525\nEpsilon = 0.0982966016583251\nEpsilon = 0.09828677199815927\nEpsilon = 0.09827694332095946\nEpsilon = 0.09826711562662736\nEpsilon = 0.0982572889150647\nEpsilon = 0.0982474631861732\nEpsilon = 0.09823763843985459\nEpsilon = 0.0982278146760106\nEpsilon = 0.098217991894543\nEpsilon = 0.09820817009535354\nEpsilon = 0.098198349278344\nAgent: ddqn_agent . Episode 1247/2000. Number of steps to finish: 20. Loss: 17.4826717376709 Reward: -10.0\nEpsilon = 0.09818852944341618\nEpsilon = 0.09817871059047184\nEpsilon = 0.0981688927194128\nEpsilon = 0.09815907583014086\nEpsilon = 0.09814925992255784\nEpsilon = 0.09813944499656559\nEpsilon = 0.09812963105206593\nEpsilon = 0.09811981808896073\nEpsilon = 0.09811000610715183\nEpsilon = 0.09810019510654111\nEpsilon = 0.09809038508703045\nEpsilon = 0.09808057604852176\nEpsilon = 0.09807076799091691\nEpsilon = 0.09806096091411781\nEpsilon = 0.0980511548180264\nEpsilon = 0.09804134970254459\nEpsilon = 0.09803154556757433\nEpsilon = 0.09802174241301757\nEpsilon = 0.09801194023877627\nEpsilon = 0.0980021390447524\nAgent: ddqn_agent . Episode 1248/2000. Number of steps to finish: 20. Loss: 17.173969268798828 Reward: -10.0\nEpsilon = 0.09799233883084793\nEpsilon = 0.09798253959696485\nEpsilon = 0.09797274134300515\nEpsilon = 0.09796294406887085\nEpsilon = 0.09795314777446397\nEpsilon = 0.09794335245968652\nEpsilon = 0.09793355812444056\nEpsilon = 0.09792376476862812\nEpsilon = 0.09791397239215126\nEpsilon = 0.09790418099491205\nEpsilon = 0.09789439057681255\nEpsilon = 0.09788460113775488\nEpsilon = 0.0978748126776411\nEpsilon = 0.09786502519637334\nEpsilon = 0.09785523869385371\nEpsilon = 0.09784545316998432\nEpsilon = 0.09783566862466732\nEpsilon = 0.09782588505780486\nEpsilon = 0.09781610246929907\nEpsilon = 0.09780632085905215\nAgent: ddqn_agent . Episode 1249/2000. Number of steps to finish: 20. Loss: 17.135942459106445 Reward: -18.0\nEpsilon = 0.09779654022696625\nEpsilon = 0.09778676057294355\nEpsilon = 0.09777698189688626\nEpsilon = 0.09776720419869657\nEpsilon = 0.0977574274782767\nEpsilon = 0.09774765173552888\nEpsilon = 0.09773787697035533\nEpsilon = 0.09772810318265829\nEpsilon = 0.09771833037234003\nEpsilon = 0.0977085585393028\nEpsilon = 0.09769878768344886\nEpsilon = 0.09768901780468052\nEpsilon = 0.09767924890290006\nEpsilon = 0.09766948097800977\nEpsilon = 0.09765971402991197\nEpsilon = 0.09764994805850898\nEpsilon = 0.09764018306370313\nEpsilon = 0.09763041904539677\nEpsilon = 0.09762065600349222\nEpsilon = 0.09761089393789187\nAgent: ddqn_agent . Episode 1250/2000. Number of steps to finish: 20. Loss: 16.473148345947266 Reward: -18.0\nEpsilon = 0.09760113284849807\nEpsilon = 0.09759137273521322\nEpsilon = 0.0975816135979397\nEpsilon = 0.0975718554365799\nEpsilon = 0.09756209825103623\nEpsilon = 0.09755234204121113\nEpsilon = 0.09754258680700702\nEpsilon = 0.09753283254832631\nEpsilon = 0.09752307926507148\nEpsilon = 0.09751332695714497\nEpsilon = 0.09750357562444927\nEpsilon = 0.09749382526688682\nEpsilon = 0.09748407588436013\nEpsilon = 0.09747432747677169\nEpsilon = 0.09746458004402402\nEpsilon = 0.09745483358601961\nEpsilon = 0.09744508810266102\nEpsilon = 0.09743534359385075\nEpsilon = 0.09742560005949137\nEpsilon = 0.09741585749948542\nAgent: ddqn_agent . Episode 1251/2000. Number of steps to finish: 20. Loss: 16.155839920043945 Reward: -14.0\nEpsilon = 0.09740611591373548\nEpsilon = 0.09739637530214411\nEpsilon = 0.09738663566461389\nEpsilon = 0.09737689700104743\nEpsilon = 0.09736715931134733\nEpsilon = 0.0973574225954162\nEpsilon = 0.09734768685315666\nEpsilon = 0.09733795208447134\nEpsilon = 0.09732821828926289\nEpsilon = 0.09731848546743396\nEpsilon = 0.09730875361888722\nEpsilon = 0.09729902274352532\nEpsilon = 0.09728929284125097\nEpsilon = 0.09727956391196685\nEpsilon = 0.09726983595557566\nEpsilon = 0.0972601089719801\nEpsilon = 0.0972503829610829\nEpsilon = 0.0972406579227868\nEpsilon = 0.09723093385699452\nEpsilon = 0.09722121076360882\nAgent: ddqn_agent . Episode 1252/2000. Number of steps to finish: 20. Loss: 16.953481674194336 Reward: -8.0\nEpsilon = 0.09721148864253246\nEpsilon = 0.0972017674936682\nEpsilon = 0.09719204731691883\nEpsilon = 0.09718232811218713\nEpsilon = 0.09717260987937591\nEpsilon = 0.09716289261838798\nEpsilon = 0.09715317632912614\nEpsilon = 0.09714346101149322\nEpsilon = 0.09713374666539208\nEpsilon = 0.09712403329072554\nEpsilon = 0.09711432088739647\nEpsilon = 0.09710460945530773\nEpsilon = 0.0970948989943622\nEpsilon = 0.09708518950446277\nEpsilon = 0.09707548098551232\nEpsilon = 0.09706577343741377\nEpsilon = 0.09705606686007003\nEpsilon = 0.09704636125338402\nEpsilon = 0.09703665661725869\nEpsilon = 0.09702695295159697\nAgent: ddqn_agent . Episode 1253/2000. Number of steps to finish: 20. Loss: 16.909122467041016 Reward: -14.0\nEpsilon = 0.09701725025630181\nEpsilon = 0.09700754853127619\nEpsilon = 0.09699784777642306\nEpsilon = 0.09698814799164542\nEpsilon = 0.09697844917684625\nEpsilon = 0.09696875133192857\nEpsilon = 0.09695905445679538\nEpsilon = 0.0969493585513497\nEpsilon = 0.09693966361549457\nEpsilon = 0.09692996964913302\nEpsilon = 0.0969202766521681\nEpsilon = 0.09691058462450289\nEpsilon = 0.09690089356604044\nEpsilon = 0.09689120347668384\nEpsilon = 0.09688151435633617\nEpsilon = 0.09687182620490054\nEpsilon = 0.09686213902228005\nEpsilon = 0.09685245280837783\nEpsilon = 0.096842767563097\nEpsilon = 0.09683308328634069\nAgent: ddqn_agent . Episode 1254/2000. Number of steps to finish: 20. Loss: 15.954906463623047 Reward: -16.0\nEpsilon = 0.09682339997801205\nEpsilon = 0.09681371763801425\nEpsilon = 0.09680403626625045\nEpsilon = 0.09679435586262382\nEpsilon = 0.09678467642703756\nEpsilon = 0.09677499795939486\nEpsilon = 0.09676532045959892\nEpsilon = 0.09675564392755295\nEpsilon = 0.0967459683631602\nEpsilon = 0.09673629376632388\nEpsilon = 0.09672662013694724\nEpsilon = 0.09671694747493355\nEpsilon = 0.09670727578018605\nEpsilon = 0.09669760505260803\nEpsilon = 0.09668793529210278\nEpsilon = 0.09667826649857357\nEpsilon = 0.09666859867192372\nEpsilon = 0.09665893181205652\nEpsilon = 0.09664926591887532\nEpsilon = 0.09663960099228343\nAgent: ddqn_agent . Episode 1255/2000. Number of steps to finish: 20. Loss: 17.31389045715332 Reward: -12.0\nEpsilon = 0.0966299370321842\nEpsilon = 0.09662027403848099\nEpsilon = 0.09661061201107714\nEpsilon = 0.09660095094987603\nEpsilon = 0.09659129085478105\nEpsilon = 0.09658163172569557\nEpsilon = 0.096571973562523\nEpsilon = 0.09656231636516675\nEpsilon = 0.09655266013353024\nEpsilon = 0.0965430048675169\nEpsilon = 0.09653335056703015\nEpsilon = 0.09652369723197345\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.09651404486225025\nEpsilon = 0.09650439345776403\nEpsilon = 0.09649474301841825\nEpsilon = 0.09648509354411641\nEpsilon = 0.096475445034762\nEpsilon = 0.09646579749025853\nEpsilon = 0.0964561509105095\nEpsilon = 0.09644650529541846\nAgent: ddqn_agent . Episode 1256/2000. Number of steps to finish: 20. Loss: 16.726978302001953 Reward: -14.0\nEpsilon = 0.09643686064488892\nEpsilon = 0.09642721695882443\nEpsilon = 0.09641757423712854\nEpsilon = 0.09640793247970483\nEpsilon = 0.09639829168645686\nEpsilon = 0.09638865185728822\nEpsilon = 0.0963790129921025\nEpsilon = 0.09636937509080329\nEpsilon = 0.09635973815329421\nEpsilon = 0.09635010217947888\nEpsilon = 0.09634046716926094\nEpsilon = 0.09633083312254401\nEpsilon = 0.09632120003923175\nEpsilon = 0.09631156791922783\nEpsilon = 0.09630193676243591\nEpsilon = 0.09629230656875967\nEpsilon = 0.09628267733810279\nEpsilon = 0.09627304907036899\nEpsilon = 0.09626342176546195\nEpsilon = 0.0962537954232854\nAgent: ddqn_agent . Episode 1257/2000. Number of steps to finish: 20. Loss: 16.875370025634766 Reward: -12.0\nEpsilon = 0.09624417004374307\nEpsilon = 0.0962345456267387\nEpsilon = 0.09622492217217601\nEpsilon = 0.0962152996799588\nEpsilon = 0.0962056781499908\nEpsilon = 0.09619605758217581\nEpsilon = 0.0961864379764176\nEpsilon = 0.09617681933261996\nEpsilon = 0.09616720165068669\nEpsilon = 0.09615758493052162\nEpsilon = 0.09614796917202857\nEpsilon = 0.09613835437511137\nEpsilon = 0.09612874053967385\nEpsilon = 0.09611912766561989\nEpsilon = 0.09610951575285333\nEpsilon = 0.09609990480127804\nEpsilon = 0.09609029481079792\nEpsilon = 0.09608068578131684\nEpsilon = 0.0960710777127387\nEpsilon = 0.09606147060496743\nAgent: ddqn_agent . Episode 1258/2000. Number of steps to finish: 20. Loss: 17.28069305419922 Reward: -14.0\nEpsilon = 0.09605186445790694\nEpsilon = 0.09604225927146115\nEpsilon = 0.096032655045534\nEpsilon = 0.09602305178002946\nEpsilon = 0.09601344947485145\nEpsilon = 0.09600384812990397\nEpsilon = 0.09599424774509098\nEpsilon = 0.09598464832031647\nEpsilon = 0.09597504985548444\nEpsilon = 0.09596545235049889\nEpsilon = 0.09595585580526383\nEpsilon = 0.0959462602196833\nEpsilon = 0.09593666559366133\nEpsilon = 0.09592707192710197\nEpsilon = 0.09591747921990926\nEpsilon = 0.09590788747198727\nEpsilon = 0.09589829668324007\nEpsilon = 0.09588870685357175\nEpsilon = 0.09587911798288638\nEpsilon = 0.0958695300710881\nAgent: ddqn_agent . Episode 1259/2000. Number of steps to finish: 20. Loss: 17.324390411376953 Reward: -14.0\nEpsilon = 0.09585994311808099\nEpsilon = 0.09585035712376919\nEpsilon = 0.09584077208805682\nEpsilon = 0.09583118801084801\nEpsilon = 0.09582160489204693\nEpsilon = 0.09581202273155773\nEpsilon = 0.09580244152928458\nEpsilon = 0.09579286128513165\nEpsilon = 0.09578328199900314\nEpsilon = 0.09577370367080325\nEpsilon = 0.09576412630043617\nEpsilon = 0.09575454988780613\nEpsilon = 0.09574497443281735\nEpsilon = 0.09573539993537407\nEpsilon = 0.09572582639538053\nEpsilon = 0.095716253812741\nEpsilon = 0.09570668218735973\nEpsilon = 0.095697111519141\nEpsilon = 0.09568754180798908\nEpsilon = 0.09567797305380828\nAgent: ddqn_agent . Episode 1260/2000. Number of steps to finish: 20. Loss: 16.589540481567383 Reward: -14.0\nEpsilon = 0.0956684052565029\nEpsilon = 0.09565883841597725\nEpsilon = 0.09564927253213565\nEpsilon = 0.09563970760488244\nEpsilon = 0.09563014363412195\nEpsilon = 0.09562058061975853\nEpsilon = 0.09561101856169656\nEpsilon = 0.09560145745984039\nEpsilon = 0.09559189731409441\nEpsilon = 0.09558233812436301\nEpsilon = 0.09557277989055057\nEpsilon = 0.09556322261256152\nEpsilon = 0.09555366629030027\nEpsilon = 0.09554411092367124\nEpsilon = 0.09553455651257887\nEpsilon = 0.09552500305692761\nEpsilon = 0.09551545055662192\nEpsilon = 0.09550589901156625\nEpsilon = 0.0954963484216651\nEpsilon = 0.09548679878682294\nAgent: ddqn_agent . Episode 1261/2000. Number of steps to finish: 20. Loss: 18.123476028442383 Reward: -20.0\nEpsilon = 0.09547725010694426\nEpsilon = 0.09546770238193357\nEpsilon = 0.09545815561169538\nEpsilon = 0.0954486097961342\nEpsilon = 0.0954390649351546\nEpsilon = 0.09542952102866108\nEpsilon = 0.0954199780765582\nEpsilon = 0.09541043607875055\nEpsilon = 0.09540089503514268\nEpsilon = 0.09539135494563916\nEpsilon = 0.0953818158101446\nEpsilon = 0.09537227762856358\nEpsilon = 0.09536274040080073\nEpsilon = 0.09535320412676065\nEpsilon = 0.09534366880634798\nEpsilon = 0.09533413443946735\nEpsilon = 0.0953246010260234\nEpsilon = 0.0953150685659208\nEpsilon = 0.09530553705906421\nEpsilon = 0.0952960065053583\nAgent: ddqn_agent . Episode 1262/2000. Number of steps to finish: 20. Loss: 17.690410614013672 Reward: -16.0\nEpsilon = 0.09528647690470776\nEpsilon = 0.0952769482570173\nEpsilon = 0.09526742056219159\nEpsilon = 0.09525789382013537\nEpsilon = 0.09524836803075336\nEpsilon = 0.09523884319395029\nEpsilon = 0.0952293193096309\nEpsilon = 0.09521979637769994\nEpsilon = 0.09521027439806216\nEpsilon = 0.09520075337062237\nEpsilon = 0.09519123329528531\nEpsilon = 0.09518171417195578\nEpsilon = 0.09517219600053858\nEpsilon = 0.09516267878093854\nEpsilon = 0.09515316251306044\nEpsilon = 0.09514364719680914\nEpsilon = 0.09513413283208946\nEpsilon = 0.09512461941880626\nEpsilon = 0.09511510695686438\nEpsilon = 0.0951055954461687\nAgent: ddqn_agent . Episode 1263/2000. Number of steps to finish: 20. Loss: 16.362491607666016 Reward: -14.0\nEpsilon = 0.09509608488662409\nEpsilon = 0.09508657527813542\nEpsilon = 0.09507706662060761\nEpsilon = 0.09506755891394555\nEpsilon = 0.09505805215805416\nEpsilon = 0.09504854635283835\nEpsilon = 0.09503904149820307\nEpsilon = 0.09502953759405325\nEpsilon = 0.09502003464029385\nEpsilon = 0.09501053263682982\nEpsilon = 0.09500103158356614\nEpsilon = 0.09499153148040779\nEpsilon = 0.09498203232725974\nEpsilon = 0.09497253412402702\nEpsilon = 0.09496303687061462\nEpsilon = 0.09495354056692756\nEpsilon = 0.09494404521287088\nEpsilon = 0.09493455080834959\nEpsilon = 0.09492505735326875\nEpsilon = 0.09491556484753343\nAgent: ddqn_agent . Episode 1264/2000. Number of steps to finish: 20. Loss: 17.835315704345703 Reward: -20.0\nEpsilon = 0.09490607329104868\nEpsilon = 0.09489658268371957\nEpsilon = 0.0948870930254512\nEpsilon = 0.09487760431614865\nEpsilon = 0.09486811655571704\nEpsilon = 0.09485862974406148\nEpsilon = 0.09484914388108708\nEpsilon = 0.09483965896669896\nEpsilon = 0.09483017500080229\nEpsilon = 0.09482069198330222\nEpsilon = 0.09481120991410388\nEpsilon = 0.09480172879311248\nEpsilon = 0.09479224862023317\nEpsilon = 0.09478276939537114\nEpsilon = 0.09477329111843161\nEpsilon = 0.09476381378931976\nEpsilon = 0.09475433740794083\nEpsilon = 0.09474486197420004\nEpsilon = 0.09473538748800261\nEpsilon = 0.09472591394925381\nAgent: ddqn_agent . Episode 1265/2000. Number of steps to finish: 20. Loss: 17.674644470214844 Reward: -14.0\nEpsilon = 0.09471644135785888\nEpsilon = 0.0947069697137231\nEpsilon = 0.09469749901675173\nEpsilon = 0.09468802926685005\nEpsilon = 0.09467856046392337\nEpsilon = 0.09466909260787698\nEpsilon = 0.09465962569861619\nEpsilon = 0.09465015973604633\nEpsilon = 0.09464069472007273\nEpsilon = 0.09463123065060072\nEpsilon = 0.09462176752753566\nEpsilon = 0.09461230535078291\nEpsilon = 0.09460284412024783\nEpsilon = 0.0945933838358358\nEpsilon = 0.09458392449745222\nEpsilon = 0.09457446610500247\nEpsilon = 0.09456500865839197\nEpsilon = 0.09455555215752613\nEpsilon = 0.09454609660231038\nEpsilon = 0.09453664199265015\nAgent: ddqn_agent . Episode 1266/2000. Number of steps to finish: 20. Loss: 17.031688690185547 Reward: -16.0\nEpsilon = 0.09452718832845089\nEpsilon = 0.09451773560961804\nEpsilon = 0.09450828383605708\nEpsilon = 0.09449883300767348\nEpsilon = 0.09448938312437272\nEpsilon = 0.09447993418606028\nEpsilon = 0.09447048619264167\nEpsilon = 0.09446103914402242\nEpsilon = 0.09445159304010801\nEpsilon = 0.094442147880804\nEpsilon = 0.09443270366601593\nEpsilon = 0.09442326039564933\nEpsilon = 0.09441381806960976\nEpsilon = 0.0944043766878028\nEpsilon = 0.09439493625013402\nEpsilon = 0.09438549675650901\nEpsilon = 0.09437605820683335\nEpsilon = 0.09436662060101267\nEpsilon = 0.09435718393895257\nEpsilon = 0.09434774822055868\nAgent: ddqn_agent . Episode 1267/2000. Number of steps to finish: 20. Loss: 16.537818908691406 Reward: -18.0\nEpsilon = 0.09433831344573662\nEpsilon = 0.09432887961439206\nEpsilon = 0.09431944672643061\nEpsilon = 0.09431001478175798\nEpsilon = 0.0943005837802798\nEpsilon = 0.09429115372190178\nEpsilon = 0.09428172460652959\nEpsilon = 0.09427229643406894\nEpsilon = 0.09426286920442553\nEpsilon = 0.09425344291750509\nEpsilon = 0.09424401757321334\nEpsilon = 0.09423459317145602\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.09422516971213887\nEpsilon = 0.09421574719516766\nEpsilon = 0.09420632562044814\nEpsilon = 0.0941969049878861\nEpsilon = 0.09418748529738731\nEpsilon = 0.09417806654885758\nEpsilon = 0.0941686487422027\nEpsilon = 0.09415923187732847\nAgent: ddqn_agent . Episode 1268/2000. Number of steps to finish: 20. Loss: 17.507566452026367 Reward: -10.0\nEpsilon = 0.09414981595414074\nEpsilon = 0.09414040097254532\nEpsilon = 0.09413098693244806\nEpsilon = 0.09412157383375482\nEpsilon = 0.09411216167637144\nEpsilon = 0.0941027504602038\nEpsilon = 0.09409334018515779\nEpsilon = 0.09408393085113928\nEpsilon = 0.09407452245805417\nEpsilon = 0.09406511500580837\nEpsilon = 0.09405570849430779\nEpsilon = 0.09404630292345836\nEpsilon = 0.09403689829316601\nEpsilon = 0.0940274946033367\nEpsilon = 0.09401809185387637\nEpsilon = 0.09400869004469098\nEpsilon = 0.0939992891756865\nEpsilon = 0.09398988924676895\nEpsilon = 0.09398049025784427\nEpsilon = 0.0939710922088185\nAgent: ddqn_agent . Episode 1269/2000. Number of steps to finish: 20. Loss: 17.19470977783203 Reward: -12.0\nEpsilon = 0.09396169509959762\nEpsilon = 0.09395229893008766\nEpsilon = 0.09394290370019466\nEpsilon = 0.09393350940982464\nEpsilon = 0.09392411605888366\nEpsilon = 0.09391472364727778\nEpsilon = 0.09390533217491305\nEpsilon = 0.09389594164169555\nEpsilon = 0.09388655204753138\nEpsilon = 0.09387716339232663\nEpsilon = 0.09386777567598739\nEpsilon = 0.0938583888984198\nEpsilon = 0.09384900305952995\nEpsilon = 0.093839618159224\nEpsilon = 0.09383023419740807\nEpsilon = 0.09382085117398833\nEpsilon = 0.09381146908887093\nEpsilon = 0.09380208794196204\nEpsilon = 0.09379270773316785\nEpsilon = 0.09378332846239454\nAgent: ddqn_agent . Episode 1270/2000. Number of steps to finish: 20. Loss: 17.963939666748047 Reward: -10.0\nEpsilon = 0.0937739501295483\nEpsilon = 0.09376457273453534\nEpsilon = 0.09375519627726189\nEpsilon = 0.09374582075763417\nEpsilon = 0.0937364461755584\nEpsilon = 0.09372707253094084\nEpsilon = 0.09371769982368774\nEpsilon = 0.09370832805370538\nEpsilon = 0.0936989572209\nEpsilon = 0.09368958732517792\nEpsilon = 0.0936802183664454\nEpsilon = 0.09367085034460876\nEpsilon = 0.0936614832595743\nEpsilon = 0.09365211711124834\nEpsilon = 0.09364275189953722\nEpsilon = 0.09363338762434727\nEpsilon = 0.09362402428558483\nEpsilon = 0.09361466188315627\nEpsilon = 0.09360530041696796\nEpsilon = 0.09359593988692627\nAgent: ddqn_agent . Episode 1271/2000. Number of steps to finish: 20. Loss: 17.134756088256836 Reward: -12.0\nEpsilon = 0.09358658029293758\nEpsilon = 0.09357722163490828\nEpsilon = 0.0935678639127448\nEpsilon = 0.09355850712635352\nEpsilon = 0.0935491512756409\nEpsilon = 0.09353979636051334\nEpsilon = 0.09353044238087728\nEpsilon = 0.09352108933663919\nEpsilon = 0.09351173722770553\nEpsilon = 0.09350238605398277\nEpsilon = 0.09349303581537738\nEpsilon = 0.09348368651179584\nEpsilon = 0.09347433814314465\nEpsilon = 0.09346499070933034\nEpsilon = 0.09345564421025941\nEpsilon = 0.09344629864583838\nEpsilon = 0.0934369540159738\nEpsilon = 0.0934276103205722\nEpsilon = 0.09341826755954015\nEpsilon = 0.0934089257327842\nAgent: ddqn_agent . Episode 1272/2000. Number of steps to finish: 20. Loss: 17.755325317382812 Reward: -14.0\nEpsilon = 0.09339958484021092\nEpsilon = 0.0933902448817269\nEpsilon = 0.09338090585723872\nEpsilon = 0.09337156776665301\nEpsilon = 0.09336223060987635\nEpsilon = 0.09335289438681536\nEpsilon = 0.09334355909737668\nEpsilon = 0.09333422474146695\nEpsilon = 0.09332489131899281\nEpsilon = 0.09331555882986091\nEpsilon = 0.09330622727397793\nEpsilon = 0.09329689665125053\nEpsilon = 0.09328756696158541\nEpsilon = 0.09327823820488926\nEpsilon = 0.09326891038106877\nEpsilon = 0.09325958349003066\nEpsilon = 0.09325025753168166\nEpsilon = 0.09324093250592849\nEpsilon = 0.0932316084126779\nEpsilon = 0.09322228525183664\nAgent: ddqn_agent . Episode 1273/2000. Number of steps to finish: 20. Loss: 15.766464233398438 Reward: -14.0\nEpsilon = 0.09321296302331146\nEpsilon = 0.09320364172700912\nEpsilon = 0.09319432136283642\nEpsilon = 0.09318500193070015\nEpsilon = 0.09317568343050707\nEpsilon = 0.09316636586216402\nEpsilon = 0.09315704922557781\nEpsilon = 0.09314773352065525\nEpsilon = 0.09313841874730319\nEpsilon = 0.09312910490542846\nEpsilon = 0.09311979199493792\nEpsilon = 0.09311048001573842\nEpsilon = 0.09310116896773685\nEpsilon = 0.09309185885084008\nEpsilon = 0.093082549664955\nEpsilon = 0.0930732414099885\nEpsilon = 0.09306393408584751\nEpsilon = 0.09305462769243893\nEpsilon = 0.09304532222966969\nEpsilon = 0.09303601769744672\nAgent: ddqn_agent . Episode 1274/2000. Number of steps to finish: 20. Loss: 16.439655303955078 Reward: -12.0\nEpsilon = 0.09302671409567698\nEpsilon = 0.09301741142426741\nEpsilon = 0.09300810968312498\nEpsilon = 0.09299880887215667\nEpsilon = 0.09298950899126945\nEpsilon = 0.09298021004037033\nEpsilon = 0.09297091201936629\nEpsilon = 0.09296161492816435\nEpsilon = 0.09295231876667154\nEpsilon = 0.09294302353479487\nEpsilon = 0.09293372923244139\nEpsilon = 0.09292443585951815\nEpsilon = 0.0929151434159322\nEpsilon = 0.0929058519015906\nEpsilon = 0.09289656131640045\nEpsilon = 0.0928872716602688\nEpsilon = 0.09287798293310277\nEpsilon = 0.09286869513480947\nEpsilon = 0.09285940826529598\nEpsilon = 0.09285012232446946\nAgent: ddqn_agent . Episode 1275/2000. Number of steps to finish: 20. Loss: 17.675647735595703 Reward: -12.0\nEpsilon = 0.09284083731223701\nEpsilon = 0.09283155322850578\nEpsilon = 0.09282227007318293\nEpsilon = 0.09281298784617562\nEpsilon = 0.092803706547391\nEpsilon = 0.09279442617673625\nEpsilon = 0.09278514673411858\nEpsilon = 0.09277586821944517\nEpsilon = 0.09276659063262323\nEpsilon = 0.09275731397355996\nEpsilon = 0.09274803824216261\nEpsilon = 0.0927387634383384\nEpsilon = 0.09272948956199456\nEpsilon = 0.09272021661303836\nEpsilon = 0.09271094459137706\nEpsilon = 0.09270167349691792\nEpsilon = 0.09269240332956823\nEpsilon = 0.09268313408923527\nEpsilon = 0.09267386577582636\nEpsilon = 0.09266459838924877\nAgent: ddqn_agent . Episode 1276/2000. Number of steps to finish: 20. Loss: 17.18862533569336 Reward: -20.0\nEpsilon = 0.09265533192940985\nEpsilon = 0.0926460663962169\nEpsilon = 0.09263680178957728\nEpsilon = 0.09262753810939833\nEpsilon = 0.09261827535558739\nEpsilon = 0.09260901352805183\nEpsilon = 0.09259975262669902\nEpsilon = 0.09259049265143635\nEpsilon = 0.0925812336021712\nEpsilon = 0.09257197547881098\nEpsilon = 0.0925627182812631\nEpsilon = 0.09255346200943497\nEpsilon = 0.09254420666323403\nEpsilon = 0.0925349522425677\nEpsilon = 0.09252569874734345\nEpsilon = 0.09251644617746872\nEpsilon = 0.09250719453285097\nEpsilon = 0.09249794381339768\nEpsilon = 0.09248869401901634\nEpsilon = 0.09247944514961444\nAgent: ddqn_agent . Episode 1277/2000. Number of steps to finish: 20. Loss: 16.51629066467285 Reward: -16.0\nEpsilon = 0.09247019720509948\nEpsilon = 0.09246095018537898\nEpsilon = 0.09245170409036044\nEpsilon = 0.0924424589199514\nEpsilon = 0.0924332146740594\nEpsilon = 0.092423971352592\nEpsilon = 0.09241472895545674\nEpsilon = 0.0924054874825612\nEpsilon = 0.09239624693381294\nEpsilon = 0.09238700730911957\nEpsilon = 0.09237776860838866\nEpsilon = 0.09236853083152782\nEpsilon = 0.09235929397844467\nEpsilon = 0.09235005804904682\nEpsilon = 0.09234082304324191\nEpsilon = 0.09233158896093759\nEpsilon = 0.0923223558020415\nEpsilon = 0.09231312356646129\nEpsilon = 0.09230389225410465\nEpsilon = 0.09229466186487924\nAgent: ddqn_agent . Episode 1278/2000. Number of steps to finish: 20. Loss: 16.836563110351562 Reward: -18.0\nEpsilon = 0.09228543239869275\nEpsilon = 0.09227620385545288\nEpsilon = 0.09226697623506733\nEpsilon = 0.09225774953744383\nEpsilon = 0.09224852376249008\nEpsilon = 0.09223929891011384\nEpsilon = 0.09223007498022283\nEpsilon = 0.09222085197272481\nEpsilon = 0.09221162988752754\nEpsilon = 0.09220240872453879\nEpsilon = 0.09219318848366634\nEpsilon = 0.09218396916481797\nEpsilon = 0.09217475076790149\nEpsilon = 0.0921655332928247\nEpsilon = 0.09215631673949543\nEpsilon = 0.09214710110782148\nEpsilon = 0.0921378863977107\nEpsilon = 0.09212867260907093\nEpsilon = 0.09211945974181002\nEpsilon = 0.09211024779583583\nAgent: ddqn_agent . Episode 1279/2000. Number of steps to finish: 20. Loss: 17.022232055664062 Reward: -14.0\nEpsilon = 0.09210103677105626\nEpsilon = 0.09209182666737915\nEpsilon = 0.09208261748471241\nEpsilon = 0.09207340922296395\nEpsilon = 0.09206420188204165\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.09205499546185344\nEpsilon = 0.09204578996230726\nEpsilon = 0.09203658538331104\nEpsilon = 0.0920273817247727\nEpsilon = 0.09201817898660022\nEpsilon = 0.09200897716870156\nEpsilon = 0.09199977627098468\nEpsilon = 0.09199057629335759\nEpsilon = 0.09198137723572826\nEpsilon = 0.09197217909800469\nEpsilon = 0.09196298188009489\nEpsilon = 0.09195378558190688\nEpsilon = 0.09194459020334869\nEpsilon = 0.09193539574432835\nEpsilon = 0.09192620220475392\nAgent: ddqn_agent . Episode 1280/2000. Number of steps to finish: 20. Loss: 16.864055633544922 Reward: -16.0\nEpsilon = 0.09191700958453344\nEpsilon = 0.091907817883575\nEpsilon = 0.09189862710178663\nEpsilon = 0.09188943723907646\nEpsilon = 0.09188024829535255\nEpsilon = 0.09187106027052301\nEpsilon = 0.09186187316449596\nEpsilon = 0.0918526869771795\nEpsilon = 0.0918435017084818\nEpsilon = 0.09183431735831095\nEpsilon = 0.09182513392657513\nEpsilon = 0.09181595141318247\nEpsilon = 0.09180676981804115\nEpsilon = 0.09179758914105934\nEpsilon = 0.09178840938214523\nEpsilon = 0.09177923054120701\nEpsilon = 0.0917700526181529\nEpsilon = 0.09176087561289108\nEpsilon = 0.09175169952532979\nEpsilon = 0.09174252435537725\nAgent: ddqn_agent . Episode 1281/2000. Number of steps to finish: 20. Loss: 15.676337242126465 Reward: -12.0\nEpsilon = 0.09173335010294172\nEpsilon = 0.09172417676793142\nEpsilon = 0.09171500435025462\nEpsilon = 0.09170583284981959\nEpsilon = 0.09169666226653461\nEpsilon = 0.09168749260030797\nEpsilon = 0.09167832385104793\nEpsilon = 0.09166915601866282\nEpsilon = 0.09165998910306096\nEpsilon = 0.09165082310415065\nEpsilon = 0.09164165802184024\nEpsilon = 0.09163249385603806\nEpsilon = 0.09162333060665245\nEpsilon = 0.09161416827359178\nEpsilon = 0.09160500685676443\nEpsilon = 0.09159584635607874\nEpsilon = 0.09158668677144313\nEpsilon = 0.091577528102766\nEpsilon = 0.09156837034995571\nEpsilon = 0.09155921351292072\nAgent: ddqn_agent . Episode 1282/2000. Number of steps to finish: 20. Loss: 15.868233680725098 Reward: -14.0\nEpsilon = 0.09155005759156942\nEpsilon = 0.09154090258581027\nEpsilon = 0.09153174849555169\nEpsilon = 0.09152259532070214\nEpsilon = 0.09151344306117007\nEpsilon = 0.09150429171686396\nEpsilon = 0.09149514128769227\nEpsilon = 0.09148599177356351\nEpsilon = 0.09147684317438615\nEpsilon = 0.09146769549006871\nEpsilon = 0.0914585487205197\nEpsilon = 0.09144940286564765\nEpsilon = 0.09144025792536109\nEpsilon = 0.09143111389956855\nEpsilon = 0.0914219707881786\nEpsilon = 0.09141282859109978\nEpsilon = 0.09140368730824067\nEpsilon = 0.09139454693950984\nEpsilon = 0.09138540748481588\nEpsilon = 0.0913762689440674\nAgent: ddqn_agent . Episode 1283/2000. Number of steps to finish: 20. Loss: 17.962848663330078 Reward: -14.0\nEpsilon = 0.091367131317173\nEpsilon = 0.09135799460404129\nEpsilon = 0.09134885880458088\nEpsilon = 0.09133972391870042\nEpsilon = 0.09133058994630855\nEpsilon = 0.09132145688731393\nEpsilon = 0.09131232474162519\nEpsilon = 0.09130319350915103\nEpsilon = 0.09129406318980011\nEpsilon = 0.09128493378348114\nEpsilon = 0.09127580529010279\nEpsilon = 0.09126667770957378\nEpsilon = 0.09125755104180282\nEpsilon = 0.09124842528669865\nEpsilon = 0.09123930044416997\nEpsilon = 0.09123017651412556\nEpsilon = 0.09122105349647415\nEpsilon = 0.0912119313911245\nEpsilon = 0.09120281019798539\nEpsilon = 0.0911936899169656\nAgent: ddqn_agent . Episode 1284/2000. Number of steps to finish: 20. Loss: 18.091360092163086 Reward: -14.0\nEpsilon = 0.0911845705479739\nEpsilon = 0.0911754520909191\nEpsilon = 0.09116633454571\nEpsilon = 0.09115721791225544\nEpsilon = 0.09114810219046421\nEpsilon = 0.09113898738024516\nEpsilon = 0.09112987348150714\nEpsilon = 0.09112076049415899\nEpsilon = 0.09111164841810958\nEpsilon = 0.09110253725326777\nEpsilon = 0.09109342699954244\nEpsilon = 0.09108431765684248\nEpsilon = 0.0910752092250768\nEpsilon = 0.09106610170415429\nEpsilon = 0.09105699509398388\nEpsilon = 0.09104788939447447\nEpsilon = 0.09103878460553502\nEpsilon = 0.09102968072707447\nEpsilon = 0.09102057775900177\nEpsilon = 0.09101147570122586\nAgent: ddqn_agent . Episode 1285/2000. Number of steps to finish: 20. Loss: 17.326208114624023 Reward: -12.0\nEpsilon = 0.09100237455365574\nEpsilon = 0.09099327431620038\nEpsilon = 0.09098417498876876\nEpsilon = 0.09097507657126988\nEpsilon = 0.09096597906361276\nEpsilon = 0.09095688246570639\nEpsilon = 0.09094778677745982\nEpsilon = 0.09093869199878207\nEpsilon = 0.09092959812958219\nEpsilon = 0.09092050516976924\nEpsilon = 0.09091141311925226\nEpsilon = 0.09090232197794033\nEpsilon = 0.09089323174574254\nEpsilon = 0.09088414242256797\nEpsilon = 0.09087505400832571\nEpsilon = 0.09086596650292487\nEpsilon = 0.09085687990627458\nEpsilon = 0.09084779421828396\nEpsilon = 0.09083870943886213\nEpsilon = 0.09082962556791825\nAgent: ddqn_agent . Episode 1286/2000. Number of steps to finish: 20. Loss: 17.05335807800293 Reward: -16.0\nEpsilon = 0.09082054260536146\nEpsilon = 0.09081146055110093\nEpsilon = 0.09080237940504582\nEpsilon = 0.09079329916710531\nEpsilon = 0.0907842198371886\nEpsilon = 0.09077514141520487\nEpsilon = 0.09076606390106336\nEpsilon = 0.09075698729467326\nEpsilon = 0.0907479115959438\nEpsilon = 0.0907388368047842\nEpsilon = 0.09072976292110373\nEpsilon = 0.09072068994481161\nEpsilon = 0.09071161787581714\nEpsilon = 0.09070254671402955\nEpsilon = 0.09069347645935814\nEpsilon = 0.0906844071117122\nEpsilon = 0.09067533867100103\nEpsilon = 0.09066627113713392\nEpsilon = 0.09065720451002021\nEpsilon = 0.0906481387895692\nAgent: ddqn_agent . Episode 1287/2000. Number of steps to finish: 20. Loss: 16.752464294433594 Reward: -20.0\nEpsilon = 0.09063907397569025\nEpsilon = 0.09063001006829269\nEpsilon = 0.09062094706728586\nEpsilon = 0.09061188497257913\nEpsilon = 0.09060282378408188\nEpsilon = 0.09059376350170348\nEpsilon = 0.09058470412535331\nEpsilon = 0.09057564565494078\nEpsilon = 0.09056658809037528\nEpsilon = 0.09055753143156624\nEpsilon = 0.09054847567842308\nEpsilon = 0.09053942083085524\nEpsilon = 0.09053036688877215\nEpsilon = 0.09052131385208327\nEpsilon = 0.09051226172069807\nEpsilon = 0.090503210494526\nEpsilon = 0.09049416017347656\nEpsilon = 0.09048511075745921\nEpsilon = 0.09047606224638347\nEpsilon = 0.09046701464015883\nAgent: ddqn_agent . Episode 1288/2000. Number of steps to finish: 20. Loss: 17.1527156829834 Reward: -12.0\nEpsilon = 0.09045796793869482\nEpsilon = 0.09044892214190095\nEpsilon = 0.09043987724968676\nEpsilon = 0.09043083326196179\nEpsilon = 0.0904217901786356\nEpsilon = 0.09041274799961774\nEpsilon = 0.09040370672481778\nEpsilon = 0.0903946663541453\nEpsilon = 0.09038562688750988\nEpsilon = 0.09037658832482114\nEpsilon = 0.09036755066598866\nEpsilon = 0.09035851391092206\nEpsilon = 0.09034947805953097\nEpsilon = 0.09034044311172502\nEpsilon = 0.09033140906741385\nEpsilon = 0.09032237592650712\nEpsilon = 0.09031334368891447\nEpsilon = 0.09030431235454559\nEpsilon = 0.09029528192331013\nEpsilon = 0.0902862523951178\nAgent: ddqn_agent . Episode 1289/2000. Number of steps to finish: 20. Loss: 16.22584342956543 Reward: -20.0\nEpsilon = 0.09027722376987829\nEpsilon = 0.0902681960475013\nEpsilon = 0.09025916922789655\nEpsilon = 0.09025014331097377\nEpsilon = 0.09024111829664266\nEpsilon = 0.090232094184813\nEpsilon = 0.09022307097539452\nEpsilon = 0.09021404866829698\nEpsilon = 0.09020502726343015\nEpsilon = 0.0901960067607038\nEpsilon = 0.09018698716002774\nEpsilon = 0.09017796846131174\nEpsilon = 0.0901689506644656\nEpsilon = 0.09015993376939915\nEpsilon = 0.09015091777602222\nEpsilon = 0.09014190268424462\nEpsilon = 0.09013288849397619\nEpsilon = 0.0901238752051268\nEpsilon = 0.09011486281760628\nEpsilon = 0.09010585133132452\nAgent: ddqn_agent . Episode 1290/2000. Number of steps to finish: 20. Loss: 18.16492462158203 Reward: -18.0\nEpsilon = 0.0900968407461914\nEpsilon = 0.09008783106211678\nEpsilon = 0.09007882227901057\nEpsilon = 0.09006981439678267\nEpsilon = 0.090060807415343\nEpsilon = 0.09005180133460147\nEpsilon = 0.09004279615446802\nEpsilon = 0.09003379187485257\nEpsilon = 0.09002478849566509\nEpsilon = 0.09001578601681552\nEpsilon = 0.09000678443821385\nEpsilon = 0.08999778375977002\nEpsilon = 0.08998878398139405\nEpsilon = 0.08997978510299591\nEpsilon = 0.08997078712448561\nEpsilon = 0.08996179004577316\nEpsilon = 0.08995279386676859\nEpsilon = 0.08994379858738191\nEpsilon = 0.08993480420752317\nEpsilon = 0.08992581072710241\nAgent: ddqn_agent . Episode 1291/2000. Number of steps to finish: 20. Loss: 16.504613876342773 Reward: -14.0\nEpsilon = 0.08991681814602971\nEpsilon = 0.08990782646421511\nEpsilon = 0.08989883568156869\nEpsilon = 0.08988984579800054\nEpsilon = 0.08988085681342074\nEpsilon = 0.0898718687277394\nAgent: ddqn_agent . Episode 1292/2000. Number of steps to finish: 6. Loss: 5.578426361083984 Reward: 6.0\nEpsilon = 0.08986288154086663\nEpsilon = 0.08985389525271255\nEpsilon = 0.08984490986318727\nEpsilon = 0.08983592537220095\nEpsilon = 0.08982694177966373\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.08981795908548576\nEpsilon = 0.08980897728957721\nEpsilon = 0.08979999639184826\nEpsilon = 0.08979101639220907\nEpsilon = 0.08978203729056985\nEpsilon = 0.0897730590868408\nEpsilon = 0.08976408178093212\nEpsilon = 0.08975510537275402\nEpsilon = 0.08974612986221675\nEpsilon = 0.08973715524923052\nEpsilon = 0.0897281815337056\nEpsilon = 0.08971920871555222\nEpsilon = 0.08971023679468067\nEpsilon = 0.0897012657710012\nEpsilon = 0.0896922956444241\nAgent: ddqn_agent . Episode 1293/2000. Number of steps to finish: 20. Loss: 19.389856338500977 Reward: -12.0\nEpsilon = 0.08968332641485965\nEpsilon = 0.08967435808221817\nEpsilon = 0.08966539064640995\nEpsilon = 0.08965642410734531\nEpsilon = 0.08964745846493458\nEpsilon = 0.08963849371908808\nEpsilon = 0.08962952986971617\nEpsilon = 0.0896205669167292\nEpsilon = 0.08961160486003754\nEpsilon = 0.08960264369955154\nEpsilon = 0.08959368343518158\nEpsilon = 0.08958472406683807\nEpsilon = 0.08957576559443138\nEpsilon = 0.08956680801787194\nEpsilon = 0.08955785133707016\nEpsilon = 0.08954889555193646\nEpsilon = 0.08953994066238126\nEpsilon = 0.08953098666831502\nEpsilon = 0.08952203356964819\nEpsilon = 0.08951308136629123\nAgent: ddqn_agent . Episode 1294/2000. Number of steps to finish: 20. Loss: 17.226512908935547 Reward: -12.0\nEpsilon = 0.0895041300581546\nEpsilon = 0.08949517964514878\nEpsilon = 0.08948623012718426\nEpsilon = 0.08947728150417154\nEpsilon = 0.08946833377602113\nEpsilon = 0.08945938694264353\nEpsilon = 0.08945044100394928\nEpsilon = 0.08944149595984888\nEpsilon = 0.08943255181025289\nEpsilon = 0.08942360855507187\nEpsilon = 0.08941466619421636\nEpsilon = 0.08940572472759693\nEpsilon = 0.08939678415512417\nEpsilon = 0.08938784447670865\nAgent: ddqn_agent . Episode 1295/2000. Number of steps to finish: 14. Loss: 11.842750549316406 Reward: -2.0\nEpsilon = 0.08937890569226098\nEpsilon = 0.08936996780169175\nEpsilon = 0.08936103080491158\nEpsilon = 0.08935209470183109\nEpsilon = 0.08934315949236091\nEpsilon = 0.08933422517641167\nEpsilon = 0.08932529175389403\nEpsilon = 0.08931635922471864\nEpsilon = 0.08930742758879617\nEpsilon = 0.0892984968460373\nEpsilon = 0.0892895669963527\nEpsilon = 0.08928063803965307\nEpsilon = 0.0892717099758491\nEpsilon = 0.08926278280485152\nEpsilon = 0.08925385652657103\nEpsilon = 0.08924493114091837\nEpsilon = 0.08923600664780429\nEpsilon = 0.08922708304713951\nEpsilon = 0.0892181603388348\nEpsilon = 0.08920923852280091\nAgent: ddqn_agent . Episode 1296/2000. Number of steps to finish: 20. Loss: 15.931251525878906 Reward: -18.0\nEpsilon = 0.08920031759894863\nEpsilon = 0.08919139756718873\nEpsilon = 0.08918247842743202\nEpsilon = 0.08917356017958927\nEpsilon = 0.08916464282357131\nEpsilon = 0.08915572635928895\nEpsilon = 0.08914681078665303\nEpsilon = 0.08913789610557436\nEpsilon = 0.0891289823159638\nEpsilon = 0.0891200694177322\nEpsilon = 0.08911115741079044\nEpsilon = 0.08910224629504936\nEpsilon = 0.08909333607041985\nEpsilon = 0.0890844267368128\nEpsilon = 0.08907551829413912\nEpsilon = 0.0890666107423097\nEpsilon = 0.08905770408123548\nEpsilon = 0.08904879831082736\nEpsilon = 0.08903989343099628\nEpsilon = 0.08903098944165318\nAgent: ddqn_agent . Episode 1297/2000. Number of steps to finish: 20. Loss: 17.933706283569336 Reward: -16.0\nEpsilon = 0.08902208634270901\nEpsilon = 0.08901318413407475\nEpsilon = 0.08900428281566133\nEpsilon = 0.08899538238737977\nEpsilon = 0.08898648284914103\nEpsilon = 0.08897758420085611\nEpsilon = 0.08896868644243602\nEpsilon = 0.08895978957379178\nEpsilon = 0.0889508935948344\nEpsilon = 0.0889419985054749\nEpsilon = 0.08893310430562436\nEpsilon = 0.08892421099519379\nEpsilon = 0.08891531857409428\nEpsilon = 0.08890642704223688\nEpsilon = 0.08889753639953266\nEpsilon = 0.0888886466458927\nEpsilon = 0.08887975778122811\nEpsilon = 0.08887086980544999\nEpsilon = 0.08886198271846944\nEpsilon = 0.0888530965201976\nAgent: ddqn_agent . Episode 1298/2000. Number of steps to finish: 20. Loss: 17.553611755371094 Reward: -14.0\nEpsilon = 0.08884421121054559\nEpsilon = 0.08883532678942453\nEpsilon = 0.08882644325674559\nEpsilon = 0.08881756061241991\nEpsilon = 0.08880867885635867\nEpsilon = 0.08879979798847303\nEpsilon = 0.08879091800867418\nEpsilon = 0.08878203891687332\nEpsilon = 0.08877316071298164\nEpsilon = 0.08876428339691034\nEpsilon = 0.08875540696857065\nEpsilon = 0.0887465314278738\nEpsilon = 0.08873765677473101\nEpsilon = 0.08872878300905354\nEpsilon = 0.08871991013075263\nEpsilon = 0.08871103813973956\nEpsilon = 0.08870216703592558\nEpsilon = 0.08869329681922199\nEpsilon = 0.08868442748954007\nEpsilon = 0.08867555904679111\nAgent: ddqn_agent . Episode 1299/2000. Number of steps to finish: 20. Loss: 16.563093185424805 Reward: -10.0\nEpsilon = 0.08866669149088643\nEpsilon = 0.08865782482173734\nEpsilon = 0.08864895903925517\nEpsilon = 0.08864009414335125\nEpsilon = 0.08863123013393692\nEpsilon = 0.08862236701092353\nEpsilon = 0.08861350477422245\nEpsilon = 0.08860464342374502\nEpsilon = 0.08859578295940265\nEpsilon = 0.08858692338110671\nEpsilon = 0.08857806468876861\nEpsilon = 0.08856920688229973\nEpsilon = 0.0885603499616115\nEpsilon = 0.08855149392661534\nEpsilon = 0.08854263877722267\nEpsilon = 0.08853378451334494\nEpsilon = 0.08852493113489361\nEpsilon = 0.08851607864178013\nEpsilon = 0.08850722703391595\nEpsilon = 0.08849837631121256\nAgent: ddqn_agent . Episode 1300/2000. Number of steps to finish: 20. Loss: 17.75114631652832 Reward: -16.0\nEpsilon = 0.08848952647358144\nEpsilon = 0.08848067752093408\nEpsilon = 0.088471829453182\nEpsilon = 0.08846298227023668\nEpsilon = 0.08845413597200966\nEpsilon = 0.08844529055841245\nEpsilon = 0.08843644602935662\nEpsilon = 0.08842760238475368\nEpsilon = 0.0884187596245152\nEpsilon = 0.08840991774855275\nEpsilon = 0.0884010767567779\nEpsilon = 0.08839223664910223\nEpsilon = 0.08838339742543733\nEpsilon = 0.08837455908569479\nEpsilon = 0.08836572162978622\nEpsilon = 0.08835688505762324\nEpsilon = 0.08834804936911747\nEpsilon = 0.08833921456418056\nEpsilon = 0.08833038064272415\nEpsilon = 0.08832154760465988\nAgent: ddqn_agent . Episode 1301/2000. Number of steps to finish: 20. Loss: 15.779623031616211 Reward: -14.0\nEpsilon = 0.08831271544989941\nEpsilon = 0.08830388417835441\nEpsilon = 0.08829505378993657\nEpsilon = 0.08828622428455758\nEpsilon = 0.08827739566212912\nEpsilon = 0.08826856792256291\nAgent: ddqn_agent . Episode 1302/2000. Number of steps to finish: 6. Loss: 5.39888334274292 Reward: 6.0\nEpsilon = 0.08825974106577066\nEpsilon = 0.08825091509166408\nEpsilon = 0.08824209000015491\nEpsilon = 0.0882332657911549\nEpsilon = 0.08822444246457578\nEpsilon = 0.08821562002032933\nEpsilon = 0.0882067984583273\nEpsilon = 0.08819797777848147\nEpsilon = 0.08818915798070362\nEpsilon = 0.08818033906490555\nEpsilon = 0.08817152103099905\nEpsilon = 0.08816270387889595\nEpsilon = 0.08815388760850806\nEpsilon = 0.08814507221974721\nEpsilon = 0.08813625771252524\nEpsilon = 0.08812744408675399\nEpsilon = 0.08811863134234532\nEpsilon = 0.0881098194792111\nEpsilon = 0.08810100849726317\nEpsilon = 0.08809219839641344\nAgent: ddqn_agent . Episode 1303/2000. Number of steps to finish: 20. Loss: 18.00896453857422 Reward: -16.0\nEpsilon = 0.0880833891765738\nEpsilon = 0.08807458083765614\nEpsilon = 0.08806577337957237\nEpsilon = 0.08805696680223442\nEpsilon = 0.08804816110555419\nEpsilon = 0.08803935628944363\nEpsilon = 0.0880305523538147\nEpsilon = 0.08802174929857931\nEpsilon = 0.08801294712364946\nEpsilon = 0.08800414582893709\nEpsilon = 0.08799534541435419\nEpsilon = 0.08798654587981276\nEpsilon = 0.08797774722522478\nEpsilon = 0.08796894945050225\nEpsilon = 0.0879601525555572\nEpsilon = 0.08795135654030165\nEpsilon = 0.08794256140464761\nEpsilon = 0.08793376714850715\nEpsilon = 0.0879249737717923\nEpsilon = 0.08791618127441513\nAgent: ddqn_agent . Episode 1304/2000. Number of steps to finish: 20. Loss: 18.468456268310547 Reward: -18.0\nEpsilon = 0.08790738965628768\nEpsilon = 0.08789859891732205\nEpsilon = 0.08788980905743031\nEpsilon = 0.08788102007652457\nEpsilon = 0.08787223197451692\nEpsilon = 0.08786344475131946\nEpsilon = 0.08785465840684434\nEpsilon = 0.08784587294100366\nEpsilon = 0.08783708835370956\nEpsilon = 0.08782830464487419\nEpsilon = 0.0878195218144097\nEpsilon = 0.08781073986222826\nEpsilon = 0.08780195878824204\nEpsilon = 0.08779317859236321\nEpsilon = 0.08778439927450397\nEpsilon = 0.08777562083457653\nEpsilon = 0.08776684327249308\nEpsilon = 0.08775806658816583\nEpsilon = 0.08774929078150702\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.08774051585242887\nAgent: ddqn_agent . Episode 1305/2000. Number of steps to finish: 20. Loss: 18.146989822387695 Reward: -18.0\nEpsilon = 0.08773174180084363\nEpsilon = 0.08772296862666355\nEpsilon = 0.08771419632980089\nEpsilon = 0.0877054249101679\nEpsilon = 0.08769665436767689\nEpsilon = 0.08768788470224012\nEpsilon = 0.0876791159137699\nEpsilon = 0.08767034800217852\nEpsilon = 0.0876615809673783\nEpsilon = 0.08765281480928157\nEpsilon = 0.08764404952780064\nEpsilon = 0.08763528512284786\nEpsilon = 0.08762652159433558\nEpsilon = 0.08761775894217615\nEpsilon = 0.08760899716628193\nEpsilon = 0.0876002362665653\nEpsilon = 0.08759147624293864\nEpsilon = 0.08758271709531434\nEpsilon = 0.08757395882360482\nEpsilon = 0.08756520142772246\nAgent: ddqn_agent . Episode 1306/2000. Number of steps to finish: 20. Loss: 17.586984634399414 Reward: -18.0\nEpsilon = 0.08755644490757969\nEpsilon = 0.08754768926308894\nEpsilon = 0.08753893449416263\nEpsilon = 0.08753018060071321\nEpsilon = 0.08752142758265315\nEpsilon = 0.08751267543989488\nEpsilon = 0.0875039241723509\nEpsilon = 0.08749517377993365\nEpsilon = 0.08748642426255566\nEpsilon = 0.08747767562012941\nEpsilon = 0.0874689278525674\nEpsilon = 0.08746018095978214\nEpsilon = 0.08745143494168617\nEpsilon = 0.087442689798192\nEpsilon = 0.08743394552921219\nEpsilon = 0.08742520213465926\nEpsilon = 0.0874164596144458\nEpsilon = 0.08740771796848436\nEpsilon = 0.08739897719668752\nEpsilon = 0.08739023729896785\nAgent: ddqn_agent . Episode 1307/2000. Number of steps to finish: 20. Loss: 16.6020450592041 Reward: -14.0\nEpsilon = 0.08738149827523795\nEpsilon = 0.08737276012541043\nEpsilon = 0.08736402284939788\nEpsilon = 0.08735528644711295\nEpsilon = 0.08734655091846824\nEpsilon = 0.08733781626337639\nEpsilon = 0.08732908248175006\nEpsilon = 0.08732034957350189\nEpsilon = 0.08731161753854454\nEpsilon = 0.08730288637679068\nEpsilon = 0.087294156088153\nEpsilon = 0.08728542667254419\nEpsilon = 0.08727669812987693\nEpsilon = 0.08726797046006395\nEpsilon = 0.08725924366301795\nEpsilon = 0.08725051773865164\nEpsilon = 0.08724179268687778\nEpsilon = 0.08723306850760909\nEpsilon = 0.08722434520075834\nEpsilon = 0.08721562276623826\nAgent: ddqn_agent . Episode 1308/2000. Number of steps to finish: 20. Loss: 17.87619400024414 Reward: -12.0\nEpsilon = 0.08720690120396164\nEpsilon = 0.08719818051384125\nEpsilon = 0.08718946069578987\nEpsilon = 0.08718074174972029\nEpsilon = 0.08717202367554532\nEpsilon = 0.08716330647317776\nEpsilon = 0.08715459014253044\nEpsilon = 0.08714587468351619\nEpsilon = 0.08713716009604784\nEpsilon = 0.08712844638003824\nEpsilon = 0.08711973353540023\nEpsilon = 0.08711102156204668\nEpsilon = 0.08710231045989048\nEpsilon = 0.08709360022884449\nEpsilon = 0.08708489086882161\nEpsilon = 0.08707618237973473\nEpsilon = 0.08706747476149676\nEpsilon = 0.08705876801402061\nEpsilon = 0.0870500621372192\nEpsilon = 0.08704135713100548\nAgent: ddqn_agent . Episode 1309/2000. Number of steps to finish: 20. Loss: 16.860065460205078 Reward: -18.0\nEpsilon = 0.08703265299529238\nEpsilon = 0.08702394972999285\nEpsilon = 0.08701524733501985\nEpsilon = 0.08700654581028634\nEpsilon = 0.08699784515570531\nEpsilon = 0.08698914537118975\nEpsilon = 0.08698044645665264\nEpsilon = 0.08697174841200697\nEpsilon = 0.08696305123716577\nEpsilon = 0.08695435493204205\nEpsilon = 0.08694565949654885\nEpsilon = 0.0869369649305992\nEpsilon = 0.08692827123410614\nEpsilon = 0.08691957840698272\nEpsilon = 0.08691088644914202\nEpsilon = 0.0869021953604971\nEpsilon = 0.08689350514096106\nEpsilon = 0.08688481579044696\nEpsilon = 0.08687612730886791\nEpsilon = 0.08686743969613703\nAgent: ddqn_agent . Episode 1310/2000. Number of steps to finish: 20. Loss: 17.13684844970703 Reward: -20.0\nEpsilon = 0.08685875295216741\nEpsilon = 0.08685006707687219\nEpsilon = 0.08684138207016451\nEpsilon = 0.0868326979319575\nEpsilon = 0.0868240146621643\nEpsilon = 0.08681533226069808\nEpsilon = 0.08680665072747201\nEpsilon = 0.08679797006239927\nEpsilon = 0.08678929026539303\nEpsilon = 0.0867806113363665\nEpsilon = 0.08677193327523286\nEpsilon = 0.08676325608190534\nEpsilon = 0.08675457975629715\nEpsilon = 0.08674590429832152\nEpsilon = 0.08673722970789169\nEpsilon = 0.0867285559849209\nEpsilon = 0.0867198831293224\nEpsilon = 0.08671121114100946\nEpsilon = 0.08670254001989536\nEpsilon = 0.08669386976589337\nAgent: ddqn_agent . Episode 1311/2000. Number of steps to finish: 20. Loss: 18.091358184814453 Reward: -14.0\nEpsilon = 0.08668520037891678\nEpsilon = 0.08667653185887889\nEpsilon = 0.086667864205693\nEpsilon = 0.08665919741927243\nEpsilon = 0.0866505314995305\nEpsilon = 0.08664186644638056\nEpsilon = 0.08663320225973592\nEpsilon = 0.08662453893950994\nEpsilon = 0.08661587648561599\nEpsilon = 0.08660721489796742\nEpsilon = 0.08659855417647763\nEpsilon = 0.08658989432105998\nEpsilon = 0.08658123533162788\nEpsilon = 0.08657257720809472\nEpsilon = 0.08656391995037391\nEpsilon = 0.08655526355837886\nEpsilon = 0.08654660803202303\nEpsilon = 0.08653795337121983\nEpsilon = 0.0865292995758827\nEpsilon = 0.08652064664592513\nAgent: ddqn_agent . Episode 1312/2000. Number of steps to finish: 20. Loss: 17.897953033447266 Reward: -18.0\nEpsilon = 0.08651199458126053\nEpsilon = 0.0865033433818024\nEpsilon = 0.08649469304746422\nEpsilon = 0.08648604357815948\nEpsilon = 0.08647739497380166\nEpsilon = 0.08646874723430428\nEpsilon = 0.08646010035958085\nEpsilon = 0.08645145434954489\nEpsilon = 0.08644280920410993\nEpsilon = 0.08643416492318952\nEpsilon = 0.0864255215066972\nEpsilon = 0.08641687895454653\nEpsilon = 0.08640823726665108\nEpsilon = 0.08639959644292441\nEpsilon = 0.08639095648328013\nEpsilon = 0.0863823173876318\nEpsilon = 0.08637367915589304\nEpsilon = 0.08636504178797745\nEpsilon = 0.08635640528379865\nEpsilon = 0.08634776964327028\nAgent: ddqn_agent . Episode 1313/2000. Number of steps to finish: 20. Loss: 17.850942611694336 Reward: -20.0\nEpsilon = 0.08633913486630596\nEpsilon = 0.08633050095281933\nEpsilon = 0.08632186790272404\nEpsilon = 0.08631323571593377\nEpsilon = 0.08630460439236218\nEpsilon = 0.08629597393192294\nEpsilon = 0.08628734433452975\nEpsilon = 0.0862787156000963\nEpsilon = 0.0862700877285363\nEpsilon = 0.08626146071976344\nEpsilon = 0.08625283457369147\nEpsilon = 0.0862442092902341\nEpsilon = 0.08623558486930508\nEpsilon = 0.08622696131081815\nEpsilon = 0.08621833861468707\nEpsilon = 0.0862097167808256\nEpsilon = 0.08620109580914752\nEpsilon = 0.0861924756995666\nEpsilon = 0.08618385645199665\nEpsilon = 0.08617523806635145\nAgent: ddqn_agent . Episode 1314/2000. Number of steps to finish: 20. Loss: 16.69615936279297 Reward: -12.0\nEpsilon = 0.08616662054254481\nEpsilon = 0.08615800388049057\nEpsilon = 0.08614938808010252\nEpsilon = 0.08614077314129451\nEpsilon = 0.08613215906398038\nEpsilon = 0.08612354584807398\nEpsilon = 0.08611493349348917\nEpsilon = 0.08610632200013982\nEpsilon = 0.08609771136793981\nEpsilon = 0.08608910159680301\nEpsilon = 0.08608049268664333\nEpsilon = 0.08607188463737467\nEpsilon = 0.08606327744891093\nEpsilon = 0.08605467112116603\nEpsilon = 0.08604606565405391\nEpsilon = 0.08603746104748851\nEpsilon = 0.08602885730138377\nEpsilon = 0.08602025441565363\nEpsilon = 0.08601165239021206\nEpsilon = 0.08600305122497305\nAgent: ddqn_agent . Episode 1315/2000. Number of steps to finish: 20. Loss: 18.36579704284668 Reward: -12.0\nEpsilon = 0.08599445091985056\nEpsilon = 0.08598585147475857\nEpsilon = 0.0859772528896111\nEpsilon = 0.08596865516432213\nEpsilon = 0.0859600582988057\nEpsilon = 0.08595146229297582\nEpsilon = 0.08594286714674652\nEpsilon = 0.08593427286003184\nEpsilon = 0.08592567943274584\nEpsilon = 0.08591708686480257\nEpsilon = 0.0859084951561161\nEpsilon = 0.08589990430660048\nEpsilon = 0.08589131431616982\nEpsilon = 0.0858827251847382\nEpsilon = 0.08587413691221973\nEpsilon = 0.08586554949852851\nEpsilon = 0.08585696294357865\nEpsilon = 0.0858483772472843\nEpsilon = 0.08583979240955956\nEpsilon = 0.0858312084303186\nAgent: ddqn_agent . Episode 1316/2000. Number of steps to finish: 20. Loss: 17.691293716430664 Reward: -14.0\nEpsilon = 0.08582262530947557\nEpsilon = 0.08581404304694462\nEpsilon = 0.08580546164263993\nEpsilon = 0.08579688109647567\nEpsilon = 0.08578830140836602\nEpsilon = 0.08577972257822518\nEpsilon = 0.08577114460596735\nEpsilon = 0.08576256749150676\nEpsilon = 0.08575399123475762\nEpsilon = 0.08574541583563414\nEpsilon = 0.08573684129405057\nEpsilon = 0.08572826760992117\nEpsilon = 0.08571969478316017\nEpsilon = 0.08571112281368186\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.0857025517014005\nEpsilon = 0.08569398144623036\nEpsilon = 0.08568541204808573\nEpsilon = 0.08567684350688093\nEpsilon = 0.08566827582253024\nEpsilon = 0.08565970899494799\nAgent: ddqn_agent . Episode 1317/2000. Number of steps to finish: 20. Loss: 19.516822814941406 Reward: -10.0\nEpsilon = 0.0856511430240485\nEpsilon = 0.0856425779097461\nEpsilon = 0.08563401365195512\nEpsilon = 0.08562545025058993\nEpsilon = 0.08561688770556487\nEpsilon = 0.08560832601679431\nEpsilon = 0.08559976518419264\nEpsilon = 0.08559120520767423\nEpsilon = 0.08558264608715346\nEpsilon = 0.08557408782254475\nEpsilon = 0.0855655304137625\nEpsilon = 0.08555697386072113\nEpsilon = 0.08554841816333505\nEpsilon = 0.08553986332151872\nEpsilon = 0.08553130933518657\nEpsilon = 0.08552275620425305\nEpsilon = 0.08551420392863263\nEpsilon = 0.08550565250823976\nEpsilon = 0.08549710194298893\nEpsilon = 0.08548855223279463\nAgent: ddqn_agent . Episode 1318/2000. Number of steps to finish: 20. Loss: 16.758996963500977 Reward: -12.0\nEpsilon = 0.08548000337757135\nEpsilon = 0.0854714553772336\nEpsilon = 0.08546290823169587\nEpsilon = 0.0854543619408727\nEpsilon = 0.08544581650467863\nEpsilon = 0.08543727192302816\nEpsilon = 0.08542872819583586\nEpsilon = 0.08542018532301628\nEpsilon = 0.08541164330448398\nEpsilon = 0.08540310214015352\nEpsilon = 0.08539456182993951\nEpsilon = 0.08538602237375652\nEpsilon = 0.08537748377151914\nEpsilon = 0.08536894602314199\nEpsilon = 0.08536040912853968\nEpsilon = 0.08535187308762682\nEpsilon = 0.08534333790031806\nEpsilon = 0.08533480356652803\nEpsilon = 0.08532627008617139\nEpsilon = 0.08531773745916277\nAgent: ddqn_agent . Episode 1319/2000. Number of steps to finish: 20. Loss: 17.656492233276367 Reward: -14.0\nEpsilon = 0.08530920568541685\nEpsilon = 0.0853006747648483\nEpsilon = 0.08529214469737181\nEpsilon = 0.08528361548290207\nEpsilon = 0.08527508712135379\nEpsilon = 0.08526655961264165\nEpsilon = 0.08525803295668039\nEpsilon = 0.08524950715338472\nEpsilon = 0.08524098220266939\nEpsilon = 0.08523245810444913\nEpsilon = 0.08522393485863869\nEpsilon = 0.08521541246515282\nEpsilon = 0.0852068909239063\nEpsilon = 0.08519837023481391\nEpsilon = 0.08518985039779044\nEpsilon = 0.08518133141275065\nEpsilon = 0.08517281327960938\nEpsilon = 0.08516429599828142\nEpsilon = 0.08515577956868159\nEpsilon = 0.08514726399072473\nAgent: ddqn_agent . Episode 1320/2000. Number of steps to finish: 20. Loss: 18.26222038269043 Reward: -8.0\nEpsilon = 0.08513874926432566\nEpsilon = 0.08513023538939922\nEpsilon = 0.08512172236586028\nEpsilon = 0.0851132101936237\nEpsilon = 0.08510469887260433\nEpsilon = 0.08509618840271707\nEpsilon = 0.0850876787838768\nEpsilon = 0.0850791700159984\nEpsilon = 0.0850706620989968\nEpsilon = 0.0850621550327869\nEpsilon = 0.08505364881728363\nEpsilon = 0.0850451434524019\nEpsilon = 0.08503663893805666\nEpsilon = 0.08502813527416285\nEpsilon = 0.08501963246063543\nEpsilon = 0.08501113049738937\nEpsilon = 0.08500262938433963\nEpsilon = 0.0849941291214012\nEpsilon = 0.08498562970848905\nEpsilon = 0.0849771311455182\nAgent: ddqn_agent . Episode 1321/2000. Number of steps to finish: 20. Loss: 16.06546401977539 Reward: -14.0\nEpsilon = 0.08496863343240366\nEpsilon = 0.08496013656906042\nEpsilon = 0.08495164055540351\nEpsilon = 0.08494314539134797\nEpsilon = 0.08493465107680884\nEpsilon = 0.08492615761170115\nEpsilon = 0.08491766499593999\nEpsilon = 0.08490917322944039\nEpsilon = 0.08490068231211745\nEpsilon = 0.08489219224388624\nEpsilon = 0.08488370302466185\nEpsilon = 0.08487521465435938\nEpsilon = 0.08486672713289393\nEpsilon = 0.08485824046018065\nEpsilon = 0.08484975463613463\nEpsilon = 0.08484126966067101\nEpsilon = 0.08483278553370495\nEpsilon = 0.08482430225515158\nEpsilon = 0.08481581982492607\nEpsilon = 0.08480733824294358\nAgent: ddqn_agent . Episode 1322/2000. Number of steps to finish: 20. Loss: 16.99403190612793 Reward: -14.0\nEpsilon = 0.08479885750911928\nEpsilon = 0.08479037762336837\nEpsilon = 0.08478189858560603\nEpsilon = 0.08477342039574746\nEpsilon = 0.0847649430537079\nEpsilon = 0.08475646655940253\nEpsilon = 0.08474799091274658\nEpsilon = 0.08473951611365531\nEpsilon = 0.08473104216204394\nEpsilon = 0.08472256905782774\nEpsilon = 0.08471409680092196\nEpsilon = 0.08470562539124186\nEpsilon = 0.08469715482870274\nEpsilon = 0.08468868511321988\nEpsilon = 0.08468021624470856\nEpsilon = 0.0846717482230841\nEpsilon = 0.08466328104826179\nEpsilon = 0.08465481472015697\nEpsilon = 0.08464634923868496\nEpsilon = 0.0846378846037611\nAgent: ddqn_agent . Episode 1323/2000. Number of steps to finish: 20. Loss: 17.326637268066406 Reward: -18.0\nEpsilon = 0.08462942081530071\nEpsilon = 0.08462095787321919\nEpsilon = 0.08461249577743186\nEpsilon = 0.08460403452785412\nEpsilon = 0.08459557412440133\nEpsilon = 0.08458711456698889\nEpsilon = 0.08457865585553219\nEpsilon = 0.08457019798994664\nEpsilon = 0.08456174097014764\nEpsilon = 0.08455328479605062\nEpsilon = 0.08454482946757101\nEpsilon = 0.08453637498462425\nEpsilon = 0.08452792134712579\nEpsilon = 0.08451946855499108\nEpsilon = 0.08451101660813558\nEpsilon = 0.08450256550647477\nEpsilon = 0.08449411524992412\nEpsilon = 0.08448566583839913\nEpsilon = 0.08447721727181529\nEpsilon = 0.0844687695500881\nAgent: ddqn_agent . Episode 1324/2000. Number of steps to finish: 20. Loss: 17.63808822631836 Reward: -14.0\nEpsilon = 0.0844603226731331\nEpsilon = 0.08445187664086579\nEpsilon = 0.0844434314532017\nEpsilon = 0.08443498711005638\nEpsilon = 0.08442654361134538\nEpsilon = 0.08441810095698425\nEpsilon = 0.08440965914688855\nEpsilon = 0.08440121818097386\nEpsilon = 0.08439277805915577\nEpsilon = 0.08438433878134985\nEpsilon = 0.08437590034747172\nEpsilon = 0.08436746275743698\nEpsilon = 0.08435902601116124\nEpsilon = 0.08435059010856012\nEpsilon = 0.08434215504954927\nEpsilon = 0.08433372083404432\nEpsilon = 0.08432528746196091\nEpsilon = 0.08431685493321472\nEpsilon = 0.0843084232477214\nEpsilon = 0.08429999240539664\nAgent: ddqn_agent . Episode 1325/2000. Number of steps to finish: 20. Loss: 16.623279571533203 Reward: -18.0\nEpsilon = 0.0842915624061561\nEpsilon = 0.08428313324991549\nEpsilon = 0.08427470493659049\nEpsilon = 0.08426627746609683\nEpsilon = 0.08425785083835022\nEpsilon = 0.08424942505326638\nEpsilon = 0.08424100011076105\nEpsilon = 0.08423257601074997\nEpsilon = 0.0842241527531489\nEpsilon = 0.08421573033787359\nEpsilon = 0.08420730876483981\nEpsilon = 0.08419888803396333\nEpsilon = 0.08419046814515993\nEpsilon = 0.08418204909834541\nEpsilon = 0.08417363089343557\nEpsilon = 0.08416521353034623\nEpsilon = 0.0841567970089932\nEpsilon = 0.0841483813292923\nEpsilon = 0.08413996649115937\nEpsilon = 0.08413155249451026\nAgent: ddqn_agent . Episode 1326/2000. Number of steps to finish: 20. Loss: 17.415149688720703 Reward: -12.0\nEpsilon = 0.08412313933926081\nEpsilon = 0.0841147270253269\nEpsilon = 0.08410631555262436\nEpsilon = 0.0840979049210691\nEpsilon = 0.084089495130577\nEpsilon = 0.08408108618106394\nEpsilon = 0.08407267807244584\nEpsilon = 0.08406427080463859\nEpsilon = 0.08405586437755813\nEpsilon = 0.08404745879112038\nEpsilon = 0.08403905404524127\nEpsilon = 0.08403065013983675\nEpsilon = 0.08402224707482277\nEpsilon = 0.08401384485011529\nEpsilon = 0.08400544346563028\nEpsilon = 0.08399704292128372\nEpsilon = 0.0839886432169916\nEpsilon = 0.08398024435266989\nEpsilon = 0.08397184632823462\nEpsilon = 0.0839634491436018\nAgent: ddqn_agent . Episode 1327/2000. Number of steps to finish: 20. Loss: 17.816770553588867 Reward: -20.0\nEpsilon = 0.08395505279868745\nEpsilon = 0.08394665729340758\nEpsilon = 0.08393826262767824\nEpsilon = 0.08392986880141548\nEpsilon = 0.08392147581453534\nEpsilon = 0.08391308366695388\nEpsilon = 0.08390469235858719\nEpsilon = 0.08389630188935134\nEpsilon = 0.0838879122591624\nEpsilon = 0.08387952346793648\nEpsilon = 0.08387113551558968\nEpsilon = 0.08386274840203813\nEpsilon = 0.08385436212719792\nEpsilon = 0.0838459766909852\nEpsilon = 0.08383759209331611\nEpsilon = 0.08382920833410677\nEpsilon = 0.08382082541327336\nEpsilon = 0.08381244333073204\nEpsilon = 0.08380406208639897\nEpsilon = 0.08379568168019033\nAgent: ddqn_agent . Episode 1328/2000. Number of steps to finish: 20. Loss: 17.565637588500977 Reward: -14.0\nEpsilon = 0.08378730211202232\nEpsilon = 0.08377892338181112\nEpsilon = 0.08377054548947294\nEpsilon = 0.08376216843492398\nEpsilon = 0.08375379221808049\nEpsilon = 0.08374541683885868\nEpsilon = 0.0837370422971748\nEpsilon = 0.08372866859294507\nEpsilon = 0.08372029572608577\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.08371192369651317\nEpsilon = 0.08370355250414352\nEpsilon = 0.0836951821488931\nEpsilon = 0.08368681263067822\nEpsilon = 0.08367844394941515\nEpsilon = 0.08367007610502021\nEpsilon = 0.0836617090974097\nEpsilon = 0.08365334292649997\nEpsilon = 0.08364497759220732\nEpsilon = 0.0836366130944481\nEpsilon = 0.08362824943313865\nAgent: ddqn_agent . Episode 1329/2000. Number of steps to finish: 20. Loss: 16.63316535949707 Reward: -14.0\nEpsilon = 0.08361988660819533\nEpsilon = 0.08361152461953451\nEpsilon = 0.08360316346707256\nEpsilon = 0.08359480315072584\nEpsilon = 0.08358644367041078\nEpsilon = 0.08357808502604373\nEpsilon = 0.08356972721754113\nEpsilon = 0.08356137024481938\nEpsilon = 0.0835530141077949\nEpsilon = 0.08354465880638413\nEpsilon = 0.08353630434050349\nEpsilon = 0.08352795071006944\nEpsilon = 0.08351959791499843\nEpsilon = 0.08351124595520693\nEpsilon = 0.08350289483061141\nEpsilon = 0.08349454454112834\nEpsilon = 0.08348619508667422\nEpsilon = 0.08347784646716555\nEpsilon = 0.08346949868251884\nEpsilon = 0.08346115173265059\nAgent: ddqn_agent . Episode 1330/2000. Number of steps to finish: 20. Loss: 17.551740646362305 Reward: -20.0\nEpsilon = 0.08345280561747732\nEpsilon = 0.08344446033691558\nEpsilon = 0.08343611589088189\nEpsilon = 0.0834277722792928\nEpsilon = 0.08341942950206487\nEpsilon = 0.08341108755911467\nEpsilon = 0.08340274645035875\nEpsilon = 0.08339440617571371\nEpsilon = 0.08338606673509615\nEpsilon = 0.08337772812842263\nEpsilon = 0.0833693903556098\nEpsilon = 0.08336105341657424\nEpsilon = 0.08335271731123259\nEpsilon = 0.08334438203950147\nEpsilon = 0.08333604760129752\nEpsilon = 0.08332771399653739\nEpsilon = 0.08331938122513774\nEpsilon = 0.08331104928701523\nEpsilon = 0.08330271818208652\nEpsilon = 0.08329438791026832\nAgent: ddqn_agent . Episode 1331/2000. Number of steps to finish: 20. Loss: 19.2421875 Reward: -20.0\nEpsilon = 0.08328605847147728\nEpsilon = 0.08327772986563013\nEpsilon = 0.08326940209264357\nEpsilon = 0.0832610751524343\nEpsilon = 0.08325274904491907\nEpsilon = 0.08324442377001458\nEpsilon = 0.08323609932763758\nEpsilon = 0.08322777571770482\nEpsilon = 0.08321945294013304\nEpsilon = 0.08321113099483902\nEpsilon = 0.08320280988173955\nEpsilon = 0.08319448960075138\nEpsilon = 0.0831861701517913\nEpsilon = 0.08317785153477612\nEpsilon = 0.08316953374962265\nEpsilon = 0.08316121679624769\nEpsilon = 0.08315290067456807\nEpsilon = 0.08314458538450062\nEpsilon = 0.08313627092596217\nEpsilon = 0.08312795729886957\nAgent: ddqn_agent . Episode 1332/2000. Number of steps to finish: 20. Loss: 17.41094970703125 Reward: -12.0\nEpsilon = 0.08311964450313969\nEpsilon = 0.08311133253868938\nEpsilon = 0.08310302140543552\nEpsilon = 0.08309471110329497\nEpsilon = 0.08308640163218464\nEpsilon = 0.08307809299202143\nEpsilon = 0.08306978518272223\nEpsilon = 0.08306147820420397\nEpsilon = 0.08305317205638355\nEpsilon = 0.08304486673917791\nEpsilon = 0.083036562252504\nEpsilon = 0.08302825859627874\nEpsilon = 0.08301995577041911\nEpsilon = 0.08301165377484207\nEpsilon = 0.08300335260946459\nEpsilon = 0.08299505227420365\nEpsilon = 0.08298675276897623\nEpsilon = 0.08297845409369933\nEpsilon = 0.08297015624828996\nEpsilon = 0.08296185923266514\nAgent: ddqn_agent . Episode 1333/2000. Number of steps to finish: 20. Loss: 18.505901336669922 Reward: -18.0\nEpsilon = 0.08295356304674187\nEpsilon = 0.0829452676904372\nEpsilon = 0.08293697316366816\nEpsilon = 0.0829286794663518\nEpsilon = 0.08292038659840516\nEpsilon = 0.08291209455974531\nEpsilon = 0.08290380335028934\nEpsilon = 0.08289551296995432\nEpsilon = 0.08288722341865733\nEpsilon = 0.08287893469631546\nEpsilon = 0.08287064680284584\nEpsilon = 0.08286235973816555\nEpsilon = 0.08285407350219173\nEpsilon = 0.08284578809484151\nEpsilon = 0.08283750351603203\nEpsilon = 0.08282921976568043\nEpsilon = 0.08282093684370387\nEpsilon = 0.0828126547500195\nEpsilon = 0.0828043734845445\nEpsilon = 0.08279609304719605\nAgent: ddqn_agent . Episode 1334/2000. Number of steps to finish: 20. Loss: 16.731670379638672 Reward: -16.0\nEpsilon = 0.08278781343789134\nEpsilon = 0.08277953465654755\nEpsilon = 0.0827712567030819\nEpsilon = 0.08276297957741159\nEpsilon = 0.08275470327945385\nEpsilon = 0.0827464278091259\nEpsilon = 0.08273815316634499\nEpsilon = 0.08272987935102835\nEpsilon = 0.08272160636309325\nEpsilon = 0.08271333420245694\nEpsilon = 0.0827050628690367\nEpsilon = 0.0826967923627498\nEpsilon = 0.08268852268351352\nEpsilon = 0.08268025383124518\nEpsilon = 0.08267198580586206\nEpsilon = 0.08266371860728147\nEpsilon = 0.08265545223542074\nEpsilon = 0.0826471866901972\nEpsilon = 0.08263892197152818\nEpsilon = 0.08263065807933102\nAgent: ddqn_agent . Episode 1335/2000. Number of steps to finish: 20. Loss: 18.62041473388672 Reward: -10.0\nEpsilon = 0.08262239501352309\nEpsilon = 0.08261413277402173\nEpsilon = 0.08260587136074432\nEpsilon = 0.08259761077360825\nEpsilon = 0.08258935101253088\nEpsilon = 0.08258109207742963\nEpsilon = 0.08257283396822189\nEpsilon = 0.08256457668482507\nEpsilon = 0.08255632022715659\nEpsilon = 0.08254806459513388\nEpsilon = 0.08253980978867437\nEpsilon = 0.0825315558076955\nEpsilon = 0.08252330265211473\nEpsilon = 0.08251505032184953\nEpsilon = 0.08250679881681734\nEpsilon = 0.08249854813693566\nEpsilon = 0.08249029828212197\nEpsilon = 0.08248204925229376\nEpsilon = 0.08247380104736853\nEpsilon = 0.08246555366726378\nAgent: ddqn_agent . Episode 1336/2000. Number of steps to finish: 20. Loss: 16.265560150146484 Reward: -14.0\nEpsilon = 0.08245730711189705\nEpsilon = 0.08244906138118586\nEpsilon = 0.08244081647504775\nEpsilon = 0.08243257239340025\nEpsilon = 0.0824243291361609\nEpsilon = 0.0824160867032473\nEpsilon = 0.08240784509457696\nEpsilon = 0.08239960431006751\nEpsilon = 0.08239136434963651\nEpsilon = 0.08238312521320154\nEpsilon = 0.08237488690068022\nEpsilon = 0.08236664941199015\nEpsilon = 0.08235841274704896\nEpsilon = 0.08235017690577426\nEpsilon = 0.08234194188808368\nEpsilon = 0.08233370769389486\nEpsilon = 0.08232547432312548\nEpsilon = 0.08231724177569318\nEpsilon = 0.08230901005151561\nEpsilon = 0.08230077915051046\nAgent: ddqn_agent . Episode 1337/2000. Number of steps to finish: 20. Loss: 16.888826370239258 Reward: -14.0\nEpsilon = 0.08229254907259541\nEpsilon = 0.08228431981768815\nEpsilon = 0.08227609138570638\nEpsilon = 0.0822678637765678\nEpsilon = 0.08225963699019015\nEpsilon = 0.08225141102649113\nEpsilon = 0.08224318588538848\nEpsilon = 0.08223496156679995\nEpsilon = 0.08222673807064328\nEpsilon = 0.08221851539683621\nEpsilon = 0.08221029354529652\nEpsilon = 0.08220207251594198\nEpsilon = 0.08219385230869039\nEpsilon = 0.08218563292345953\nEpsilon = 0.08217741436016718\nEpsilon = 0.08216919661873116\nEpsilon = 0.0821609796990693\nAgent: ddqn_agent . Episode 1338/2000. Number of steps to finish: 17. Loss: 14.110466003417969 Reward: -5.0\nEpsilon = 0.0821527636010994\nEpsilon = 0.08214454832473929\nEpsilon = 0.08213633386990682\nEpsilon = 0.08212812023651983\nEpsilon = 0.08211990742449618\nEpsilon = 0.08211169543375373\nEpsilon = 0.08210348426421035\nEpsilon = 0.08209527391578393\nEpsilon = 0.08208706438839235\nEpsilon = 0.08207885568195351\nEpsilon = 0.08207064779638532\nEpsilon = 0.08206244073160568\nEpsilon = 0.08205423448753252\nEpsilon = 0.08204602906408377\nEpsilon = 0.08203782446117737\nEpsilon = 0.08202962067873125\nEpsilon = 0.08202141771666338\nEpsilon = 0.08201321557489172\nEpsilon = 0.08200501425333423\nEpsilon = 0.0819968137519089\nAgent: ddqn_agent . Episode 1339/2000. Number of steps to finish: 20. Loss: 15.507601737976074 Reward: -14.0\nEpsilon = 0.0819886140705337\nEpsilon = 0.08198041520912665\nEpsilon = 0.08197221716760573\nEpsilon = 0.08196401994588898\nEpsilon = 0.08195582354389438\nEpsilon = 0.08194762796154\nEpsilon = 0.08193943319874385\nEpsilon = 0.08193123925542398\nEpsilon = 0.08192304613149844\nEpsilon = 0.08191485382688529\nEpsilon = 0.08190666234150261\nEpsilon = 0.08189847167526847\nEpsilon = 0.08189028182810094\nEpsilon = 0.08188209279991814\nEpsilon = 0.08187390459063815\nEpsilon = 0.08186571720017909\nEpsilon = 0.08185753062845907\nEpsilon = 0.08184934487539623\nEpsilon = 0.0818411599409087\nEpsilon = 0.08183297582491461\nAgent: ddqn_agent . Episode 1340/2000. Number of steps to finish: 20. Loss: 19.12560272216797 Reward: -10.0\nEpsilon = 0.08182479252733212\nEpsilon = 0.08181661004807939\nEpsilon = 0.08180842838707458\nEpsilon = 0.08180024754423587\nEpsilon = 0.08179206751948145\nEpsilon = 0.08178388831272951\nEpsilon = 0.08177570992389824\nEpsilon = 0.08176753235290585\nEpsilon = 0.08175935559967056\nEpsilon = 0.0817511796641106\nEpsilon = 0.08174300454614418\nEpsilon = 0.08173483024568956\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.081726656762665\nEpsilon = 0.08171848409698873\nEpsilon = 0.08171031224857903\nEpsilon = 0.08170214121735417\nEpsilon = 0.08169397100323243\nEpsilon = 0.0816858016061321\nEpsilon = 0.08167763302597149\nEpsilon = 0.0816694652626689\nAgent: ddqn_agent . Episode 1341/2000. Number of steps to finish: 20. Loss: 18.251928329467773 Reward: -18.0\nEpsilon = 0.08166129831614263\nEpsilon = 0.08165313218631103\nEpsilon = 0.08164496687309239\nEpsilon = 0.08163680237640508\nEpsilon = 0.08162863869616745\nEpsilon = 0.08162047583229783\nEpsilon = 0.08161231378471459\nEpsilon = 0.08160415255333613\nEpsilon = 0.08159599213808079\nEpsilon = 0.08158783253886698\nEpsilon = 0.08157967375561309\nEpsilon = 0.08157151578823753\nEpsilon = 0.0815633586366587\nEpsilon = 0.08155520230079503\nEpsilon = 0.08154704678056496\nEpsilon = 0.0815388920758869\nEpsilon = 0.0815307381866793\nEpsilon = 0.08152258511286063\nEpsilon = 0.08151443285434935\nEpsilon = 0.08150628141106392\nAgent: ddqn_agent . Episode 1342/2000. Number of steps to finish: 20. Loss: 17.021562576293945 Reward: -20.0\nEpsilon = 0.08149813078292281\nEpsilon = 0.08148998096984451\nEpsilon = 0.08148183197174753\nEpsilon = 0.08147368378855036\nEpsilon = 0.0814655364201715\nEpsilon = 0.08145738986652948\nEpsilon = 0.08144924412754283\nEpsilon = 0.08144109920313007\nEpsilon = 0.08143295509320976\nEpsilon = 0.08142481179770045\nEpsilon = 0.08141666931652068\nEpsilon = 0.08140852764958903\nEpsilon = 0.08140038679682407\nEpsilon = 0.08139224675814438\nEpsilon = 0.08138410753346857\nEpsilon = 0.08137596912271522\nEpsilon = 0.08136783152580294\nEpsilon = 0.08135969474265037\nEpsilon = 0.08135155877317611\nEpsilon = 0.0813434236172988\nAgent: ddqn_agent . Episode 1343/2000. Number of steps to finish: 20. Loss: 18.353322982788086 Reward: -14.0\nEpsilon = 0.08133528927493706\nEpsilon = 0.08132715574600957\nEpsilon = 0.08131902303043498\nEpsilon = 0.08131089112813193\nEpsilon = 0.08130276003901912\nEpsilon = 0.08129462976301523\nEpsilon = 0.08128650030003892\nEpsilon = 0.08127837165000892\nEpsilon = 0.08127024381284392\nEpsilon = 0.08126211678846264\nEpsilon = 0.08125399057678379\nEpsilon = 0.08124586517772611\nEpsilon = 0.08123774059120834\nEpsilon = 0.08122961681714921\nEpsilon = 0.0812214938554675\nEpsilon = 0.08121337170608195\nEpsilon = 0.08120525036891134\nEpsilon = 0.08119712984387445\nEpsilon = 0.08118901013089007\nEpsilon = 0.08118089122987698\nAgent: ddqn_agent . Episode 1344/2000. Number of steps to finish: 20. Loss: 17.27389144897461 Reward: -10.0\nEpsilon = 0.081172773140754\nEpsilon = 0.08116465586343992\nEpsilon = 0.08115653939785357\nEpsilon = 0.08114842374391379\nEpsilon = 0.08114030890153939\nEpsilon = 0.08113219487064924\nAgent: ddqn_agent . Episode 1345/2000. Number of steps to finish: 6. Loss: 5.420600414276123 Reward: 6.0\nEpsilon = 0.08112408165116218\nEpsilon = 0.08111596924299706\nEpsilon = 0.08110785764607277\nEpsilon = 0.08109974686030816\nEpsilon = 0.08109163688562213\nEpsilon = 0.08108352772193357\nEpsilon = 0.08107541936916138\nEpsilon = 0.08106731182722446\nEpsilon = 0.08105920509604174\nEpsilon = 0.08105109917553213\nEpsilon = 0.08104299406561458\nEpsilon = 0.08103488976620801\nEpsilon = 0.0810267862772314\nEpsilon = 0.08101868359860367\nEpsilon = 0.08101058173024382\nEpsilon = 0.0810024806720708\nEpsilon = 0.0809943804240036\nEpsilon = 0.0809862809859612\nEpsilon = 0.0809781823578626\nEpsilon = 0.08097008453962681\nAgent: ddqn_agent . Episode 1346/2000. Number of steps to finish: 20. Loss: 16.615581512451172 Reward: -16.0\nEpsilon = 0.08096198753117285\nEpsilon = 0.08095389133241974\nEpsilon = 0.0809457959432865\nEpsilon = 0.08093770136369217\nEpsilon = 0.0809296075935558\nEpsilon = 0.08092151463279645\nEpsilon = 0.08091342248133318\nEpsilon = 0.08090533113908505\nEpsilon = 0.08089724060597114\nEpsilon = 0.08088915088191055\nEpsilon = 0.08088106196682236\nEpsilon = 0.08087297386062568\nEpsilon = 0.08086488656323962\nEpsilon = 0.08085680007458329\nEpsilon = 0.08084871439457583\nEpsilon = 0.08084062952313638\nEpsilon = 0.08083254546018406\nEpsilon = 0.08082446220563805\nEpsilon = 0.08081637975941748\nEpsilon = 0.08080829812144154\nAgent: ddqn_agent . Episode 1347/2000. Number of steps to finish: 20. Loss: 18.02187728881836 Reward: -18.0\nEpsilon = 0.0808002172916294\nEpsilon = 0.08079213726990023\nEpsilon = 0.08078405805617324\nEpsilon = 0.08077597965036762\nEpsilon = 0.08076790205240258\nEpsilon = 0.08075982526219734\nEpsilon = 0.08075174927967112\nEpsilon = 0.08074367410474315\nEpsilon = 0.08073559973733267\nEpsilon = 0.08072752617735894\nEpsilon = 0.08071945342474121\nEpsilon = 0.08071138147939874\nEpsilon = 0.0807033103412508\nEpsilon = 0.08069524001021668\nEpsilon = 0.08068717048621565\nEpsilon = 0.08067910176916704\nEpsilon = 0.08067103385899012\nEpsilon = 0.08066296675560422\nEpsilon = 0.08065490045892866\nEpsilon = 0.08064683496888277\nAgent: ddqn_agent . Episode 1348/2000. Number of steps to finish: 20. Loss: 18.784067153930664 Reward: -18.0\nEpsilon = 0.08063877028538588\nEpsilon = 0.08063070640835734\nEpsilon = 0.08062264333771652\nEpsilon = 0.08061458107338275\nEpsilon = 0.08060651961527542\nEpsilon = 0.08059845896331388\nEpsilon = 0.08059039911741755\nEpsilon = 0.08058234007750581\nEpsilon = 0.08057428184349806\nEpsilon = 0.0805662244153137\nEpsilon = 0.08055816779287217\nEpsilon = 0.08055011197609288\nEpsilon = 0.08054205696489528\nEpsilon = 0.08053400275919878\nEpsilon = 0.08052594935892286\nEpsilon = 0.08051789676398698\nEpsilon = 0.08050984497431057\nEpsilon = 0.08050179398981315\nEpsilon = 0.08049374381041417\nEpsilon = 0.08048569443603314\nAgent: ddqn_agent . Episode 1349/2000. Number of steps to finish: 20. Loss: 17.99050521850586 Reward: -10.0\nEpsilon = 0.08047764586658954\nEpsilon = 0.08046959810200288\nEpsilon = 0.08046155114219268\nEpsilon = 0.08045350498707846\nEpsilon = 0.08044545963657974\nEpsilon = 0.08043741509061608\nEpsilon = 0.08042937134910702\nEpsilon = 0.08042132841197211\nEpsilon = 0.08041328627913091\nEpsilon = 0.080405244950503\nEpsilon = 0.08039720442600795\nEpsilon = 0.08038916470556536\nEpsilon = 0.0803811257890948\nEpsilon = 0.0803730876765159\nEpsilon = 0.08036505036774824\nEpsilon = 0.08035701386271146\nEpsilon = 0.08034897816132519\nEpsilon = 0.08034094326350906\nEpsilon = 0.0803329091691827\nEpsilon = 0.08032487587826578\nAgent: ddqn_agent . Episode 1350/2000. Number of steps to finish: 20. Loss: 18.001121520996094 Reward: -12.0\nEpsilon = 0.08031684339067796\nEpsilon = 0.0803088117063389\nEpsilon = 0.08030078082516827\nEpsilon = 0.08029275074708575\nEpsilon = 0.08028472147201104\nEpsilon = 0.08027669299986384\nEpsilon = 0.08026866533056386\nEpsilon = 0.0802606384640308\nEpsilon = 0.08025261240018439\nEpsilon = 0.08024458713894436\nEpsilon = 0.08023656268023047\nEpsilon = 0.08022853902396244\nEpsilon = 0.08022051617006005\nEpsilon = 0.08021249411844306\nEpsilon = 0.08020447286903121\nEpsilon = 0.08019645242174431\nEpsilon = 0.08018843277650213\nEpsilon = 0.08018041393322448\nEpsilon = 0.08017239589183116\nEpsilon = 0.08016437865224198\nAgent: ddqn_agent . Episode 1351/2000. Number of steps to finish: 20. Loss: 17.328174591064453 Reward: -20.0\nEpsilon = 0.08015636221437675\nEpsilon = 0.08014834657815531\nEpsilon = 0.0801403317434975\nEpsilon = 0.08013231771032314\nEpsilon = 0.08012430447855211\nEpsilon = 0.08011629204810425\nEpsilon = 0.08010828041889943\nEpsilon = 0.08010026959085755\nEpsilon = 0.08009225956389845\nEpsilon = 0.08008425033794206\nEpsilon = 0.08007624191290827\nEpsilon = 0.08006823428871698\nEpsilon = 0.08006022746528811\nEpsilon = 0.08005222144254158\nEpsilon = 0.08004421622039733\nEpsilon = 0.08003621179877529\nEpsilon = 0.08002820817759541\nEpsilon = 0.08002020535677766\nEpsilon = 0.08001220333624198\nEpsilon = 0.08000420211590836\nAgent: ddqn_agent . Episode 1352/2000. Number of steps to finish: 20. Loss: 17.562368392944336 Reward: -12.0\nEpsilon = 0.07999620169569677\nEpsilon = 0.0799882020755272\nEpsilon = 0.07998020325531965\nEpsilon = 0.07997220523499413\nEpsilon = 0.07996420801447063\nEpsilon = 0.07995621159366918\nEpsilon = 0.07994821597250981\nEpsilon = 0.07994022115091257\nEpsilon = 0.07993222712879748\nEpsilon = 0.07992423390608461\nEpsilon = 0.079916241482694\nEpsilon = 0.07990824985854572\nEpsilon = 0.07990025903355986\nEpsilon = 0.07989226900765652\nEpsilon = 0.07988427978075575\nEpsilon = 0.07987629135277768\nEpsilon = 0.07986830372364241\nEpsilon = 0.07986031689327004\nEpsilon = 0.07985233086158072\nEpsilon = 0.07984434562849456\nAgent: ddqn_agent . Episode 1353/2000. Number of steps to finish: 20. Loss: 16.90021324157715 Reward: -10.0\nEpsilon = 0.07983636119393171\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.07982837755781232\nEpsilon = 0.07982039472005653\nEpsilon = 0.07981241268058453\nEpsilon = 0.07980443143931648\nEpsilon = 0.07979645099617255\nEpsilon = 0.07978847135107293\nEpsilon = 0.07978049250393783\nEpsilon = 0.07977251445468744\nEpsilon = 0.07976453720324198\nEpsilon = 0.07975656074952166\nEpsilon = 0.0797485850934467\nEpsilon = 0.07974061023493736\nEpsilon = 0.07973263617391387\nEpsilon = 0.07972466291029648\nEpsilon = 0.07971669044400545\nEpsilon = 0.07970871877496105\nEpsilon = 0.07970074790308355\nEpsilon = 0.07969277782829325\nEpsilon = 0.07968480855051041\nAgent: ddqn_agent . Episode 1354/2000. Number of steps to finish: 20. Loss: 18.690673828125 Reward: -20.0\nEpsilon = 0.07967684006965536\nEpsilon = 0.0796688723856484\nEpsilon = 0.07966090549840985\nEpsilon = 0.07965293940786\nEpsilon = 0.07964497411391921\nEpsilon = 0.07963700961650783\nEpsilon = 0.07962904591554618\nEpsilon = 0.07962108301095462\nEpsilon = 0.07961312090265353\nEpsilon = 0.07960515959056326\nEpsilon = 0.0795971990746042\nEpsilon = 0.07958923935469675\nEpsilon = 0.07958128043076128\nEpsilon = 0.0795733223027182\nEpsilon = 0.07956536497048793\nEpsilon = 0.07955740843399088\nEpsilon = 0.07954945269314748\nEpsilon = 0.07954149774787816\nEpsilon = 0.07953354359810338\nEpsilon = 0.07952559024374357\nAgent: ddqn_agent . Episode 1355/2000. Number of steps to finish: 20. Loss: 19.10198402404785 Reward: -12.0\nEpsilon = 0.0795176376847192\nEpsilon = 0.07950968592095073\nEpsilon = 0.07950173495235864\nEpsilon = 0.0794937847788634\nEpsilon = 0.07948583540038552\nEpsilon = 0.07947788681684548\nAgent: ddqn_agent . Episode 1356/2000. Number of steps to finish: 6. Loss: 4.976335048675537 Reward: 6.0\nEpsilon = 0.0794699390281638\nEpsilon = 0.07946199203426098\nEpsilon = 0.07945404583505755\nEpsilon = 0.07944610043047405\nEpsilon = 0.07943815582043101\nEpsilon = 0.07943021200484897\nEpsilon = 0.07942226898364849\nEpsilon = 0.07941432675675013\nEpsilon = 0.07940638532407446\nEpsilon = 0.07939844468554205\nEpsilon = 0.0793905048410735\nEpsilon = 0.0793825657905894\nEpsilon = 0.07937462753401034\nEpsilon = 0.07936669007125693\nEpsilon = 0.07935875340224981\nEpsilon = 0.07935081752690959\nEpsilon = 0.0793428824451569\nEpsilon = 0.07933494815691239\nEpsilon = 0.0793270146620967\nEpsilon = 0.07931908196063049\nAgent: ddqn_agent . Episode 1357/2000. Number of steps to finish: 20. Loss: 16.82257652282715 Reward: -18.0\nEpsilon = 0.07931115005243443\nEpsilon = 0.07930321893742918\nEpsilon = 0.07929528861553545\nEpsilon = 0.07928735908667389\nEpsilon = 0.07927943035076522\nEpsilon = 0.07927150240773015\nEpsilon = 0.07926357525748938\nEpsilon = 0.07925564889996363\nEpsilon = 0.07924772333507363\nEpsilon = 0.07923979856274012\nEpsilon = 0.07923187458288385\nEpsilon = 0.07922395139542555\nEpsilon = 0.07921602900028601\nEpsilon = 0.07920810739738599\nEpsilon = 0.07920018658664625\nEpsilon = 0.07919226656798758\nEpsilon = 0.07918434734133079\nEpsilon = 0.07917642890659665\nEpsilon = 0.079168511263706\nEpsilon = 0.07916059441257962\nAgent: ddqn_agent . Episode 1358/2000. Number of steps to finish: 20. Loss: 15.970099449157715 Reward: -14.0\nEpsilon = 0.07915267835313837\nEpsilon = 0.07914476308530306\nEpsilon = 0.07913684860899453\nEpsilon = 0.07912893492413363\nEpsilon = 0.07912102203064121\nEpsilon = 0.07911310992843815\nEpsilon = 0.0791051986174453\nEpsilon = 0.07909728809758357\nEpsilon = 0.0790893783687738\nEpsilon = 0.07908146943093693\nEpsilon = 0.07907356128399383\nEpsilon = 0.07906565392786544\nEpsilon = 0.07905774736247265\nEpsilon = 0.0790498415877364\nEpsilon = 0.07904193660357763\nEpsilon = 0.07903403240991727\nEpsilon = 0.07902612900667629\nEpsilon = 0.07901822639377562\nEpsilon = 0.07901032457113624\nEpsilon = 0.07900242353867913\nAgent: ddqn_agent . Episode 1359/2000. Number of steps to finish: 20. Loss: 16.93583106994629 Reward: -18.0\nEpsilon = 0.07899452329632527\nEpsilon = 0.07898662384399563\nEpsilon = 0.07897872518161123\nEpsilon = 0.07897082730909306\nEpsilon = 0.07896293022636215\nEpsilon = 0.07895503393333951\nEpsilon = 0.07894713842994618\nEpsilon = 0.07893924371610318\nEpsilon = 0.07893134979173157\nEpsilon = 0.07892345665675239\nEpsilon = 0.07891556431108672\nEpsilon = 0.07890767275465561\nEpsilon = 0.07889978198738015\nEpsilon = 0.07889189200918141\nEpsilon = 0.0788840028199805\nEpsilon = 0.0788761144196985\nEpsilon = 0.07886822680825653\nEpsilon = 0.0788603399855757\nEpsilon = 0.07885245395157715\nEpsilon = 0.07884456870618199\nAgent: ddqn_agent . Episode 1360/2000. Number of steps to finish: 20. Loss: 18.848012924194336 Reward: -10.0\nEpsilon = 0.07883668424931138\nEpsilon = 0.07882880058088645\nEpsilon = 0.07882091770082837\nEpsilon = 0.0788130356090583\nEpsilon = 0.0788051543054974\nEpsilon = 0.07879727379006685\nEpsilon = 0.07878939406268784\nEpsilon = 0.07878151512328158\nEpsilon = 0.07877363697176924\nEpsilon = 0.07876575960807207\nEpsilon = 0.07875788303211126\nEpsilon = 0.07875000724380805\nEpsilon = 0.07874213224308367\nEpsilon = 0.07873425802985937\nEpsilon = 0.07872638460405638\nEpsilon = 0.07871851196559598\nEpsilon = 0.07871064011439942\nEpsilon = 0.07870276905038798\nEpsilon = 0.07869489877348294\nEpsilon = 0.0786870292836056\nAgent: ddqn_agent . Episode 1361/2000. Number of steps to finish: 20. Loss: 17.098487854003906 Reward: -16.0\nEpsilon = 0.07867916058067724\nEpsilon = 0.07867129266461917\nEpsilon = 0.0786634255353527\nEpsilon = 0.07865555919279917\nEpsilon = 0.07864769363687989\nEpsilon = 0.0786398288675162\nEpsilon = 0.07863196488462945\nEpsilon = 0.07862410168814099\nEpsilon = 0.07861623927797218\nEpsilon = 0.07860837765404438\nEpsilon = 0.07860051681627898\nEpsilon = 0.07859265676459735\nEpsilon = 0.0785847974989209\nEpsilon = 0.07857693901917101\nEpsilon = 0.07856908132526909\nEpsilon = 0.07856122441713656\nEpsilon = 0.07855336829469485\nEpsilon = 0.07854551295786538\nEpsilon = 0.0785376584065696\nEpsilon = 0.07852980464072894\nAgent: ddqn_agent . Episode 1362/2000. Number of steps to finish: 20. Loss: 17.252233505249023 Reward: -10.0\nEpsilon = 0.07852195166026486\nEpsilon = 0.07851409946509884\nEpsilon = 0.07850624805515233\nEpsilon = 0.07849839743034681\nEpsilon = 0.07849054759060378\nEpsilon = 0.07848269853584472\nEpsilon = 0.07847485026599113\nEpsilon = 0.07846700278096454\nEpsilon = 0.07845915608068645\nEpsilon = 0.07845131016507838\nEpsilon = 0.07844346503406187\nEpsilon = 0.07843562068755847\nEpsilon = 0.0784277771254897\nEpsilon = 0.07841993434777715\nEpsilon = 0.07841209235434238\nEpsilon = 0.07840425114510695\nEpsilon = 0.07839641071999244\nEpsilon = 0.07838857107892044\nEpsilon = 0.07838073222181255\nEpsilon = 0.07837289414859037\nAgent: ddqn_agent . Episode 1363/2000. Number of steps to finish: 20. Loss: 17.40880012512207 Reward: -14.0\nEpsilon = 0.07836505685917551\nEpsilon = 0.07835722035348959\nEpsilon = 0.07834938463145424\nEpsilon = 0.0783415496929911\nEpsilon = 0.0783337155380218\nEpsilon = 0.078325882166468\nEpsilon = 0.07831804957825136\nEpsilon = 0.07831021777329354\nEpsilon = 0.07830238675151621\nEpsilon = 0.07829455651284106\nEpsilon = 0.07828672705718978\nEpsilon = 0.07827889838448407\nEpsilon = 0.07827107049464563\nEpsilon = 0.07826324338759616\nEpsilon = 0.07825541706325741\nEpsilon = 0.07824759152155109\nEpsilon = 0.07823976676239894\nEpsilon = 0.0782319427857227\nEpsilon = 0.07822411959144412\nEpsilon = 0.07821629717948499\nAgent: ddqn_agent . Episode 1364/2000. Number of steps to finish: 20. Loss: 17.965835571289062 Reward: -20.0\nEpsilon = 0.07820847554976704\nEpsilon = 0.07820065470221206\nEpsilon = 0.07819283463674184\nEpsilon = 0.07818501535327817\nEpsilon = 0.07817719685174283\nEpsilon = 0.07816937913205765\nEpsilon = 0.07816156219414445\nEpsilon = 0.07815374603792503\nEpsilon = 0.07814593066332125\nEpsilon = 0.07813811607025492\nEpsilon = 0.0781303022586479\nEpsilon = 0.07812248922842203\nEpsilon = 0.0781146769794992\nEpsilon = 0.07810686551180125\nEpsilon = 0.07809905482525008\nEpsilon = 0.07809124491976756\nEpsilon = 0.07808343579527559\nEpsilon = 0.07807562745169606\nEpsilon = 0.0780678198889509\nEpsilon = 0.078060013106962\nAgent: ddqn_agent . Episode 1365/2000. Number of steps to finish: 20. Loss: 19.13591766357422 Reward: -16.0\nEpsilon = 0.0780522071056513\nEpsilon = 0.07804440188494075\nEpsilon = 0.07803659744475225\nEpsilon = 0.07802879378500778\nEpsilon = 0.07802099090562928\nEpsilon = 0.07801318880653872\nEpsilon = 0.07800538748765806\nEpsilon = 0.07799758694890929\nEpsilon = 0.0779897871902144\nEpsilon = 0.07798198821149538\nEpsilon = 0.07797419001267424\nEpsilon = 0.07796639259367297\nEpsilon = 0.0779585959544136\nEpsilon = 0.07795080009481815\nEpsilon = 0.07794300501480868\nEpsilon = 0.0779352107143072\nEpsilon = 0.07792741719323577\nEpsilon = 0.07791962445151646\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.07791183248907131\nEpsilon = 0.0779040413058224\nAgent: ddqn_agent . Episode 1366/2000. Number of steps to finish: 20. Loss: 19.162446975708008 Reward: -18.0\nEpsilon = 0.07789625090169182\nEpsilon = 0.07788846127660165\nEpsilon = 0.07788067243047399\nEpsilon = 0.07787288436323093\nEpsilon = 0.07786509707479461\nEpsilon = 0.07785731056508713\nEpsilon = 0.07784952483403063\nEpsilon = 0.07784173988154722\nEpsilon = 0.07783395570755906\nEpsilon = 0.07782617231198831\nEpsilon = 0.07781838969475711\nEpsilon = 0.07781060785578764\nEpsilon = 0.07780282679500206\nEpsilon = 0.07779504651232255\nEpsilon = 0.07778726700767132\nEpsilon = 0.07777948828097055\nEpsilon = 0.07777171033214245\nEpsilon = 0.07776393316110923\nEpsilon = 0.07775615676779311\nEpsilon = 0.07774838115211634\nAgent: ddqn_agent . Episode 1367/2000. Number of steps to finish: 20. Loss: 17.275407791137695 Reward: -14.0\nEpsilon = 0.07774060631400113\nEpsilon = 0.07773283225336973\nEpsilon = 0.0777250589701444\nEpsilon = 0.07771728646424739\nEpsilon = 0.07770951473560096\nEpsilon = 0.0777017437841274\nEpsilon = 0.07769397360974899\nEpsilon = 0.07768620421238802\nEpsilon = 0.07767843559196679\nEpsilon = 0.07767066774840758\nEpsilon = 0.07766290068163274\nEpsilon = 0.07765513439156459\nEpsilon = 0.07764736887812543\nEpsilon = 0.07763960414123762\nEpsilon = 0.0776318401808235\nEpsilon = 0.07762407699680542\nEpsilon = 0.07761631458910574\nEpsilon = 0.07760855295764683\nEpsilon = 0.07760079210235107\nEpsilon = 0.07759303202314084\nAgent: ddqn_agent . Episode 1368/2000. Number of steps to finish: 20. Loss: 16.368228912353516 Reward: -12.0\nEpsilon = 0.07758527271993854\nEpsilon = 0.07757751419266655\nEpsilon = 0.07756975644124728\nEpsilon = 0.07756199946560316\nEpsilon = 0.0775542432656566\nEpsilon = 0.07754648784133004\nEpsilon = 0.07753873319254591\nEpsilon = 0.07753097931922666\nEpsilon = 0.07752322622129473\nEpsilon = 0.07751547389867261\nEpsilon = 0.07750772235128274\nEpsilon = 0.07749997157904762\nEpsilon = 0.07749222158188972\nEpsilon = 0.07748447235973153\nEpsilon = 0.07747672391249556\nEpsilon = 0.07746897624010432\nEpsilon = 0.07746122934248031\nEpsilon = 0.07745348321954607\nEpsilon = 0.07744573787122411\nEpsilon = 0.07743799329743699\nAgent: ddqn_agent . Episode 1369/2000. Number of steps to finish: 20. Loss: 18.228771209716797 Reward: -20.0\nEpsilon = 0.07743024949810724\nEpsilon = 0.07742250647315743\nEpsilon = 0.07741476422251012\nEpsilon = 0.07740702274608786\nEpsilon = 0.07739928204381326\nEpsilon = 0.07739154211560888\nEpsilon = 0.07738380296139731\nEpsilon = 0.07737606458110118\nEpsilon = 0.07736832697464306\nEpsilon = 0.0773605901419456\nEpsilon = 0.07735285408293141\nEpsilon = 0.07734511879752312\nEpsilon = 0.07733738428564337\nEpsilon = 0.0773296505472148\nEpsilon = 0.07732191758216007\nEpsilon = 0.07731418539040186\nEpsilon = 0.07730645397186282\nEpsilon = 0.07729872332646563\nEpsilon = 0.07729099345413298\nEpsilon = 0.07728326435478756\nAgent: ddqn_agent . Episode 1370/2000. Number of steps to finish: 20. Loss: 17.914186477661133 Reward: -10.0\nEpsilon = 0.07727553602835209\nEpsilon = 0.07726780847474926\nEpsilon = 0.07726008169390179\nEpsilon = 0.0772523556857324\nEpsilon = 0.07724463045016382\nEpsilon = 0.0772369059871188\nEpsilon = 0.0772291822965201\nEpsilon = 0.07722145937829045\nEpsilon = 0.07721373723235261\nEpsilon = 0.07720601585862938\nEpsilon = 0.07719829525704351\nEpsilon = 0.07719057542751781\nEpsilon = 0.07718285636997506\nEpsilon = 0.07717513808433807\nEpsilon = 0.07716742057052964\nEpsilon = 0.07715970382847259\nEpsilon = 0.07715198785808974\nEpsilon = 0.07714427265930393\nEpsilon = 0.07713655823203801\nEpsilon = 0.0771288445762148\nAgent: ddqn_agent . Episode 1371/2000. Number of steps to finish: 20. Loss: 15.655375480651855 Reward: -16.0\nEpsilon = 0.07712113169175717\nEpsilon = 0.077113419578588\nEpsilon = 0.07710570823663014\nEpsilon = 0.07709799766580648\nEpsilon = 0.0770902878660399\nEpsilon = 0.0770825788372533\nEpsilon = 0.07707487057936957\nEpsilon = 0.07706716309231164\nEpsilon = 0.07705945637600241\nEpsilon = 0.07705175043036482\nEpsilon = 0.07704404525532178\nEpsilon = 0.07703634085079625\nEpsilon = 0.07702863721671117\nEpsilon = 0.0770209343529895\nEpsilon = 0.0770132322595542\nEpsilon = 0.07700553093632824\nEpsilon = 0.07699783038323461\nEpsilon = 0.07699013060019629\nEpsilon = 0.07698243158713626\nEpsilon = 0.07697473334397754\nAgent: ddqn_agent . Episode 1372/2000. Number of steps to finish: 20. Loss: 17.333545684814453 Reward: -14.0\nEpsilon = 0.07696703587064314\nEpsilon = 0.07695933916705608\nEpsilon = 0.07695164323313937\nEpsilon = 0.07694394806881606\nEpsilon = 0.07693625367400918\nEpsilon = 0.07692856004864178\nEpsilon = 0.07692086719263691\nEpsilon = 0.07691317510591765\nEpsilon = 0.07690548378840706\nEpsilon = 0.07689779324002823\nEpsilon = 0.07689010346070423\nEpsilon = 0.07688241445035815\nEpsilon = 0.07687472620891311\nEpsilon = 0.07686703873629222\nEpsilon = 0.07685935203241859\nEpsilon = 0.07685166609721535\nEpsilon = 0.07684398093060563\nEpsilon = 0.07683629653251256\nEpsilon = 0.0768286129028593\nEpsilon = 0.07682093004156902\nAgent: ddqn_agent . Episode 1373/2000. Number of steps to finish: 20. Loss: 16.882198333740234 Reward: -16.0\nEpsilon = 0.07681324794856487\nEpsilon = 0.07680556662377001\nEpsilon = 0.07679788606710763\nEpsilon = 0.07679020627850093\nEpsilon = 0.07678252725787309\nEpsilon = 0.0767748490051473\nEpsilon = 0.07676717152024679\nEpsilon = 0.07675949480309475\nEpsilon = 0.07675181885361444\nEpsilon = 0.07674414367172908\nEpsilon = 0.0767364692573619\nEpsilon = 0.07672879561043616\nEpsilon = 0.07672112273087511\nEpsilon = 0.07671345061860203\nEpsilon = 0.07670577927354018\nEpsilon = 0.07669810869561282\nEpsilon = 0.07669043888474326\nEpsilon = 0.07668276984085479\nEpsilon = 0.0766751015638707\nEpsilon = 0.0766674340537143\nAgent: ddqn_agent . Episode 1374/2000. Number of steps to finish: 20. Loss: 17.917373657226562 Reward: -16.0\nEpsilon = 0.07665976731030893\nEpsilon = 0.0766521013335779\nEpsilon = 0.07664443612344454\nEpsilon = 0.0766367716798322\nEpsilon = 0.07662910800266422\nEpsilon = 0.07662144509186396\nEpsilon = 0.07661378294735477\nEpsilon = 0.07660612156906003\nEpsilon = 0.07659846095690313\nEpsilon = 0.07659080111080745\nEpsilon = 0.07658314203069637\nEpsilon = 0.0765754837164933\nEpsilon = 0.07656782616812165\nEpsilon = 0.07656016938550483\nEpsilon = 0.07655251336856628\nEpsilon = 0.07654485811722943\nEpsilon = 0.07653720363141771\nEpsilon = 0.07652954991105457\nEpsilon = 0.07652189695606346\nEpsilon = 0.07651424476636785\nAgent: ddqn_agent . Episode 1375/2000. Number of steps to finish: 20. Loss: 17.78162956237793 Reward: -18.0\nEpsilon = 0.07650659334189122\nEpsilon = 0.07649894268255704\nEpsilon = 0.07649129278828878\nEpsilon = 0.07648364365900996\nEpsilon = 0.07647599529464406\nEpsilon = 0.0764683476951146\nEpsilon = 0.07646070086034508\nEpsilon = 0.07645305479025905\nEpsilon = 0.07644540948478001\nEpsilon = 0.07643776494383153\nEpsilon = 0.07643012116733715\nEpsilon = 0.07642247815522041\nEpsilon = 0.07641483590740489\nEpsilon = 0.07640719442381415\nEpsilon = 0.07639955370437176\nEpsilon = 0.07639191374900133\nEpsilon = 0.07638427455762642\nEpsilon = 0.07637663613017066\nEpsilon = 0.07636899846655765\nEpsilon = 0.07636136156671099\nAgent: ddqn_agent . Episode 1376/2000. Number of steps to finish: 20. Loss: 17.591644287109375 Reward: -16.0\nEpsilon = 0.07635372543055431\nEpsilon = 0.07634609005801125\nEpsilon = 0.07633845544900546\nEpsilon = 0.07633082160346055\nEpsilon = 0.07632318852130021\nEpsilon = 0.07631555620244808\nEpsilon = 0.07630792464682784\nEpsilon = 0.07630029385436315\nEpsilon = 0.07629266382497772\nEpsilon = 0.07628503455859523\nAgent: ddqn_agent . Episode 1377/2000. Number of steps to finish: 10. Loss: 8.236069679260254 Reward: 2.0\nEpsilon = 0.07627740605513937\nEpsilon = 0.07626977831453385\nEpsilon = 0.0762621513367024\nEpsilon = 0.07625452512156873\nEpsilon = 0.07624689966905658\nEpsilon = 0.07623927497908968\nEpsilon = 0.07623165105159177\nEpsilon = 0.0762240278864866\nEpsilon = 0.07621640548369796\nEpsilon = 0.0762087838431496\nEpsilon = 0.07620116296476528\nEpsilon = 0.07619354284846881\nEpsilon = 0.07618592349418396\nEpsilon = 0.07617830490183454\nEpsilon = 0.07617068707134436\nEpsilon = 0.07616307000263722\nEpsilon = 0.07615545369563696\nEpsilon = 0.0761478381502674\nEpsilon = 0.07614022336645238\nEpsilon = 0.07613260934411573\nAgent: ddqn_agent . Episode 1378/2000. Number of steps to finish: 20. Loss: 18.403291702270508 Reward: -12.0\nEpsilon = 0.07612499608318131\nEpsilon = 0.07611738358357299\nEpsilon = 0.07610977184521464\nEpsilon = 0.07610216086803012\nEpsilon = 0.07609455065194332\nEpsilon = 0.07608694119687812\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.07607933250275843\nEpsilon = 0.07607172456950816\nEpsilon = 0.07606411739705121\nEpsilon = 0.0760565109853115\nEpsilon = 0.07604890533421298\nEpsilon = 0.07604130044367956\nEpsilon = 0.07603369631363519\nEpsilon = 0.07602609294400382\nEpsilon = 0.07601849033470942\nEpsilon = 0.07601088848567596\nEpsilon = 0.0760032873968274\nEpsilon = 0.07599568706808771\nEpsilon = 0.0759880874993809\nEpsilon = 0.07598048869063095\nAgent: ddqn_agent . Episode 1379/2000. Number of steps to finish: 20. Loss: 18.78531265258789 Reward: -14.0\nEpsilon = 0.0759728906417619\nEpsilon = 0.07596529335269772\nEpsilon = 0.07595769682336245\nEpsilon = 0.07595010105368012\nEpsilon = 0.07594250604357475\nEpsilon = 0.07593491179297039\nEpsilon = 0.0759273183017911\nEpsilon = 0.07591972556996092\nEpsilon = 0.07591213359740392\nEpsilon = 0.07590454238404419\nEpsilon = 0.07589695192980579\nEpsilon = 0.07588936223461282\nEpsilon = 0.07588177329838935\nEpsilon = 0.07587418512105952\nEpsilon = 0.07586659770254742\nEpsilon = 0.07585901104277716\nEpsilon = 0.07585142514167288\nEpsilon = 0.07584383999915871\nEpsilon = 0.07583625561515879\nEpsilon = 0.07582867198959728\nAgent: ddqn_agent . Episode 1380/2000. Number of steps to finish: 20. Loss: 17.590641021728516 Reward: -20.0\nEpsilon = 0.07582108912239832\nEpsilon = 0.07581350701348609\nEpsilon = 0.07580592566278474\nEpsilon = 0.07579834507021846\nEpsilon = 0.07579076523571143\nEpsilon = 0.07578318615918786\nEpsilon = 0.07577560784057194\nEpsilon = 0.07576803027978789\nEpsilon = 0.07576045347675991\nEpsilon = 0.07575287743141224\nEpsilon = 0.07574530214366909\nEpsilon = 0.07573772761345472\nEpsilon = 0.07573015384069338\nEpsilon = 0.07572258082530932\nEpsilon = 0.0757150085672268\nEpsilon = 0.07570743706637008\nEpsilon = 0.07569986632266344\nEpsilon = 0.07569229633603118\nEpsilon = 0.07568472710639758\nEpsilon = 0.07567715863368694\nAgent: ddqn_agent . Episode 1381/2000. Number of steps to finish: 20. Loss: 17.542451858520508 Reward: -16.0\nEpsilon = 0.07566959091782358\nEpsilon = 0.0756620239587318\nEpsilon = 0.07565445775633592\nEpsilon = 0.07564689231056028\nEpsilon = 0.07563932762132923\nEpsilon = 0.0756317636885671\nEpsilon = 0.07562420051219824\nEpsilon = 0.07561663809214703\nEpsilon = 0.07560907642833781\nEpsilon = 0.07560151552069497\nEpsilon = 0.0755939553691429\nEpsilon = 0.07558639597360599\nEpsilon = 0.07557883733400862\nEpsilon = 0.07557127945027522\nEpsilon = 0.0755637223223302\nEpsilon = 0.07555616595009797\nEpsilon = 0.07554861033350296\nEpsilon = 0.07554105547246961\nEpsilon = 0.07553350136692237\nEpsilon = 0.07552594801678568\nAgent: ddqn_agent . Episode 1382/2000. Number of steps to finish: 20. Loss: 18.73459815979004 Reward: -16.0\nEpsilon = 0.075518395421984\nEpsilon = 0.07551084358244181\nEpsilon = 0.07550329249808357\nEpsilon = 0.07549574216883377\nEpsilon = 0.07548819259461689\nEpsilon = 0.07548064377535743\nEpsilon = 0.07547309571097989\nEpsilon = 0.07546554840140879\nEpsilon = 0.07545800184656865\nEpsilon = 0.07545045604638399\nEpsilon = 0.07544291100077935\nEpsilon = 0.07543536670967928\nEpsilon = 0.07542782317300831\nEpsilon = 0.07542028039069101\nEpsilon = 0.07541273836265194\nEpsilon = 0.07540519708881568\nEpsilon = 0.0753976565691068\nEpsilon = 0.07539011680344988\nEpsilon = 0.07538257779176954\nEpsilon = 0.07537503953399037\nAgent: ddqn_agent . Episode 1383/2000. Number of steps to finish: 20. Loss: 17.67328453063965 Reward: -16.0\nEpsilon = 0.07536750203003698\nEpsilon = 0.07535996527983398\nEpsilon = 0.07535242928330599\nEpsilon = 0.07534489404037766\nEpsilon = 0.07533735955097362\nEpsilon = 0.07532982581501853\nEpsilon = 0.07532229283243702\nEpsilon = 0.07531476060315377\nEpsilon = 0.07530722912709345\nEpsilon = 0.07529969840418074\nEpsilon = 0.07529216843434032\nEpsilon = 0.07528463921749688\nEpsilon = 0.07527711075357513\nEpsilon = 0.07526958304249977\nEpsilon = 0.07526205608419552\nEpsilon = 0.0752545298785871\nEpsilon = 0.07524700442559924\nEpsilon = 0.07523947972515668\nEpsilon = 0.07523195577718417\nEpsilon = 0.07522443258160645\nAgent: ddqn_agent . Episode 1384/2000. Number of steps to finish: 20. Loss: 15.710187911987305 Reward: -12.0\nEpsilon = 0.07521691013834829\nEpsilon = 0.07520938844733445\nEpsilon = 0.07520186750848971\nEpsilon = 0.07519434732173887\nEpsilon = 0.07518682788700669\nEpsilon = 0.07517930920421799\nEpsilon = 0.07517179127329757\nEpsilon = 0.07516427409417024\nEpsilon = 0.07515675766676083\nEpsilon = 0.07514924199099415\nEpsilon = 0.07514172706679506\nEpsilon = 0.07513421289408838\nEpsilon = 0.07512669947279897\nEpsilon = 0.07511918680285169\nEpsilon = 0.0751116748841714\nEpsilon = 0.07510416371668299\nEpsilon = 0.07509665330031133\nEpsilon = 0.0750891436349813\nEpsilon = 0.07508163472061781\nEpsilon = 0.07507412655714575\nAgent: ddqn_agent . Episode 1385/2000. Number of steps to finish: 20. Loss: 16.789487838745117 Reward: -18.0\nEpsilon = 0.07506661914449003\nEpsilon = 0.07505911248257559\nEpsilon = 0.07505160657132733\nEpsilon = 0.0750441014106702\nEpsilon = 0.07503659700052913\nEpsilon = 0.07502909334082908\nEpsilon = 0.07502159043149499\nEpsilon = 0.07501408827245185\nEpsilon = 0.0750065868636246\nEpsilon = 0.07499908620493824\nEpsilon = 0.07499158629631775\nEpsilon = 0.07498408713768812\nEpsilon = 0.07497658872897435\nEpsilon = 0.07496909107010145\nEpsilon = 0.07496159416099445\nEpsilon = 0.07495409800157835\nEpsilon = 0.07494660259177818\nEpsilon = 0.074939107931519\nEpsilon = 0.07493161402072586\nEpsilon = 0.07492412085932379\nAgent: ddqn_agent . Episode 1386/2000. Number of steps to finish: 20. Loss: 18.072429656982422 Reward: -20.0\nEpsilon = 0.07491662844723786\nEpsilon = 0.07490913678439313\nEpsilon = 0.0749016458707147\nEpsilon = 0.07489415570612763\nEpsilon = 0.07488666629055703\nEpsilon = 0.07487917762392797\nEpsilon = 0.07487168970616558\nEpsilon = 0.07486420253719496\nEpsilon = 0.07485671611694124\nEpsilon = 0.07484923044532954\nEpsilon = 0.07484174552228501\nEpsilon = 0.07483426134773279\nEpsilon = 0.07482677792159802\nEpsilon = 0.07481929524380586\nEpsilon = 0.07481181331428148\nEpsilon = 0.07480433213295005\nEpsilon = 0.07479685169973677\nEpsilon = 0.0747893720145668\nEpsilon = 0.07478189307736534\nEpsilon = 0.0747744148880576\nAgent: ddqn_agent . Episode 1387/2000. Number of steps to finish: 20. Loss: 18.13129425048828 Reward: -14.0\nEpsilon = 0.07476693744656879\nEpsilon = 0.07475946075282414\nEpsilon = 0.07475198480674886\nEpsilon = 0.07474450960826819\nEpsilon = 0.07473703515730735\nEpsilon = 0.07472956145379163\nEpsilon = 0.07472208849764625\nEpsilon = 0.07471461628879648\nEpsilon = 0.0747071448271676\nEpsilon = 0.07469967411268488\nEpsilon = 0.07469220414527361\nEpsilon = 0.07468473492485908\nEpsilon = 0.07467726645136659\nEpsilon = 0.07466979872472146\nEpsilon = 0.07466233174484899\nEpsilon = 0.07465486551167451\nEpsilon = 0.07464740002512334\nEpsilon = 0.07463993528512082\nEpsilon = 0.07463247129159231\nEpsilon = 0.07462500804446315\nAgent: ddqn_agent . Episode 1388/2000. Number of steps to finish: 20. Loss: 16.598705291748047 Reward: -12.0\nEpsilon = 0.07461754554365871\nEpsilon = 0.07461008378910435\nEpsilon = 0.07460262278072544\nEpsilon = 0.07459516251844736\nEpsilon = 0.07458770300219551\nEpsilon = 0.0745802442318953\nEpsilon = 0.07457278620747211\nEpsilon = 0.07456532892885137\nEpsilon = 0.07455787239595849\nEpsilon = 0.07455041660871889\nEpsilon = 0.07454296156705802\nEpsilon = 0.07453550727090132\nEpsilon = 0.07452805372017422\nEpsilon = 0.07452060091480221\nEpsilon = 0.07451314885471073\nEpsilon = 0.07450569753982526\nEpsilon = 0.07449824697007128\nEpsilon = 0.07449079714537427\nEpsilon = 0.07448334806565973\nEpsilon = 0.07447589973085317\nAgent: ddqn_agent . Episode 1389/2000. Number of steps to finish: 20. Loss: 18.55215835571289 Reward: -20.0\nEpsilon = 0.07446845214088009\nEpsilon = 0.074461005295666\nEpsilon = 0.07445355919513644\nEpsilon = 0.07444611383921693\nEpsilon = 0.074438669227833\nEpsilon = 0.07443122536091022\nEpsilon = 0.07442378223837413\nEpsilon = 0.0744163398601503\nEpsilon = 0.07440889822616428\nEpsilon = 0.07440145733634167\nEpsilon = 0.07439401719060804\nEpsilon = 0.07438657778888898\nEpsilon = 0.07437913913111009\nEpsilon = 0.07437170121719698\nEpsilon = 0.07436426404707526\nEpsilon = 0.07435682762067056\nEpsilon = 0.07434939193790849\nEpsilon = 0.0743419569987147\nEpsilon = 0.07433452280301482\nEpsilon = 0.07432708935073452\nAgent: ddqn_agent . Episode 1390/2000. Number of steps to finish: 20. Loss: 16.13228416442871 Reward: -14.0\nEpsilon = 0.07431965664179944\nEpsilon = 0.07431222467613527\nEpsilon = 0.07430479345366765\nEpsilon = 0.07429736297432228\nEpsilon = 0.07428993323802485\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.07428250424470105\nEpsilon = 0.07427507599427659\nEpsilon = 0.07426764848667716\nEpsilon = 0.0742602217218285\nEpsilon = 0.07425279569965632\nEpsilon = 0.07424537042008635\nEpsilon = 0.07423794588304433\nEpsilon = 0.07423052208845603\nEpsilon = 0.07422309903624719\nEpsilon = 0.07421567672634356\nEpsilon = 0.07420825515867092\nEpsilon = 0.07420083433315505\nEpsilon = 0.07419341424972174\nEpsilon = 0.07418599490829678\nEpsilon = 0.07417857630880595\nAgent: ddqn_agent . Episode 1391/2000. Number of steps to finish: 20. Loss: 18.441667556762695 Reward: -10.0\nEpsilon = 0.07417115845117507\nEpsilon = 0.07416374133532995\nEpsilon = 0.07415632496119642\nEpsilon = 0.0741489093287003\nEpsilon = 0.07414149443776744\nEpsilon = 0.07413408028832366\nEpsilon = 0.07412666688029483\nEpsilon = 0.07411925421360681\nEpsilon = 0.07411184228818544\nEpsilon = 0.07410443110395663\nEpsilon = 0.07409702066084624\nEpsilon = 0.07408961095878015\nEpsilon = 0.07408220199768427\nEpsilon = 0.07407479377748451\nEpsilon = 0.07406738629810676\nEpsilon = 0.07405997955947695\nEpsilon = 0.074052573561521\nEpsilon = 0.07404516830416485\nEpsilon = 0.07403776378733444\nEpsilon = 0.07403036001095571\nAgent: ddqn_agent . Episode 1392/2000. Number of steps to finish: 20. Loss: 18.01108169555664 Reward: -14.0\nEpsilon = 0.07402295697495462\nEpsilon = 0.07401555467925712\nEpsilon = 0.0740081531237892\nEpsilon = 0.07400075230847682\nEpsilon = 0.07399335223324598\nEpsilon = 0.07398595289802265\nEpsilon = 0.07397855430273285\nEpsilon = 0.07397115644730258\nEpsilon = 0.07396375933165784\nEpsilon = 0.07395636295572468\nEpsilon = 0.07394896731942911\nEpsilon = 0.07394157242269717\nEpsilon = 0.0739341782654549\nEpsilon = 0.07392678484762835\nEpsilon = 0.07391939216914359\nEpsilon = 0.07391200022992668\nEpsilon = 0.07390460902990369\nEpsilon = 0.0738972185690007\nEpsilon = 0.07388982884714379\nEpsilon = 0.07388243986425908\nAgent: ddqn_agent . Episode 1393/2000. Number of steps to finish: 20. Loss: 18.043615341186523 Reward: -14.0\nEpsilon = 0.07387505162027265\nEpsilon = 0.07386766411511063\nEpsilon = 0.07386027734869911\nEpsilon = 0.07385289132096425\nEpsilon = 0.07384550603183215\nEpsilon = 0.07383812148122897\nEpsilon = 0.07383073766908084\nEpsilon = 0.07382335459531393\nEpsilon = 0.0738159722598544\nEpsilon = 0.07380859066262842\nEpsilon = 0.07380120980356215\nEpsilon = 0.0737938296825818\nEpsilon = 0.07378645029961355\nEpsilon = 0.07377907165458358\nEpsilon = 0.07377169374741813\nEpsilon = 0.07376431657804339\nEpsilon = 0.07375694014638558\nEpsilon = 0.07374956445237094\nEpsilon = 0.0737421894959257\nEpsilon = 0.07373481527697612\nAgent: ddqn_agent . Episode 1394/2000. Number of steps to finish: 20. Loss: 18.996728897094727 Reward: -14.0\nEpsilon = 0.07372744179544842\nEpsilon = 0.07372006905126888\nEpsilon = 0.07371269704436376\nEpsilon = 0.07370532577465932\nEpsilon = 0.07369795524208186\nEpsilon = 0.07369058544655765\nEpsilon = 0.073683216388013\nEpsilon = 0.0736758480663742\nEpsilon = 0.07366848048156756\nEpsilon = 0.0736611136335194\nEpsilon = 0.07365374752215605\nEpsilon = 0.07364638214740384\nEpsilon = 0.0736390175091891\nEpsilon = 0.07363165360743817\nEpsilon = 0.07362429044207743\nEpsilon = 0.07361692801303321\nEpsilon = 0.07360956632023191\nEpsilon = 0.07360220536359989\nEpsilon = 0.07359484514306353\nEpsilon = 0.07358748565854922\nAgent: ddqn_agent . Episode 1395/2000. Number of steps to finish: 20. Loss: 20.200626373291016 Reward: -10.0\nEpsilon = 0.07358012690998336\nEpsilon = 0.07357276889729236\nEpsilon = 0.07356541162040263\nEpsilon = 0.07355805507924058\nEpsilon = 0.07355069927373266\nEpsilon = 0.07354334420380529\nEpsilon = 0.07353598986938491\nEpsilon = 0.07352863627039798\nEpsilon = 0.07352128340677094\nEpsilon = 0.07351393127843027\nEpsilon = 0.07350657988530243\nEpsilon = 0.0734992292273139\nEpsilon = 0.07349187930439117\nEpsilon = 0.07348453011646074\nEpsilon = 0.07347718166344909\nEpsilon = 0.07346983394528274\nEpsilon = 0.07346248696188822\nEpsilon = 0.07345514071319202\nEpsilon = 0.0734477951991207\nEpsilon = 0.07344045041960079\nAgent: ddqn_agent . Episode 1396/2000. Number of steps to finish: 20. Loss: 17.845596313476562 Reward: -20.0\nEpsilon = 0.07343310637455883\nEpsilon = 0.07342576306392137\nEpsilon = 0.07341842048761499\nEpsilon = 0.07341107864556623\nEpsilon = 0.07340373753770167\nEpsilon = 0.0733963971639479\nEpsilon = 0.0733890575242315\nEpsilon = 0.07338171861847907\nEpsilon = 0.07337438044661722\nEpsilon = 0.07336704300857255\nEpsilon = 0.0733597063042717\nEpsilon = 0.07335237033364128\nEpsilon = 0.07334503509660792\nEpsilon = 0.07333770059309826\nEpsilon = 0.07333036682303895\nEpsilon = 0.07332303378635664\nEpsilon = 0.073315701482978\nEpsilon = 0.0733083699128297\nEpsilon = 0.07330103907583842\nEpsilon = 0.07329370897193084\nAgent: ddqn_agent . Episode 1397/2000. Number of steps to finish: 20. Loss: 15.888684272766113 Reward: -12.0\nEpsilon = 0.07328637960103365\nEpsilon = 0.07327905096307355\nEpsilon = 0.07327172305797724\nEpsilon = 0.07326439588567145\nEpsilon = 0.07325706944608289\nEpsilon = 0.07324974373913828\nEpsilon = 0.07324241876476437\nEpsilon = 0.07323509452288789\nEpsilon = 0.0732277710134356\nEpsilon = 0.07322044823633425\nEpsilon = 0.07321312619151062\nEpsilon = 0.07320580487889147\nEpsilon = 0.07319848429840359\nEpsilon = 0.07319116444997374\nEpsilon = 0.07318384533352874\nEpsilon = 0.07317652694899539\nEpsilon = 0.07316920929630048\nEpsilon = 0.07316189237537085\nEpsilon = 0.07315457618613332\nEpsilon = 0.0731472607285147\nAgent: ddqn_agent . Episode 1398/2000. Number of steps to finish: 20. Loss: 19.951871871948242 Reward: -10.0\nEpsilon = 0.07313994600244185\nEpsilon = 0.0731326320078416\nEpsilon = 0.07312531874464082\nEpsilon = 0.07311800621276636\nEpsilon = 0.07311069441214509\nEpsilon = 0.07310338334270387\nEpsilon = 0.0730960730043696\nEpsilon = 0.07308876339706916\nEpsilon = 0.07308145452072945\nEpsilon = 0.07307414637527737\nEpsilon = 0.07306683896063984\nEpsilon = 0.07305953227674378\nEpsilon = 0.0730522263235161\nEpsilon = 0.07304492110088374\nEpsilon = 0.07303761660877366\nEpsilon = 0.07303031284711278\nEpsilon = 0.07302300981582807\nEpsilon = 0.07301570751484648\nEpsilon = 0.073008405944095\nEpsilon = 0.07300110510350058\nAgent: ddqn_agent . Episode 1399/2000. Number of steps to finish: 20. Loss: 16.848106384277344 Reward: -14.0\nEpsilon = 0.07299380499299023\nEpsilon = 0.07298650561249093\nEpsilon = 0.07297920696192968\nEpsilon = 0.07297190904123349\nEpsilon = 0.07296461185032936\nEpsilon = 0.07295731538914432\nEpsilon = 0.0729500196576054\nEpsilon = 0.07294272465563964\nEpsilon = 0.07293543038317407\nEpsilon = 0.07292813684013576\nEpsilon = 0.07292084402645174\nEpsilon = 0.0729135519420491\nEpsilon = 0.0729062605868549\nEpsilon = 0.07289896996079621\nEpsilon = 0.07289168006380013\nEpsilon = 0.07288439089579375\nEpsilon = 0.07287710245670417\nEpsilon = 0.0728698147464585\nEpsilon = 0.07286252776498385\nEpsilon = 0.07285524151220735\nAgent: ddqn_agent . Episode 1400/2000. Number of steps to finish: 20. Loss: 18.13517189025879 Reward: -20.0\nEpsilon = 0.07284795598805613\nEpsilon = 0.07284067119245732\nEpsilon = 0.07283338712533807\nEpsilon = 0.07282610378662555\nEpsilon = 0.07281882117624688\nEpsilon = 0.07281153929412926\nEpsilon = 0.07280425814019985\nEpsilon = 0.07279697771438583\nEpsilon = 0.07278969801661439\nEpsilon = 0.07278241904681274\nEpsilon = 0.07277514080490806\nEpsilon = 0.07276786329082757\nEpsilon = 0.07276058650449849\nEpsilon = 0.07275331044584804\nEpsilon = 0.07274603511480346\nEpsilon = 0.07273876051129198\nEpsilon = 0.07273148663524086\nEpsilon = 0.07272421348657733\nEpsilon = 0.07271694106522868\nEpsilon = 0.07270966937112215\nAgent: ddqn_agent . Episode 1401/2000. Number of steps to finish: 20. Loss: 15.182807922363281 Reward: -14.0\nEpsilon = 0.07270239840418503\nEpsilon = 0.07269512816434462\nEpsilon = 0.07268785865152819\nEpsilon = 0.07268058986566303\nEpsilon = 0.07267332180667646\nEpsilon = 0.0726660544744958\nEpsilon = 0.07265878786904835\nEpsilon = 0.07265152199026144\nEpsilon = 0.07264425683806242\nEpsilon = 0.07263699241237861\nEpsilon = 0.07262972871313737\nEpsilon = 0.07262246574026605\nEpsilon = 0.07261520349369203\nEpsilon = 0.07260794197334267\nEpsilon = 0.07260068117914534\nEpsilon = 0.07259342111102743\nEpsilon = 0.07258616176891633\nEpsilon = 0.07257890315273943\nEpsilon = 0.07257164526242416\nEpsilon = 0.07256438809789792\nAgent: ddqn_agent . Episode 1402/2000. Number of steps to finish: 20. Loss: 16.900728225708008 Reward: -20.0\nEpsilon = 0.07255713165908813\nEpsilon = 0.07254987594592222\nEpsilon = 0.07254262095832763\nEpsilon = 0.0725353666962318\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.07252811315956217\nEpsilon = 0.07252086034824622\nEpsilon = 0.0725136082622114\nEpsilon = 0.07250635690138518\nEpsilon = 0.07249910626569504\nEpsilon = 0.07249185635506847\nEpsilon = 0.07248460716943296\nEpsilon = 0.07247735870871602\nEpsilon = 0.07247011097284516\nEpsilon = 0.07246286396174788\nEpsilon = 0.0724556176753517\nEpsilon = 0.07244837211358417\nEpsilon = 0.07244112727637281\nEpsilon = 0.07243388316364517\nEpsilon = 0.0724266397753288\nEpsilon = 0.07241939711135127\nAgent: ddqn_agent . Episode 1403/2000. Number of steps to finish: 20. Loss: 18.818138122558594 Reward: -14.0\nEpsilon = 0.07241215517164014\nEpsilon = 0.07240491395612297\nEpsilon = 0.07239767346472736\nEpsilon = 0.0723904336973809\nEpsilon = 0.07238319465401116\nEpsilon = 0.07237595633454576\nEpsilon = 0.0723687187389123\nEpsilon = 0.07236148186703842\nEpsilon = 0.07235424571885171\nEpsilon = 0.07234701029427983\nEpsilon = 0.0723397755932504\nEpsilon = 0.07233254161569108\nEpsilon = 0.07232530836152952\nEpsilon = 0.07231807583069337\nEpsilon = 0.0723108440231103\nEpsilon = 0.07230361293870799\nEpsilon = 0.07229638257741412\nEpsilon = 0.07228915293915639\nEpsilon = 0.07228192402386248\nEpsilon = 0.0722746958314601\nAgent: ddqn_agent . Episode 1404/2000. Number of steps to finish: 20. Loss: 18.60975456237793 Reward: -14.0\nEpsilon = 0.07226746836187695\nEpsilon = 0.07226024161504076\nEpsilon = 0.07225301559087925\nEpsilon = 0.07224579028932017\nEpsilon = 0.07223856571029123\nEpsilon = 0.07223134185372021\nEpsilon = 0.07222411871953484\nEpsilon = 0.07221689630766288\nEpsilon = 0.07220967461803211\nEpsilon = 0.0722024536505703\nEpsilon = 0.07219523340520526\nEpsilon = 0.07218801388186473\nEpsilon = 0.07218079508047655\nEpsilon = 0.0721735770009685\nEpsilon = 0.07216635964326841\nEpsilon = 0.07215914300730408\nEpsilon = 0.07215192709300336\nEpsilon = 0.07214471190029406\nEpsilon = 0.07213749742910404\nEpsilon = 0.07213028367936113\nAgent: ddqn_agent . Episode 1405/2000. Number of steps to finish: 20. Loss: 17.545467376708984 Reward: -14.0\nEpsilon = 0.07212307065099319\nEpsilon = 0.07211585834392809\nEpsilon = 0.0721086467580937\nEpsilon = 0.07210143589341789\nEpsilon = 0.07209422574982854\nEpsilon = 0.07208701632725356\nEpsilon = 0.07207980762562084\nEpsilon = 0.07207259964485828\nEpsilon = 0.07206539238489379\nEpsilon = 0.0720581858456553\nEpsilon = 0.07205098002707074\nEpsilon = 0.07204377492906804\nEpsilon = 0.07203657055157513\nEpsilon = 0.07202936689451997\nEpsilon = 0.07202216395783052\nEpsilon = 0.07201496174143474\nEpsilon = 0.0720077602452606\nEpsilon = 0.07200055946923607\nEpsilon = 0.07199335941328915\nEpsilon = 0.07198616007734782\nAgent: ddqn_agent . Episode 1406/2000. Number of steps to finish: 20. Loss: 17.460479736328125 Reward: -20.0\nEpsilon = 0.07197896146134009\nEpsilon = 0.07197176356519396\nEpsilon = 0.07196456638883744\nEpsilon = 0.07195736993219856\nEpsilon = 0.07195017419520534\nEpsilon = 0.07194297917778582\nEpsilon = 0.07193578487986804\nEpsilon = 0.07192859130138006\nEpsilon = 0.07192139844224993\nEpsilon = 0.0719142063024057\nEpsilon = 0.07190701488177546\nEpsilon = 0.07189982418028729\nEpsilon = 0.07189263419786926\nEpsilon = 0.07188544493444947\nEpsilon = 0.07187825638995603\nEpsilon = 0.07187106856431703\nEpsilon = 0.0718638814574606\nEpsilon = 0.07185669506931486\nEpsilon = 0.07184950939980793\nEpsilon = 0.07184232444886794\nAgent: ddqn_agent . Episode 1407/2000. Number of steps to finish: 20. Loss: 16.559133529663086 Reward: -18.0\nEpsilon = 0.07183514021642305\nEpsilon = 0.07182795670240141\nEpsilon = 0.07182077390673118\nEpsilon = 0.07181359182934051\nEpsilon = 0.07180641047015758\nEpsilon = 0.07179922982911056\nEpsilon = 0.07179204990612766\nEpsilon = 0.07178487070113704\nEpsilon = 0.07177769221406692\nEpsilon = 0.07177051444484552\nEpsilon = 0.07176333739340103\nEpsilon = 0.07175616105966169\nEpsilon = 0.07174898544355572\nEpsilon = 0.07174181054501137\nEpsilon = 0.07173463636395687\nEpsilon = 0.07172746290032048\nEpsilon = 0.07172029015403045\nEpsilon = 0.07171311812501505\nEpsilon = 0.07170594681320254\nEpsilon = 0.07169877621852122\nAgent: ddqn_agent . Episode 1408/2000. Number of steps to finish: 20. Loss: 18.513126373291016 Reward: -20.0\nEpsilon = 0.07169160634089937\nEpsilon = 0.07168443718026528\nEpsilon = 0.07167726873654726\nEpsilon = 0.0716701010096736\nEpsilon = 0.07166293399957263\nEpsilon = 0.07165576770617267\nEpsilon = 0.07164860212940205\nEpsilon = 0.07164143726918912\nEpsilon = 0.0716342731254622\nEpsilon = 0.07162710969814964\nEpsilon = 0.07161994698717983\nEpsilon = 0.07161278499248111\nEpsilon = 0.07160562371398187\nEpsilon = 0.07159846315161048\nEpsilon = 0.07159130330529531\nEpsilon = 0.07158414417496478\nEpsilon = 0.07157698576054729\nEpsilon = 0.07156982806197124\nEpsilon = 0.07156267107916504\nEpsilon = 0.07155551481205713\nAgent: ddqn_agent . Episode 1409/2000. Number of steps to finish: 20. Loss: 16.9323673248291 Reward: -16.0\nEpsilon = 0.07154835926057593\nEpsilon = 0.07154120442464987\nEpsilon = 0.0715340503042074\nEpsilon = 0.07152689689917698\nEpsilon = 0.07151974420948706\nEpsilon = 0.07151259223506612\nEpsilon = 0.07150544097584262\nEpsilon = 0.07149829043174503\nEpsilon = 0.07149114060270186\nEpsilon = 0.07148399148864158\nEpsilon = 0.07147684308949272\nEpsilon = 0.07146969540518378\nEpsilon = 0.07146254843564326\nEpsilon = 0.0714554021807997\nEpsilon = 0.07144825664058162\nEpsilon = 0.07144111181491757\nEpsilon = 0.07143396770373607\nEpsilon = 0.0714268243069657\nEpsilon = 0.071419681624535\nEpsilon = 0.07141253965637255\nAgent: ddqn_agent . Episode 1410/2000. Number of steps to finish: 20. Loss: 17.819169998168945 Reward: -16.0\nEpsilon = 0.07140539840240691\nEpsilon = 0.07139825786256666\nEpsilon = 0.0713911180367804\nEpsilon = 0.07138397892497672\nEpsilon = 0.07137684052708422\nEpsilon = 0.07136970284303151\nEpsilon = 0.07136256587274721\nEpsilon = 0.07135542961615994\nEpsilon = 0.07134829407319833\nEpsilon = 0.07134115924379102\nEpsilon = 0.07133402512786664\nEpsilon = 0.07132689172535386\nEpsilon = 0.07131975903618133\nEpsilon = 0.07131262706027772\nEpsilon = 0.07130549579757169\nEpsilon = 0.07129836524799193\nEpsilon = 0.07129123541146713\nEpsilon = 0.07128410628792599\nEpsilon = 0.0712769778772972\nEpsilon = 0.07126985017950947\nAgent: ddqn_agent . Episode 1411/2000. Number of steps to finish: 20. Loss: 17.699932098388672 Reward: -10.0\nEpsilon = 0.07126272319449152\nEpsilon = 0.07125559692217207\nEpsilon = 0.07124847136247985\nEpsilon = 0.0712413465153436\nEpsilon = 0.07123422238069206\nEpsilon = 0.071227098958454\nEpsilon = 0.07121997624855815\nEpsilon = 0.0712128542509333\nEpsilon = 0.0712057329655082\nEpsilon = 0.07119861239221165\nEpsilon = 0.07119149253097243\nEpsilon = 0.07118437338171933\nEpsilon = 0.07117725494438117\nEpsilon = 0.07117013721888672\nEpsilon = 0.07116302020516484\nEpsilon = 0.07115590390314432\nEpsilon = 0.07114878831275401\nEpsilon = 0.07114167343392273\nEpsilon = 0.07113455926657934\nEpsilon = 0.07112744581065268\nAgent: ddqn_agent . Episode 1412/2000. Number of steps to finish: 20. Loss: 17.223369598388672 Reward: -12.0\nEpsilon = 0.07112033306607161\nEpsilon = 0.071113221032765\nEpsilon = 0.07110610971066172\nEpsilon = 0.07109899909969065\nEpsilon = 0.07109188919978068\nEpsilon = 0.0710847800108607\nEpsilon = 0.07107767153285961\nEpsilon = 0.07107056376570632\nEpsilon = 0.07106345670932974\nEpsilon = 0.07105635036365882\nEpsilon = 0.07104924472862245\nEpsilon = 0.07104213980414958\nEpsilon = 0.07103503559016917\nEpsilon = 0.07102793208661015\nEpsilon = 0.07102082929340149\nEpsilon = 0.07101372721047215\nEpsilon = 0.07100662583775111\nEpsilon = 0.07099952517516733\nEpsilon = 0.07099242522264981\nEpsilon = 0.07098532598012755\nAgent: ddqn_agent . Episode 1413/2000. Number of steps to finish: 20. Loss: 19.60260772705078 Reward: -18.0\nEpsilon = 0.07097822744752953\nEpsilon = 0.07097112962478477\nEpsilon = 0.07096403251182229\nEpsilon = 0.07095693610857111\nEpsilon = 0.07094984041496025\nEpsilon = 0.07094274543091875\nEpsilon = 0.07093565115637565\nEpsilon = 0.07092855759126002\nEpsilon = 0.07092146473550089\nEpsilon = 0.07091437258902733\nEpsilon = 0.07090728115176843\nEpsilon = 0.07090019042365325\nEpsilon = 0.07089310040461089\nEpsilon = 0.07088601109457043\nEpsilon = 0.07087892249346098\nEpsilon = 0.07087183460121163\nEpsilon = 0.0708647474177515\nEpsilon = 0.07085766094300973\nEpsilon = 0.07085057517691543\nEpsilon = 0.07084349011939774\nAgent: ddqn_agent . Episode 1414/2000. Number of steps to finish: 20. Loss: 20.28315544128418 Reward: -20.0\nEpsilon = 0.0708364057703858\nEpsilon = 0.07082932212980876\nEpsilon = 0.07082223919759578\nEpsilon = 0.07081515697367602\nEpsilon = 0.07080807545797865\nEpsilon = 0.07080099465043285\nEpsilon = 0.07079391455096781\nEpsilon = 0.07078683515951271\nEpsilon = 0.07077975647599677\nEpsilon = 0.07077267850034917\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.07076560123249914\nEpsilon = 0.0707585246723759\nEpsilon = 0.07075144881990866\nEpsilon = 0.07074437367502667\nEpsilon = 0.07073729923765917\nEpsilon = 0.0707302255077354\nEpsilon = 0.07072315248518463\nEpsilon = 0.07071608016993611\nEpsilon = 0.07070900856191911\nEpsilon = 0.07070193766106292\nAgent: ddqn_agent . Episode 1415/2000. Number of steps to finish: 20. Loss: 16.529722213745117 Reward: -20.0\nEpsilon = 0.0706948674672968\nEpsilon = 0.07068779798055008\nEpsilon = 0.07068072920075202\nEpsilon = 0.07067366112783195\nEpsilon = 0.07066659376171917\nEpsilon = 0.07065952710234301\nEpsilon = 0.07065246114963278\nEpsilon = 0.07064539590351782\nEpsilon = 0.07063833136392747\nEpsilon = 0.07063126753079108\nEpsilon = 0.070624204404038\nEpsilon = 0.07061714198359759\nEpsilon = 0.07061008026939923\nEpsilon = 0.07060301926137229\nEpsilon = 0.07059595895944615\nEpsilon = 0.0705888993635502\nEpsilon = 0.07058184047361385\nEpsilon = 0.07057478228956648\nEpsilon = 0.07056772481133752\nEpsilon = 0.07056066803885638\nAgent: ddqn_agent . Episode 1416/2000. Number of steps to finish: 20. Loss: 17.79352569580078 Reward: -12.0\nEpsilon = 0.0705536119720525\nEpsilon = 0.0705465566108553\nEpsilon = 0.0705395019551942\nEpsilon = 0.0705324480049987\nEpsilon = 0.0705253947601982\nEpsilon = 0.07051834222072219\nEpsilon = 0.07051129038650011\nEpsilon = 0.07050423925746147\nEpsilon = 0.07049718883353571\nAgent: ddqn_agent . Episode 1417/2000. Number of steps to finish: 9. Loss: 8.201216697692871 Reward: 3.0\nEpsilon = 0.07049013911465236\nEpsilon = 0.0704830901007409\nEpsilon = 0.07047604179173082\nEpsilon = 0.07046899418755165\nEpsilon = 0.07046194728813289\nEpsilon = 0.07045490109340408\nEpsilon = 0.07044785560329474\nEpsilon = 0.07044081081773441\nEpsilon = 0.07043376673665264\nEpsilon = 0.07042672335997897\nEpsilon = 0.07041968068764298\nEpsilon = 0.07041263871957422\nEpsilon = 0.07040559745570227\nEpsilon = 0.0703985568959567\nEpsilon = 0.07039151704026711\nEpsilon = 0.07038447788856309\nEpsilon = 0.07037743944077422\nEpsilon = 0.07037040169683015\nEpsilon = 0.07036336465666046\nEpsilon = 0.0703563283201948\nAgent: ddqn_agent . Episode 1418/2000. Number of steps to finish: 20. Loss: 17.592391967773438 Reward: -12.0\nEpsilon = 0.07034929268736277\nEpsilon = 0.07034225775809404\nEpsilon = 0.07033522353231823\nEpsilon = 0.070328190009965\nEpsilon = 0.070321157190964\nEpsilon = 0.07031412507524491\nEpsilon = 0.0703070936627374\nEpsilon = 0.07030006295337111\nEpsilon = 0.07029303294707578\nEpsilon = 0.07028600364378108\nEpsilon = 0.07027897504341671\nEpsilon = 0.07027194714591237\nEpsilon = 0.07026491995119778\nEpsilon = 0.07025789345920266\nEpsilon = 0.07025086766985675\nEpsilon = 0.07024384258308976\nEpsilon = 0.07023681819883144\nEpsilon = 0.07022979451701156\nEpsilon = 0.07022277153755986\nEpsilon = 0.07021574926040611\nAgent: ddqn_agent . Episode 1419/2000. Number of steps to finish: 20. Loss: 18.196489334106445 Reward: -8.0\nEpsilon = 0.07020872768548007\nEpsilon = 0.07020170681271153\nEpsilon = 0.07019468664203025\nEpsilon = 0.07018766717336605\nEpsilon = 0.07018064840664871\nEpsilon = 0.07017363034180805\nEpsilon = 0.07016661297877387\nEpsilon = 0.070159596317476\nEpsilon = 0.07015258035784425\nEpsilon = 0.07014556509980846\nEpsilon = 0.07013855054329848\nEpsilon = 0.07013153668824415\nEpsilon = 0.07012452353457532\nEpsilon = 0.07011751108222186\nEpsilon = 0.07011049933111364\nEpsilon = 0.07010348828118053\nEpsilon = 0.07009647793235241\nEpsilon = 0.07008946828455917\nEpsilon = 0.07008245933773072\nEpsilon = 0.07007545109179694\nAgent: ddqn_agent . Episode 1420/2000. Number of steps to finish: 20. Loss: 17.71751594543457 Reward: -14.0\nEpsilon = 0.07006844354668776\nEpsilon = 0.07006143670233309\nEpsilon = 0.07005443055866285\nEpsilon = 0.07004742511560699\nEpsilon = 0.07004042037309544\nEpsilon = 0.07003341633105813\nEpsilon = 0.07002641298942502\nEpsilon = 0.07001941034812609\nEpsilon = 0.07001240840709128\nEpsilon = 0.07000540716625056\nEpsilon = 0.06999840662553394\nEpsilon = 0.0699914067848714\nEpsilon = 0.0699844076441929\nEpsilon = 0.06997740920342849\nEpsilon = 0.06997041146250814\nEpsilon = 0.06996341442136189\nEpsilon = 0.06995641807991976\nEpsilon = 0.06994942243811177\nEpsilon = 0.06994242749586796\nEpsilon = 0.06993543325311838\nAgent: ddqn_agent . Episode 1421/2000. Number of steps to finish: 20. Loss: 18.338497161865234 Reward: -12.0\nEpsilon = 0.06992843970979307\nEpsilon = 0.06992144686582209\nEpsilon = 0.06991445472113551\nEpsilon = 0.0699074632756634\nEpsilon = 0.06990047252933583\nEpsilon = 0.0698934824820829\nEpsilon = 0.06988649313383469\nEpsilon = 0.06987950448452131\nEpsilon = 0.06987251653407285\nEpsilon = 0.06986552928241944\nEpsilon = 0.0698585427294912\nEpsilon = 0.06985155687521825\nEpsilon = 0.06984457171953073\nEpsilon = 0.06983758726235878\nEpsilon = 0.06983060350363254\nEpsilon = 0.06982362044328218\nEpsilon = 0.06981663808123785\nEpsilon = 0.06980965641742973\nEpsilon = 0.06980267545178799\nEpsilon = 0.0697956951842428\nAgent: ddqn_agent . Episode 1422/2000. Number of steps to finish: 20. Loss: 19.055932998657227 Reward: -10.0\nEpsilon = 0.06978871561472438\nEpsilon = 0.06978173674316292\nEpsilon = 0.0697747585694886\nEpsilon = 0.06976778109363166\nEpsilon = 0.0697608043155223\nEpsilon = 0.06975382823509074\nEpsilon = 0.06974685285226724\nEpsilon = 0.06973987816698202\nEpsilon = 0.06973290417916532\nEpsilon = 0.0697259308887474\nEpsilon = 0.06971895829565852\nEpsilon = 0.06971198639982895\nEpsilon = 0.06970501520118896\nEpsilon = 0.06969804469966885\nEpsilon = 0.06969107489519888\nEpsilon = 0.06968410578770937\nEpsilon = 0.06967713737713059\nEpsilon = 0.06967016966339287\nEpsilon = 0.06966320264642653\nEpsilon = 0.06965623632616189\nAgent: ddqn_agent . Episode 1423/2000. Number of steps to finish: 20. Loss: 17.294897079467773 Reward: -18.0\nEpsilon = 0.06964927070252927\nEpsilon = 0.06964230577545902\nEpsilon = 0.06963534154488148\nEpsilon = 0.069628378010727\nEpsilon = 0.06962141517292592\nEpsilon = 0.06961445303140862\nEpsilon = 0.06960749158610548\nEpsilon = 0.06960053083694687\nEpsilon = 0.06959357078386318\nEpsilon = 0.0695866114267848\nEpsilon = 0.06957965276564211\nEpsilon = 0.06957269480036554\nEpsilon = 0.0695657375308855\nEpsilon = 0.06955878095713242\nEpsilon = 0.0695518250790367\nEpsilon = 0.0695448698965288\nEpsilon = 0.06953791540953916\nEpsilon = 0.06953096161799821\nEpsilon = 0.0695240085218364\nEpsilon = 0.06951705612098422\nAgent: ddqn_agent . Episode 1424/2000. Number of steps to finish: 20. Loss: 18.008129119873047 Reward: -16.0\nEpsilon = 0.06951010441537211\nEpsilon = 0.06950315340493057\nEpsilon = 0.06949620308959008\nEpsilon = 0.06948925346928111\nEpsilon = 0.06948230454393418\nEpsilon = 0.06947535631347979\nEpsilon = 0.06946840877784843\nEpsilon = 0.06946146193697066\nEpsilon = 0.06945451579077697\nEpsilon = 0.06944757033919789\nEpsilon = 0.06944062558216398\nEpsilon = 0.06943368151960576\nEpsilon = 0.0694267381514538\nEpsilon = 0.06941979547763866\nEpsilon = 0.0694128534980909\nEpsilon = 0.06940591221274109\nEpsilon = 0.06939897162151981\nEpsilon = 0.06939203172435766\nEpsilon = 0.06938509252118523\nEpsilon = 0.0693781540119331\nAgent: ddqn_agent . Episode 1425/2000. Number of steps to finish: 20. Loss: 17.791221618652344 Reward: -14.0\nEpsilon = 0.06937121619653192\nEpsilon = 0.06936427907491226\nEpsilon = 0.06935734264700477\nEpsilon = 0.06935040691274007\nEpsilon = 0.0693434718720488\nEpsilon = 0.06933653752486159\nEpsilon = 0.0693296038711091\nEpsilon = 0.069322670910722\nEpsilon = 0.06931573864363093\nEpsilon = 0.06930880706976657\nEpsilon = 0.06930187618905959\nEpsilon = 0.06929494600144068\nEpsilon = 0.06928801650684054\nEpsilon = 0.06928108770518986\nEpsilon = 0.06927415959641933\nEpsilon = 0.06926723218045969\nEpsilon = 0.06926030545724164\nEpsilon = 0.06925337942669592\nEpsilon = 0.06924645408875325\nEpsilon = 0.06923952944334438\nAgent: ddqn_agent . Episode 1426/2000. Number of steps to finish: 20. Loss: 17.996522903442383 Reward: -18.0\nEpsilon = 0.06923260549040004\nEpsilon = 0.06922568222985101\nEpsilon = 0.06921875966162802\nEpsilon = 0.06921183778566185\nEpsilon = 0.06920491660188328\nEpsilon = 0.06919799611022309\nEpsilon = 0.06919107631061207\nEpsilon = 0.069184157202981\nEpsilon = 0.0691772387872607\nEpsilon = 0.06917032106338197\nEpsilon = 0.06916340403127563\nEpsilon = 0.0691564876908725\nEpsilon = 0.06914957204210341\nEpsilon = 0.0691426570848992\nEpsilon = 0.0691357428191907\nEpsilon = 0.06912882924490878\nEpsilon = 0.06912191636198428\nEpsilon = 0.06911500417034809\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.06910809266993105\nEpsilon = 0.06910118186066407\nAgent: ddqn_agent . Episode 1427/2000. Number of steps to finish: 20. Loss: 18.93527603149414 Reward: -12.0\nEpsilon = 0.069094271742478\nEpsilon = 0.06908736231530374\nEpsilon = 0.0690804535790722\nEpsilon = 0.0690735455337143\nEpsilon = 0.06906663817916092\nEpsilon = 0.069059731515343\nEpsilon = 0.06905282554219147\nEpsilon = 0.06904592025963725\nEpsilon = 0.0690390156676113\nAgent: ddqn_agent . Episode 1428/2000. Number of steps to finish: 9. Loss: 8.690282821655273 Reward: 3.0\nEpsilon = 0.06903211176604454\nEpsilon = 0.06902520855486793\nEpsilon = 0.06901830603401245\nEpsilon = 0.06901140420340905\nEpsilon = 0.0690045030629887\nEpsilon = 0.06899760261268241\nEpsilon = 0.06899070285242115\nEpsilon = 0.06898380378213591\nEpsilon = 0.0689769054017577\nEpsilon = 0.06897000771121753\nEpsilon = 0.0689631107104464\nEpsilon = 0.06895621439937535\nEpsilon = 0.06894931877793542\nEpsilon = 0.06894242384605763\nEpsilon = 0.06893552960367302\nEpsilon = 0.06892863605071266\nEpsilon = 0.06892174318710759\nEpsilon = 0.06891485101278888\nEpsilon = 0.0689079595276876\nEpsilon = 0.06890106873173484\nAgent: ddqn_agent . Episode 1429/2000. Number of steps to finish: 20. Loss: 17.64044189453125 Reward: -14.0\nEpsilon = 0.06889417862486166\nEpsilon = 0.06888728920699917\nEpsilon = 0.06888040047807847\nEpsilon = 0.06887351243803067\nEpsilon = 0.06886662508678686\nEpsilon = 0.06885973842427817\nEpsilon = 0.06885285245043575\nEpsilon = 0.0688459671651907\nEpsilon = 0.06883908256847418\nEpsilon = 0.06883219866021734\nEpsilon = 0.06882531544035131\nEpsilon = 0.06881843290880728\nEpsilon = 0.0688115510655164\nEpsilon = 0.06880466991040984\nEpsilon = 0.0687977894434188\nEpsilon = 0.06879090966447446\nEpsilon = 0.06878403057350801\nEpsilon = 0.06877715217045066\nEpsilon = 0.06877027445523362\nEpsilon = 0.0687633974277881\nAgent: ddqn_agent . Episode 1430/2000. Number of steps to finish: 20. Loss: 17.737045288085938 Reward: -10.0\nEpsilon = 0.06875652108804532\nEpsilon = 0.06874964543593652\nEpsilon = 0.06874277047139293\nEpsilon = 0.0687358961943458\nEpsilon = 0.06872902260472637\nEpsilon = 0.0687221497024659\nEpsilon = 0.06871527748749566\nEpsilon = 0.06870840595974691\nEpsilon = 0.06870153511915093\nEpsilon = 0.06869466496563902\nEpsilon = 0.06868779549914246\nEpsilon = 0.06868092671959254\nEpsilon = 0.06867405862692058\nEpsilon = 0.0686671912210579\nEpsilon = 0.0686603245019358\nEpsilon = 0.0686534584694856\nEpsilon = 0.06864659312363866\nEpsilon = 0.0686397284643263\nEpsilon = 0.06863286449147986\nEpsilon = 0.0686260012050307\nAgent: ddqn_agent . Episode 1431/2000. Number of steps to finish: 20. Loss: 18.866697311401367 Reward: -16.0\nEpsilon = 0.0686191386049102\nEpsilon = 0.06861227669104972\nEpsilon = 0.06860541546338061\nEpsilon = 0.06859855492183428\nEpsilon = 0.06859169506634209\nEpsilon = 0.06858483589683546\nEpsilon = 0.06857797741324578\nEpsilon = 0.06857111961550445\nEpsilon = 0.0685642625035429\nEpsilon = 0.06855740607729255\nEpsilon = 0.06855055033668482\nEpsilon = 0.06854369528165115\nEpsilon = 0.06853684091212299\nEpsilon = 0.06852998722803177\nEpsilon = 0.06852313422930897\nEpsilon = 0.06851628191588605\nEpsilon = 0.06850943028769446\nEpsilon = 0.06850257934466569\nEpsilon = 0.06849572908673122\nEpsilon = 0.06848887951382256\nAgent: ddqn_agent . Episode 1432/2000. Number of steps to finish: 20. Loss: 19.40372085571289 Reward: -16.0\nEpsilon = 0.06848203062587117\nEpsilon = 0.06847518242280859\nEpsilon = 0.0684683349045663\nEpsilon = 0.06846148807107584\nEpsilon = 0.06845464192226873\nEpsilon = 0.0684477964580765\nEpsilon = 0.06844095167843069\nEpsilon = 0.06843410758326285\nEpsilon = 0.06842726417250453\nEpsilon = 0.06842042144608727\nEpsilon = 0.06841357940394266\nEpsilon = 0.06840673804600227\nEpsilon = 0.06839989737219766\nEpsilon = 0.06839305738246045\nEpsilon = 0.0683862180767222\nEpsilon = 0.06837937945491453\nEpsilon = 0.06837254151696903\nEpsilon = 0.06836570426281734\nEpsilon = 0.06835886769239105\nEpsilon = 0.06835203180562181\nAgent: ddqn_agent . Episode 1433/2000. Number of steps to finish: 20. Loss: 18.639469146728516 Reward: -16.0\nEpsilon = 0.06834519660244125\nEpsilon = 0.068338362082781\nEpsilon = 0.06833152824657272\nEpsilon = 0.06832469509374807\nEpsilon = 0.0683178626242387\nEpsilon = 0.06831103083797627\nEpsilon = 0.06830419973489248\nEpsilon = 0.06829736931491899\nEpsilon = 0.0682905395779875\nEpsilon = 0.0682837105240297\nEpsilon = 0.06827688215297731\nEpsilon = 0.06827005446476202\nEpsilon = 0.06826322745931554\nEpsilon = 0.06825640113656961\nEpsilon = 0.06824957549645595\nEpsilon = 0.06824275053890631\nEpsilon = 0.06823592626385243\nEpsilon = 0.06822910267122605\nEpsilon = 0.06822227976095893\nEpsilon = 0.06821545753298283\nAgent: ddqn_agent . Episode 1434/2000. Number of steps to finish: 20. Loss: 18.088045120239258 Reward: -18.0\nEpsilon = 0.06820863598722954\nEpsilon = 0.06820181512363081\nEpsilon = 0.06819499494211845\nEpsilon = 0.06818817544262423\nEpsilon = 0.06818135662507997\nEpsilon = 0.06817453848941747\nEpsilon = 0.06816772103556853\nEpsilon = 0.06816090426346498\nEpsilon = 0.06815408817303864\nEpsilon = 0.06814727276422133\nEpsilon = 0.0681404580369449\nEpsilon = 0.06813364399114122\nEpsilon = 0.0681268306267421\nEpsilon = 0.06812001794367943\nEpsilon = 0.06811320594188507\nEpsilon = 0.06810639462129088\nEpsilon = 0.06809958398182875\nEpsilon = 0.06809277402343057\nEpsilon = 0.06808596474602822\nEpsilon = 0.06807915614955362\nAgent: ddqn_agent . Episode 1435/2000. Number of steps to finish: 20. Loss: 17.66902732849121 Reward: -20.0\nEpsilon = 0.06807234823393866\nEpsilon = 0.06806554099911527\nEpsilon = 0.06805873444501535\nEpsilon = 0.06805192857157086\nEpsilon = 0.0680451233787137\nEpsilon = 0.06803831886637583\nEpsilon = 0.06803151503448919\nEpsilon = 0.06802471188298574\nEpsilon = 0.06801790941179744\nEpsilon = 0.06801110762085627\nEpsilon = 0.06800430651009419\nEpsilon = 0.06799750607944319\nEpsilon = 0.06799070632883525\nEpsilon = 0.06798390725820237\nEpsilon = 0.06797710886747656\nEpsilon = 0.06797031115658982\nEpsilon = 0.06796351412547416\nEpsilon = 0.06795671777406162\nEpsilon = 0.06794992210228422\nEpsilon = 0.067943127110074\nAgent: ddqn_agent . Episode 1436/2000. Number of steps to finish: 20. Loss: 18.535554885864258 Reward: -14.0\nEpsilon = 0.067936332797363\nEpsilon = 0.06792953916408326\nEpsilon = 0.06792274621016685\nEpsilon = 0.06791595393554584\nEpsilon = 0.06790916234015228\nEpsilon = 0.06790237142391826\nEpsilon = 0.06789558118677587\nEpsilon = 0.06788879162865719\nAgent: ddqn_agent . Episode 1437/2000. Number of steps to finish: 8. Loss: 7.8882622718811035 Reward: 4.0\nEpsilon = 0.06788200274949432\nEpsilon = 0.06787521454921938\nEpsilon = 0.06786842702776447\nEpsilon = 0.06786164018506169\nEpsilon = 0.06785485402104319\nEpsilon = 0.06784806853564108\nEpsilon = 0.06784128372878752\nEpsilon = 0.06783449960041464\nEpsilon = 0.0678277161504546\nEpsilon = 0.06782093337883956\nEpsilon = 0.06781415128550167\nEpsilon = 0.06780736987037311\nEpsilon = 0.06780058913338607\nEpsilon = 0.06779380907447273\nEpsilon = 0.06778702969356529\nEpsilon = 0.06778025099059592\nEpsilon = 0.06777347296549686\nEpsilon = 0.06776669561820031\nEpsilon = 0.06775991894863849\nEpsilon = 0.06775314295674363\nAgent: ddqn_agent . Episode 1438/2000. Number of steps to finish: 20. Loss: 18.398134231567383 Reward: -14.0\nEpsilon = 0.06774636764244796\nEpsilon = 0.06773959300568372\nEpsilon = 0.06773281904638315\nEpsilon = 0.0677260457644785\nEpsilon = 0.06771927315990206\nEpsilon = 0.06771250123258607\nEpsilon = 0.06770572998246281\nEpsilon = 0.06769895940946456\nEpsilon = 0.06769218951352361\nEpsilon = 0.06768542029457227\nEpsilon = 0.06767865175254281\nEpsilon = 0.06767188388736756\nEpsilon = 0.06766511669897882\nEpsilon = 0.06765835018730892\nEpsilon = 0.0676515843522902\nEpsilon = 0.06764481919385497\nEpsilon = 0.06763805471193558\nEpsilon = 0.06763129090646439\nEpsilon = 0.06762452777737374\nEpsilon = 0.067617765324596\nAgent: ddqn_agent . Episode 1439/2000. Number of steps to finish: 20. Loss: 17.878854751586914 Reward: -16.0\nEpsilon = 0.06761100354806354\nEpsilon = 0.06760424244770874\nEpsilon = 0.06759748202346397\nEpsilon = 0.06759072227526162\nEpsilon = 0.0675839632030341\nEpsilon = 0.0675772048067138\nEpsilon = 0.06757044708623312\nEpsilon = 0.0675636900415245\nEpsilon = 0.06755693367252034\nEpsilon = 0.0675501779791531\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.06754342296135518\nEpsilon = 0.06753666861905905\nEpsilon = 0.06752991495219714\nEpsilon = 0.06752316196070192\nEpsilon = 0.06751640964450585\nEpsilon = 0.0675096580035414\nEpsilon = 0.06750290703774105\nEpsilon = 0.06749615674703728\nEpsilon = 0.06748940713136257\nEpsilon = 0.06748265819064944\nAgent: ddqn_agent . Episode 1440/2000. Number of steps to finish: 20. Loss: 19.3311824798584 Reward: -20.0\nEpsilon = 0.06747590992483038\nEpsilon = 0.0674691623338379\nEpsilon = 0.06746241541760452\nEpsilon = 0.06745566917606276\nEpsilon = 0.06744892360914516\nEpsilon = 0.06744217871678425\nEpsilon = 0.06743543449891257\nEpsilon = 0.06742869095546268\nEpsilon = 0.06742194808636713\nEpsilon = 0.0674152058915585\nEpsilon = 0.06740846437096934\nEpsilon = 0.06740172352453225\nEpsilon = 0.06739498335217979\nEpsilon = 0.06738824385384458\nEpsilon = 0.0673815050294592\nEpsilon = 0.06737476687895624\nEpsilon = 0.06736802940226835\nEpsilon = 0.06736129259932813\nEpsilon = 0.0673545564700682\nEpsilon = 0.0673478210144212\nAgent: ddqn_agent . Episode 1441/2000. Number of steps to finish: 20. Loss: 18.17357635498047 Reward: -10.0\nEpsilon = 0.06734108623231976\nEpsilon = 0.06733435212369653\nEpsilon = 0.06732761868848415\nEpsilon = 0.06732088592661531\nEpsilon = 0.06731415383802265\nEpsilon = 0.06730742242263885\nEpsilon = 0.06730069168039658\nEpsilon = 0.06729396161122854\nEpsilon = 0.06728723221506741\nEpsilon = 0.0672805034918459\nEpsilon = 0.06727377544149672\nEpsilon = 0.06726704806395258\nEpsilon = 0.06726032135914618\nEpsilon = 0.06725359532701027\nEpsilon = 0.06724686996747757\nEpsilon = 0.06724014528048082\nEpsilon = 0.06723342126595278\nEpsilon = 0.06722669792382618\nEpsilon = 0.06721997525403381\nEpsilon = 0.06721325325650841\nAgent: ddqn_agent . Episode 1442/2000. Number of steps to finish: 20. Loss: 17.21507453918457 Reward: -16.0\nEpsilon = 0.06720653193118276\nEpsilon = 0.06719981127798964\nEpsilon = 0.06719309129686185\nEpsilon = 0.06718637198773217\nEpsilon = 0.0671796533505334\nEpsilon = 0.06717293538519835\nEpsilon = 0.06716621809165983\nEpsilon = 0.06715950146985067\nEpsilon = 0.06715278551970369\nEpsilon = 0.06714607024115173\nEpsilon = 0.06713935563412761\nEpsilon = 0.0671326416985642\nEpsilon = 0.06712592843439434\nEpsilon = 0.06711921584155091\nEpsilon = 0.06711250391996676\nEpsilon = 0.06710579266957477\nEpsilon = 0.06709908209030781\nEpsilon = 0.06709237218209878\nEpsilon = 0.06708566294488057\nEpsilon = 0.06707895437858608\nAgent: ddqn_agent . Episode 1443/2000. Number of steps to finish: 20. Loss: 18.98950958251953 Reward: -18.0\nEpsilon = 0.06707224648314822\nEpsilon = 0.06706553925849991\nEpsilon = 0.06705883270457406\nEpsilon = 0.06705212682130361\nEpsilon = 0.06704542160862148\nEpsilon = 0.06703871706646061\nEpsilon = 0.06703201319475396\nEpsilon = 0.06702530999343449\nEpsilon = 0.06701860746243515\nEpsilon = 0.06701190560168892\nEpsilon = 0.06700520441112875\nEpsilon = 0.06699850389068765\nEpsilon = 0.06699180404029859\nEpsilon = 0.06698510485989456\nEpsilon = 0.06697840634940858\nEpsilon = 0.06697170850877364\nEpsilon = 0.06696501133792276\nEpsilon = 0.06695831483678898\nEpsilon = 0.0669516190053053\nEpsilon = 0.06694492384340477\nAgent: ddqn_agent . Episode 1444/2000. Number of steps to finish: 20. Loss: 17.003273010253906 Reward: -14.0\nEpsilon = 0.06693822935102042\nEpsilon = 0.06693153552808533\nEpsilon = 0.06692484237453251\nEpsilon = 0.06691814989029506\nEpsilon = 0.06691145807530603\nEpsilon = 0.0669047669294985\nEpsilon = 0.06689807645280554\nEpsilon = 0.06689138664516027\nEpsilon = 0.06688469750649575\nEpsilon = 0.06687800903674511\nEpsilon = 0.06687132123584144\nEpsilon = 0.06686463410371786\nEpsilon = 0.06685794764030749\nEpsilon = 0.06685126184554346\nEpsilon = 0.06684457671935891\nEpsilon = 0.06683789226168697\nEpsilon = 0.06683120847246081\nEpsilon = 0.06682452535161357\nEpsilon = 0.06681784289907841\nEpsilon = 0.0668111611147885\nAgent: ddqn_agent . Episode 1445/2000. Number of steps to finish: 20. Loss: 17.2893123626709 Reward: -14.0\nEpsilon = 0.06680447999867703\nEpsilon = 0.06679779955067716\nEpsilon = 0.0667911197707221\nEpsilon = 0.06678444065874502\nEpsilon = 0.06677776221467914\nEpsilon = 0.06677108443845767\nEpsilon = 0.06676440733001382\nEpsilon = 0.06675773088928082\nEpsilon = 0.06675105511619189\nEpsilon = 0.06674438001068027\nEpsilon = 0.0667377055726792\nEpsilon = 0.06673103180212193\nEpsilon = 0.06672435869894172\nEpsilon = 0.06671768626307183\nEpsilon = 0.06671101449444553\nEpsilon = 0.06670434339299608\nEpsilon = 0.06669767295865678\nEpsilon = 0.06669100319136091\nEpsilon = 0.06668433409104178\nEpsilon = 0.06667766565763268\nAgent: ddqn_agent . Episode 1446/2000. Number of steps to finish: 20. Loss: 18.039777755737305 Reward: -18.0\nEpsilon = 0.06667099789106692\nEpsilon = 0.06666433079127781\nEpsilon = 0.06665766435819868\nEpsilon = 0.06665099859176286\nEpsilon = 0.06664433349190368\nEpsilon = 0.06663766905855449\nEpsilon = 0.06663100529164864\nEpsilon = 0.06662434219111948\nEpsilon = 0.06661767975690037\nEpsilon = 0.06661101798892469\nEpsilon = 0.06660435688712579\nEpsilon = 0.06659769645143708\nEpsilon = 0.06659103668179193\nEpsilon = 0.06658437757812376\nEpsilon = 0.06657771914036595\nEpsilon = 0.06657106136845191\nEpsilon = 0.06656440426231507\nEpsilon = 0.06655774782188884\nEpsilon = 0.06655109204710664\nEpsilon = 0.06654443693790194\nAgent: ddqn_agent . Episode 1447/2000. Number of steps to finish: 20. Loss: 20.476688385009766 Reward: -12.0\nEpsilon = 0.06653778249420815\nEpsilon = 0.06653112871595873\nEpsilon = 0.06652447560308714\nEpsilon = 0.06651782315552683\nEpsilon = 0.06651117137321128\nEpsilon = 0.06650452025607395\nEpsilon = 0.06649786980404834\nEpsilon = 0.06649122001706793\nEpsilon = 0.06648457089506622\nEpsilon = 0.06647792243797672\nEpsilon = 0.06647127464573292\nEpsilon = 0.06646462751826834\nEpsilon = 0.06645798105551652\nEpsilon = 0.06645133525741097\nEpsilon = 0.06644469012388522\nEpsilon = 0.06643804565487284\nEpsilon = 0.06643140185030735\nEpsilon = 0.06642475871012232\nEpsilon = 0.06641811623425131\nEpsilon = 0.06641147442262789\nAgent: ddqn_agent . Episode 1448/2000. Number of steps to finish: 20. Loss: 20.692378997802734 Reward: -20.0\nEpsilon = 0.06640483327518562\nEpsilon = 0.0663981927918581\nEpsilon = 0.06639155297257891\nEpsilon = 0.06638491381728165\nEpsilon = 0.06637827532589992\nEpsilon = 0.06637163749836733\nEpsilon = 0.0663650003346175\nEpsilon = 0.06635836383458403\nEpsilon = 0.06635172799820058\nEpsilon = 0.06634509282540076\nEpsilon = 0.06633845831611822\nEpsilon = 0.06633182447028661\nEpsilon = 0.06632519128783959\nEpsilon = 0.0663185587687108\nEpsilon = 0.06631192691283393\nEpsilon = 0.06630529572014264\nEpsilon = 0.06629866519057062\nEpsilon = 0.06629203532405156\nEpsilon = 0.06628540612051916\nEpsilon = 0.0662787775799071\nAgent: ddqn_agent . Episode 1449/2000. Number of steps to finish: 20. Loss: 19.1773738861084 Reward: -12.0\nEpsilon = 0.06627214970214912\nEpsilon = 0.0662655224871789\nEpsilon = 0.06625889593493019\nEpsilon = 0.0662522700453367\nEpsilon = 0.06624564481833216\nEpsilon = 0.06623902025385033\nEpsilon = 0.06623239635182494\nEpsilon = 0.06622577311218976\nEpsilon = 0.06621915053487853\nEpsilon = 0.06621252861982505\nEpsilon = 0.06620590736696307\nEpsilon = 0.06619928677622637\nEpsilon = 0.06619266684754875\nEpsilon = 0.066186047580864\nEpsilon = 0.0661794289761059\nEpsilon = 0.0661728110332083\nEpsilon = 0.06616619375210497\nEpsilon = 0.06615957713272977\nEpsilon = 0.0661529611750165\nEpsilon = 0.06614634587889899\nAgent: ddqn_agent . Episode 1450/2000. Number of steps to finish: 20. Loss: 17.148162841796875 Reward: -10.0\nEpsilon = 0.0661397312443111\nEpsilon = 0.06613311727118668\nEpsilon = 0.06612650395945956\nEpsilon = 0.06611989130906361\nEpsilon = 0.0661132793199327\nEpsilon = 0.06610666799200071\nEpsilon = 0.06610005732520151\nEpsilon = 0.06609344731946899\nEpsilon = 0.06608683797473705\nEpsilon = 0.06608022929093958\nEpsilon = 0.06607362126801049\nEpsilon = 0.06606701390588368\nEpsilon = 0.06606040720449309\nEpsilon = 0.06605380116377264\nEpsilon = 0.06604719578365627\nEpsilon = 0.0660405910640779\nEpsilon = 0.0660339870049715\nEpsilon = 0.066027383606271\nEpsilon = 0.06602078086791037\nEpsilon = 0.06601417878982357\nAgent: ddqn_agent . Episode 1451/2000. Number of steps to finish: 20. Loss: 19.841808319091797 Reward: -14.0\nEpsilon = 0.0660075773719446\nEpsilon = 0.06600097661420741\nEpsilon = 0.065994376516546\nEpsilon = 0.06598777707889435\nEpsilon = 0.06598117830118645\nEpsilon = 0.06597458018335634\nEpsilon = 0.065967982725338\nEpsilon = 0.06596138592706548\nEpsilon = 0.06595478978847277\nEpsilon = 0.06594819430949393\nEpsilon = 0.06594159949006298\nEpsilon = 0.06593500533011397\nEpsilon = 0.06592841182958095\nEpsilon = 0.065921818988398\nEpsilon = 0.06591522680649915\nEpsilon = 0.0659086352838185\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.06590204442029013\nEpsilon = 0.0658954542158481\nEpsilon = 0.06588886467042651\nEpsilon = 0.06588227578395947\nAgent: ddqn_agent . Episode 1452/2000. Number of steps to finish: 20. Loss: 17.106563568115234 Reward: -20.0\nEpsilon = 0.06587568755638108\nEpsilon = 0.06586909998762544\nEpsilon = 0.06586251307762668\nEpsilon = 0.06585592682631893\nEpsilon = 0.0658493412336363\nEpsilon = 0.06584275629951293\nEpsilon = 0.06583617202388298\nEpsilon = 0.06582958840668059\nEpsilon = 0.06582300544783992\nEpsilon = 0.06581642314729513\nEpsilon = 0.0658098415049804\nEpsilon = 0.0658032605208299\nEpsilon = 0.06579668019477782\nEpsilon = 0.06579010052675834\nEpsilon = 0.06578352151670566\nEpsilon = 0.06577694316455399\nEpsilon = 0.06577036547023753\nEpsilon = 0.0657637884336905\nEpsilon = 0.06575721205484714\nEpsilon = 0.06575063633364166\nAgent: ddqn_agent . Episode 1453/2000. Number of steps to finish: 20. Loss: 18.502405166625977 Reward: -18.0\nEpsilon = 0.0657440612700083\nEpsilon = 0.0657374868638813\nEpsilon = 0.06573091311519491\nEpsilon = 0.06572434002388339\nEpsilon = 0.065717767589881\nEpsilon = 0.06571119581312201\nEpsilon = 0.0657046246935407\nEpsilon = 0.06569805423107135\nEpsilon = 0.06569148442564825\nEpsilon = 0.06568491527720569\nEpsilon = 0.06567834678567797\nEpsilon = 0.0656717789509994\nEpsilon = 0.0656652117731043\nEpsilon = 0.065658645251927\nEpsilon = 0.06565207938740181\nEpsilon = 0.06564551417946307\nEpsilon = 0.06563894962804513\nEpsilon = 0.06563238573308233\nAgent: ddqn_agent . Episode 1454/2000. Number of steps to finish: 18. Loss: 15.078755378723145 Reward: -6.0\nEpsilon = 0.06562582249450903\nEpsilon = 0.06561925991225959\nEpsilon = 0.06561269798626836\nEpsilon = 0.06560613671646973\nEpsilon = 0.06559957610279808\nEpsilon = 0.0655930161451878\nEpsilon = 0.06558645684357328\nEpsilon = 0.06557989819788893\nEpsilon = 0.06557334020806914\nEpsilon = 0.06556678287404834\nEpsilon = 0.06556022619576093\nEpsilon = 0.06555367017314136\nEpsilon = 0.06554711480612405\nEpsilon = 0.06554056009464343\nEpsilon = 0.06553400603863396\nEpsilon = 0.0655274526380301\nEpsilon = 0.0655208998927663\nEpsilon = 0.06551434780277703\nEpsilon = 0.06550779636799675\nEpsilon = 0.06550124558835996\nAgent: ddqn_agent . Episode 1455/2000. Number of steps to finish: 20. Loss: 19.407609939575195 Reward: -10.0\nEpsilon = 0.06549469546380113\nEpsilon = 0.06548814599425475\nEpsilon = 0.06548159717965532\nEpsilon = 0.06547504901993735\nEpsilon = 0.06546850151503536\nEpsilon = 0.06546195466488386\nEpsilon = 0.06545540846941737\nEpsilon = 0.06544886292857043\nEpsilon = 0.06544231804227757\nEpsilon = 0.06543577381047334\nEpsilon = 0.0654292302330923\nEpsilon = 0.06542268731006899\nEpsilon = 0.06541614504133798\nEpsilon = 0.06540960342683386\nEpsilon = 0.06540306246649118\nEpsilon = 0.06539652216024452\nEpsilon = 0.06538998250802851\nEpsilon = 0.0653834435097777\nEpsilon = 0.06537690516542673\nEpsilon = 0.06537036747491018\nAgent: ddqn_agent . Episode 1456/2000. Number of steps to finish: 20. Loss: 20.616262435913086 Reward: -14.0\nEpsilon = 0.06536383043816268\nEpsilon = 0.06535729405511886\nEpsilon = 0.06535075832571335\nEpsilon = 0.06534422324988079\nEpsilon = 0.0653376888275558\nEpsilon = 0.06533115505867305\nEpsilon = 0.06532462194316718\nEpsilon = 0.06531808948097287\nEpsilon = 0.06531155767202478\nAgent: ddqn_agent . Episode 1457/2000. Number of steps to finish: 9. Loss: 8.456056594848633 Reward: 3.0\nEpsilon = 0.06530502651625758\nEpsilon = 0.06529849601360596\nEpsilon = 0.0652919661640046\nEpsilon = 0.0652854369673882\nEpsilon = 0.06527890842369147\nEpsilon = 0.0652723805328491\nEpsilon = 0.06526585329479583\nEpsilon = 0.06525932670946635\nEpsilon = 0.0652528007767954\nEpsilon = 0.06524627549671773\nEpsilon = 0.06523975086916806\nEpsilon = 0.06523322689408113\nEpsilon = 0.06522670357139172\nEpsilon = 0.06522018090103458\nEpsilon = 0.06521365888294448\nEpsilon = 0.06520713751705619\nEpsilon = 0.06520061680330448\nEpsilon = 0.06519409674162416\nEpsilon = 0.06518757733194999\nEpsilon = 0.0651810585742168\nAgent: ddqn_agent . Episode 1458/2000. Number of steps to finish: 20. Loss: 19.834808349609375 Reward: -10.0\nEpsilon = 0.06517454046835938\nEpsilon = 0.06516802301431254\nEpsilon = 0.06516150621201111\nEpsilon = 0.06515499006138992\nEpsilon = 0.06514847456238378\nEpsilon = 0.06514195971492753\nEpsilon = 0.06513544551895604\nEpsilon = 0.06512893197440416\nEpsilon = 0.06512241908120672\nEpsilon = 0.0651159068392986\nEpsilon = 0.06510939524861467\nEpsilon = 0.06510288430908981\nEpsilon = 0.0650963740206589\nEpsilon = 0.06508986438325684\nEpsilon = 0.06508335539681852\nEpsilon = 0.06507684706127884\nEpsilon = 0.06507033937657271\nEpsilon = 0.06506383234263505\nEpsilon = 0.06505732595940078\nEpsilon = 0.06505082022680483\nAgent: ddqn_agent . Episode 1459/2000. Number of steps to finish: 20. Loss: 16.031429290771484 Reward: -14.0\nEpsilon = 0.06504431514478215\nEpsilon = 0.06503781071326767\nEpsilon = 0.06503130693219634\nEpsilon = 0.06502480380150312\nEpsilon = 0.06501830132112298\nEpsilon = 0.06501179949099087\nEpsilon = 0.06500529831104176\nEpsilon = 0.06499879778121066\nEpsilon = 0.06499229790143254\nEpsilon = 0.0649857986716424\nEpsilon = 0.06497930009177524\nEpsilon = 0.06497280216176607\nEpsilon = 0.06496630488154989\nEpsilon = 0.06495980825106173\nEpsilon = 0.06495331227023662\nEpsilon = 0.0649468169390096\nEpsilon = 0.06494032225731569\nEpsilon = 0.06493382822508996\nEpsilon = 0.06492733484226745\nEpsilon = 0.06492084210878322\nAgent: ddqn_agent . Episode 1460/2000. Number of steps to finish: 20. Loss: 19.441110610961914 Reward: -16.0\nEpsilon = 0.06491435002457235\nEpsilon = 0.06490785858956989\nEpsilon = 0.06490136780371093\nEpsilon = 0.06489487766693056\nEpsilon = 0.06488838817916387\nEpsilon = 0.06488189934034595\nEpsilon = 0.06487541115041191\nEpsilon = 0.06486892360929687\nEpsilon = 0.06486243671693594\nEpsilon = 0.06485595047326424\nEpsilon = 0.06484946487821691\nEpsilon = 0.06484297993172909\nEpsilon = 0.06483649563373592\nEpsilon = 0.06483001198417254\nEpsilon = 0.06482352898297412\nEpsilon = 0.06481704663007583\nEpsilon = 0.06481056492541282\nEpsilon = 0.06480408386892028\nEpsilon = 0.06479760346053338\nEpsilon = 0.06479112370018733\nAgent: ddqn_agent . Episode 1461/2000. Number of steps to finish: 20. Loss: 18.587833404541016 Reward: -20.0\nEpsilon = 0.06478464458781731\nEpsilon = 0.06477816612335853\nEpsilon = 0.06477168830674619\nEpsilon = 0.06476521113791552\nEpsilon = 0.06475873461680172\nEpsilon = 0.06475225874334004\nEpsilon = 0.0647457835174657\nEpsilon = 0.06473930893911395\nEpsilon = 0.06473283500822004\nEpsilon = 0.06472636172471921\nEpsilon = 0.06471988908854674\nEpsilon = 0.06471341709963789\nEpsilon = 0.06470694575792793\nEpsilon = 0.06470047506335214\nEpsilon = 0.0646940050158458\nEpsilon = 0.06468753561534421\nEpsilon = 0.06468106686178268\nEpsilon = 0.0646745987550965\nEpsilon = 0.06466813129522099\nEpsilon = 0.06466166448209147\nAgent: ddqn_agent . Episode 1462/2000. Number of steps to finish: 20. Loss: 18.100955963134766 Reward: -18.0\nEpsilon = 0.06465519831564326\nEpsilon = 0.0646487327958117\nEpsilon = 0.06464226792253211\nEpsilon = 0.06463580369573986\nEpsilon = 0.06462934011537029\nEpsilon = 0.06462287718135876\nEpsilon = 0.06461641489364062\nEpsilon = 0.06460995325215126\nEpsilon = 0.06460349225682605\nEpsilon = 0.06459703190760037\nEpsilon = 0.06459057220440961\nEpsilon = 0.06458411314718918\nEpsilon = 0.06457765473587446\nEpsilon = 0.06457119697040087\nEpsilon = 0.06456473985070382\nEpsilon = 0.06455828337671875\nEpsilon = 0.06455182754838108\nEpsilon = 0.06454537236562624\nEpsilon = 0.06453891782838968\nEpsilon = 0.06453246393660685\nAgent: ddqn_agent . Episode 1463/2000. Number of steps to finish: 20. Loss: 18.484228134155273 Reward: -10.0\nEpsilon = 0.06452601069021319\nEpsilon = 0.06451955808914417\nEpsilon = 0.06451310613333526\nEpsilon = 0.06450665482272193\nEpsilon = 0.06450020415723966\nEpsilon = 0.06449375413682393\nEpsilon = 0.06448730476141025\nEpsilon = 0.0644808560309341\nEpsilon = 0.06447440794533102\nEpsilon = 0.06446796050453649\nEpsilon = 0.06446151370848603\nEpsilon = 0.06445506755711518\nEpsilon = 0.06444862205035946\nEpsilon = 0.06444217718815443\nEpsilon = 0.06443573297043562\nEpsilon = 0.06442928939713857\nEpsilon = 0.06442284646819886\nEpsilon = 0.06441640418355205\nEpsilon = 0.06440996254313369\nEpsilon = 0.06440352154687938\nAgent: ddqn_agent . Episode 1464/2000. Number of steps to finish: 20. Loss: 17.90308380126953 Reward: -14.0\nEpsilon = 0.06439708119472469\nEpsilon = 0.06439064148660521\nEpsilon = 0.06438420242245656\nEpsilon = 0.06437776400221432\nEpsilon = 0.0643713262258141\nEpsilon = 0.06436488909319152\nEpsilon = 0.0643584526042822\nEpsilon = 0.06435201675902177\nEpsilon = 0.06434558155734588\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.06433914699919015\nEpsilon = 0.06433271308449023\nEpsilon = 0.06432627981318179\nEpsilon = 0.06431984718520047\nEpsilon = 0.06431341520048195\nEpsilon = 0.0643069838589619\nEpsilon = 0.06430055316057601\nEpsilon = 0.06429412310525995\nEpsilon = 0.06428769369294943\nEpsilon = 0.06428126492358013\nEpsilon = 0.06427483679708777\nAgent: ddqn_agent . Episode 1465/2000. Number of steps to finish: 20. Loss: 20.099346160888672 Reward: -10.0\nEpsilon = 0.06426840931340806\nEpsilon = 0.06426198247247672\nEpsilon = 0.06425555627422948\nEpsilon = 0.06424913071860205\nEpsilon = 0.0642427058055302\nEpsilon = 0.06423628153494965\nEpsilon = 0.06422985790679615\nEpsilon = 0.06422343492100548\nEpsilon = 0.06421701257751337\nEpsilon = 0.06421059087625562\nEpsilon = 0.06420416981716799\nEpsilon = 0.06419774940018627\nEpsilon = 0.06419132962524625\nEpsilon = 0.06418491049228373\nEpsilon = 0.0641784920012345\nEpsilon = 0.06417207415203438\nEpsilon = 0.06416565694461918\nEpsilon = 0.06415924037892472\nEpsilon = 0.06415282445488682\nEpsilon = 0.06414640917244133\nAgent: ddqn_agent . Episode 1466/2000. Number of steps to finish: 20. Loss: 19.047767639160156 Reward: -10.0\nEpsilon = 0.06413999453152408\nEpsilon = 0.06413358053207092\nEpsilon = 0.06412716717401772\nEpsilon = 0.06412075445730031\nEpsilon = 0.06411434238185458\nEpsilon = 0.0641079309476164\nEpsilon = 0.06410152015452164\nEpsilon = 0.06409511000250619\nEpsilon = 0.06408870049150593\nEpsilon = 0.06408229162145679\nEpsilon = 0.06407588339229464\nEpsilon = 0.06406947580395542\nEpsilon = 0.06406306885637503\nEpsilon = 0.06405666254948938\nEpsilon = 0.06405025688323443\nEpsilon = 0.0640438518575461\nEpsilon = 0.06403744747236034\nEpsilon = 0.0640310437276131\nEpsilon = 0.06402464062324034\nEpsilon = 0.06401823815917802\nAgent: ddqn_agent . Episode 1467/2000. Number of steps to finish: 20. Loss: 17.019214630126953 Reward: -16.0\nEpsilon = 0.0640118363353621\nEpsilon = 0.06400543515172856\nEpsilon = 0.06399903460821339\nEpsilon = 0.06399263470475257\nEpsilon = 0.0639862354412821\nEpsilon = 0.06397983681773797\nEpsilon = 0.0639734388340562\nEpsilon = 0.06396704149017281\nEpsilon = 0.06396064478602378\nEpsilon = 0.06395424872154519\nEpsilon = 0.06394785329667303\nEpsilon = 0.06394145851134336\nEpsilon = 0.06393506436549223\nEpsilon = 0.06392867085905568\nEpsilon = 0.06392227799196977\nEpsilon = 0.06391588576417058\nEpsilon = 0.06390949417559416\nEpsilon = 0.06390310322617661\nEpsilon = 0.06389671291585398\nEpsilon = 0.0638903232445624\nAgent: ddqn_agent . Episode 1468/2000. Number of steps to finish: 20. Loss: 17.280872344970703 Reward: -10.0\nEpsilon = 0.06388393421223794\nEpsilon = 0.06387754581881672\nEpsilon = 0.06387115806423484\nEpsilon = 0.06386477094842842\nEpsilon = 0.06385838447133357\nEpsilon = 0.06385199863288644\nEpsilon = 0.06384561343302315\nEpsilon = 0.06383922887167985\nEpsilon = 0.06383284494879268\nEpsilon = 0.0638264616642978\nEpsilon = 0.06382007901813137\nEpsilon = 0.06381369701022956\nEpsilon = 0.06380731564052854\nEpsilon = 0.06380093490896449\nEpsilon = 0.0637945548154736\nEpsilon = 0.06378817535999205\nEpsilon = 0.06378179654245605\nEpsilon = 0.0637754183628018\nEpsilon = 0.06376904082096552\nEpsilon = 0.06376266391688343\nAgent: ddqn_agent . Episode 1469/2000. Number of steps to finish: 20. Loss: 18.171663284301758 Reward: -10.0\nEpsilon = 0.06375628765049174\nEpsilon = 0.0637499120217267\nEpsilon = 0.06374353703052453\nEpsilon = 0.06373716267682147\nEpsilon = 0.06373078896055379\nEpsilon = 0.06372441588165773\nEpsilon = 0.06371804344006957\nEpsilon = 0.06371167163572555\nEpsilon = 0.06370530046856199\nEpsilon = 0.06369892993851513\nEpsilon = 0.06369256004552128\nEpsilon = 0.06368619078951672\nEpsilon = 0.06367982217043777\nEpsilon = 0.06367345418822073\nEpsilon = 0.06366708684280191\nEpsilon = 0.06366072013411762\nEpsilon = 0.06365435406210421\nEpsilon = 0.063647988626698\nEpsilon = 0.06364162382783534\nEpsilon = 0.06363525966545255\nAgent: ddqn_agent . Episode 1470/2000. Number of steps to finish: 20. Loss: 19.020008087158203 Reward: -14.0\nEpsilon = 0.06362889613948601\nEpsilon = 0.06362253324987206\nEpsilon = 0.06361617099654708\nEpsilon = 0.06360980937944742\nEpsilon = 0.06360344839850948\nEpsilon = 0.06359708805366963\nEpsilon = 0.06359072834486426\nEpsilon = 0.06358436927202978\nEpsilon = 0.06357801083510257\nEpsilon = 0.06357165303401907\nEpsilon = 0.06356529586871568\nEpsilon = 0.0635589393391288\nEpsilon = 0.06355258344519489\nEpsilon = 0.06354622818685038\nEpsilon = 0.0635398735640317\nEpsilon = 0.06353351957667529\nEpsilon = 0.06352716622471762\nEpsilon = 0.06352081350809514\nEpsilon = 0.06351446142674434\nEpsilon = 0.06350810998060166\nAgent: ddqn_agent . Episode 1471/2000. Number of steps to finish: 20. Loss: 16.618427276611328 Reward: -12.0\nEpsilon = 0.0635017591696036\nEpsilon = 0.06349540899368664\nEpsilon = 0.06348905945278727\nEpsilon = 0.06348271054684199\nEpsilon = 0.0634763622757873\nEpsilon = 0.06347001463955973\nEpsilon = 0.06346366763809577\nEpsilon = 0.06345732127133197\nEpsilon = 0.06345097553920484\nEpsilon = 0.06344463044165093\nEpsilon = 0.06343828597860676\nEpsilon = 0.0634319421500089\nEpsilon = 0.0634255989557939\nEpsilon = 0.06341925639589832\nEpsilon = 0.06341291447025874\nEpsilon = 0.06340657317881171\nEpsilon = 0.06340023252149384\nEpsilon = 0.06339389249824169\nEpsilon = 0.06338755310899187\nEpsilon = 0.06338121435368098\nAgent: ddqn_agent . Episode 1472/2000. Number of steps to finish: 20. Loss: 17.658470153808594 Reward: -10.0\nEpsilon = 0.06337487623224561\nEpsilon = 0.06336853874462238\nEpsilon = 0.06336220189074791\nEpsilon = 0.06335586567055884\nEpsilon = 0.06334953008399179\nEpsilon = 0.06334319513098338\nEpsilon = 0.06333686081147029\nEpsilon = 0.06333052712538914\nEpsilon = 0.0633241940726766\nAgent: ddqn_agent . Episode 1473/2000. Number of steps to finish: 9. Loss: 8.64688491821289 Reward: 3.0\nEpsilon = 0.06331786165326933\nEpsilon = 0.063311529867104\nEpsilon = 0.0633051987141173\nEpsilon = 0.06329886819424589\nEpsilon = 0.06329253830742647\nEpsilon = 0.06328620905359572\nEpsilon = 0.06327988043269037\nEpsilon = 0.0632735524446471\nEpsilon = 0.06326722508940263\nEpsilon = 0.06326089836689369\nEpsilon = 0.063254572277057\nEpsilon = 0.0632482468198293\nEpsilon = 0.06324192199514732\nEpsilon = 0.06323559780294781\nEpsilon = 0.06322927424316752\nEpsilon = 0.0632229513157432\nEpsilon = 0.06321662902061162\nEpsilon = 0.06321030735770956\nEpsilon = 0.0632039863269738\nEpsilon = 0.0631976659283411\nAgent: ddqn_agent . Episode 1474/2000. Number of steps to finish: 20. Loss: 18.8947696685791 Reward: -12.0\nEpsilon = 0.06319134616174826\nEpsilon = 0.06318502702713209\nEpsilon = 0.06317870852442938\nEpsilon = 0.06317239065357694\nEpsilon = 0.06316607341451158\nEpsilon = 0.06315975680717013\nEpsilon = 0.06315344083148942\nEpsilon = 0.06314712548740628\nEpsilon = 0.06314081077485754\nEpsilon = 0.06313449669378006\nEpsilon = 0.06312818324411068\nEpsilon = 0.06312187042578626\nEpsilon = 0.06311555823874368\nEpsilon = 0.06310924668291981\nEpsilon = 0.06310293575825152\nEpsilon = 0.06309662546467569\nEpsilon = 0.06309031580212922\nEpsilon = 0.06308400677054901\nEpsilon = 0.06307769836987195\nEpsilon = 0.06307139060003497\nAgent: ddqn_agent . Episode 1475/2000. Number of steps to finish: 20. Loss: 18.687097549438477 Reward: -16.0\nEpsilon = 0.06306508346097497\nEpsilon = 0.06305877695262888\nEpsilon = 0.06305247107493361\nEpsilon = 0.06304616582782611\nEpsilon = 0.06303986121124333\nEpsilon = 0.0630335572251222\nEpsilon = 0.0630272538693997\nEpsilon = 0.06302095114401275\nEpsilon = 0.06301464904889835\nEpsilon = 0.06300834758399346\nEpsilon = 0.06300204674923505\nEpsilon = 0.06299574654456014\nEpsilon = 0.06298944696990569\nEpsilon = 0.0629831480252087\nEpsilon = 0.06297684971040618\nEpsilon = 0.06297055202543514\nEpsilon = 0.0629642549702326\nEpsilon = 0.06295795854473557\nEpsilon = 0.0629516627488811\nEpsilon = 0.06294536758260622\nAgent: ddqn_agent . Episode 1476/2000. Number of steps to finish: 20. Loss: 19.332141876220703 Reward: -14.0\nEpsilon = 0.06293907304584795\nEpsilon = 0.06293277913854337\nEpsilon = 0.06292648586062952\nEpsilon = 0.06292019321204345\nEpsilon = 0.06291390119272225\nEpsilon = 0.06290760980260297\nEpsilon = 0.06290131904162272\nEpsilon = 0.06289502890971856\nEpsilon = 0.06288873940682758\nEpsilon = 0.06288245053288691\nEpsilon = 0.06287616228783362\nEpsilon = 0.06286987467160483\nEpsilon = 0.06286358768413768\nEpsilon = 0.06285730132536926\nEpsilon = 0.06285101559523673\nEpsilon = 0.06284473049367721\nEpsilon = 0.06283844602062784\nEpsilon = 0.06283216217602577\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.06282587895980818\nEpsilon = 0.0628195963719122\nAgent: ddqn_agent . Episode 1477/2000. Number of steps to finish: 20. Loss: 18.63594627380371 Reward: -16.0\nEpsilon = 0.06281331441227501\nEpsilon = 0.06280703308083378\nEpsilon = 0.0628007523775257\nEpsilon = 0.06279447230228795\nEpsilon = 0.06278819285505773\nEpsilon = 0.06278191403577223\nEpsilon = 0.06277563584436865\nEpsilon = 0.06276935828078421\nEpsilon = 0.06276308134495613\nEpsilon = 0.06275680503682164\nEpsilon = 0.06275052935631796\nEpsilon = 0.06274425430338233\nEpsilon = 0.06273797987795199\nEpsilon = 0.0627317060799642\nEpsilon = 0.0627254329093562\nEpsilon = 0.06271916036606526\nEpsilon = 0.06271288845002865\nEpsilon = 0.06270661716118364\nEpsilon = 0.06270034649946753\nEpsilon = 0.06269407646481757\nAgent: ddqn_agent . Episode 1478/2000. Number of steps to finish: 20. Loss: 17.81358528137207 Reward: -14.0\nEpsilon = 0.0626878070571711\nEpsilon = 0.06268153827646539\nEpsilon = 0.06267527012263774\nEpsilon = 0.06266900259562548\nEpsilon = 0.06266273569536591\nEpsilon = 0.06265646942179638\nEpsilon = 0.0626502037748542\nEpsilon = 0.06264393875447671\nEpsilon = 0.06263767436060126\nEpsilon = 0.0626314105931652\nEpsilon = 0.06262514745210589\nEpsilon = 0.06261888493736067\nEpsilon = 0.06261262304886694\nEpsilon = 0.06260636178656206\nEpsilon = 0.0626001011503834\nEpsilon = 0.06259384114026836\nEpsilon = 0.06258758175615434\nEpsilon = 0.06258132299797872\nEpsilon = 0.06257506486567893\nEpsilon = 0.06256880735919236\nAgent: ddqn_agent . Episode 1479/2000. Number of steps to finish: 20. Loss: 17.043909072875977 Reward: -14.0\nEpsilon = 0.06256255047845644\nEpsilon = 0.0625562942234086\nEpsilon = 0.06255003859398625\nEpsilon = 0.06254378359012686\nEpsilon = 0.06253752921176785\nEpsilon = 0.06253127545884667\nEpsilon = 0.06252502233130078\nEpsilon = 0.06251876982906765\nEpsilon = 0.06251251795208475\nEpsilon = 0.06250626670028954\nEpsilon = 0.0625000160736195\nEpsilon = 0.062493766072012145\nEpsilon = 0.06248751669540494\nEpsilon = 0.0624812679437354\nEpsilon = 0.06247501981694103\nEpsilon = 0.062468772314959335\nEpsilon = 0.06246252543772784\nEpsilon = 0.062456279185184066\nEpsilon = 0.06245003355726555\nEpsilon = 0.06244378855390982\nAgent: ddqn_agent . Episode 1480/2000. Number of steps to finish: 20. Loss: 17.894527435302734 Reward: -20.0\nEpsilon = 0.062437544175054434\nEpsilon = 0.06243130042063693\nEpsilon = 0.06242505729059487\nEpsilon = 0.06241881478486581\nEpsilon = 0.06241257290338732\nEpsilon = 0.062406331646096984\nEpsilon = 0.062400091012932375\nEpsilon = 0.06239385100383108\nEpsilon = 0.0623876116187307\nEpsilon = 0.062381372857568826\nEpsilon = 0.06237513472028307\nEpsilon = 0.06236889720681104\nEpsilon = 0.06236266031709036\nEpsilon = 0.06235642405105865\nEpsilon = 0.06235018840865354\nEpsilon = 0.062343953389812676\nEpsilon = 0.0623377189944737\nEpsilon = 0.06233148522257425\nEpsilon = 0.06232525207405199\nEpsilon = 0.062319019548844584\nAgent: ddqn_agent . Episode 1481/2000. Number of steps to finish: 20. Loss: 18.207313537597656 Reward: -16.0\nEpsilon = 0.0623127876468897\nEpsilon = 0.062306556368125016\nEpsilon = 0.062300325712488204\nEpsilon = 0.06229409567991696\nEpsilon = 0.06228786627034896\nEpsilon = 0.06228163748372193\nEpsilon = 0.06227540931997356\nEpsilon = 0.06226918177904156\nEpsilon = 0.062262954860863653\nEpsilon = 0.06225672856537757\nEpsilon = 0.06225050289252103\nEpsilon = 0.06224427784223178\nEpsilon = 0.062238053414447554\nEpsilon = 0.06223182960910611\nEpsilon = 0.0622256064261452\nEpsilon = 0.06221938386550258\nEpsilon = 0.06221316192711603\nEpsilon = 0.06220694061092332\nEpsilon = 0.06220071991686223\nEpsilon = 0.06219449984487054\nAgent: ddqn_agent . Episode 1482/2000. Number of steps to finish: 20. Loss: 17.32521629333496 Reward: -14.0\nEpsilon = 0.06218828039488605\nEpsilon = 0.062182061566846566\nEpsilon = 0.06217584336068988\nEpsilon = 0.062169625776353814\nEpsilon = 0.06216340881377618\nEpsilon = 0.06215719247289481\nEpsilon = 0.062150976753647516\nEpsilon = 0.06214476165597215\nEpsilon = 0.062138547179806554\nEpsilon = 0.06213233332508857\nEpsilon = 0.06212612009175606\nEpsilon = 0.06211990747974688\nEpsilon = 0.06211369548899891\nEpsilon = 0.06210748411945001\nEpsilon = 0.06210127337103807\nEpsilon = 0.06209506324370096\nEpsilon = 0.06208885373737659\nEpsilon = 0.06208264485200285\nEpsilon = 0.06207643658751765\nEpsilon = 0.0620702289438589\nAgent: ddqn_agent . Episode 1483/2000. Number of steps to finish: 20. Loss: 16.110326766967773 Reward: -12.0\nEpsilon = 0.06206402192096452\nEpsilon = 0.06205781551877242\nEpsilon = 0.06205160973722055\nEpsilon = 0.062045404576246826\nEpsilon = 0.0620392000357892\nEpsilon = 0.062032996115785624\nAgent: ddqn_agent . Episode 1484/2000. Number of steps to finish: 6. Loss: 5.246440410614014 Reward: 6.0\nEpsilon = 0.06202679281617405\nEpsilon = 0.062020590136892435\nEpsilon = 0.06201438807787875\nEpsilon = 0.06200818663907096\nEpsilon = 0.06200198582040706\nEpsilon = 0.06199578562182502\nEpsilon = 0.06198958604326284\nEpsilon = 0.06198338708465851\nEpsilon = 0.06197718874595005\nEpsilon = 0.061970991027075456\nEpsilon = 0.06196479392797275\nEpsilon = 0.061958597448579955\nEpsilon = 0.061952401588835096\nEpsilon = 0.06194620634867622\nEpsilon = 0.06194001172804135\nEpsilon = 0.06193381772686855\nEpsilon = 0.06192762434509586\nEpsilon = 0.061921431582661356\nEpsilon = 0.06191523943950309\nEpsilon = 0.061909047915559144\nAgent: ddqn_agent . Episode 1485/2000. Number of steps to finish: 20. Loss: 15.869034767150879 Reward: -20.0\nEpsilon = 0.06190285701076759\nEpsilon = 0.06189666672506651\nEpsilon = 0.061890477058394004\nEpsilon = 0.06188428801068817\nEpsilon = 0.0618780995818871\nEpsilon = 0.06187191177192891\nEpsilon = 0.06186572458075172\nEpsilon = 0.06185953800829365\nEpsilon = 0.06185335205449282\nEpsilon = 0.061847166719287375\nEpsilon = 0.061840982002615445\nEpsilon = 0.061834797904415185\nEpsilon = 0.06182861442462474\nEpsilon = 0.06182243156318228\nEpsilon = 0.06181624932002597\nEpsilon = 0.061810067695093965\nEpsilon = 0.061803886688324455\nEpsilon = 0.06179770629965562\nEpsilon = 0.061791526529025656\nEpsilon = 0.06178534737637276\nAgent: ddqn_agent . Episode 1486/2000. Number of steps to finish: 20. Loss: 18.818275451660156 Reward: -18.0\nEpsilon = 0.06177916884163512\nEpsilon = 0.061772990924750956\nEpsilon = 0.06176681362565848\nEpsilon = 0.06176063694429591\nEpsilon = 0.061754460880601485\nEpsilon = 0.061748285434513425\nEpsilon = 0.06174211060596997\nEpsilon = 0.061735936394909374\nEpsilon = 0.061729762801269884\nEpsilon = 0.06172358982498976\nEpsilon = 0.061717417466007264\nEpsilon = 0.06171124572426066\nEpsilon = 0.061705074599688235\nEpsilon = 0.061698904092228266\nEpsilon = 0.06169273420181905\nEpsilon = 0.061686564928398864\nEpsilon = 0.061680396271906024\nEpsilon = 0.06167422823227883\nEpsilon = 0.061668060809455603\nEpsilon = 0.06166189400337466\nAgent: ddqn_agent . Episode 1487/2000. Number of steps to finish: 20. Loss: 19.124515533447266 Reward: -14.0\nEpsilon = 0.06165572781397432\nEpsilon = 0.061649562241192925\nEpsilon = 0.06164339728496881\nEpsilon = 0.06163723294524031\nEpsilon = 0.061631069221945785\nEpsilon = 0.06162490611502359\nEpsilon = 0.06161874362441209\nEpsilon = 0.06161258175004965\nEpsilon = 0.06160642049187464\nEpsilon = 0.06160025984982545\nEpsilon = 0.06159409982384047\nEpsilon = 0.06158794041385809\nEpsilon = 0.0615817816198167\nEpsilon = 0.061575623441654716\nEpsilon = 0.06156946587931055\nEpsilon = 0.06156330893272262\nEpsilon = 0.06155715260182935\nEpsilon = 0.06155099688656916\nEpsilon = 0.06154484178688051\nEpsilon = 0.061538687302701824\nAgent: ddqn_agent . Episode 1488/2000. Number of steps to finish: 20. Loss: 20.106531143188477 Reward: -10.0\nEpsilon = 0.061532533433971554\nEpsilon = 0.06152638018062816\nEpsilon = 0.06152022754261009\nEpsilon = 0.06151407551985583\nEpsilon = 0.061507924112303845\nEpsilon = 0.061501773319892615\nEpsilon = 0.06149562314256063\nEpsilon = 0.061489473580246375\nEpsilon = 0.06148332463288835\nEpsilon = 0.06147717630042506\nEpsilon = 0.06147102858279502\nEpsilon = 0.06146488147993674\nEpsilon = 0.06145873499178875\nEpsilon = 0.061452589118289574\nEpsilon = 0.061446443859377746\nEpsilon = 0.061440299214991806\nEpsilon = 0.06143415518507031\nEpsilon = 0.0614280117695518\nEpsilon = 0.06142186896837485\nEpsilon = 0.06141572678147801\nAgent: ddqn_agent . Episode 1489/2000. Number of steps to finish: 20. Loss: 17.729351043701172 Reward: -20.0\nEpsilon = 0.06140958520879986\nEpsilon = 0.06140344425027898\nEpsilon = 0.061397303905853955\nEpsilon = 0.06139116417546337\nEpsilon = 0.06138502505904583\nEpsilon = 0.061378886556539926\nEpsilon = 0.06137274866788427\nEpsilon = 0.06136661139301749\nEpsilon = 0.06136047473187819\nEpsilon = 0.061354338684405\nEpsilon = 0.06134820325053656\nEpsilon = 0.061342068430211505\nEpsilon = 0.061335934223368485\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.06132980062994615\nEpsilon = 0.06132366764988315\nEpsilon = 0.06131753528311817\nAgent: ddqn_agent . Episode 1490/2000. Number of steps to finish: 16. Loss: 14.619568824768066 Reward: -4.0\nEpsilon = 0.06131140352958986\nEpsilon = 0.0613052723892369\nEpsilon = 0.061299141861997974\nEpsilon = 0.061293011947811775\nEpsilon = 0.061286882646617\nEpsilon = 0.061280753958352335\nEpsilon = 0.0612746258829565\nEpsilon = 0.0612684984203682\nEpsilon = 0.061262371570526164\nEpsilon = 0.06125624533336911\nEpsilon = 0.061250119708835774\nEpsilon = 0.06124399469686489\nEpsilon = 0.061237870297395205\nEpsilon = 0.06123174651036547\nEpsilon = 0.061225623335714435\nEpsilon = 0.06121950077338086\nEpsilon = 0.06121337882330353\nEpsilon = 0.0612072574854212\nEpsilon = 0.061201136759672654\nEpsilon = 0.061195016645996685\nAgent: ddqn_agent . Episode 1491/2000. Number of steps to finish: 20. Loss: 18.706851959228516 Reward: -14.0\nEpsilon = 0.06118889714433209\nEpsilon = 0.061182778254617654\nEpsilon = 0.06117665997679219\nEpsilon = 0.06117054231079451\nEpsilon = 0.06116442525656343\nEpsilon = 0.061158308814037776\nEpsilon = 0.06115219298315637\nEpsilon = 0.06114607776385805\nEpsilon = 0.06113996315608167\nEpsilon = 0.06113384915976606\nEpsilon = 0.06112773577485008\nEpsilon = 0.0611216230012726\nEpsilon = 0.06111551083897247\nEpsilon = 0.061109399287888576\nEpsilon = 0.061103288347959785\nEpsilon = 0.06109717801912499\nEpsilon = 0.061091068301323076\nEpsilon = 0.061084959194492944\nEpsilon = 0.061078850698573495\nEpsilon = 0.06107274281350364\nAgent: ddqn_agent . Episode 1492/2000. Number of steps to finish: 20. Loss: 20.90180206298828 Reward: -12.0\nEpsilon = 0.061066635539222286\nEpsilon = 0.06106052887566837\nEpsilon = 0.0610544228227808\nEpsilon = 0.06104831738049852\nEpsilon = 0.06104221254876047\nEpsilon = 0.0610361083275056\nEpsilon = 0.061030004716672845\nEpsilon = 0.06102390171620118\nEpsilon = 0.06101779932602956\nEpsilon = 0.06101169754609696\nEpsilon = 0.061005596376342355\nEpsilon = 0.06099949581670472\nEpsilon = 0.060993395867123046\nEpsilon = 0.06098729652753634\nEpsilon = 0.060981197797883586\nEpsilon = 0.0609750996781038\nEpsilon = 0.06096900216813599\nEpsilon = 0.06096290526791918\nEpsilon = 0.060956808977392384\nEpsilon = 0.060950713296494645\nAgent: ddqn_agent . Episode 1493/2000. Number of steps to finish: 20. Loss: 19.06562042236328 Reward: -16.0\nEpsilon = 0.060944618225165\nEpsilon = 0.06093852376334248\nEpsilon = 0.060932429910966146\nEpsilon = 0.06092633666797505\nEpsilon = 0.06092024403430825\nEpsilon = 0.06091415200990482\nEpsilon = 0.06090806059470383\nEpsilon = 0.06090196978864436\nEpsilon = 0.060895879591665494\nEpsilon = 0.060889790003706326\nEpsilon = 0.06088370102470596\nEpsilon = 0.06087761265460349\nEpsilon = 0.060871524893338025\nEpsilon = 0.06086543774084869\nEpsilon = 0.060859351197074606\nEpsilon = 0.0608532652619549\nEpsilon = 0.0608471799354287\nEpsilon = 0.06084109521743516\nEpsilon = 0.060835011107913416\nEpsilon = 0.06082892760680263\nAgent: ddqn_agent . Episode 1494/2000. Number of steps to finish: 20. Loss: 17.938814163208008 Reward: -12.0\nEpsilon = 0.060822844714041945\nEpsilon = 0.06081676242957054\nEpsilon = 0.06081068075332759\nEpsilon = 0.06080459968525225\nEpsilon = 0.060798519225283724\nEpsilon = 0.0607924393733612\nEpsilon = 0.060786360129423865\nEpsilon = 0.06078028149341092\nEpsilon = 0.06077420346526158\nEpsilon = 0.06076812604491505\nEpsilon = 0.06076204923231056\nEpsilon = 0.060755973027387326\nEpsilon = 0.06074989743008459\nEpsilon = 0.060743822440341586\nEpsilon = 0.06073774805809755\nEpsilon = 0.06073167428329174\nEpsilon = 0.060725601115863406\nEpsilon = 0.06071952855575182\nEpsilon = 0.060713456602896246\nEpsilon = 0.06070738525723596\nAgent: ddqn_agent . Episode 1495/2000. Number of steps to finish: 20. Loss: 19.089025497436523 Reward: -16.0\nEpsilon = 0.060701314518710234\nEpsilon = 0.06069524438725837\nEpsilon = 0.06068917486281964\nEpsilon = 0.06068310594533336\nEpsilon = 0.06067703763473883\nEpsilon = 0.06067096993097535\nEpsilon = 0.06066490283398225\nEpsilon = 0.060658836343698855\nEpsilon = 0.06065277046006449\nEpsilon = 0.060646705183018484\nEpsilon = 0.060640640512500184\nEpsilon = 0.06063457644844893\nEpsilon = 0.06062851299080409\nEpsilon = 0.060622450139505006\nEpsilon = 0.06061638789449106\nEpsilon = 0.06061032625570161\nEpsilon = 0.06060426522307604\nEpsilon = 0.060598204796553735\nEpsilon = 0.06059214497607408\nEpsilon = 0.06058608576157647\nAgent: ddqn_agent . Episode 1496/2000. Number of steps to finish: 20. Loss: 17.54380989074707 Reward: -12.0\nEpsilon = 0.060580027153000315\nEpsilon = 0.06057396915028502\nEpsilon = 0.06056791175336999\nEpsilon = 0.060561854962194656\nEpsilon = 0.060555798776698436\nEpsilon = 0.060549743196820764\nEpsilon = 0.060543688222501085\nEpsilon = 0.060537633853678836\nEpsilon = 0.06053158009029347\nEpsilon = 0.06052552693228444\nEpsilon = 0.06051947437959121\nEpsilon = 0.06051342243215325\nEpsilon = 0.060507371089910035\nEpsilon = 0.06050132035280104\nEpsilon = 0.060495270220765764\nEpsilon = 0.06048922069374369\nEpsilon = 0.06048317177167432\nEpsilon = 0.06047712345449715\nEpsilon = 0.0604710757421517\nEpsilon = 0.060465028634577485\nAgent: ddqn_agent . Episode 1497/2000. Number of steps to finish: 20. Loss: 18.739341735839844 Reward: -16.0\nEpsilon = 0.06045898213171403\nEpsilon = 0.060452936233500856\nEpsilon = 0.060446890939877505\nEpsilon = 0.06044084625078352\nEpsilon = 0.060434802166158444\nEpsilon = 0.06042875868594183\nEpsilon = 0.06042271581007324\nEpsilon = 0.060416673538492234\nEpsilon = 0.06041063187113838\nEpsilon = 0.06040459080795127\nEpsilon = 0.060398550348870476\nEpsilon = 0.06039251049383559\nEpsilon = 0.060386471242786205\nEpsilon = 0.06038043259566193\nEpsilon = 0.060374394552402365\nEpsilon = 0.06036835711294713\nEpsilon = 0.060362320277235836\nEpsilon = 0.06035628404520811\nEpsilon = 0.060350248416803594\nEpsilon = 0.060344213391961915\nAgent: ddqn_agent . Episode 1498/2000. Number of steps to finish: 20. Loss: 17.800399780273438 Reward: -12.0\nEpsilon = 0.06033817897062272\nEpsilon = 0.06033214515272566\nEpsilon = 0.06032611193821039\nEpsilon = 0.06032007932701657\nEpsilon = 0.06031404731908387\nEpsilon = 0.06030801591435197\nEpsilon = 0.06030198511276053\nEpsilon = 0.06029595491424926\nEpsilon = 0.060289925318757834\nEpsilon = 0.06028389632622596\nEpsilon = 0.060277867936593336\nEpsilon = 0.06027184014979968\nEpsilon = 0.0602658129657847\nEpsilon = 0.06025978638448812\nEpsilon = 0.06025376040584967\nEpsilon = 0.06024773502980908\nEpsilon = 0.060241710256306105\nEpsilon = 0.060235686085280474\nEpsilon = 0.060229662516671945\nEpsilon = 0.06022363955042028\nAgent: ddqn_agent . Episode 1499/2000. Number of steps to finish: 20. Loss: 18.57434844970703 Reward: -18.0\nEpsilon = 0.06021761718646524\nEpsilon = 0.06021159542474659\nEpsilon = 0.06020557426520412\nEpsilon = 0.0601995537077776\nEpsilon = 0.060193533752406825\nEpsilon = 0.06018751439903158\nEpsilon = 0.06018149564759168\nEpsilon = 0.06017547749802692\nEpsilon = 0.06016945995027712\nEpsilon = 0.06016344300428209\nEpsilon = 0.060157426659981666\nEpsilon = 0.06015141091731567\nEpsilon = 0.06014539577622394\nEpsilon = 0.06013938123664631\nEpsilon = 0.06013336729852265\nEpsilon = 0.0601273539617928\nEpsilon = 0.06012134122639662\nEpsilon = 0.06011532909227398\nEpsilon = 0.06010931755936476\nEpsilon = 0.06010330662760882\nAgent: ddqn_agent . Episode 1500/2000. Number of steps to finish: 20. Loss: 17.502267837524414 Reward: -20.0\nEpsilon = 0.06009729629694606\nEpsilon = 0.06009128656731637\nEpsilon = 0.060085277438659636\nEpsilon = 0.06007926891091577\nEpsilon = 0.06007326098402468\nEpsilon = 0.06006725365792628\nEpsilon = 0.060061246932560486\nEpsilon = 0.06005524080786723\nEpsilon = 0.06004923528378644\nEpsilon = 0.06004323036025806\nEpsilon = 0.06003722603722204\nEpsilon = 0.06003122231461831\nEpsilon = 0.060025219192386854\nEpsilon = 0.06001921667046762\nEpsilon = 0.06001321474880057\nEpsilon = 0.06000721342732569\nEpsilon = 0.06000121270598296\nEpsilon = 0.05999521258471236\nEpsilon = 0.05998921306345389\nEpsilon = 0.059983214142147544\nAgent: ddqn_agent . Episode 1501/2000. Number of steps to finish: 20. Loss: 18.3613338470459 Reward: -14.0\nEpsilon = 0.05997721582073333\nEpsilon = 0.059971218099151256\nEpsilon = 0.05996522097734134\nEpsilon = 0.059959224455243605\nEpsilon = 0.05995322853279808\nEpsilon = 0.0599472332099448\nEpsilon = 0.059941238486623805\nEpsilon = 0.05993524436277514\nEpsilon = 0.05992925083833887\nEpsilon = 0.059923257913255035\nEpsilon = 0.05991726558746371\nEpsilon = 0.05991127386090496\nEpsilon = 0.05990528273351887\nEpsilon = 0.059899292205245515\nEpsilon = 0.05989330227602499\nEpsilon = 0.05988731294579739\nEpsilon = 0.05988132421450281\nAgent: ddqn_agent . Episode 1502/2000. Number of steps to finish: 17. Loss: 15.824381828308105 Reward: -5.0\nEpsilon = 0.059875336082081364\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.05986934854847316\nEpsilon = 0.05986336161361831\nEpsilon = 0.05985737527745695\nEpsilon = 0.0598513895399292\nEpsilon = 0.05984540440097521\nEpsilon = 0.05983941986053511\nEpsilon = 0.05983343591854906\nEpsilon = 0.0598274525749572\nEpsilon = 0.05982146982969971\nEpsilon = 0.05981548768271674\nEpsilon = 0.05980950613394847\nEpsilon = 0.05980352518333507\nEpsilon = 0.05979754483081674\nEpsilon = 0.05979156507633366\nEpsilon = 0.05978558591982603\nEpsilon = 0.05977960736123405\nEpsilon = 0.05977362940049793\nEpsilon = 0.05976765203755788\nEpsilon = 0.059761675272354126\nAgent: ddqn_agent . Episode 1503/2000. Number of steps to finish: 20. Loss: 20.53152847290039 Reward: -12.0\nEpsilon = 0.05975569910482689\nEpsilon = 0.05974972353491641\nEpsilon = 0.05974374856256292\nEpsilon = 0.05973777418770666\nEpsilon = 0.059731800410287895\nEpsilon = 0.059725827230246865\nEpsilon = 0.05971985464752384\nEpsilon = 0.05971388266205909\nEpsilon = 0.05970791127379289\nEpsilon = 0.05970194048266551\nEpsilon = 0.05969597028861724\nEpsilon = 0.059690000691588384\nEpsilon = 0.05968403169151922\nEpsilon = 0.05967806328835007\nEpsilon = 0.05967209548202124\nEpsilon = 0.059666128272473036\nEpsilon = 0.05966016165964579\nEpsilon = 0.05965419564347983\nEpsilon = 0.05964823022391549\nEpsilon = 0.059642265400893096\nAgent: ddqn_agent . Episode 1504/2000. Number of steps to finish: 20. Loss: 17.538074493408203 Reward: -18.0\nEpsilon = 0.05963630117435301\nEpsilon = 0.05963033754423557\nEpsilon = 0.05962437451048115\nEpsilon = 0.059618412073030105\nEpsilon = 0.0596124502318228\nEpsilon = 0.05960648898679962\nEpsilon = 0.05960052833790094\nEpsilon = 0.05959456828506715\nEpsilon = 0.059588608828238646\nEpsilon = 0.05958264996735582\nEpsilon = 0.05957669170235909\nEpsilon = 0.059570734033188855\nEpsilon = 0.059564776959785536\nEpsilon = 0.05955882048208956\nEpsilon = 0.059552864600041354\nEpsilon = 0.05954690931358135\nEpsilon = 0.059540954622649994\nEpsilon = 0.05953500052718773\nEpsilon = 0.05952904702713501\nEpsilon = 0.0595230941224323\nAgent: ddqn_agent . Episode 1505/2000. Number of steps to finish: 20. Loss: 18.198993682861328 Reward: -12.0\nEpsilon = 0.05951714181302006\nEpsilon = 0.059511190098838755\nEpsilon = 0.05950523897982887\nEpsilon = 0.05949928845593089\nEpsilon = 0.05949333852708529\nEpsilon = 0.059487389193232584\nEpsilon = 0.059481440454313264\nEpsilon = 0.05947549231026783\nEpsilon = 0.0594695447610368\nEpsilon = 0.0594635978065607\nEpsilon = 0.059457651446780044\nEpsilon = 0.05945170568163537\nEpsilon = 0.0594457605110672\nEpsilon = 0.0594398159350161\nEpsilon = 0.0594338719534226\nEpsilon = 0.05942792856622726\nEpsilon = 0.059421985773370635\nEpsilon = 0.0594160435747933\nEpsilon = 0.05941010197043582\nEpsilon = 0.05940416096023878\nAgent: ddqn_agent . Episode 1506/2000. Number of steps to finish: 20. Loss: 19.42413330078125 Reward: -16.0\nEpsilon = 0.05939822054414276\nEpsilon = 0.059392280722088346\nEpsilon = 0.059386341494016136\nEpsilon = 0.05938040285986673\nEpsilon = 0.059374464819580745\nEpsilon = 0.059368527373098785\nEpsilon = 0.059362590520361476\nAgent: ddqn_agent . Episode 1507/2000. Number of steps to finish: 7. Loss: 5.395239353179932 Reward: 5.0\nEpsilon = 0.05935665426130944\nEpsilon = 0.05935071859588331\nEpsilon = 0.059344783524023725\nEpsilon = 0.05933884904567133\nEpsilon = 0.05933291516076676\nEpsilon = 0.059326981869250686\nEpsilon = 0.05932104917106376\nEpsilon = 0.05931511706614666\nEpsilon = 0.05930918555444004\nEpsilon = 0.0593032546358846\nEpsilon = 0.059297324310421014\nEpsilon = 0.05929139457798997\nEpsilon = 0.05928546543853217\nEpsilon = 0.05927953689198832\nEpsilon = 0.05927360893829912\nEpsilon = 0.059267681577405286\nEpsilon = 0.059261754809247544\nEpsilon = 0.05925582863376662\nEpsilon = 0.05924990305090324\nEpsilon = 0.05924397806059815\nAgent: ddqn_agent . Episode 1508/2000. Number of steps to finish: 20. Loss: 17.98712158203125 Reward: -14.0\nEpsilon = 0.05923805366279209\nEpsilon = 0.05923212985742581\nEpsilon = 0.05922620664444007\nEpsilon = 0.059220284023775625\nEpsilon = 0.05921436199537325\nEpsilon = 0.059208440559173715\nEpsilon = 0.0592025197151178\nEpsilon = 0.05919659946314629\nEpsilon = 0.059190679803199975\nEpsilon = 0.05918476073521966\nEpsilon = 0.05917884225914614\nEpsilon = 0.05917292437492022\nEpsilon = 0.05916700708248273\nEpsilon = 0.05916109038177448\nEpsilon = 0.059155174272736304\nEpsilon = 0.059149258755309034\nEpsilon = 0.0591433438294335\nEpsilon = 0.05913742949505056\nEpsilon = 0.05913151575210106\nEpsilon = 0.05912560260052585\nAgent: ddqn_agent . Episode 1509/2000. Number of steps to finish: 20. Loss: 19.08745574951172 Reward: -18.0\nEpsilon = 0.0591196900402658\nEpsilon = 0.05911377807126177\nEpsilon = 0.05910786669345465\nEpsilon = 0.059101955906785304\nEpsilon = 0.059096045711194624\nEpsilon = 0.0590901361066235\nEpsilon = 0.059084227093012844\nEpsilon = 0.05907831867030354\nEpsilon = 0.059072410838436507\nEpsilon = 0.05906650359735267\nEpsilon = 0.059060596946992935\nEpsilon = 0.059054690887298235\nEpsilon = 0.05904878541820951\nEpsilon = 0.059042880539667685\nEpsilon = 0.05903697625161372\nEpsilon = 0.05903107255398856\nEpsilon = 0.05902516944673316\nEpsilon = 0.05901926692978849\nEpsilon = 0.059013365003095515\nEpsilon = 0.059007463666595206\nAgent: ddqn_agent . Episode 1510/2000. Number of steps to finish: 20. Loss: 18.19104766845703 Reward: -18.0\nEpsilon = 0.05900156292022855\nEpsilon = 0.05899566276393652\nEpsilon = 0.05898976319766013\nEpsilon = 0.05898386422134037\nEpsilon = 0.05897796583491823\nEpsilon = 0.05897206803833474\nEpsilon = 0.058966170831530905\nEpsilon = 0.05896027421444775\nEpsilon = 0.058954378187026306\nEpsilon = 0.0589484827492076\nEpsilon = 0.05894258790093268\nEpsilon = 0.05893669364214259\nEpsilon = 0.05893079997277837\nEpsilon = 0.05892490689278109\nEpsilon = 0.05891901440209182\nEpsilon = 0.05891312250065161\nEpsilon = 0.05890723118840154\nEpsilon = 0.058901340465282705\nEpsilon = 0.05889545033123618\nEpsilon = 0.058889560786203055\nAgent: ddqn_agent . Episode 1511/2000. Number of steps to finish: 20. Loss: 19.09681510925293 Reward: -20.0\nEpsilon = 0.05888367183012443\nEpsilon = 0.058877783462941424\nEpsilon = 0.05887189568459513\nEpsilon = 0.05886600849502667\nEpsilon = 0.05886012189417717\nEpsilon = 0.05885423588198775\nEpsilon = 0.058848350458399555\nEpsilon = 0.05884246562335371\nEpsilon = 0.058836581376791375\nEpsilon = 0.05883069771865369\nEpsilon = 0.058824814648881826\nEpsilon = 0.05881893216741694\nEpsilon = 0.058813050274200196\nEpsilon = 0.05880716896917278\nEpsilon = 0.05880128825227586\nEpsilon = 0.05879540812345063\nEpsilon = 0.058789528582638285\nEpsilon = 0.05878364962978002\nEpsilon = 0.05877777126481704\nEpsilon = 0.05877189348769056\nAgent: ddqn_agent . Episode 1512/2000. Number of steps to finish: 20. Loss: 17.050743103027344 Reward: -10.0\nEpsilon = 0.05876601629834179\nEpsilon = 0.05876013969671196\nEpsilon = 0.058754263682742294\nEpsilon = 0.05874838825637402\nEpsilon = 0.05874251341754838\nEpsilon = 0.058736639166206625\nEpsilon = 0.058730765502290005\nEpsilon = 0.05872489242573978\nEpsilon = 0.058719019936497203\nEpsilon = 0.05871314803450355\nEpsilon = 0.0587072767197001\nEpsilon = 0.05870140599202813\nEpsilon = 0.05869553585142893\nEpsilon = 0.058689666297843786\nEpsilon = 0.058683797331214003\nEpsilon = 0.058677928951480884\nEpsilon = 0.05867206115858574\nEpsilon = 0.05866619395246988\nEpsilon = 0.058660327333074636\nEpsilon = 0.058654461300341326\nAgent: ddqn_agent . Episode 1513/2000. Number of steps to finish: 20. Loss: 18.362003326416016 Reward: -14.0\nEpsilon = 0.05864859585421129\nEpsilon = 0.05864273099462587\nEpsilon = 0.05863686672152641\nEpsilon = 0.05863100303485426\nEpsilon = 0.05862513993455078\nEpsilon = 0.05861927742055732\nEpsilon = 0.05861341549281527\nEpsilon = 0.05860755415126599\nEpsilon = 0.05860169339585087\nEpsilon = 0.05859583322651128\nEpsilon = 0.05858997364318863\nEpsilon = 0.05858411464582431\nEpsilon = 0.05857825623435973\nEpsilon = 0.05857239840873629\nEpsilon = 0.05856654116889542\nEpsilon = 0.05856068451477853\nEpsilon = 0.05855482844632705\nEpsilon = 0.05854897296348242\nEpsilon = 0.05854311806618607\nEpsilon = 0.058537263754379454\nAgent: ddqn_agent . Episode 1514/2000. Number of steps to finish: 20. Loss: 18.091951370239258 Reward: -14.0\nEpsilon = 0.058531410028004015\nEpsilon = 0.058525556887001215\nEpsilon = 0.05851970433131252\nEpsilon = 0.05851385236087939\nEpsilon = 0.0585080009756433\nEpsilon = 0.058502150175545733\nEpsilon = 0.05849629996052818\nEpsilon = 0.05849045033053213\nEpsilon = 0.05848460128549907\nEpsilon = 0.058478752825370524\nEpsilon = 0.05847290495008799\nEpsilon = 0.05846705765959298\nEpsilon = 0.058461210953827024\nEpsilon = 0.05845536483273164\nEpsilon = 0.05844951929624837\nEpsilon = 0.05844367434431875\nEpsilon = 0.05843782997688432\nEpsilon = 0.058431986193886634\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.058426142995267244\nEpsilon = 0.05842030038096772\nAgent: ddqn_agent . Episode 1515/2000. Number of steps to finish: 20. Loss: 19.473007202148438 Reward: -10.0\nEpsilon = 0.05841445835092962\nEpsilon = 0.05840861690509453\nEpsilon = 0.05840277604340402\nEpsilon = 0.05839693576579968\nEpsilon = 0.0583910960722231\nEpsilon = 0.05838525696261588\nEpsilon = 0.05837941843691962\nEpsilon = 0.05837358049507593\nEpsilon = 0.058367743137026425\nEpsilon = 0.058361906362712726\nEpsilon = 0.05835607017207645\nEpsilon = 0.05835023456505924\nEpsilon = 0.05834439954160274\nEpsilon = 0.05833856510164858\nEpsilon = 0.058332731245138415\nEpsilon = 0.0583268979720139\nEpsilon = 0.0583210652822167\nEpsilon = 0.05831523317568848\nEpsilon = 0.05830940165237091\nEpsilon = 0.058303570712205675\nAgent: ddqn_agent . Episode 1516/2000. Number of steps to finish: 20. Loss: 18.86162567138672 Reward: -16.0\nEpsilon = 0.05829774035513446\nEpsilon = 0.05829191058109894\nEpsilon = 0.05828608139004083\nEpsilon = 0.058280252781901826\nEpsilon = 0.05827442475662364\nEpsilon = 0.05826859731414798\nEpsilon = 0.058262770454416565\nEpsilon = 0.05825694417737112\nEpsilon = 0.05825111848295338\nEpsilon = 0.05824529337110509\nEpsilon = 0.05823946884176798\nEpsilon = 0.0582336448948838\nEpsilon = 0.05822782153039431\nEpsilon = 0.05822199874824127\nEpsilon = 0.05821617654836645\nEpsilon = 0.05821035493071162\nEpsilon = 0.05820453389521855\nEpsilon = 0.058198713441829024\nEpsilon = 0.05819289357048484\nEpsilon = 0.05818707428112779\nAgent: ddqn_agent . Episode 1517/2000. Number of steps to finish: 20. Loss: 20.705902099609375 Reward: -18.0\nEpsilon = 0.05818125557369968\nEpsilon = 0.05817543744814231\nEpsilon = 0.0581696199043975\nEpsilon = 0.05816380294240706\nEpsilon = 0.05815798656211282\nEpsilon = 0.05815217076345661\nEpsilon = 0.05814635554638027\nEpsilon = 0.058140540910825635\nEpsilon = 0.058134726856734556\nEpsilon = 0.05812891338404888\nEpsilon = 0.058123100492710474\nEpsilon = 0.0581172881826612\nEpsilon = 0.05811147645384294\nEpsilon = 0.058105665306197554\nEpsilon = 0.05809985473966694\nEpsilon = 0.05809404475419297\nEpsilon = 0.05808823534971755\nEpsilon = 0.058082426526182576\nEpsilon = 0.05807661828352996\nEpsilon = 0.05807081062170161\nAgent: ddqn_agent . Episode 1518/2000. Number of steps to finish: 20. Loss: 17.935359954833984 Reward: -10.0\nEpsilon = 0.05806500354063944\nEpsilon = 0.05805919704028538\nEpsilon = 0.05805339112058135\nEpsilon = 0.0580475857814693\nEpsilon = 0.058041781022891155\nEpsilon = 0.05803597684478887\nEpsilon = 0.05803017324710439\nEpsilon = 0.05802437022977968\nEpsilon = 0.0580185677927567\nEpsilon = 0.05801276593597743\nEpsilon = 0.05800696465938383\nEpsilon = 0.05800116396291789\nEpsilon = 0.057995363846521604\nEpsilon = 0.05798956431013695\nEpsilon = 0.05798376535370594\nEpsilon = 0.05797796697717057\nEpsilon = 0.057972169180472856\nEpsilon = 0.05796637196355481\nEpsilon = 0.057960575326358454\nEpsilon = 0.05795477926882582\nAgent: ddqn_agent . Episode 1519/2000. Number of steps to finish: 20. Loss: 20.066606521606445 Reward: -16.0\nEpsilon = 0.05794898379089894\nEpsilon = 0.05794318889251985\nEpsilon = 0.0579373945736306\nEpsilon = 0.05793160083417324\nEpsilon = 0.05792580767408982\nEpsilon = 0.05792001509332241\nEpsilon = 0.05791422309181308\nEpsilon = 0.0579084316695039\nEpsilon = 0.05790264082633695\nEpsilon = 0.05789685056225431\nEpsilon = 0.057891060877198086\nEpsilon = 0.057885271771110365\nEpsilon = 0.05787948324393326\nEpsilon = 0.05787369529560887\nEpsilon = 0.05786790792607931\nEpsilon = 0.0578621211352867\nEpsilon = 0.05785633492317317\nEpsilon = 0.05785054928968086\nEpsilon = 0.05784476423475189\nEpsilon = 0.05783897975832842\nAgent: ddqn_agent . Episode 1520/2000. Number of steps to finish: 20. Loss: 19.294387817382812 Reward: -20.0\nEpsilon = 0.057833195860352585\nEpsilon = 0.05782741254076655\nEpsilon = 0.057821629799512476\nEpsilon = 0.057815847636532525\nEpsilon = 0.057810066051768874\nEpsilon = 0.057804285045163696\nEpsilon = 0.05779850461665918\nEpsilon = 0.05779272476619752\nEpsilon = 0.0577869454937209\nEpsilon = 0.05778116679917152\nEpsilon = 0.05777538868249161\nEpsilon = 0.05776961114362336\nEpsilon = 0.057763834182509\nEpsilon = 0.05775805779909075\nEpsilon = 0.057752281993310846\nEpsilon = 0.05774650676511152\nEpsilon = 0.057740732114435006\nEpsilon = 0.05773495804122356\nEpsilon = 0.05772918454541944\nEpsilon = 0.0577234116269649\nAgent: ddqn_agent . Episode 1521/2000. Number of steps to finish: 20. Loss: 19.634571075439453 Reward: -14.0\nEpsilon = 0.0577176392858022\nEpsilon = 0.05771186752187362\nEpsilon = 0.057706096335121436\nEpsilon = 0.05770032572548792\nEpsilon = 0.05769455569291537\nEpsilon = 0.05768878623734608\nEpsilon = 0.05768301735872235\nEpsilon = 0.05767724905698648\nEpsilon = 0.05767148133208078\nEpsilon = 0.05766571418394757\nEpsilon = 0.057659947612529176\nEpsilon = 0.057654181617767926\nEpsilon = 0.05764841619960615\nEpsilon = 0.05764265135798619\nEpsilon = 0.05763688709285039\nEpsilon = 0.05763112340414111\nEpsilon = 0.057625360291800695\nEpsilon = 0.057619597755771516\nEpsilon = 0.05761383579599594\nEpsilon = 0.05760807441241634\nAgent: ddqn_agent . Episode 1522/2000. Number of steps to finish: 20. Loss: 18.24087905883789 Reward: -18.0\nEpsilon = 0.0576023136049751\nEpsilon = 0.057596553373614606\nEpsilon = 0.057590793718277244\nEpsilon = 0.057585034638905415\nEpsilon = 0.057579276135441525\nEpsilon = 0.05757351820782798\nEpsilon = 0.0575677608560072\nEpsilon = 0.0575620040799216\nEpsilon = 0.057556247879513606\nEpsilon = 0.05755049225472565\nEpsilon = 0.05754473720550018\nEpsilon = 0.057538982731779625\nEpsilon = 0.05753322883350645\nEpsilon = 0.0575274755106231\nEpsilon = 0.05752172276307204\nEpsilon = 0.05751597059079573\nEpsilon = 0.05751021899373665\nEpsilon = 0.057504467971837274\nEpsilon = 0.05749871752504009\nEpsilon = 0.05749296765328759\nAgent: ddqn_agent . Episode 1523/2000. Number of steps to finish: 20. Loss: 19.678722381591797 Reward: -12.0\nEpsilon = 0.05748721835652226\nEpsilon = 0.05748146963468661\nEpsilon = 0.05747572148772314\nEpsilon = 0.05746997391557437\nEpsilon = 0.05746422691818281\nEpsilon = 0.05745848049549099\nEpsilon = 0.05745273464744144\nEpsilon = 0.057446989373976694\nEpsilon = 0.0574412446750393\nEpsilon = 0.0574355005505718\nEpsilon = 0.05742975700051674\nEpsilon = 0.057424014024816686\nEpsilon = 0.0574182716234142\nEpsilon = 0.05741252979625186\nEpsilon = 0.05740678854327223\nEpsilon = 0.05740104786441791\nEpsilon = 0.05739530775963147\nEpsilon = 0.057389568228855506\nEpsilon = 0.05738382927203262\nEpsilon = 0.05737809088910542\nAgent: ddqn_agent . Episode 1524/2000. Number of steps to finish: 20. Loss: 18.814062118530273 Reward: -14.0\nEpsilon = 0.05737235308001651\nEpsilon = 0.05736661584470851\nEpsilon = 0.05736087918312404\nEpsilon = 0.05735514309520573\nEpsilon = 0.05734940758089621\nEpsilon = 0.05734367264013812\nEpsilon = 0.05733793827287411\nEpsilon = 0.05733220447904682\nEpsilon = 0.05732647125859892\nEpsilon = 0.057320738611473056\nEpsilon = 0.05731500653761191\nEpsilon = 0.057309275036958145\nEpsilon = 0.05730354410945445\nEpsilon = 0.0572978137550435\nEpsilon = 0.057292083973668\nEpsilon = 0.057286354765270636\nEpsilon = 0.05728062612979411\nEpsilon = 0.057274898067181126\nEpsilon = 0.057269170577374406\nEpsilon = 0.05726344366031667\nAgent: ddqn_agent . Episode 1525/2000. Number of steps to finish: 20. Loss: 18.827667236328125 Reward: -20.0\nEpsilon = 0.05725771731595063\nEpsilon = 0.05725199154421904\nEpsilon = 0.05724626634506462\nEpsilon = 0.05724054171843011\nEpsilon = 0.057234817664258274\nEpsilon = 0.05722909418249185\nEpsilon = 0.0572233712730736\nEpsilon = 0.057217648935946296\nEpsilon = 0.0572119271710527\nEpsilon = 0.057206205978335596\nEpsilon = 0.057200485357737765\nEpsilon = 0.05719476530920199\nEpsilon = 0.05718904583267107\nEpsilon = 0.05718332692808781\nEpsilon = 0.057177608595395\nEpsilon = 0.057171890834535465\nEpsilon = 0.05716617364545201\nEpsilon = 0.05716045702808747\nEpsilon = 0.05715474098238466\nEpsilon = 0.057149025508286426\nAgent: ddqn_agent . Episode 1526/2000. Number of steps to finish: 20. Loss: 17.070697784423828 Reward: -16.0\nEpsilon = 0.057143310605735596\nEpsilon = 0.057137596274675025\nEpsilon = 0.057131882515047555\nEpsilon = 0.05712616932679605\nEpsilon = 0.057120456709863376\nEpsilon = 0.05711474466419239\nEpsilon = 0.057109033189725975\nEpsilon = 0.057103322286407\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.05709761195417836\nEpsilon = 0.05709190219298294\nEpsilon = 0.057086193002763644\nEpsilon = 0.057080484383463366\nEpsilon = 0.05707477633502502\nEpsilon = 0.05706906885739152\nEpsilon = 0.05706336195050578\nEpsilon = 0.057057655614310726\nEpsilon = 0.0570519498487493\nEpsilon = 0.05704624465376442\nEpsilon = 0.05704054002929904\nEpsilon = 0.057034835975296114\nAgent: ddqn_agent . Episode 1527/2000. Number of steps to finish: 20. Loss: 18.05875015258789 Reward: -14.0\nEpsilon = 0.05702913249169859\nEpsilon = 0.05702342957844942\nEpsilon = 0.057017727235491575\nEpsilon = 0.057012025462768026\nEpsilon = 0.05700632426022175\nEpsilon = 0.05700062362779573\nEpsilon = 0.05699492356543295\nEpsilon = 0.05698922407307641\nEpsilon = 0.0569835251506691\nEpsilon = 0.05697782679815404\nEpsilon = 0.05697212901547422\nEpsilon = 0.05696643180257267\nEpsilon = 0.056960735159392414\nEpsilon = 0.05695503908587648\nEpsilon = 0.05694934358196789\nEpsilon = 0.056943648647609695\nEpsilon = 0.05693795428274494\nEpsilon = 0.056932260487316665\nEpsilon = 0.056926567261267935\nEpsilon = 0.05692087460454181\nAgent: ddqn_agent . Episode 1528/2000. Number of steps to finish: 20. Loss: 19.572877883911133 Reward: -14.0\nEpsilon = 0.05691518251708136\nEpsilon = 0.05690949099882965\nEpsilon = 0.056903800049729766\nEpsilon = 0.05689810966972479\nEpsilon = 0.05689241985875782\nEpsilon = 0.05688673061677195\nEpsilon = 0.05688104194371027\nEpsilon = 0.0568753538395159\nEpsilon = 0.05686966630413195\nEpsilon = 0.056863979337501536\nEpsilon = 0.05685829293956779\nEpsilon = 0.05685260711027383\nEpsilon = 0.056846921849562804\nEpsilon = 0.05684123715737785\nEpsilon = 0.05683555303366211\nEpsilon = 0.05682986947835875\nEpsilon = 0.05682418649141091\nEpsilon = 0.05681850407276177\nEpsilon = 0.056812822222354495\nEpsilon = 0.05680714094013226\nAgent: ddqn_agent . Episode 1529/2000. Number of steps to finish: 20. Loss: 18.78955841064453 Reward: -10.0\nEpsilon = 0.05680146022603825\nEpsilon = 0.056795780080015644\nEpsilon = 0.05679010050200764\nEpsilon = 0.05678442149195744\nEpsilon = 0.05677874304980825\nEpsilon = 0.05677306517550327\nEpsilon = 0.05676738786898572\nEpsilon = 0.05676171113019882\nEpsilon = 0.0567560349590858\nEpsilon = 0.05675035935558989\nEpsilon = 0.056744684319654334\nEpsilon = 0.05673900985122237\nEpsilon = 0.056733335950237246\nEpsilon = 0.05672766261664222\nEpsilon = 0.05672198985038056\nEpsilon = 0.05671631765139552\nEpsilon = 0.056710646019630384\nAgent: ddqn_agent . Episode 1530/2000. Number of steps to finish: 17. Loss: 18.090862274169922 Reward: -5.0\nEpsilon = 0.05670497495502842\nEpsilon = 0.05669930445753292\nEpsilon = 0.056693634527087165\nEpsilon = 0.05668796516363446\nEpsilon = 0.0566822963671181\nEpsilon = 0.05667662813748139\nEpsilon = 0.05667096047466764\nEpsilon = 0.05666529337862017\nEpsilon = 0.05665962684928231\nEpsilon = 0.05665396088659738\nEpsilon = 0.056648295490508725\nEpsilon = 0.056642630660959675\nEpsilon = 0.05663696639789358\nEpsilon = 0.05663130270125379\nEpsilon = 0.05662563957098367\nEpsilon = 0.05661997700702657\nEpsilon = 0.056614315009325865\nEpsilon = 0.056608653577824936\nEpsilon = 0.05660299271246715\nEpsilon = 0.05659733241319591\nAgent: ddqn_agent . Episode 1531/2000. Number of steps to finish: 20. Loss: 18.366683959960938 Reward: -12.0\nEpsilon = 0.05659167267995459\nEpsilon = 0.05658601351268659\nEpsilon = 0.056580354911335326\nEpsilon = 0.056574696875844196\nEpsilon = 0.05656903940615661\nEpsilon = 0.05656338250221599\nEpsilon = 0.056557726163965774\nEpsilon = 0.05655207039134938\nEpsilon = 0.05654641518431024\nEpsilon = 0.056540760542791814\nEpsilon = 0.05653510646673753\nEpsilon = 0.05652945295609086\nEpsilon = 0.05652380001079525\nEpsilon = 0.05651814763079417\nEpsilon = 0.05651249581603109\nEpsilon = 0.05650684456644949\nEpsilon = 0.056501193881992845\nEpsilon = 0.05649554376260465\nEpsilon = 0.05648989420822839\nEpsilon = 0.05648424521880757\nAgent: ddqn_agent . Episode 1532/2000. Number of steps to finish: 20. Loss: 17.18475341796875 Reward: -14.0\nEpsilon = 0.05647859679428569\nEpsilon = 0.05647294893460626\nEpsilon = 0.0564673016397128\nEpsilon = 0.05646165490954883\nEpsilon = 0.05645600874405788\nEpsilon = 0.056450363143183475\nEpsilon = 0.05644471810686916\nEpsilon = 0.05643907363505847\nEpsilon = 0.05643342972769497\nEpsilon = 0.0564277863847222\nEpsilon = 0.056422143606083726\nEpsilon = 0.05641650139172312\nEpsilon = 0.05641085974158395\nEpsilon = 0.05640521865560979\nEpsilon = 0.05639957813374423\nEpsilon = 0.05639393817593086\nEpsilon = 0.056388298782113266\nEpsilon = 0.056382659952235056\nEpsilon = 0.056377021686239835\nEpsilon = 0.05637138398407121\nAgent: ddqn_agent . Episode 1533/2000. Number of steps to finish: 20. Loss: 20.173465728759766 Reward: -12.0\nEpsilon = 0.056365746845672804\nEpsilon = 0.05636011027098824\nEpsilon = 0.05635447425996114\nEpsilon = 0.05634883881253514\nEpsilon = 0.05634320392865389\nEpsilon = 0.05633756960826103\nEpsilon = 0.056331935851300204\nEpsilon = 0.05632630265771507\nEpsilon = 0.0563206700274493\nEpsilon = 0.05631503796044656\nEpsilon = 0.056309406456650515\nEpsilon = 0.05630377551600485\nEpsilon = 0.05629814513845325\nEpsilon = 0.056292515323939404\nEpsilon = 0.05628688607240701\nEpsilon = 0.05628125738379977\nEpsilon = 0.05627562925806139\nEpsilon = 0.05627000169513559\nEpsilon = 0.05626437469496608\nEpsilon = 0.056258748257496584\nAgent: ddqn_agent . Episode 1534/2000. Number of steps to finish: 20. Loss: 18.242835998535156 Reward: -16.0\nEpsilon = 0.056253122382670835\nEpsilon = 0.05624749707043257\nEpsilon = 0.05624187232072553\nEpsilon = 0.056236248133493455\nEpsilon = 0.056230624508680105\nEpsilon = 0.05622500144622924\nEpsilon = 0.056219378946084614\nEpsilon = 0.05621375700819001\nEpsilon = 0.05620813563248919\nEpsilon = 0.05620251481892594\nEpsilon = 0.056196894567444054\nEpsilon = 0.05619127487798731\nEpsilon = 0.05618565575049951\nEpsilon = 0.05618003718492446\nEpsilon = 0.05617441918120597\nEpsilon = 0.05616880173928785\nEpsilon = 0.056163184859113924\nEpsilon = 0.05615756854062801\nEpsilon = 0.056151952783773953\nEpsilon = 0.056146337588495576\nAgent: ddqn_agent . Episode 1535/2000. Number of steps to finish: 20. Loss: 21.11516571044922 Reward: -10.0\nEpsilon = 0.056140722954736724\nEpsilon = 0.05613510888244125\nEpsilon = 0.056129495371553005\nEpsilon = 0.05612388242201585\nEpsilon = 0.05611827003377365\nEpsilon = 0.05611265820677027\nEpsilon = 0.0561070469409496\nEpsilon = 0.056101436236255506\nEpsilon = 0.05609582609263188\nEpsilon = 0.05609021651002262\nEpsilon = 0.056084607488371616\nEpsilon = 0.056078999027622776\nEpsilon = 0.056073391127720015\nEpsilon = 0.056067783788607245\nEpsilon = 0.056062177010228385\nEpsilon = 0.05605657079252736\nEpsilon = 0.05605096513544811\nEpsilon = 0.056045360038934566\nEpsilon = 0.05603975550293067\nEpsilon = 0.05603415152738038\nAgent: ddqn_agent . Episode 1536/2000. Number of steps to finish: 20. Loss: 17.41999626159668 Reward: -14.0\nEpsilon = 0.05602854811222764\nEpsilon = 0.05602294525741642\nEpsilon = 0.056017342962890675\nEpsilon = 0.05601174122859438\nEpsilon = 0.05600614005447153\nEpsilon = 0.05600053944046608\nEpsilon = 0.05599493938652204\nEpsilon = 0.05598933989258339\nEpsilon = 0.05598374095859413\nEpsilon = 0.05597814258449827\nEpsilon = 0.05597254477023982\nEpsilon = 0.0559669475157628\nEpsilon = 0.055961350821011224\nEpsilon = 0.05595575468592912\nEpsilon = 0.05595015911046053\nEpsilon = 0.05594456409454948\nEpsilon = 0.055938969638140025\nEpsilon = 0.05593337574117621\nEpsilon = 0.05592778240360209\nEpsilon = 0.05592218962536173\nAgent: ddqn_agent . Episode 1537/2000. Number of steps to finish: 20. Loss: 17.03191375732422 Reward: -16.0\nEpsilon = 0.0559165974063992\nEpsilon = 0.05591100574665856\nEpsilon = 0.055905414646083895\nEpsilon = 0.055899824104619286\nEpsilon = 0.05589423412220883\nEpsilon = 0.055888644698796605\nEpsilon = 0.05588305583432673\nEpsilon = 0.05587746752874329\nEpsilon = 0.055871879781990416\nEpsilon = 0.05586629259401222\nEpsilon = 0.055860705964752816\nEpsilon = 0.05585511989415634\nEpsilon = 0.05584953438216693\nEpsilon = 0.055843949428728716\nEpsilon = 0.05583836503378584\nEpsilon = 0.055832781197282466\nEpsilon = 0.05582719791916274\nEpsilon = 0.05582161519937082\nEpsilon = 0.05581603303785089\nEpsilon = 0.055810451434547104\nAgent: ddqn_agent . Episode 1538/2000. Number of steps to finish: 20. Loss: 18.53900718688965 Reward: -10.0\nEpsilon = 0.05580487038940365\nEpsilon = 0.055799289902364706\nEpsilon = 0.05579370997337447\nEpsilon = 0.05578813060237713\nEpsilon = 0.055782551789316896\nEpsilon = 0.055776973534137964\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.05577139583678455\nEpsilon = 0.055765818697200875\nEpsilon = 0.055760242115331156\nEpsilon = 0.05575466609111962\nEpsilon = 0.05574909062451051\nEpsilon = 0.055743515715448055\nEpsilon = 0.05573794136387651\nEpsilon = 0.05573236756974012\nEpsilon = 0.05572679433298315\nEpsilon = 0.055721221653549854\nEpsilon = 0.0557156495313845\nEpsilon = 0.05571007796643136\nEpsilon = 0.05570450695863472\nEpsilon = 0.055698936507938855\nAgent: ddqn_agent . Episode 1539/2000. Number of steps to finish: 20. Loss: 20.225318908691406 Reward: -12.0\nEpsilon = 0.05569336661428806\nEpsilon = 0.055687797277626634\nEpsilon = 0.055682228497898875\nEpsilon = 0.055676660275049084\nEpsilon = 0.05567109260902158\nEpsilon = 0.05566552549976068\nEpsilon = 0.0556599589472107\nEpsilon = 0.05565439295131598\nEpsilon = 0.05564882751202085\nEpsilon = 0.05564326262926965\nEpsilon = 0.05563769830300672\nEpsilon = 0.055632134533176426\nEpsilon = 0.05562657131972311\nEpsilon = 0.055621008662591134\nEpsilon = 0.055615446561724874\nEpsilon = 0.055609885017068705\nEpsilon = 0.055604324028567\nEpsilon = 0.05559876359616414\nEpsilon = 0.05559320371980452\nEpsilon = 0.05558764439943254\nAgent: ddqn_agent . Episode 1540/2000. Number of steps to finish: 20. Loss: 19.571645736694336 Reward: -18.0\nEpsilon = 0.055582085634992597\nEpsilon = 0.0555765274264291\nEpsilon = 0.05557096977368646\nEpsilon = 0.05556541267670909\nEpsilon = 0.05555985613544142\nEpsilon = 0.05555430014982787\nEpsilon = 0.055548744719812894\nEpsilon = 0.055543189845340915\nEpsilon = 0.055537635526356384\nAgent: ddqn_agent . Episode 1541/2000. Number of steps to finish: 9. Loss: 8.107587814331055 Reward: 3.0\nEpsilon = 0.05553208176280375\nEpsilon = 0.05552652855462747\nEpsilon = 0.055520975901772004\nEpsilon = 0.05551542380418183\nEpsilon = 0.05550987226180141\nEpsilon = 0.055504321274575226\nEpsilon = 0.05549877084244777\nEpsilon = 0.05549322096536352\nEpsilon = 0.055487671643266984\nEpsilon = 0.05548212287610266\nEpsilon = 0.055476574663815045\nEpsilon = 0.055471027006348664\nEpsilon = 0.05546547990364803\nEpsilon = 0.055459933355657666\nEpsilon = 0.055454387362322104\nEpsilon = 0.05544884192358587\nEpsilon = 0.05544329703939351\nEpsilon = 0.05543775270968957\nEpsilon = 0.0554322089344186\nEpsilon = 0.05542666571352516\nAgent: ddqn_agent . Episode 1542/2000. Number of steps to finish: 20. Loss: 18.19210433959961 Reward: -18.0\nEpsilon = 0.05542112304695381\nEpsilon = 0.055415580934649114\nEpsilon = 0.05541003937655565\nEpsilon = 0.055404498372617995\nEpsilon = 0.05539895792278073\nEpsilon = 0.05539341802698845\nEpsilon = 0.05538787868518576\nEpsilon = 0.05538233989731724\nEpsilon = 0.05537680166332751\nEpsilon = 0.055371263983161174\nEpsilon = 0.05536572685676286\nEpsilon = 0.05536019028407718\nEpsilon = 0.05535465426504878\nEpsilon = 0.055349118799622274\nEpsilon = 0.05534358388774231\nEpsilon = 0.05533804952935354\nEpsilon = 0.05533251572440061\nEpsilon = 0.05532698247282817\nEpsilon = 0.05532144977458089\nEpsilon = 0.05531591762960343\nAgent: ddqn_agent . Episode 1543/2000. Number of steps to finish: 20. Loss: 19.022348403930664 Reward: -12.0\nEpsilon = 0.05531038603784047\nEpsilon = 0.05530485499923669\nEpsilon = 0.055299324513736765\nEpsilon = 0.055293794581285395\nEpsilon = 0.055288265201827266\nEpsilon = 0.05528273637530708\nEpsilon = 0.055277208101669556\nEpsilon = 0.05527168038085939\nEpsilon = 0.0552661532128213\nEpsilon = 0.05526062659750002\nEpsilon = 0.05525510053484027\nEpsilon = 0.055249575024786786\nEpsilon = 0.055244050067284306\nEpsilon = 0.05523852566227758\nEpsilon = 0.05523300180971135\nEpsilon = 0.05522747850953038\nEpsilon = 0.055221955761679424\nEpsilon = 0.05521643356610326\nEpsilon = 0.055210911922746644\nEpsilon = 0.05520539083155437\nAgent: ddqn_agent . Episode 1544/2000. Number of steps to finish: 20. Loss: 18.09188461303711 Reward: -12.0\nEpsilon = 0.05519987029247121\nEpsilon = 0.055194350305441967\nEpsilon = 0.055188830870411425\nEpsilon = 0.05518331198732438\nEpsilon = 0.05517779365612565\nEpsilon = 0.05517227587676004\nEpsilon = 0.055166758649172365\nEpsilon = 0.05516124197330745\nEpsilon = 0.05515572584911012\nEpsilon = 0.05515021027652521\nEpsilon = 0.05514469525549756\nEpsilon = 0.05513918078597201\nEpsilon = 0.055133666867893415\nEpsilon = 0.055128153501206624\nEpsilon = 0.055122640685856504\nEpsilon = 0.05511712842178792\nEpsilon = 0.05511161670894574\nEpsilon = 0.055106105547274846\nEpsilon = 0.05510059493672012\nEpsilon = 0.05509508487722645\nAgent: ddqn_agent . Episode 1545/2000. Number of steps to finish: 20. Loss: 20.229028701782227 Reward: -14.0\nEpsilon = 0.05508957536873872\nEpsilon = 0.05508406641120185\nEpsilon = 0.05507855800456073\nEpsilon = 0.05507305014876028\nEpsilon = 0.055067542843745404\nEpsilon = 0.05506203608946103\nEpsilon = 0.055056529885852086\nEpsilon = 0.0550510242328635\nEpsilon = 0.055045519130440215\nEpsilon = 0.05504001457852717\nEpsilon = 0.05503451057706932\nEpsilon = 0.05502900712601161\nEpsilon = 0.05502350422529901\nEpsilon = 0.05501800187487648\nEpsilon = 0.055012500074689\nEpsilon = 0.055006998824681526\nEpsilon = 0.055001498124799056\nEpsilon = 0.054995997974986575\nEpsilon = 0.05499049837518908\nEpsilon = 0.054984999325351565\nAgent: ddqn_agent . Episode 1546/2000. Number of steps to finish: 20. Loss: 18.624048233032227 Reward: -10.0\nEpsilon = 0.05497950082541903\nEpsilon = 0.05497400287533649\nEpsilon = 0.05496850547504896\nEpsilon = 0.05496300862450146\nEpsilon = 0.05495751232363901\nEpsilon = 0.05495201657240665\nEpsilon = 0.05494652137074941\nEpsilon = 0.05494102671861233\nEpsilon = 0.054935532615940474\nEpsilon = 0.05493003906267888\nEpsilon = 0.054924546058772616\nEpsilon = 0.05491905360416674\nEpsilon = 0.054913561698806325\nEpsilon = 0.05490807034263644\nEpsilon = 0.05490257953560218\nEpsilon = 0.05489708927764862\nEpsilon = 0.05489159956872086\nEpsilon = 0.05488611040876399\nEpsilon = 0.054880621797723114\nEpsilon = 0.05487513373554334\nAgent: ddqn_agent . Episode 1547/2000. Number of steps to finish: 20. Loss: 20.125566482543945 Reward: -14.0\nEpsilon = 0.05486964622216979\nEpsilon = 0.054864159257547576\nEpsilon = 0.054858672841621825\nEpsilon = 0.054853186974337664\nEpsilon = 0.05484770165564023\nEpsilon = 0.054842216885474665\nEpsilon = 0.054836732663786115\nEpsilon = 0.054831248990519735\nEpsilon = 0.054825765865620685\nEpsilon = 0.05482028328903412\nEpsilon = 0.05481480126070522\nEpsilon = 0.054809319780579145\nEpsilon = 0.054803838848601086\nEpsilon = 0.05479835846471623\nEpsilon = 0.05479287862886976\nEpsilon = 0.05478739934100687\nEpsilon = 0.054781920601072766\nEpsilon = 0.05477644240901266\nEpsilon = 0.05477096476477176\nEpsilon = 0.05476548766829528\nAgent: ddqn_agent . Episode 1548/2000. Number of steps to finish: 20. Loss: 19.98023223876953 Reward: -10.0\nEpsilon = 0.054760011119528454\nEpsilon = 0.0547545351184165\nEpsilon = 0.054749059664904655\nEpsilon = 0.05474358475893817\nEpsilon = 0.054738110400462274\nEpsilon = 0.05473263658942223\nEpsilon = 0.05472716332576329\nEpsilon = 0.05472169060943071\nEpsilon = 0.054716218440369765\nEpsilon = 0.05471074681852573\nEpsilon = 0.05470527574384388\nEpsilon = 0.0546998052162695\nEpsilon = 0.054694335235747876\nEpsilon = 0.054688865802224304\nEpsilon = 0.054683396915644084\nEpsilon = 0.05467792857595252\nEpsilon = 0.05467246078309493\nEpsilon = 0.05466699353701662\nEpsilon = 0.05466152683766292\nEpsilon = 0.054656060684979156\nAgent: ddqn_agent . Episode 1549/2000. Number of steps to finish: 20. Loss: 20.25391960144043 Reward: -14.0\nEpsilon = 0.05465059507891066\nEpsilon = 0.05464513001940277\nEpsilon = 0.054639665506400835\nEpsilon = 0.05463420153985019\nEpsilon = 0.05462873811969621\nEpsilon = 0.054623275245884235\nEpsilon = 0.05461781291835965\nEpsilon = 0.05461235113706781\nEpsilon = 0.05460688990195411\nEpsilon = 0.05460142921296391\nEpsilon = 0.05459596907004261\nEpsilon = 0.05459050947313561\nEpsilon = 0.054585050422188294\nEpsilon = 0.05457959191714608\nEpsilon = 0.054574133957954366\nEpsilon = 0.05456867654455857\nEpsilon = 0.05456321967690411\nEpsilon = 0.05455776335493642\nEpsilon = 0.05455230757860093\nEpsilon = 0.054546852347843074\nAgent: ddqn_agent . Episode 1550/2000. Number of steps to finish: 20. Loss: 20.419574737548828 Reward: -10.0\nEpsilon = 0.05454139766260829\nEpsilon = 0.05453594352284203\nEpsilon = 0.054530489928489745\nEpsilon = 0.0545250368794969\nEpsilon = 0.05451958437580895\nEpsilon = 0.054514132417371367\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.05450868100412963\nEpsilon = 0.054503230136029215\nEpsilon = 0.054497779813015616\nEpsilon = 0.054492330035034314\nEpsilon = 0.05448688080203081\nEpsilon = 0.05448143211395061\nAgent: ddqn_agent . Episode 1551/2000. Number of steps to finish: 12. Loss: 11.76232624053955 Reward: 0.0\nEpsilon = 0.05447598397073922\nEpsilon = 0.05447053637234214\nEpsilon = 0.05446508931870491\nEpsilon = 0.05445964280977304\nEpsilon = 0.05445419684549207\nEpsilon = 0.05444875142580752\nEpsilon = 0.05444330655066494\nEpsilon = 0.054437862220009875\nEpsilon = 0.05443241843378788\nEpsilon = 0.0544269751919445\nEpsilon = 0.0544215324944253\nEpsilon = 0.05441609034117586\nEpsilon = 0.05441064873214174\nEpsilon = 0.05440520766726853\nEpsilon = 0.054399767146501805\nEpsilon = 0.054394327169787156\nEpsilon = 0.05438888773707018\nEpsilon = 0.054383448848296476\nEpsilon = 0.05437801050341164\nEpsilon = 0.0543725727023613\nAgent: ddqn_agent . Episode 1552/2000. Number of steps to finish: 20. Loss: 19.240962982177734 Reward: -14.0\nEpsilon = 0.054367135445091065\nEpsilon = 0.05436169873154656\nEpsilon = 0.054356262561673405\nEpsilon = 0.054350826935417235\nEpsilon = 0.05434539185272369\nEpsilon = 0.05433995731353842\nEpsilon = 0.05433452331780707\nEpsilon = 0.054329089865475286\nEpsilon = 0.05432365695648874\nEpsilon = 0.05431822459079309\nEpsilon = 0.05431279276833401\nEpsilon = 0.05430736148905718\nEpsilon = 0.054301930752908276\nEpsilon = 0.05429650055983298\nEpsilon = 0.054291070909777\nEpsilon = 0.054285641802686024\nEpsilon = 0.05428021323850576\nEpsilon = 0.05427478521718191\nEpsilon = 0.05426935773866019\nEpsilon = 0.05426393080288633\nAgent: ddqn_agent . Episode 1553/2000. Number of steps to finish: 20. Loss: 18.436594009399414 Reward: -14.0\nEpsilon = 0.05425850440980604\nEpsilon = 0.054253078559365064\nEpsilon = 0.054247653251509126\nEpsilon = 0.05424222848618398\nEpsilon = 0.05423680426333536\nEpsilon = 0.05423138058290903\nEpsilon = 0.05422595744485074\nEpsilon = 0.05422053484910625\nEpsilon = 0.05421511279562134\nEpsilon = 0.054209691284341784\nEpsilon = 0.05420427031521335\nEpsilon = 0.05419884988818183\nEpsilon = 0.05419343000319301\nEpsilon = 0.05418801066019269\nEpsilon = 0.05418259185912667\nEpsilon = 0.05417717359994076\nEpsilon = 0.05417175588258077\nEpsilon = 0.05416633870699251\nEpsilon = 0.05416092207312181\nEpsilon = 0.054155505980914495\nAgent: ddqn_agent . Episode 1554/2000. Number of steps to finish: 20. Loss: 19.76578140258789 Reward: -20.0\nEpsilon = 0.054150090430316405\nEpsilon = 0.054144675421273374\nEpsilon = 0.05413926095373125\nEpsilon = 0.05413384702763588\nEpsilon = 0.05412843364293311\nEpsilon = 0.05412302079956882\nEpsilon = 0.05411760849748887\nEpsilon = 0.05411219673663912\nEpsilon = 0.054106785516965454\nEpsilon = 0.054101374838413756\nEpsilon = 0.054095964700929915\nEpsilon = 0.05409055510445982\nEpsilon = 0.05408514604894937\nEpsilon = 0.05407973753434448\nEpsilon = 0.05407432956059105\nEpsilon = 0.05406892212763499\nEpsilon = 0.05406351523542223\nEpsilon = 0.05405810888389869\nEpsilon = 0.0540527030730103\nEpsilon = 0.054047297802703\nAgent: ddqn_agent . Episode 1555/2000. Number of steps to finish: 20. Loss: 20.659244537353516 Reward: -10.0\nEpsilon = 0.05404189307292273\nEpsilon = 0.05403648888361544\nEpsilon = 0.05403108523472708\nEpsilon = 0.05402568212620361\nEpsilon = 0.05402027955799099\nEpsilon = 0.05401487753003519\nEpsilon = 0.05400947604228219\nEpsilon = 0.054004075094677965\nEpsilon = 0.0539986746871685\nEpsilon = 0.05399327481969978\nEpsilon = 0.05398787549221781\nEpsilon = 0.05398247670466859\nEpsilon = 0.053977078456998125\nEpsilon = 0.05397168074915243\nEpsilon = 0.053966283581077516\nEpsilon = 0.05396088695271941\nEpsilon = 0.05395549086402414\nEpsilon = 0.05395009531493774\nEpsilon = 0.05394470030540625\nEpsilon = 0.053939305835375706\nAgent: ddqn_agent . Episode 1556/2000. Number of steps to finish: 20. Loss: 17.35517120361328 Reward: -14.0\nEpsilon = 0.053933911904792166\nEpsilon = 0.05392851851360169\nEpsilon = 0.05392312566175033\nEpsilon = 0.053917733349184156\nEpsilon = 0.05391234157584924\nEpsilon = 0.053906950341691656\nEpsilon = 0.053901559646657486\nEpsilon = 0.05389616949069282\nEpsilon = 0.05389077987374375\nEpsilon = 0.053885390795756376\nEpsilon = 0.0538800022566768\nEpsilon = 0.05387461425645113\nEpsilon = 0.05386922679502549\nEpsilon = 0.053863839872345984\nEpsilon = 0.05385845348835875\nEpsilon = 0.053853067643009914\nEpsilon = 0.053847682336245616\nEpsilon = 0.05384229756801199\nEpsilon = 0.05383691333825519\nEpsilon = 0.05383152964692136\nAgent: ddqn_agent . Episode 1557/2000. Number of steps to finish: 20. Loss: 20.525733947753906 Reward: -18.0\nEpsilon = 0.053826146493956666\nEpsilon = 0.05382076387930727\nEpsilon = 0.053815381802919345\nEpsilon = 0.053810000264739054\nEpsilon = 0.05380461926471258\nEpsilon = 0.053799238802786115\nEpsilon = 0.05379385887890584\nEpsilon = 0.053788479493017946\nEpsilon = 0.053783100645068645\nEpsilon = 0.053777722335004136\nEpsilon = 0.053772344562770635\nEpsilon = 0.05376696732831436\nEpsilon = 0.05376159063158153\nEpsilon = 0.05375621447251837\nEpsilon = 0.05375083885107112\nEpsilon = 0.05374546376718601\nEpsilon = 0.05374008922080929\nEpsilon = 0.05373471521188721\nEpsilon = 0.053729341740366025\nEpsilon = 0.05372396880619199\nAgent: ddqn_agent . Episode 1558/2000. Number of steps to finish: 20. Loss: 18.503463745117188 Reward: -14.0\nEpsilon = 0.05371859640931138\nEpsilon = 0.053713224549670445\nEpsilon = 0.053707853227215475\nEpsilon = 0.053702482441892754\nEpsilon = 0.05369711219364857\nEpsilon = 0.0536917424824292\nEpsilon = 0.05368637330818096\nEpsilon = 0.053681004670850144\nEpsilon = 0.05367563657038306\nEpsilon = 0.05367026900672602\nEpsilon = 0.05366490197982535\nEpsilon = 0.05365953548962737\nEpsilon = 0.053654169536078405\nEpsilon = 0.0536488041191248\nEpsilon = 0.05364343923871289\nEpsilon = 0.05363807489478902\nEpsilon = 0.05363271108729954\nEpsilon = 0.053627347816190815\nEpsilon = 0.053621985081409196\nEpsilon = 0.053616622882901054\nAgent: ddqn_agent . Episode 1559/2000. Number of steps to finish: 20. Loss: 17.79827308654785 Reward: -18.0\nEpsilon = 0.053611261220612764\nEpsilon = 0.053605900094490705\nEpsilon = 0.05360053950448126\nEpsilon = 0.053595179450530814\nEpsilon = 0.05358981993258576\nEpsilon = 0.05358446095059251\nEpsilon = 0.05357910250449745\nEpsilon = 0.053573744594247\nEpsilon = 0.05356838721978757\nEpsilon = 0.05356303038106559\nEpsilon = 0.05355767407802749\nEpsilon = 0.053552318310619684\nEpsilon = 0.053546963078788624\nEpsilon = 0.053541608382480746\nEpsilon = 0.0535362542216425\nEpsilon = 0.053530900596220336\nEpsilon = 0.05352554750616072\nEpsilon = 0.053520194951410104\nEpsilon = 0.05351484293191496\nEpsilon = 0.05350949144762177\nAgent: ddqn_agent . Episode 1560/2000. Number of steps to finish: 20. Loss: 17.718441009521484 Reward: -20.0\nEpsilon = 0.05350414049847701\nEpsilon = 0.053498790084427164\nEpsilon = 0.05349344020541872\nEpsilon = 0.05348809086139818\nEpsilon = 0.053482742052312036\nEpsilon = 0.053477393778106806\nEpsilon = 0.053472046038728994\nEpsilon = 0.05346669883412512\nAgent: ddqn_agent . Episode 1561/2000. Number of steps to finish: 8. Loss: 7.907462120056152 Reward: 4.0\nEpsilon = 0.053461352164241714\nEpsilon = 0.05345600602902529\nEpsilon = 0.053450660428422385\nEpsilon = 0.05344531536237954\nEpsilon = 0.05343997083084331\nEpsilon = 0.053434626833760226\nEpsilon = 0.05342928337107685\nEpsilon = 0.053423940442739744\nEpsilon = 0.05341859804869547\nEpsilon = 0.0534132561888906\nEpsilon = 0.05340791486327171\nEpsilon = 0.053402574071785384\nEpsilon = 0.053397233814378206\nEpsilon = 0.05339189409099677\nEpsilon = 0.05338655490158767\nEpsilon = 0.05338121624609751\nEpsilon = 0.053375878124472906\nEpsilon = 0.05337054053666046\nEpsilon = 0.053365203482606795\nEpsilon = 0.053359866962258536\nAgent: ddqn_agent . Episode 1562/2000. Number of steps to finish: 20. Loss: 17.863645553588867 Reward: -14.0\nEpsilon = 0.05335453097556231\nEpsilon = 0.053349195522464754\nEpsilon = 0.05334386060291251\nEpsilon = 0.05333852621685222\nEpsilon = 0.053333192364230536\nEpsilon = 0.05332785904499411\nEpsilon = 0.05332252625908961\nEpsilon = 0.0533171940064637\nEpsilon = 0.053311862287063055\nEpsilon = 0.05330653110083435\nEpsilon = 0.05330120044772427\nEpsilon = 0.0532958703276795\nEpsilon = 0.05329054074064673\nEpsilon = 0.053285211686572666\nEpsilon = 0.05327988316540401\nEpsilon = 0.05327455517708747\nEpsilon = 0.053269227721569765\nEpsilon = 0.05326390079879761\nEpsilon = 0.05325857440871773\nEpsilon = 0.05325324855127686\nAgent: ddqn_agent . Episode 1563/2000. Number of steps to finish: 20. Loss: 20.627344131469727 Reward: -14.0\nEpsilon = 0.05324792322642173\nEpsilon = 0.05324259843409909\nEpsilon = 0.05323727417425568\nEpsilon = 0.053231950446838254\nEpsilon = 0.05322662725179357\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.053221304589068395\nEpsilon = 0.053215982458609486\nEpsilon = 0.05321066086036363\nEpsilon = 0.05320533979427759\nEpsilon = 0.053200019260298166\nEpsilon = 0.053194699258372134\nEpsilon = 0.053189379788446296\nEpsilon = 0.05318406085046745\nEpsilon = 0.05317874244438241\nAgent: ddqn_agent . Episode 1564/2000. Number of steps to finish: 14. Loss: 15.113895416259766 Reward: -2.0\nEpsilon = 0.05317342457013797\nEpsilon = 0.05316810722768096\nEpsilon = 0.05316279041695819\nEpsilon = 0.053157474137916494\nEpsilon = 0.053152158390502705\nEpsilon = 0.05314684317466366\nEpsilon = 0.053141528490346195\nEpsilon = 0.05313621433749716\nEpsilon = 0.05313090071606341\nEpsilon = 0.05312558762599181\nEpsilon = 0.05312027506722921\nEpsilon = 0.05311496303972249\nEpsilon = 0.05310965154341852\nEpsilon = 0.05310434057826418\nEpsilon = 0.053099030144206354\nEpsilon = 0.05309372024119193\nEpsilon = 0.05308841086916781\nEpsilon = 0.05308310202808089\nEpsilon = 0.05307779371787808\nEpsilon = 0.053072485938506296\nAgent: ddqn_agent . Episode 1565/2000. Number of steps to finish: 20. Loss: 20.15242576599121 Reward: -18.0\nEpsilon = 0.053067178689912446\nEpsilon = 0.053061871972043456\nEpsilon = 0.05305656578484625\nEpsilon = 0.05305126012826777\nEpsilon = 0.05304595500225494\nEpsilon = 0.053040650406754715\nEpsilon = 0.05303534634171404\nEpsilon = 0.053030042807079875\nEpsilon = 0.05302473980279917\nEpsilon = 0.05301943732881889\nEpsilon = 0.05301413538508601\nEpsilon = 0.0530088339715475\nEpsilon = 0.05300353308815035\nEpsilon = 0.052998232734841534\nEpsilon = 0.05299293291156805\nEpsilon = 0.052987633618276894\nEpsilon = 0.05298233485491507\nEpsilon = 0.05297703662142958\nEpsilon = 0.052971738917767436\nEpsilon = 0.05296644174387566\nAgent: ddqn_agent . Episode 1566/2000. Number of steps to finish: 20. Loss: 18.734689712524414 Reward: -14.0\nEpsilon = 0.05296114509970127\nEpsilon = 0.052955848985191296\nEpsilon = 0.052950553400292776\nEpsilon = 0.05294525834495275\nEpsilon = 0.05293996381911825\nEpsilon = 0.05293466982273634\nEpsilon = 0.052929376355754065\nEpsilon = 0.05292408341811849\nEpsilon = 0.05291879100977668\nAgent: ddqn_agent . Episode 1567/2000. Number of steps to finish: 9. Loss: 9.60330867767334 Reward: 3.0\nEpsilon = 0.0529134991306757\nEpsilon = 0.052908207780762634\nEpsilon = 0.05290291695998456\nEpsilon = 0.05289762666828856\nEpsilon = 0.05289233690562173\nEpsilon = 0.05288704767193117\nEpsilon = 0.052881758967163975\nEpsilon = 0.05287647079126726\nEpsilon = 0.052871183144188136\nEpsilon = 0.052865896025873715\nEpsilon = 0.052860609436271126\nEpsilon = 0.0528553233753275\nEpsilon = 0.05285003784298997\nEpsilon = 0.05284475283920567\nEpsilon = 0.052839468363921754\nEpsilon = 0.052834184417085366\nEpsilon = 0.05282890099864366\nEpsilon = 0.052823618108543795\nEpsilon = 0.05281833574673294\nEpsilon = 0.05281305391315827\nAgent: ddqn_agent . Episode 1568/2000. Number of steps to finish: 20. Loss: 19.350629806518555 Reward: -18.0\nEpsilon = 0.05280777260776696\nEpsilon = 0.052802491830506185\nEpsilon = 0.05279721158132313\nEpsilon = 0.052791931860165\nEpsilon = 0.05278665266697899\nEpsilon = 0.05278137400171229\nEpsilon = 0.05277609586431212\nEpsilon = 0.05277081825472569\nEpsilon = 0.052765541172900214\nEpsilon = 0.05276026461878292\nEpsilon = 0.05275498859232104\nEpsilon = 0.05274971309346181\nEpsilon = 0.052744438122152466\nEpsilon = 0.05273916367834025\nEpsilon = 0.052733889761972415\nEpsilon = 0.052728616372996216\nEpsilon = 0.05272334351135892\nEpsilon = 0.05271807117700778\nEpsilon = 0.05271279936989008\nEpsilon = 0.05270752808995309\nAgent: ddqn_agent . Episode 1569/2000. Number of steps to finish: 20. Loss: 17.579069137573242 Reward: -16.0\nEpsilon = 0.05270225733714409\nEpsilon = 0.05269698711141038\nEpsilon = 0.05269171741269924\nEpsilon = 0.05268644824095797\nEpsilon = 0.05268117959613388\nEpsilon = 0.05267591147817427\nEpsilon = 0.052670643887026455\nEpsilon = 0.052665376822637756\nEpsilon = 0.05266011028495549\nEpsilon = 0.052654844273927\nEpsilon = 0.05264957878949961\nEpsilon = 0.05264431383162066\nEpsilon = 0.052639049400237496\nEpsilon = 0.05263378549529747\nEpsilon = 0.05262852211674794\nEpsilon = 0.052623259264536267\nEpsilon = 0.052617996938609815\nEpsilon = 0.052612735138915956\nEpsilon = 0.052607473865402064\nEpsilon = 0.05260221311801552\nAgent: ddqn_agent . Episode 1570/2000. Number of steps to finish: 20. Loss: 18.530988693237305 Reward: -18.0\nEpsilon = 0.05259695289670372\nEpsilon = 0.052591693201414054\nEpsilon = 0.05258643403209391\nEpsilon = 0.052581175388690705\nEpsilon = 0.05257591727115184\nEpsilon = 0.052570659679424724\nEpsilon = 0.05256540261345678\nEpsilon = 0.05256014607319544\nEpsilon = 0.05255489005858812\nEpsilon = 0.05254963456958226\nEpsilon = 0.052544379606125305\nEpsilon = 0.05253912516816469\nEpsilon = 0.052533871255647874\nEpsilon = 0.05252861786852231\nEpsilon = 0.052523365006735455\nEpsilon = 0.05251811267023478\nEpsilon = 0.05251286085896776\nEpsilon = 0.052507609572881864\nEpsilon = 0.052502358811924575\nEpsilon = 0.05249710857604338\nAgent: ddqn_agent . Episode 1571/2000. Number of steps to finish: 20. Loss: 19.246501922607422 Reward: -12.0\nEpsilon = 0.05249185886518578\nEpsilon = 0.05248660967929926\nEpsilon = 0.05248136101833133\nEpsilon = 0.052476112882229496\nEpsilon = 0.052470865270941275\nEpsilon = 0.052465618184414184\nEpsilon = 0.052460371622595744\nEpsilon = 0.052455125585433485\nEpsilon = 0.05244988007287494\nEpsilon = 0.05244463508486765\nEpsilon = 0.05243939062135917\nEpsilon = 0.05243414668229703\nEpsilon = 0.0524289032676288\nEpsilon = 0.05242366037730204\nEpsilon = 0.05241841801126431\nEpsilon = 0.052413176169463185\nEpsilon = 0.05240793485184624\nEpsilon = 0.05240269405836106\nEpsilon = 0.052397453788955224\nEpsilon = 0.05239221404357633\nAgent: ddqn_agent . Episode 1572/2000. Number of steps to finish: 20. Loss: 19.416000366210938 Reward: -20.0\nEpsilon = 0.05238697482217197\nEpsilon = 0.05238173612468976\nEpsilon = 0.05237649795107729\nEpsilon = 0.05237126030128218\nEpsilon = 0.052366023175252054\nEpsilon = 0.052360786572934526\nEpsilon = 0.05235555049427723\nEpsilon = 0.0523503149392278\nEpsilon = 0.05234507990773388\nEpsilon = 0.0523398453997431\nEpsilon = 0.05233461141520313\nEpsilon = 0.05232937795406161\nEpsilon = 0.052324145016266206\nEpsilon = 0.05231891260176458\nEpsilon = 0.0523136807105044\nEpsilon = 0.05230844934243335\nEpsilon = 0.05230321849749911\nEpsilon = 0.05229798817564936\nEpsilon = 0.0522927583768318\nEpsilon = 0.052287529100994115\nAgent: ddqn_agent . Episode 1573/2000. Number of steps to finish: 20. Loss: 19.024446487426758 Reward: -12.0\nEpsilon = 0.05228230034808402\nEpsilon = 0.05227707211804921\nEpsilon = 0.052271844410837404\nEpsilon = 0.05226661722639632\nEpsilon = 0.05226139056467368\nEpsilon = 0.052256164425617216\nEpsilon = 0.052250938809174655\nEpsilon = 0.05224571371529374\nEpsilon = 0.05224048914392221\nEpsilon = 0.05223526509500782\nEpsilon = 0.052230041568498316\nEpsilon = 0.05222481856434147\nEpsilon = 0.05221959608248503\nEpsilon = 0.052214374122876786\nEpsilon = 0.0522091526854645\nEpsilon = 0.05220393177019595\nEpsilon = 0.05219871137701893\nEpsilon = 0.05219349150588123\nEpsilon = 0.05218827215673064\nEpsilon = 0.05218305332951497\nAgent: ddqn_agent . Episode 1574/2000. Number of steps to finish: 20. Loss: 20.0139217376709 Reward: -12.0\nEpsilon = 0.05217783502418202\nEpsilon = 0.052172617240679606\nEpsilon = 0.05216739997895554\nEpsilon = 0.052162183238957646\nEpsilon = 0.05215696702063375\nEpsilon = 0.05215175132393169\nEpsilon = 0.0521465361487993\nEpsilon = 0.05214132149518442\nEpsilon = 0.0521361073630349\nEpsilon = 0.052130893752298596\nEpsilon = 0.05212568066292337\nEpsilon = 0.05212046809485708\nAgent: ddqn_agent . Episode 1575/2000. Number of steps to finish: 12. Loss: 10.867195129394531 Reward: 0.0\nEpsilon = 0.05211525604804759\nEpsilon = 0.05211004452244279\nEpsilon = 0.05210483351799055\nEpsilon = 0.05209962303463875\nEpsilon = 0.05209441307233528\nEpsilon = 0.05208920363102805\nEpsilon = 0.052083994710664945\nEpsilon = 0.05207878631119388\nEpsilon = 0.05207357843256276\nEpsilon = 0.052068371074719504\nEpsilon = 0.05206316423761203\nEpsilon = 0.052057957921188266\nEpsilon = 0.052052752125396146\nEpsilon = 0.052047546850183604\nEpsilon = 0.05204234209549859\nEpsilon = 0.05203713786128904\nEpsilon = 0.05203193414750291\nEpsilon = 0.052026730954088156\nEpsilon = 0.05202152828099275\nEpsilon = 0.05201632612816465\nAgent: ddqn_agent . Episode 1576/2000. Number of steps to finish: 20. Loss: 19.16875648498535 Reward: -12.0\nEpsilon = 0.052011124495551835\nEpsilon = 0.05200592338310228\nEpsilon = 0.052000722790763974\nEpsilon = 0.0519955227184849\nEpsilon = 0.05199032316621305\nEpsilon = 0.05198512413389643\nEpsilon = 0.05197992562148304\nEpsilon = 0.05197472762892089\nEpsilon = 0.051969530156158\nEpsilon = 0.051964333203142386\nEpsilon = 0.05195913676982207\nEpsilon = 0.05195394085614509\nEpsilon = 0.05194874546205948\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.05194355058751327\nEpsilon = 0.05193835623245452\nEpsilon = 0.051933162396831274\nEpsilon = 0.05192796908059159\nEpsilon = 0.05192277628368353\nEpsilon = 0.051917584006055166\nEpsilon = 0.05191239224765456\nAgent: ddqn_agent . Episode 1577/2000. Number of steps to finish: 20. Loss: 20.040002822875977 Reward: -14.0\nEpsilon = 0.05190720100842979\nEpsilon = 0.05190201028832895\nEpsilon = 0.051896820087300115\nEpsilon = 0.051891630405291385\nEpsilon = 0.05188644124225086\nEpsilon = 0.051881252598126634\nEpsilon = 0.05187606447286682\nEpsilon = 0.05187087686641954\nEpsilon = 0.0518656897787329\nEpsilon = 0.05186050320975502\nEpsilon = 0.05185531715943405\nEpsilon = 0.05185013162771811\nEpsilon = 0.05184494661455533\nEpsilon = 0.05183976211989388\nEpsilon = 0.051834578143681885\nEpsilon = 0.051829394685867516\nEpsilon = 0.05182421174639893\nEpsilon = 0.051819029325224295\nEpsilon = 0.051813847422291774\nEpsilon = 0.05180866603754954\nAgent: ddqn_agent . Episode 1578/2000. Number of steps to finish: 20. Loss: 19.46831703186035 Reward: -10.0\nEpsilon = 0.05180348517094579\nEpsilon = 0.0517983048224287\nEpsilon = 0.05179312499194646\nEpsilon = 0.051787945679447264\nEpsilon = 0.051782766884879323\nEpsilon = 0.05177758860819084\nEpsilon = 0.05177241084933002\nEpsilon = 0.05176723360824509\nEpsilon = 0.051762056884884264\nEpsilon = 0.051756880679195774\nEpsilon = 0.051751704991127856\nAgent: ddqn_agent . Episode 1579/2000. Number of steps to finish: 11. Loss: 12.274442672729492 Reward: 1.0\nEpsilon = 0.051746529820628745\nEpsilon = 0.051741355167646684\nEpsilon = 0.05173618103212992\nEpsilon = 0.05173100741402671\nEpsilon = 0.051725834313285306\nEpsilon = 0.05172066172985398\nEpsilon = 0.051715489663680995\nEpsilon = 0.051710318114714625\nEpsilon = 0.051705147082903156\nEpsilon = 0.05169997656819487\nEpsilon = 0.05169480657053805\nEpsilon = 0.051689637089880995\nEpsilon = 0.05168446812617201\nEpsilon = 0.05167929967935939\nEpsilon = 0.05167413174939146\nEpsilon = 0.05166896433621652\nEpsilon = 0.051663797439782895\nEpsilon = 0.051658631060038915\nEpsilon = 0.05165346519693291\nEpsilon = 0.05164829985041322\nAgent: ddqn_agent . Episode 1580/2000. Number of steps to finish: 20. Loss: 21.06839370727539 Reward: -18.0\nEpsilon = 0.05164313502042818\nEpsilon = 0.051637970706926134\nEpsilon = 0.051632806909855444\nEpsilon = 0.05162764362916446\nEpsilon = 0.05162248086480154\nEpsilon = 0.05161731861671506\nEpsilon = 0.05161215688485339\nEpsilon = 0.05160699566916491\nEpsilon = 0.05160183496959799\nEpsilon = 0.051596674786101034\nEpsilon = 0.05159151511862242\nEpsilon = 0.05158635596711056\nEpsilon = 0.051581197331513846\nEpsilon = 0.051576039211780694\nEpsilon = 0.05157088160785952\nEpsilon = 0.051565724519698736\nEpsilon = 0.05156056794724677\nEpsilon = 0.05155541189045205\nEpsilon = 0.051550256349263\nEpsilon = 0.05154510132362808\nAgent: ddqn_agent . Episode 1581/2000. Number of steps to finish: 20. Loss: 22.199134826660156 Reward: -12.0\nEpsilon = 0.05153994681349572\nEpsilon = 0.051534792818814366\nEpsilon = 0.05152963933953249\nEpsilon = 0.05152448637559853\nEpsilon = 0.051519333926960974\nEpsilon = 0.051514181993568275\nEpsilon = 0.051509030575368916\nEpsilon = 0.05150387967231138\nEpsilon = 0.05149872928434415\nEpsilon = 0.05149357941141572\nAgent: ddqn_agent . Episode 1582/2000. Number of steps to finish: 10. Loss: 10.987983703613281 Reward: 2.0\nEpsilon = 0.05148843005347458\nEpsilon = 0.05148328121046923\nEpsilon = 0.05147813288234818\nEpsilon = 0.05147298506905995\nEpsilon = 0.05146783777055305\nEpsilon = 0.05146269098677599\nEpsilon = 0.051457544717677314\nEpsilon = 0.051452398963205546\nEpsilon = 0.05144725372330922\nEpsilon = 0.05144210899793689\nEpsilon = 0.0514369647870371\nEpsilon = 0.0514318210905584\nEpsilon = 0.05142667790844934\nEpsilon = 0.0514215352406585\nEpsilon = 0.05141639308713443\nEpsilon = 0.05141125144782572\nEpsilon = 0.051406110322680935\nEpsilon = 0.05140096971164867\nEpsilon = 0.05139582961467751\nEpsilon = 0.05139069003171604\nAgent: ddqn_agent . Episode 1583/2000. Number of steps to finish: 20. Loss: 19.11785888671875 Reward: -14.0\nEpsilon = 0.05138555096271287\nEpsilon = 0.0513804124076166\nEpsilon = 0.05137527436637584\nEpsilon = 0.0513701368389392\nEpsilon = 0.0513649998252553\nEpsilon = 0.05135986332527278\nEpsilon = 0.051354727338940254\nEpsilon = 0.051349591866206364\nEpsilon = 0.05134445690701974\nEpsilon = 0.05133932246132904\nAgent: ddqn_agent . Episode 1584/2000. Number of steps to finish: 10. Loss: 10.603660583496094 Reward: 2.0\nEpsilon = 0.051334188529082905\nEpsilon = 0.05132905511023\nEpsilon = 0.05132392220471897\nEpsilon = 0.0513187898124985\nEpsilon = 0.051313657933517255\nEpsilon = 0.0513085265677239\nEpsilon = 0.05130339571506713\nEpsilon = 0.05129826537549562\nEpsilon = 0.05129313554895808\nEpsilon = 0.05128800623540318\nEpsilon = 0.05128287743477964\nEpsilon = 0.051277749147036165\nEpsilon = 0.05127262137212146\nEpsilon = 0.051267494109984255\nEpsilon = 0.051262367360573255\nEpsilon = 0.0512572411238372\nEpsilon = 0.05125211539972482\nEpsilon = 0.05124699018818484\nEpsilon = 0.051241865489166025\nEpsilon = 0.05123674130261711\nAgent: ddqn_agent . Episode 1585/2000. Number of steps to finish: 20. Loss: 19.25252342224121 Reward: -12.0\nEpsilon = 0.051231617628486846\nEpsilon = 0.051226494466724\nEpsilon = 0.051221371817277324\nEpsilon = 0.051216249680095596\nEpsilon = 0.051211128055127585\nEpsilon = 0.05120600694232207\nEpsilon = 0.05120088634162784\nEpsilon = 0.05119576625299368\nEpsilon = 0.05119064667636838\nEpsilon = 0.05118552761170074\nEpsilon = 0.05118040905893957\nEpsilon = 0.051175291018033676\nEpsilon = 0.051170173488931874\nEpsilon = 0.05116505647158298\nEpsilon = 0.051159939965935826\nEpsilon = 0.05115482397193923\nEpsilon = 0.05114970848954204\nEpsilon = 0.05114459351869309\nEpsilon = 0.05113947905934122\nEpsilon = 0.051134365111435286\nAgent: ddqn_agent . Episode 1586/2000. Number of steps to finish: 20. Loss: 20.534446716308594 Reward: -14.0\nEpsilon = 0.051129251674924145\nEpsilon = 0.05112413874975665\nEpsilon = 0.051119026335881675\nEpsilon = 0.05111391443324809\nEpsilon = 0.05110880304180476\nEpsilon = 0.05110369216150058\nEpsilon = 0.05109858179228444\nEpsilon = 0.05109347193410521\nEpsilon = 0.0510883625869118\nEpsilon = 0.05108325375065311\nEpsilon = 0.05107814542527805\nEpsilon = 0.05107303761073552\nEpsilon = 0.05106793030697444\nEpsilon = 0.05106282351394375\nEpsilon = 0.05105771723159235\nEpsilon = 0.051052611459869196\nEpsilon = 0.05104750619872321\nEpsilon = 0.05104240144810334\nEpsilon = 0.05103729720795853\nEpsilon = 0.05103219347823773\nAgent: ddqn_agent . Episode 1587/2000. Number of steps to finish: 20. Loss: 18.175661087036133 Reward: -14.0\nEpsilon = 0.05102709025888991\nEpsilon = 0.05102198754986402\nEpsilon = 0.051016885351109036\nEpsilon = 0.051011783662573924\nEpsilon = 0.051006682484207665\nEpsilon = 0.05100158181595924\nEpsilon = 0.05099648165777765\nEpsilon = 0.05099138200961187\nEpsilon = 0.05098628287141091\nEpsilon = 0.05098118424312377\nEpsilon = 0.050976086124699456\nEpsilon = 0.05097098851608699\nEpsilon = 0.05096589141723538\nEpsilon = 0.05096079482809365\nEpsilon = 0.05095569874861084\nEpsilon = 0.05095060317873598\nEpsilon = 0.05094550811841811\nEpsilon = 0.050940413567606264\nEpsilon = 0.0509353195262495\nEpsilon = 0.05093022599429688\nAgent: ddqn_agent . Episode 1588/2000. Number of steps to finish: 20. Loss: 18.54071807861328 Reward: -14.0\nEpsilon = 0.05092513297169745\nEpsilon = 0.05092004045840028\nEpsilon = 0.050914948454354444\nEpsilon = 0.05090985695950901\nEpsilon = 0.05090476597381306\nEpsilon = 0.050899675497215684\nEpsilon = 0.05089458552966596\nEpsilon = 0.050889496071113\nEpsilon = 0.05088440712150589\nEpsilon = 0.05087931868079374\nEpsilon = 0.050874230748925664\nEpsilon = 0.05086914332585077\nEpsilon = 0.050864056411518185\nEpsilon = 0.05085897000587703\nEpsilon = 0.050853884108876445\nEpsilon = 0.050848798720465556\nEpsilon = 0.05084371384059351\nEpsilon = 0.05083862946920945\nEpsilon = 0.050833545606262534\nEpsilon = 0.05082846225170191\nAgent: ddqn_agent . Episode 1589/2000. Number of steps to finish: 20. Loss: 19.930707931518555 Reward: -10.0\nEpsilon = 0.05082337940547674\nEpsilon = 0.05081829706753619\nEpsilon = 0.05081321523782944\nEpsilon = 0.050808133916305655\nEpsilon = 0.050803053102914025\nEpsilon = 0.05079797279760374\nEpsilon = 0.050792893000323976\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.05078781371102394\nEpsilon = 0.050782734929652844\nEpsilon = 0.05077765665615988\nEpsilon = 0.05077257889049427\nEpsilon = 0.05076750163260522\nEpsilon = 0.05076242488244196\nEpsilon = 0.050757348639953716\nEpsilon = 0.05075227290508972\nEpsilon = 0.05074719767779921\nEpsilon = 0.05074212295803143\nEpsilon = 0.05073704874573563\nEpsilon = 0.05073197504086106\nEpsilon = 0.05072690184335698\nAgent: ddqn_agent . Episode 1590/2000. Number of steps to finish: 20. Loss: 18.624967575073242 Reward: -12.0\nEpsilon = 0.050721829153172644\nEpsilon = 0.05071675697025733\nEpsilon = 0.0507116852945603\nEpsilon = 0.05070661412603084\nEpsilon = 0.05070154346461824\nEpsilon = 0.05069647331027178\nEpsilon = 0.050691403662940754\nEpsilon = 0.05068633452257446\nEpsilon = 0.0506812658891222\nEpsilon = 0.05067619776253329\nEpsilon = 0.05067113014275704\nEpsilon = 0.05066606302974276\nEpsilon = 0.05066099642343979\nEpsilon = 0.05065593032379745\nEpsilon = 0.05065086473076507\nEpsilon = 0.05064579964429199\nEpsilon = 0.05064073506432756\nEpsilon = 0.050635670990821134\nEpsilon = 0.05063060742372205\nEpsilon = 0.05062554436297968\nAgent: ddqn_agent . Episode 1591/2000. Number of steps to finish: 20. Loss: 18.81162452697754 Reward: -16.0\nEpsilon = 0.05062048180854339\nEpsilon = 0.05061541976036253\nEpsilon = 0.0506103582183865\nEpsilon = 0.05060529718256466\nEpsilon = 0.0506002366528464\nEpsilon = 0.050595176629181116\nEpsilon = 0.0505901171115182\nEpsilon = 0.05058505809980705\nEpsilon = 0.05057999959399707\nEpsilon = 0.05057494159403767\nEpsilon = 0.05056988409987827\nEpsilon = 0.05056482711146828\nEpsilon = 0.050559770628757134\nEpsilon = 0.05055471465169426\nEpsilon = 0.05054965918022909\nEpsilon = 0.05054460421431107\nEpsilon = 0.05053954975388964\nEpsilon = 0.05053449579891425\nEpsilon = 0.05052944234933436\nEpsilon = 0.05052438940509943\nAgent: ddqn_agent . Episode 1592/2000. Number of steps to finish: 20. Loss: 19.373023986816406 Reward: -12.0\nEpsilon = 0.05051933696615892\nEpsilon = 0.05051428503246231\nEpsilon = 0.050509233603959064\nEpsilon = 0.05050418268059867\nEpsilon = 0.05049913226233061\nEpsilon = 0.05049408234910438\nEpsilon = 0.05048903294086947\nEpsilon = 0.05048398403757538\nEpsilon = 0.05047893563917163\nEpsilon = 0.05047388774560771\nEpsilon = 0.05046884035683315\nEpsilon = 0.05046379347279747\nEpsilon = 0.05045874709345019\nEpsilon = 0.05045370121874085\nEpsilon = 0.050448655848618976\nEpsilon = 0.05044361098303411\nEpsilon = 0.05043856662193581\nEpsilon = 0.05043352276527362\nEpsilon = 0.05042847941299709\nEpsilon = 0.050423436565055796\nAgent: ddqn_agent . Episode 1593/2000. Number of steps to finish: 20. Loss: 17.759817123413086 Reward: -8.0\nEpsilon = 0.05041839422139929\nEpsilon = 0.05041335238197715\nEpsilon = 0.05040831104673896\nEpsilon = 0.05040327021563428\nEpsilon = 0.05039822988861272\nEpsilon = 0.05039319006562386\nEpsilon = 0.050388150746617295\nEpsilon = 0.050383111931542636\nEpsilon = 0.05037807362034948\nEpsilon = 0.050373035812987445\nEpsilon = 0.050367998509406146\nEpsilon = 0.050362961709555204\nEpsilon = 0.05035792541338425\nEpsilon = 0.05035288962084291\nEpsilon = 0.050347854331880826\nEpsilon = 0.05034281954644764\nEpsilon = 0.050337785264493\nEpsilon = 0.050332751485966554\nEpsilon = 0.05032771821081796\nEpsilon = 0.05032268543899688\nAgent: ddqn_agent . Episode 1594/2000. Number of steps to finish: 20. Loss: 17.135517120361328 Reward: -10.0\nEpsilon = 0.05031765317045298\nEpsilon = 0.05031262140513594\nEpsilon = 0.05030759014299543\nEpsilon = 0.05030255938398113\nEpsilon = 0.05029752912804273\nEpsilon = 0.050292499375129925\nEpsilon = 0.05028747012519241\nEpsilon = 0.050282441378179894\nEpsilon = 0.050277413134042076\nEpsilon = 0.05027238539272867\nEpsilon = 0.0502673581541894\nEpsilon = 0.05026233141837398\nEpsilon = 0.050257305185232144\nEpsilon = 0.050252279454713625\nEpsilon = 0.05024725422676815\nEpsilon = 0.05024222950134547\nEpsilon = 0.050237205278395335\nEpsilon = 0.050232181557867496\nEpsilon = 0.05022715833971171\nEpsilon = 0.05022213562387774\nAgent: ddqn_agent . Episode 1595/2000. Number of steps to finish: 20. Loss: 18.77836799621582 Reward: -14.0\nEpsilon = 0.05021711341031535\nEpsilon = 0.05021209169897432\nEpsilon = 0.050207070489804426\nEpsilon = 0.05020204978275544\nEpsilon = 0.05019702957777717\nEpsilon = 0.05019200987481939\nEpsilon = 0.050186990673831915\nEpsilon = 0.05018197197476453\nEpsilon = 0.050176953777567056\nEpsilon = 0.0501719360821893\nEpsilon = 0.05016691888858108\nEpsilon = 0.050161902196692226\nEpsilon = 0.050156886006472556\nEpsilon = 0.05015187031787191\nEpsilon = 0.05014685513084012\nEpsilon = 0.05014184044532704\nEpsilon = 0.050136826261282505\nEpsilon = 0.05013181257865638\nEpsilon = 0.05012679939739851\nEpsilon = 0.05012178671745877\nAgent: ddqn_agent . Episode 1596/2000. Number of steps to finish: 20. Loss: 19.81439971923828 Reward: -16.0\nEpsilon = 0.050116774538787025\nEpsilon = 0.05011176286133315\nEpsilon = 0.05010675168504702\nEpsilon = 0.05010174100987851\nEpsilon = 0.05009673083577752\nEpsilon = 0.05009172116269395\nEpsilon = 0.05008671199057768\nEpsilon = 0.050081703319378626\nEpsilon = 0.05007669514904669\nEpsilon = 0.050071687479531786\nEpsilon = 0.05006668031078383\nEpsilon = 0.050061673642752755\nEpsilon = 0.05005666747538848\nEpsilon = 0.050051661808640936\nEpsilon = 0.05004665664246007\nEpsilon = 0.050041651976795824\nEpsilon = 0.050036647811598146\nEpsilon = 0.05003164414681699\nEpsilon = 0.05002664098240231\nEpsilon = 0.05002163831830407\nAgent: ddqn_agent . Episode 1597/2000. Number of steps to finish: 20. Loss: 18.936750411987305 Reward: -12.0\nEpsilon = 0.050016636154472244\nEpsilon = 0.0500116344908568\nEpsilon = 0.050006633327407714\nEpsilon = 0.05000163266407497\nEpsilon = 0.04999663250080857\nEpsilon = 0.04999163283755849\nEpsilon = 0.049986633674274736\nEpsilon = 0.049981635010907306\nEpsilon = 0.04997663684740622\nEpsilon = 0.04997163918372148\nEpsilon = 0.04996664201980311\nEpsilon = 0.04996164535560113\nEpsilon = 0.04995664919106557\nEpsilon = 0.049951653526146465\nEpsilon = 0.04994665836079385\nEpsilon = 0.04994166369495777\nEpsilon = 0.04993666952858827\nEpsilon = 0.04993167586163542\nEpsilon = 0.049926682694049256\nEpsilon = 0.04992169002577985\nAgent: ddqn_agent . Episode 1598/2000. Number of steps to finish: 20. Loss: 18.2431583404541 Reward: -10.0\nEpsilon = 0.04991669785677727\nEpsilon = 0.049911706186991596\nEpsilon = 0.0499067150163729\nEpsilon = 0.049901724344871265\nEpsilon = 0.04989673417243678\nEpsilon = 0.049891744499019536\nEpsilon = 0.04988675532456963\nEpsilon = 0.04988176664903718\nEpsilon = 0.04987677847237228\nEpsilon = 0.04987179079452504\nEpsilon = 0.04986680361544559\nEpsilon = 0.04986181693508405\nEpsilon = 0.04985683075339054\nEpsilon = 0.049851845070315204\nEpsilon = 0.04984685988580817\nEpsilon = 0.04984187519981959\nEpsilon = 0.049836891012299614\nEpsilon = 0.049831907323198386\nEpsilon = 0.04982692413246607\nEpsilon = 0.04982194144005282\nAgent: ddqn_agent . Episode 1599/2000. Number of steps to finish: 20. Loss: 19.425495147705078 Reward: -12.0\nEpsilon = 0.049816959245908815\nEpsilon = 0.049811977549984225\nEpsilon = 0.049806996352229224\nEpsilon = 0.049802015652594005\nEpsilon = 0.049797035451028746\nEpsilon = 0.049792055747483646\nEpsilon = 0.0497870765419089\nEpsilon = 0.04978209783425471\nEpsilon = 0.04977711962447129\nEpsilon = 0.049772141912508845\nEpsilon = 0.049767164698317594\nEpsilon = 0.04976218798184776\nEpsilon = 0.04975721176304958\nEpsilon = 0.049752236041873274\nEpsilon = 0.049747260818269086\nEpsilon = 0.04974228609218726\nEpsilon = 0.04973731186357804\nEpsilon = 0.04973233813239168\nEpsilon = 0.049727364898578444\nEpsilon = 0.04972239216208859\nAgent: ddqn_agent . Episode 1600/2000. Number of steps to finish: 20. Loss: 18.736833572387695 Reward: -18.0\nEpsilon = 0.04971741992287238\nEpsilon = 0.049712448180880095\nEpsilon = 0.049707476936062006\nEpsilon = 0.0497025061883684\nEpsilon = 0.04969753593774956\nEpsilon = 0.04969256618415579\nEpsilon = 0.049687596927537374\nEpsilon = 0.04968262816784462\nEpsilon = 0.049677659905027836\nEpsilon = 0.049672692139037335\nEpsilon = 0.04966772486982343\nEpsilon = 0.04966275809733645\nEpsilon = 0.04965779182152672\nEpsilon = 0.04965282604234457\nEpsilon = 0.049647860759740335\nEpsilon = 0.049642895973664364\nEpsilon = 0.049637931684067\nEpsilon = 0.04963296789089859\nEpsilon = 0.0496280045941095\nEpsilon = 0.04962304179365009\nAgent: ddqn_agent . Episode 1601/2000. Number of steps to finish: 20. Loss: 19.9769344329834 Reward: -12.0\nEpsilon = 0.04961807948947072\nEpsilon = 0.049613117681521775\nEpsilon = 0.049608156369753624\nEpsilon = 0.04960319555411665\nEpsilon = 0.04959823523456124\nEpsilon = 0.049593275411037786\nEpsilon = 0.04958831608349668\nEpsilon = 0.04958335725188833\nEpsilon = 0.04957839891616314\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.04957344107627153\nEpsilon = 0.0495684837321639\nEpsilon = 0.04956352688379068\nEpsilon = 0.049558570531102304\nEpsilon = 0.049553614674049194\nEpsilon = 0.04954865931258179\nEpsilon = 0.04954370444665053\nEpsilon = 0.04953875007620587\nEpsilon = 0.04953379620119825\nEpsilon = 0.04952884282157813\nEpsilon = 0.04952388993729597\nAgent: ddqn_agent . Episode 1602/2000. Number of steps to finish: 20. Loss: 19.91361427307129 Reward: -16.0\nEpsilon = 0.04951893754830224\nEpsilon = 0.04951398565454741\nEpsilon = 0.04950903425598196\nEpsilon = 0.04950408335255636\nEpsilon = 0.049499132944221105\nEpsilon = 0.04949418303092668\nEpsilon = 0.04948923361262359\nEpsilon = 0.04948428468926233\nEpsilon = 0.049479336260793406\nEpsilon = 0.04947438832716733\nEpsilon = 0.04946944088833461\nEpsilon = 0.04946449394424578\nEpsilon = 0.049459547494851354\nEpsilon = 0.04945460154010187\nEpsilon = 0.049449656079947855\nEpsilon = 0.04944471111433986\nEpsilon = 0.049439766643228425\nEpsilon = 0.049434822666564106\nEpsilon = 0.04942987918429745\nEpsilon = 0.04942493619637902\nAgent: ddqn_agent . Episode 1603/2000. Number of steps to finish: 20. Loss: 18.950334548950195 Reward: -12.0\nEpsilon = 0.04941999370275938\nEpsilon = 0.049415051703389105\nEpsilon = 0.04941011019821877\nEpsilon = 0.04940516918719895\nEpsilon = 0.04940022867028023\nEpsilon = 0.0493952886474132\nEpsilon = 0.04939034911854846\nEpsilon = 0.04938541008363661\nEpsilon = 0.04938047154262824\nEpsilon = 0.04937553349547398\nEpsilon = 0.04937059594212443\nEpsilon = 0.049365658882530215\nEpsilon = 0.049360722316641965\nEpsilon = 0.0493557862444103\nEpsilon = 0.049350850665785866\nEpsilon = 0.04934591558071929\nEpsilon = 0.04934098098916122\nEpsilon = 0.0493360468910623\nEpsilon = 0.0493311132863732\nEpsilon = 0.04932618017504456\nAgent: ddqn_agent . Episode 1604/2000. Number of steps to finish: 20. Loss: 18.767044067382812 Reward: -16.0\nEpsilon = 0.049321247557027055\nEpsilon = 0.04931631543227135\nEpsilon = 0.04931138380072812\nEpsilon = 0.04930645266234805\nEpsilon = 0.049301522017081816\nEpsilon = 0.049296591864880106\nEpsilon = 0.049291662205693615\nEpsilon = 0.04928673303947305\nEpsilon = 0.0492818043661691\nEpsilon = 0.04927687618573248\nEpsilon = 0.04927194849811391\nEpsilon = 0.049267021303264096\nEpsilon = 0.04926209460113377\nEpsilon = 0.049257168391673656\nEpsilon = 0.04925224267483449\nEpsilon = 0.049247317450567006\nEpsilon = 0.04924239271882195\nEpsilon = 0.049237468479550066\nEpsilon = 0.04923254473270211\nEpsilon = 0.04922762147822884\nAgent: ddqn_agent . Episode 1605/2000. Number of steps to finish: 20. Loss: 24.214021682739258 Reward: -14.0\nEpsilon = 0.049222698716081016\nEpsilon = 0.04921777644620941\nEpsilon = 0.04921285466856479\nEpsilon = 0.04920793338309793\nEpsilon = 0.04920301258975962\nEpsilon = 0.049198092288500646\nEpsilon = 0.049193172479271796\nEpsilon = 0.04918825316202387\nEpsilon = 0.049183334336707664\nEpsilon = 0.049178416003273995\nEpsilon = 0.04917349816167367\nEpsilon = 0.0491685808118575\nEpsilon = 0.04916366395377632\nEpsilon = 0.04915874758738094\nEpsilon = 0.0491538317126222\nEpsilon = 0.04914891632945094\nEpsilon = 0.049144001437818\nEpsilon = 0.049139087037674216\nEpsilon = 0.04913417312897045\nEpsilon = 0.049129259711657554\nAgent: ddqn_agent . Episode 1606/2000. Number of steps to finish: 20. Loss: 21.448179244995117 Reward: -14.0\nEpsilon = 0.04912434678568639\nEpsilon = 0.049119434351007824\nEpsilon = 0.049114522407572726\nEpsilon = 0.04910961095533197\nEpsilon = 0.04910469999423644\nEpsilon = 0.04909978952423701\nEpsilon = 0.04909487954528459\nEpsilon = 0.04908997005733006\nEpsilon = 0.04908506106032433\nEpsilon = 0.0490801525542183\nEpsilon = 0.04907524453896288\nEpsilon = 0.04907033701450898\nEpsilon = 0.04906542998080753\nEpsilon = 0.04906052343780945\nEpsilon = 0.04905561738546567\nEpsilon = 0.049050711823727125\nEpsilon = 0.04904580675254475\nEpsilon = 0.0490409021718695\nEpsilon = 0.049035998081652314\nEpsilon = 0.04903109448184415\nAgent: ddqn_agent . Episode 1607/2000. Number of steps to finish: 20. Loss: 20.290523529052734 Reward: -16.0\nEpsilon = 0.049026191372395965\nEpsilon = 0.04902128875325873\nEpsilon = 0.0490163866243834\nEpsilon = 0.049011484985720964\nEpsilon = 0.049006583837222395\nEpsilon = 0.04900168317883867\nEpsilon = 0.04899678301052079\nEpsilon = 0.04899188333221974\nEpsilon = 0.04898698414388652\nEpsilon = 0.04898208544547213\nEpsilon = 0.04897718723692759\nEpsilon = 0.04897228951820389\nEpsilon = 0.04896739228925207\nEpsilon = 0.04896249555002315\nEpsilon = 0.04895759930046815\nEpsilon = 0.0489527035405381\nAgent: ddqn_agent . Episode 1608/2000. Number of steps to finish: 16. Loss: 17.09454345703125 Reward: -4.0\nEpsilon = 0.048947808270184044\nEpsilon = 0.048942913489357025\nEpsilon = 0.04893801919800809\nEpsilon = 0.04893312539608829\nEpsilon = 0.04892823208354868\nEpsilon = 0.04892333926034032\nEpsilon = 0.04891844692641429\nEpsilon = 0.04891355508172165\nEpsilon = 0.048908663726213476\nEpsilon = 0.048903772859840856\nEpsilon = 0.048898882482554876\nEpsilon = 0.04889399259430662\nEpsilon = 0.04888910319504719\nEpsilon = 0.04888421428472769\nEpsilon = 0.04887932586329922\nEpsilon = 0.04887443793071289\nEpsilon = 0.048869550486919815\nEpsilon = 0.048864663531871126\nEpsilon = 0.04885977706551794\nEpsilon = 0.04885489108781139\nAgent: ddqn_agent . Episode 1609/2000. Number of steps to finish: 20. Loss: 19.918811798095703 Reward: -12.0\nEpsilon = 0.04885000559870261\nEpsilon = 0.04884512059814274\nEpsilon = 0.04884023608608293\nEpsilon = 0.04883535206247432\nEpsilon = 0.04883046852726808\nEpsilon = 0.04882558548041535\nEpsilon = 0.04882070292186731\nEpsilon = 0.048815820851575126\nEpsilon = 0.04881093926948997\nEpsilon = 0.04880605817556302\nEpsilon = 0.04880117756974547\nEpsilon = 0.04879629745198849\nEpsilon = 0.04879141782224329\nEpsilon = 0.04878653868046107\nEpsilon = 0.048781660026593024\nEpsilon = 0.04877678186059037\nEpsilon = 0.04877190418240431\nEpsilon = 0.048767026991986075\nEpsilon = 0.048762150289286876\nEpsilon = 0.04875727407425795\nAgent: ddqn_agent . Episode 1610/2000. Number of steps to finish: 20. Loss: 19.704444885253906 Reward: -10.0\nEpsilon = 0.04875239834685052\nEpsilon = 0.04874752310701584\nEpsilon = 0.04874264835470514\nEpsilon = 0.04873777408986967\nEpsilon = 0.048732900312460685\nEpsilon = 0.04872802702242944\nEpsilon = 0.0487231542197272\nEpsilon = 0.04871828190430523\nEpsilon = 0.0487134100761148\nEpsilon = 0.04870853873510719\nEpsilon = 0.04870366788123368\nEpsilon = 0.04869879751444556\nEpsilon = 0.048693927634694115\nEpsilon = 0.04868905824193064\nEpsilon = 0.04868418933610645\nEpsilon = 0.04867932091717284\nAgent: ddqn_agent . Episode 1611/2000. Number of steps to finish: 16. Loss: 16.534343719482422 Reward: -4.0\nEpsilon = 0.04867445298508113\nEpsilon = 0.04866958553978262\nEpsilon = 0.048664718581228644\nEpsilon = 0.04865985210937052\nEpsilon = 0.04865498612415959\nEpsilon = 0.048650120625547175\nEpsilon = 0.04864525561348462\nEpsilon = 0.04864039108792328\nEpsilon = 0.048635527048814485\nEpsilon = 0.0486306634961096\nEpsilon = 0.048625800429759995\nEpsilon = 0.04862093784971702\nEpsilon = 0.04861607575593205\nEpsilon = 0.04861121414835645\nEpsilon = 0.048606353026941614\nEpsilon = 0.04860149239163892\nEpsilon = 0.048596632242399754\nEpsilon = 0.04859177257917551\nEpsilon = 0.04858691340191759\nEpsilon = 0.0485820547105774\nAgent: ddqn_agent . Episode 1612/2000. Number of steps to finish: 20. Loss: 20.073530197143555 Reward: -10.0\nEpsilon = 0.04857719650510634\nEpsilon = 0.04857233878545583\nEpsilon = 0.048567481551577286\nEpsilon = 0.048562624803422126\nEpsilon = 0.048557768540941786\nEpsilon = 0.04855291276408769\nEpsilon = 0.04854805747281128\nEpsilon = 0.048543202667064\nEpsilon = 0.048538348346797294\nEpsilon = 0.048533494511962615\nEpsilon = 0.04852864116251142\nEpsilon = 0.04852378829839517\nEpsilon = 0.048518935919565326\nEpsilon = 0.04851408402597337\nEpsilon = 0.04850923261757077\nEpsilon = 0.04850438169430901\nEpsilon = 0.04849953125613958\nEpsilon = 0.048494681303013966\nEpsilon = 0.048489831834883666\nEpsilon = 0.04848498285170018\nAgent: ddqn_agent . Episode 1613/2000. Number of steps to finish: 20. Loss: 19.799644470214844 Reward: -20.0\nEpsilon = 0.04848013435341501\nEpsilon = 0.04847528633997967\nEpsilon = 0.04847043881134567\nEpsilon = 0.04846559176746453\nEpsilon = 0.04846074520828778\nEpsilon = 0.04845589913376695\nEpsilon = 0.048451053543853574\nEpsilon = 0.048446208438499186\nAgent: ddqn_agent . Episode 1614/2000. Number of steps to finish: 8. Loss: 7.165517807006836 Reward: 4.0\nEpsilon = 0.048441363817655334\nEpsilon = 0.04843651968127357\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.04843167602930544\nEpsilon = 0.04842683286170251\nEpsilon = 0.04842199017841634\nEpsilon = 0.0484171479793985\nEpsilon = 0.04841230626460056\nEpsilon = 0.0484074650339741\nEpsilon = 0.0484026242874707\nEpsilon = 0.048397784025041954\nEpsilon = 0.04839294424663945\nEpsilon = 0.04838810495221479\nEpsilon = 0.04838326614171957\nEpsilon = 0.048378427815105395\nEpsilon = 0.04837358997232388\nEpsilon = 0.04836875261332665\nEpsilon = 0.04836391573806532\nEpsilon = 0.04835907934649151\nEpsilon = 0.04835424343855686\nEpsilon = 0.04834940801421301\nAgent: ddqn_agent . Episode 1615/2000. Number of steps to finish: 20. Loss: 20.072099685668945 Reward: -18.0\nEpsilon = 0.04834457307341159\nEpsilon = 0.04833973861610425\nEpsilon = 0.04833490464224264\nEpsilon = 0.04833007115177842\nEpsilon = 0.04832523814466324\nEpsilon = 0.04832040562084877\nEpsilon = 0.04831557358028669\nEpsilon = 0.048310742022928656\nEpsilon = 0.04830591094872636\nEpsilon = 0.04830108035763149\nEpsilon = 0.048296250249595725\nEpsilon = 0.048291420624570765\nEpsilon = 0.04828659148250831\nEpsilon = 0.04828176282336006\nEpsilon = 0.048276934647077725\nEpsilon = 0.04827210695361302\nEpsilon = 0.048267279742917654\nEpsilon = 0.04826245301494336\nEpsilon = 0.04825762676964187\nEpsilon = 0.048252801006964906\nAgent: ddqn_agent . Episode 1616/2000. Number of steps to finish: 20. Loss: 20.29840087890625 Reward: -12.0\nEpsilon = 0.04824797572686421\nEpsilon = 0.048243150929291524\nEpsilon = 0.04823832661419859\nEpsilon = 0.04823350278153717\nEpsilon = 0.04822867943125902\nEpsilon = 0.04822385656331589\nEpsilon = 0.04821903417765956\nEpsilon = 0.048214212274241795\nEpsilon = 0.04820939085301437\nEpsilon = 0.048204569913929074\nEpsilon = 0.04819974945693768\nEpsilon = 0.04819492948199199\nEpsilon = 0.04819010998904379\nEpsilon = 0.04818529097804489\nEpsilon = 0.04818047244894708\nEpsilon = 0.04817565440170219\nEpsilon = 0.04817083683626202\nEpsilon = 0.04816601975257839\nEpsilon = 0.048161203150603134\nEpsilon = 0.04815638703028807\nAgent: ddqn_agent . Episode 1617/2000. Number of steps to finish: 20. Loss: 21.73360252380371 Reward: -16.0\nEpsilon = 0.048151571391585046\nEpsilon = 0.048146756234445885\nEpsilon = 0.04814194155882244\nEpsilon = 0.048137127364666556\nEpsilon = 0.04813231365193009\nEpsilon = 0.0481275004205649\nEpsilon = 0.04812268767052284\nEpsilon = 0.04811787540175579\nEpsilon = 0.04811306361421562\nEpsilon = 0.0481082523078542\nAgent: ddqn_agent . Episode 1618/2000. Number of steps to finish: 10. Loss: 9.978852272033691 Reward: 2.0\nEpsilon = 0.04810344148262341\nEpsilon = 0.04809863113847515\nEpsilon = 0.048093821275361305\nEpsilon = 0.04808901189323377\nEpsilon = 0.04808420299204445\nEpsilon = 0.048079394571745246\nEpsilon = 0.04807458663228807\nEpsilon = 0.048069779173624844\nEpsilon = 0.04806497219570748\nEpsilon = 0.048060165698487915\nEpsilon = 0.04805535968191807\nEpsilon = 0.04805055414594988\nEpsilon = 0.048045749090535286\nEpsilon = 0.04804094451562623\nEpsilon = 0.04803614042117467\nEpsilon = 0.04803133680713255\nEpsilon = 0.04802653367345184\nEpsilon = 0.048021731020084495\nEpsilon = 0.048016928846982485\nEpsilon = 0.048012127154097785\nAgent: ddqn_agent . Episode 1619/2000. Number of steps to finish: 20. Loss: 19.560867309570312 Reward: -14.0\nEpsilon = 0.04800732594138238\nEpsilon = 0.04800252520878824\nEpsilon = 0.04799772495626736\nEpsilon = 0.04799292518377174\nEpsilon = 0.04798812589125336\nEpsilon = 0.047983327078664234\nEpsilon = 0.047978528745956366\nEpsilon = 0.04797373089308177\nEpsilon = 0.04796893351999246\nEpsilon = 0.04796413662664046\nEpsilon = 0.0479593402129778\nEpsilon = 0.0479545442789565\nEpsilon = 0.047949748824528605\nEpsilon = 0.04794495384964615\nEpsilon = 0.047940159354261186\nEpsilon = 0.04793536533832576\nEpsilon = 0.04793057180179192\nEpsilon = 0.04792577874461174\nEpsilon = 0.04792098616673728\nEpsilon = 0.04791619406812061\nAgent: ddqn_agent . Episode 1620/2000. Number of steps to finish: 20. Loss: 18.03663444519043 Reward: -12.0\nEpsilon = 0.047911402448713794\nEpsilon = 0.047906611308468926\nEpsilon = 0.04790182064733808\nEpsilon = 0.04789703046527334\nEpsilon = 0.04789224076222681\nEpsilon = 0.04788745153815059\nEpsilon = 0.04788266279299678\nEpsilon = 0.04787787452671748\nEpsilon = 0.04787308673926481\nEpsilon = 0.047868299430590885\nEpsilon = 0.04786351260064783\nAgent: ddqn_agent . Episode 1621/2000. Number of steps to finish: 11. Loss: 9.750383377075195 Reward: 1.0\nEpsilon = 0.047858726249387765\nEpsilon = 0.04785394037676283\nEpsilon = 0.04784915498272515\nEpsilon = 0.047844370067226874\nEpsilon = 0.04783958563022015\nEpsilon = 0.04783480167165713\nEpsilon = 0.047830018191489965\nEpsilon = 0.047825235189670814\nEpsilon = 0.04782045266615185\nEpsilon = 0.04781567062088524\nEpsilon = 0.04781088905382315\nEpsilon = 0.04780610796491777\nEpsilon = 0.04780132735412128\nEpsilon = 0.04779654722138587\nEpsilon = 0.04779176756666373\nEpsilon = 0.04778698838990706\nEpsilon = 0.04778220969106807\nEpsilon = 0.04777743147009896\nEpsilon = 0.047772653726951955\nEpsilon = 0.04776787646157926\nAgent: ddqn_agent . Episode 1622/2000. Number of steps to finish: 20. Loss: 21.15776252746582 Reward: -10.0\nEpsilon = 0.04776309967393311\nEpsilon = 0.04775832336396572\nEpsilon = 0.047753547531629324\nEpsilon = 0.04774877217687616\nEpsilon = 0.047743997299658475\nEpsilon = 0.04773922289992851\nEpsilon = 0.04773444897763852\nEpsilon = 0.047729675532740753\nEpsilon = 0.04772490256518748\nEpsilon = 0.047720130074930964\nEpsilon = 0.04771535806192347\nEpsilon = 0.04771058652611728\nEpsilon = 0.04770581546746467\nEpsilon = 0.04770104488591793\nEpsilon = 0.047696274781429335\nEpsilon = 0.047691505153951194\nEpsilon = 0.0476867360034358\nEpsilon = 0.047681967329835456\nEpsilon = 0.04767719913310247\nEpsilon = 0.04767243141318916\nAgent: ddqn_agent . Episode 1623/2000. Number of steps to finish: 20. Loss: 20.498435974121094 Reward: -12.0\nEpsilon = 0.04766766417004784\nEpsilon = 0.04766289740363084\nEpsilon = 0.04765813111389048\nEpsilon = 0.04765336530077909\nEpsilon = 0.047648599964249014\nEpsilon = 0.04764383510425259\nEpsilon = 0.047639070720742166\nEpsilon = 0.04763430681367009\nEpsilon = 0.04762954338298873\nEpsilon = 0.047624780428650426\nEpsilon = 0.04762001795060756\nEpsilon = 0.047615255948812496\nEpsilon = 0.04761049442321762\nEpsilon = 0.04760573337377529\nEpsilon = 0.047600972800437916\nEpsilon = 0.04759621270315787\nEpsilon = 0.04759145308188756\nEpsilon = 0.04758669393657937\nEpsilon = 0.04758193526718571\nEpsilon = 0.047577177073658994\nAgent: ddqn_agent . Episode 1624/2000. Number of steps to finish: 20. Loss: 19.8596248626709 Reward: -12.0\nEpsilon = 0.04757241935595163\nEpsilon = 0.04756766211401603\nEpsilon = 0.04756290534780463\nEpsilon = 0.04755814905726985\nEpsilon = 0.04755339324236412\nEpsilon = 0.047548637903039884\nAgent: ddqn_agent . Episode 1625/2000. Number of steps to finish: 6. Loss: 7.22960090637207 Reward: 6.0\nEpsilon = 0.04754388303924958\nEpsilon = 0.04753912865094565\nEpsilon = 0.04753437473808056\nEpsilon = 0.04752962130060675\nEpsilon = 0.04752486833847669\nEpsilon = 0.04752011585164284\nEpsilon = 0.047515363840057674\nEpsilon = 0.04751061230367367\nEpsilon = 0.047505861242443305\nEpsilon = 0.04750111065631906\nEpsilon = 0.04749636054525343\nEpsilon = 0.0474916109091989\nEpsilon = 0.047486861748107984\nEpsilon = 0.04748211306193317\nEpsilon = 0.04747736485062698\nEpsilon = 0.04747261711414192\nEpsilon = 0.047467869852430504\nEpsilon = 0.04746312306544526\nEpsilon = 0.04745837675313872\nEpsilon = 0.04745363091546341\nAgent: ddqn_agent . Episode 1626/2000. Number of steps to finish: 20. Loss: 19.585128784179688 Reward: -10.0\nEpsilon = 0.04744888555237187\nEpsilon = 0.04744414066381663\nEpsilon = 0.04743939624975025\nEpsilon = 0.04743465231012527\nEpsilon = 0.04742990884489426\nEpsilon = 0.04742516585400977\nEpsilon = 0.04742042333742437\nEpsilon = 0.04741568129509063\nAgent: ddqn_agent . Episode 1627/2000. Number of steps to finish: 8. Loss: 7.896285533905029 Reward: 4.0\nEpsilon = 0.047410939726961115\nEpsilon = 0.04740619863298842\nEpsilon = 0.047401458013125126\nEpsilon = 0.047396717867323815\nEpsilon = 0.04739197819553708\nEpsilon = 0.047387238997717525\nEpsilon = 0.047382500273817754\nEpsilon = 0.047377762023790376\nEpsilon = 0.047373024247588\nAgent: ddqn_agent . Episode 1628/2000. Number of steps to finish: 9. Loss: 9.306509971618652 Reward: 3.0\nEpsilon = 0.04736828694516324\nEpsilon = 0.04736355011646873\nEpsilon = 0.047358813761457084\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.04735407788008094\nEpsilon = 0.04734934247229293\nEpsilon = 0.0473446075380457\nEpsilon = 0.0473398730772919\nEpsilon = 0.04733513908998417\nEpsilon = 0.04733040557607517\nEpsilon = 0.04732567253551757\nEpsilon = 0.04732093996826402\nEpsilon = 0.04731620787426719\nEpsilon = 0.04731147625347976\nEpsilon = 0.04730674510585441\nEpsilon = 0.047302014431343825\nEpsilon = 0.04729728422990069\nEpsilon = 0.0472925545014777\nEpsilon = 0.04728782524602755\nEpsilon = 0.04728309646350295\nEpsilon = 0.047278368153856595\nAgent: ddqn_agent . Episode 1629/2000. Number of steps to finish: 20. Loss: 19.65608024597168 Reward: -12.0\nEpsilon = 0.04727364031704121\nEpsilon = 0.04726891295300951\nEpsilon = 0.04726418606171421\nEpsilon = 0.047259459643108036\nEpsilon = 0.047254733697143725\nEpsilon = 0.04725000822377401\nEpsilon = 0.047245283222951635\nEpsilon = 0.04724055869462934\nEpsilon = 0.04723583463875988\nEpsilon = 0.04723111105529601\nEpsilon = 0.047226387944190484\nEpsilon = 0.047221665305396066\nEpsilon = 0.04721694313886553\nEpsilon = 0.047212221444551644\nEpsilon = 0.04720750022240719\nEpsilon = 0.04720277947238495\nEpsilon = 0.04719805919443771\nEpsilon = 0.04719333938851827\nEpsilon = 0.04718862005457942\nEpsilon = 0.04718390119257396\nAgent: ddqn_agent . Episode 1630/2000. Number of steps to finish: 20. Loss: 17.86134147644043 Reward: -14.0\nEpsilon = 0.0471791828024547\nEpsilon = 0.047174464884174455\nEpsilon = 0.04716974743768604\nEpsilon = 0.04716503046294227\nEpsilon = 0.04716031395989598\nEpsilon = 0.047155597928499986\nEpsilon = 0.04715088236870714\nEpsilon = 0.04714616728047027\nEpsilon = 0.04714145266374222\nEpsilon = 0.04713673851847585\nEpsilon = 0.047132024844624004\nEpsilon = 0.04712731164213954\nEpsilon = 0.047122598910975325\nEpsilon = 0.047117886651084226\nEpsilon = 0.047113174862419116\nEpsilon = 0.04710846354493287\nEpsilon = 0.04710375269857838\nEpsilon = 0.047099042323308526\nEpsilon = 0.04709433241907619\nEpsilon = 0.04708962298583429\nAgent: ddqn_agent . Episode 1631/2000. Number of steps to finish: 20. Loss: 18.291427612304688 Reward: -12.0\nEpsilon = 0.04708491402353571\nEpsilon = 0.04708020553213335\nEpsilon = 0.04707549751158014\nEpsilon = 0.04707078996182898\nEpsilon = 0.047066082882832795\nEpsilon = 0.047061376274544514\nEpsilon = 0.04705667013691706\nEpsilon = 0.047051964469903375\nEpsilon = 0.047047259273456384\nEpsilon = 0.047042554547529036\nEpsilon = 0.04703785029207428\nEpsilon = 0.04703314650704507\nEpsilon = 0.04702844319239437\nEpsilon = 0.047023740348075126\nEpsilon = 0.04701903797404032\nEpsilon = 0.04701433607024292\nEpsilon = 0.0470096346366359\nEpsilon = 0.04700493367317223\nEpsilon = 0.047000233179804916\nEpsilon = 0.046995533156486934\nAgent: ddqn_agent . Episode 1632/2000. Number of steps to finish: 20. Loss: 18.236366271972656 Reward: -20.0\nEpsilon = 0.046990833603171286\nEpsilon = 0.04698613451981097\nEpsilon = 0.04698143590635899\nEpsilon = 0.04697673776276835\nEpsilon = 0.04697204008899208\nEpsilon = 0.04696734288498318\nEpsilon = 0.04696264615069468\nEpsilon = 0.04695794988607961\nEpsilon = 0.046953254091091\nEpsilon = 0.046948558765681894\nEpsilon = 0.046943863909805324\nEpsilon = 0.04693916952341434\nEpsilon = 0.046934475606462\nEpsilon = 0.046929782158901354\nEpsilon = 0.04692508918068546\nEpsilon = 0.046920396671767395\nEpsilon = 0.04691570463210022\nEpsilon = 0.04691101306163701\nEpsilon = 0.04690632196033084\nEpsilon = 0.04690163132813481\nAgent: ddqn_agent . Episode 1633/2000. Number of steps to finish: 20. Loss: 20.841821670532227 Reward: -14.0\nEpsilon = 0.046896941165001996\nEpsilon = 0.04689225147088549\nEpsilon = 0.0468875622457384\nEpsilon = 0.04688287348951383\nEpsilon = 0.04687818520216488\nEpsilon = 0.046873497383644665\nEpsilon = 0.0468688100339063\nEpsilon = 0.046864123152902906\nEpsilon = 0.046859436740587616\nEpsilon = 0.04685475079691356\nEpsilon = 0.04685006532183387\nEpsilon = 0.046845380315301687\nEpsilon = 0.04684069577727016\nEpsilon = 0.046836011707692436\nEpsilon = 0.046831328106521665\nEpsilon = 0.046826644973711015\nEpsilon = 0.04682196230921364\nEpsilon = 0.04681728011298272\nEpsilon = 0.04681259838497143\nEpsilon = 0.04680791712513293\nAgent: ddqn_agent . Episode 1634/2000. Number of steps to finish: 20. Loss: 21.5455379486084 Reward: -10.0\nEpsilon = 0.046803236333420414\nEpsilon = 0.04679855600978707\nEpsilon = 0.04679387615418609\nEpsilon = 0.04678919676657067\nEpsilon = 0.04678451784689402\nEpsilon = 0.04677983939510933\nEpsilon = 0.04677516141116982\nEpsilon = 0.046770483895028706\nEpsilon = 0.0467658068466392\nEpsilon = 0.04676113026595454\nEpsilon = 0.046756454152927944\nEpsilon = 0.04675177850751265\nEpsilon = 0.0467471033296619\nEpsilon = 0.04674242861932894\nEpsilon = 0.046737754376467\nEpsilon = 0.046733080601029355\nEpsilon = 0.04672840729296925\nEpsilon = 0.04672373445223996\nEpsilon = 0.04671906207879473\nEpsilon = 0.046714390172586856\nAgent: ddqn_agent . Episode 1635/2000. Number of steps to finish: 20. Loss: 19.74850845336914 Reward: -16.0\nEpsilon = 0.0467097187335696\nEpsilon = 0.04670504776169624\nEpsilon = 0.04670037725692007\nEpsilon = 0.04669570721919438\nEpsilon = 0.04669103764847246\nEpsilon = 0.04668636854470761\nEpsilon = 0.04668169990785314\nEpsilon = 0.046677031737862355\nEpsilon = 0.04667236403468857\nEpsilon = 0.046667696798285106\nEpsilon = 0.04666303002860528\nEpsilon = 0.04665836372560242\nEpsilon = 0.04665369788922986\nEpsilon = 0.046649032519440935\nEpsilon = 0.04664436761618899\nEpsilon = 0.046639703179427366\nEpsilon = 0.04663503920910943\nEpsilon = 0.04663037570518852\nEpsilon = 0.046625712667618\nEpsilon = 0.046621050096351244\nAgent: ddqn_agent . Episode 1636/2000. Number of steps to finish: 20. Loss: 18.790950775146484 Reward: -14.0\nEpsilon = 0.04661638799134161\nEpsilon = 0.046611726352542474\nEpsilon = 0.04660706517990722\nEpsilon = 0.046602404473389224\nEpsilon = 0.046597744232941885\nEpsilon = 0.04659308445851859\nEpsilon = 0.04658842515007274\nEpsilon = 0.046583766307557735\nEpsilon = 0.04657910793092698\nEpsilon = 0.04657445002013389\nEpsilon = 0.04656979257513188\nEpsilon = 0.046565135595874364\nEpsilon = 0.046560479082314775\nEpsilon = 0.04655582303440654\nEpsilon = 0.0465511674521031\nEpsilon = 0.04654651233535789\nEpsilon = 0.04654185768412436\nEpsilon = 0.046537203498355946\nEpsilon = 0.04653254977800611\nEpsilon = 0.04652789652302831\nAgent: ddqn_agent . Episode 1637/2000. Number of steps to finish: 20. Loss: 20.76634979248047 Reward: -14.0\nEpsilon = 0.046523243733376005\nEpsilon = 0.04651859140900267\nEpsilon = 0.04651393954986177\nEpsilon = 0.04650928815590678\nEpsilon = 0.046504637227091195\nEpsilon = 0.04649998676336849\nEpsilon = 0.04649533676469215\nEpsilon = 0.04649068723101568\nEpsilon = 0.04648603816229258\nEpsilon = 0.04648138955847635\nEpsilon = 0.046476741419520504\nEpsilon = 0.04647209374537855\nEpsilon = 0.046467446536004015\nEpsilon = 0.046462799791350415\nEpsilon = 0.04645815351137128\nEpsilon = 0.04645350769602014\nEpsilon = 0.046448862345250544\nEpsilon = 0.04644421745901602\nEpsilon = 0.04643957303727012\nEpsilon = 0.046434929079966394\nAgent: ddqn_agent . Episode 1638/2000. Number of steps to finish: 20. Loss: 18.96308135986328 Reward: -10.0\nEpsilon = 0.0464302855870584\nEpsilon = 0.046425642558499694\nEpsilon = 0.046420999994243844\nEpsilon = 0.04641635789424442\nEpsilon = 0.046411716258455\nEpsilon = 0.046407075086829154\nEpsilon = 0.04640243437932047\nEpsilon = 0.04639779413588254\nEpsilon = 0.04639315435646895\nEpsilon = 0.04638851504103331\nAgent: ddqn_agent . Episode 1639/2000. Number of steps to finish: 10. Loss: 9.249545097351074 Reward: 2.0\nEpsilon = 0.0463838761895292\nEpsilon = 0.04637923780191025\nEpsilon = 0.04637459987813006\nEpsilon = 0.046369962418142244\nEpsilon = 0.04636532542190043\nEpsilon = 0.046360688889358244\nEpsilon = 0.046356052820469305\nAgent: ddqn_agent . Episode 1640/2000. Number of steps to finish: 7. Loss: 6.411317825317383 Reward: 5.0\nEpsilon = 0.046351417215187256\nEpsilon = 0.046346782073465737\nEpsilon = 0.04634214739525839\nEpsilon = 0.04633751318051886\nEpsilon = 0.04633287942920081\nEpsilon = 0.04632824614125789\nEpsilon = 0.046323613316643766\nEpsilon = 0.0463189809553121\nEpsilon = 0.04631434905721657\nEpsilon = 0.04630971762231085\nEpsilon = 0.04630508665054862\nEpsilon = 0.046300456141883564\nEpsilon = 0.04629582609626938\nEpsilon = 0.04629119651365975\nEpsilon = 0.04628656739400839\nEpsilon = 0.04628193873726899\nEpsilon = 0.04627731054339526\nEpsilon = 0.04627268281234092\nEpsilon = 0.04626805554405969\nEpsilon = 0.04626342873850528\nAgent: ddqn_agent . Episode 1641/2000. Number of steps to finish: 20. Loss: 21.80706024169922 Reward: -12.0\nEpsilon = 0.04625880239563143\nEpsilon = 0.04625417651539187\nEpsilon = 0.04624955109774033\nEpsilon = 0.04624492614263056\nEpsilon = 0.046240301650016295\nEpsilon = 0.04623567761985129\nEpsilon = 0.046231054052089306\nEpsilon = 0.0462264309466841\nEpsilon = 0.04622180830358943\nEpsilon = 0.04621718612275907\nEpsilon = 0.04621256440414679\nEpsilon = 0.046207943147706376\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.046203322353391606\nEpsilon = 0.046198702021156265\nEpsilon = 0.04619408215095415\nEpsilon = 0.04618946274273905\nEpsilon = 0.04618484379646478\nEpsilon = 0.046180225312085134\nEpsilon = 0.04617560728955392\nEpsilon = 0.04617098972882497\nAgent: ddqn_agent . Episode 1642/2000. Number of steps to finish: 20. Loss: 19.942934036254883 Reward: -20.0\nEpsilon = 0.046166372629852084\nEpsilon = 0.0461617559925891\nEpsilon = 0.046157139816989844\nEpsilon = 0.046152524103008144\nEpsilon = 0.046147908850597844\nEpsilon = 0.046143294059712785\nEpsilon = 0.04613867973030682\nAgent: ddqn_agent . Episode 1643/2000. Number of steps to finish: 7. Loss: 6.7168498039245605 Reward: 5.0\nEpsilon = 0.04613406586233379\nEpsilon = 0.04612945245574756\nEpsilon = 0.04612483951050198\nEpsilon = 0.046120227026550935\nEpsilon = 0.04611561500384828\nEpsilon = 0.0461110034423479\nEpsilon = 0.046106392342003664\nEpsilon = 0.04610178170276946\nEpsilon = 0.04609717152459918\nEpsilon = 0.046092561807446725\nEpsilon = 0.04608795255126598\nEpsilon = 0.04608334375601085\nEpsilon = 0.04607873542163525\nEpsilon = 0.046074127548093084\nEpsilon = 0.04606952013533828\nEpsilon = 0.04606491318332474\nEpsilon = 0.04606030669200641\nEpsilon = 0.04605570066133721\nEpsilon = 0.04605109509127108\nEpsilon = 0.04604648998176195\nAgent: ddqn_agent . Episode 1644/2000. Number of steps to finish: 20. Loss: 21.215059280395508 Reward: -20.0\nEpsilon = 0.04604188533276378\nEpsilon = 0.0460372811442305\nEpsilon = 0.04603267741611608\nEpsilon = 0.04602807414837447\nEpsilon = 0.04602347134095963\nEpsilon = 0.04601886899382553\nEpsilon = 0.04601426710692615\nEpsilon = 0.04600966568021546\nEpsilon = 0.04600506471364744\nEpsilon = 0.04600046420717607\nEpsilon = 0.04599586416075536\nEpsilon = 0.04599126457433928\nEpsilon = 0.045986665447881846\nEpsilon = 0.04598206678133706\nEpsilon = 0.045977468574658925\nEpsilon = 0.04597287082780146\nEpsilon = 0.04596827354071868\nEpsilon = 0.04596367671336461\nEpsilon = 0.04595908034569328\nEpsilon = 0.04595448443765871\nAgent: ddqn_agent . Episode 1645/2000. Number of steps to finish: 20. Loss: 18.414974212646484 Reward: -16.0\nEpsilon = 0.04594988898921495\nEpsilon = 0.04594529400031603\nEpsilon = 0.045940699470915995\nEpsilon = 0.0459361054009689\nEpsilon = 0.04593151179042881\nEpsilon = 0.04592691863924976\nEpsilon = 0.045922325947385836\nEpsilon = 0.0459177337147911\nEpsilon = 0.04591314194141962\nEpsilon = 0.04590855062722548\nEpsilon = 0.04590395977216276\nEpsilon = 0.04589936937618554\nEpsilon = 0.045894779439247926\nEpsilon = 0.045890189961304\nEpsilon = 0.04588560094230787\nEpsilon = 0.04588101238221364\nEpsilon = 0.04587642428097542\nEpsilon = 0.045871836638547325\nEpsilon = 0.04586724945488347\nEpsilon = 0.045862662729937984\nAgent: ddqn_agent . Episode 1646/2000. Number of steps to finish: 20. Loss: 18.44729232788086 Reward: -16.0\nEpsilon = 0.04585807646366499\nEpsilon = 0.045853490656018625\nEpsilon = 0.04584890530695302\nEpsilon = 0.04584432041642233\nEpsilon = 0.04583973598438069\nEpsilon = 0.04583515201078225\nEpsilon = 0.04583056849558117\nEpsilon = 0.04582598543873161\nEpsilon = 0.04582140284018774\nEpsilon = 0.045816820699903726\nEpsilon = 0.04581223901783374\nEpsilon = 0.045807657793931957\nEpsilon = 0.04580307702815256\nEpsilon = 0.04579849672044975\nEpsilon = 0.045793916870777704\nEpsilon = 0.04578933747909063\nEpsilon = 0.04578475854534272\nEpsilon = 0.04578018006948818\nEpsilon = 0.045775602051481235\nEpsilon = 0.04577102449127609\nAgent: ddqn_agent . Episode 1647/2000. Number of steps to finish: 20. Loss: 23.366291046142578 Reward: -14.0\nEpsilon = 0.045766447388826965\nEpsilon = 0.045761870744088086\nEpsilon = 0.045757294557013675\nEpsilon = 0.04575271882755797\nEpsilon = 0.04574814355567522\nEpsilon = 0.04574356874131965\nEpsilon = 0.045738994384445515\nEpsilon = 0.045734420485007074\nEpsilon = 0.04572984704295857\nEpsilon = 0.04572527405825427\nEpsilon = 0.04572070153084845\nEpsilon = 0.045716129460695365\nEpsilon = 0.045711557847749294\nEpsilon = 0.04570698669196452\nEpsilon = 0.045702415993295324\nEpsilon = 0.045697845751696\nEpsilon = 0.045693275967120826\nEpsilon = 0.04568870663952412\nEpsilon = 0.04568413776886016\nEpsilon = 0.04567956935508328\nAgent: ddqn_agent . Episode 1648/2000. Number of steps to finish: 20. Loss: 18.589235305786133 Reward: -10.0\nEpsilon = 0.04567500139814777\nEpsilon = 0.045670433898007956\nEpsilon = 0.04566586685461815\nEpsilon = 0.04566130026793269\nEpsilon = 0.0456567341379059\nEpsilon = 0.04565216846449211\nEpsilon = 0.04564760324764566\nEpsilon = 0.045643038487320896\nEpsilon = 0.045638474183472166\nEpsilon = 0.045633910336053816\nEpsilon = 0.04562934694502021\nEpsilon = 0.04562478401032571\nEpsilon = 0.04562022153192467\nEpsilon = 0.045615659509771485\nEpsilon = 0.04561109794382051\nEpsilon = 0.04560653683402613\nEpsilon = 0.04560197618034273\nEpsilon = 0.045597415982724696\nEpsilon = 0.045592856241126425\nEpsilon = 0.04558829695550232\nAgent: ddqn_agent . Episode 1649/2000. Number of steps to finish: 20. Loss: 18.045114517211914 Reward: -16.0\nEpsilon = 0.045583738125806766\nEpsilon = 0.04557917975199419\nEpsilon = 0.04557462183401899\nEpsilon = 0.04557006437183559\nEpsilon = 0.045565507365398404\nEpsilon = 0.04556095081466186\nEpsilon = 0.045556394719580395\nEpsilon = 0.04555183908010844\nEpsilon = 0.04554728389620043\nEpsilon = 0.04554272916781081\nEpsilon = 0.04553817489489403\nEpsilon = 0.04553362107740454\nEpsilon = 0.0455290677152968\nEpsilon = 0.04552451480852527\nEpsilon = 0.04551996235704442\nEpsilon = 0.045515410360808715\nEpsilon = 0.04551085881977263\nEpsilon = 0.04550630773389066\nEpsilon = 0.04550175710311727\nEpsilon = 0.045497206927406955\nAgent: ddqn_agent . Episode 1650/2000. Number of steps to finish: 20. Loss: 21.658157348632812 Reward: -12.0\nEpsilon = 0.045492657206714214\nEpsilon = 0.04548810794099354\nEpsilon = 0.04548355913019944\nEpsilon = 0.04547901077428642\nEpsilon = 0.04547446287320899\nEpsilon = 0.04546991542692167\nEpsilon = 0.04546536843537898\nEpsilon = 0.045460821898535446\nEpsilon = 0.04545627581634559\nEpsilon = 0.04545173018876396\nEpsilon = 0.04544718501574508\nEpsilon = 0.04544264029724351\nEpsilon = 0.04543809603321378\nEpsilon = 0.045433552223610466\nEpsilon = 0.045429008868388104\nEpsilon = 0.045424465967501264\nEpsilon = 0.045419923520904515\nEpsilon = 0.04541538152855242\nEpsilon = 0.04541083999039957\nEpsilon = 0.04540629890640053\nAgent: ddqn_agent . Episode 1651/2000. Number of steps to finish: 20. Loss: 18.447359085083008 Reward: -10.0\nEpsilon = 0.04540175827650989\nEpsilon = 0.04539721810068224\nEpsilon = 0.045392678378872175\nEpsilon = 0.04538813911103429\nEpsilon = 0.04538360029712318\nEpsilon = 0.04537906193709347\nEpsilon = 0.04537452403089976\nEpsilon = 0.045369986578496675\nEpsilon = 0.045365449579838825\nEpsilon = 0.045360913034880844\nEpsilon = 0.04535637694357736\nEpsilon = 0.045351841305883\nEpsilon = 0.04534730612175242\nEpsilon = 0.04534277139114024\nEpsilon = 0.04533823711400113\nEpsilon = 0.04533370329028973\nEpsilon = 0.04532916991996071\nEpsilon = 0.04532463700296871\nEpsilon = 0.045320104539268415\nEpsilon = 0.04531557252881449\nAgent: ddqn_agent . Episode 1652/2000. Number of steps to finish: 20. Loss: 22.686031341552734 Reward: -12.0\nEpsilon = 0.04531104097156161\nEpsilon = 0.045306509867464456\nEpsilon = 0.04530197921647771\nEpsilon = 0.04529744901855606\nEpsilon = 0.04529291927365421\nEpsilon = 0.045288389981726844\nEpsilon = 0.04528386114272867\nEpsilon = 0.0452793327566144\nEpsilon = 0.04527480482333874\nEpsilon = 0.04527027734285641\nEpsilon = 0.04526575031512212\nEpsilon = 0.04526122374009061\nEpsilon = 0.0452566976177166\nEpsilon = 0.045252171947954827\nEpsilon = 0.04524764673076003\nEpsilon = 0.04524312196608696\nEpsilon = 0.04523859765389035\nEpsilon = 0.04523407379412496\nEpsilon = 0.045229550386745546\nEpsilon = 0.04522502743170687\nAgent: ddqn_agent . Episode 1653/2000. Number of steps to finish: 20. Loss: 21.434980392456055 Reward: -14.0\nEpsilon = 0.0452205049289637\nEpsilon = 0.045215982878470805\nEpsilon = 0.04521146128018296\nEpsilon = 0.04520694013405494\nEpsilon = 0.04520241944004154\nEpsilon = 0.04519789919809754\nEpsilon = 0.04519337940817773\nEpsilon = 0.04518886007023691\nEpsilon = 0.045184341184229884\nEpsilon = 0.04517982275011146\nEpsilon = 0.04517530476783645\nEpsilon = 0.045170787237359664\nEpsilon = 0.045166270158635925\nEpsilon = 0.04516175353162006\nEpsilon = 0.0451572373562669\nEpsilon = 0.04515272163253128\nEpsilon = 0.04514820636036802\nEpsilon = 0.04514369153973199\nEpsilon = 0.04513917717057801\nEpsilon = 0.04513466325286095\nAgent: ddqn_agent . Episode 1654/2000. Number of steps to finish: 20. Loss: 23.202587127685547 Reward: -14.0\nEpsilon = 0.045130149786535664\nEpsilon = 0.04512563677155701\nEpsilon = 0.04512112420787986\nEpsilon = 0.04511661209545907\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.04511210043424953\nEpsilon = 0.0451075892242061\nEpsilon = 0.045103078465283684\nEpsilon = 0.04509856815743716\nEpsilon = 0.045094058300621416\nEpsilon = 0.04508954889479135\nEpsilon = 0.04508503993990187\nEpsilon = 0.04508053143590788\nEpsilon = 0.04507602338276429\nEpsilon = 0.045071515780426015\nEpsilon = 0.04506700862884797\nEpsilon = 0.04506250192798509\nEpsilon = 0.04505799567779229\nEpsilon = 0.04505348987822451\nEpsilon = 0.04504898452923669\nEpsilon = 0.045044479630783764\nAgent: ddqn_agent . Episode 1655/2000. Number of steps to finish: 20. Loss: 22.35291290283203 Reward: -14.0\nEpsilon = 0.045039975182820685\nEpsilon = 0.045035471185302406\nEpsilon = 0.04503096763818388\nEpsilon = 0.045026464541420064\nEpsilon = 0.04502196189496592\nEpsilon = 0.04501745969877642\nEpsilon = 0.04501295795280654\nEpsilon = 0.04500845665701126\nEpsilon = 0.04500395581134556\nEpsilon = 0.044999455415764424\nEpsilon = 0.04499495547022285\nEpsilon = 0.044990455974675825\nEpsilon = 0.044985956929078356\nEpsilon = 0.044981458333385446\nEpsilon = 0.04497696018755211\nEpsilon = 0.04497246249153335\nEpsilon = 0.0449679652452842\nEpsilon = 0.044963468448759675\nEpsilon = 0.044958972101914796\nEpsilon = 0.04495447620470461\nAgent: ddqn_agent . Episode 1656/2000. Number of steps to finish: 20. Loss: 18.967632293701172 Reward: -14.0\nEpsilon = 0.04494998075708414\nEpsilon = 0.04494548575900843\nEpsilon = 0.04494099121043253\nEpsilon = 0.04493649711131149\nEpsilon = 0.04493200346160035\nEpsilon = 0.044927510261254196\nEpsilon = 0.04492301751022807\nEpsilon = 0.04491852520847705\nEpsilon = 0.0449140333559562\nEpsilon = 0.044909541952620605\nEpsilon = 0.044905050998425344\nEpsilon = 0.044900560493325505\nEpsilon = 0.04489607043727617\nEpsilon = 0.044891580830232446\nEpsilon = 0.04488709167214942\nEpsilon = 0.04488260296298221\nEpsilon = 0.04487811470268591\nEpsilon = 0.04487362689121564\nEpsilon = 0.044869139528526523\nEpsilon = 0.044864652614573675\nAgent: ddqn_agent . Episode 1657/2000. Number of steps to finish: 20. Loss: 21.048660278320312 Reward: -18.0\nEpsilon = 0.044860166149312215\nEpsilon = 0.044855680132697286\nEpsilon = 0.04485119456468402\nEpsilon = 0.04484670944522755\nEpsilon = 0.04484222477428303\nEpsilon = 0.0448377405518056\nEpsilon = 0.04483325677775042\nEpsilon = 0.044828773452072646\nEpsilon = 0.04482429057472744\nEpsilon = 0.04481980814566997\nEpsilon = 0.0448153261648554\nEpsilon = 0.044810844632238916\nEpsilon = 0.044806363547775695\nEpsilon = 0.04480188291142092\nEpsilon = 0.04479740272312978\nEpsilon = 0.04479292298285747\nEpsilon = 0.04478844369055918\nEpsilon = 0.04478396484619013\nEpsilon = 0.04477948644970551\nEpsilon = 0.04477500850106054\nAgent: ddqn_agent . Episode 1658/2000. Number of steps to finish: 20. Loss: 21.72659683227539 Reward: -12.0\nEpsilon = 0.04477053100021043\nEpsilon = 0.04476605394711041\nEpsilon = 0.0447615773417157\nEpsilon = 0.04475710118398153\nEpsilon = 0.044752625473863133\nEpsilon = 0.04474815021131575\nEpsilon = 0.04474367539629462\nAgent: ddqn_agent . Episode 1659/2000. Number of steps to finish: 7. Loss: 7.117739677429199 Reward: 5.0\nEpsilon = 0.04473920102875499\nEpsilon = 0.04473472710865212\nEpsilon = 0.044730253635941256\nEpsilon = 0.044725780610577665\nEpsilon = 0.04472130803251661\nEpsilon = 0.04471683590171336\nEpsilon = 0.04471236421812319\nEpsilon = 0.04470789298170138\nEpsilon = 0.04470342219240321\nEpsilon = 0.04469895185018397\nEpsilon = 0.044694481954998956\nEpsilon = 0.044690012506803455\nEpsilon = 0.044685543505552774\nEpsilon = 0.04468107495120222\nEpsilon = 0.044676606843707105\nEpsilon = 0.044672139183022735\nEpsilon = 0.044667671969104435\nEpsilon = 0.04466320520190752\nEpsilon = 0.044658738881387335\nEpsilon = 0.0446542730074992\nAgent: ddqn_agent . Episode 1660/2000. Number of steps to finish: 20. Loss: 18.71839714050293 Reward: -12.0\nEpsilon = 0.044649807580198445\nEpsilon = 0.044645342599440425\nEpsilon = 0.04464087806518048\nEpsilon = 0.044636413977373966\nEpsilon = 0.04463195033597623\nEpsilon = 0.04462748714094263\nEpsilon = 0.04462302439222854\nEpsilon = 0.044618562089789315\nEpsilon = 0.04461410023358034\nEpsilon = 0.04460963882355698\nEpsilon = 0.044605177859674626\nEpsilon = 0.04460071734188866\nEpsilon = 0.044596257270154475\nEpsilon = 0.04459179764442746\nEpsilon = 0.044587338464663014\nEpsilon = 0.044582879730816545\nEpsilon = 0.04457842144284346\nEpsilon = 0.04457396360069918\nEpsilon = 0.04456950620433911\nEpsilon = 0.044565049253718673\nAgent: ddqn_agent . Episode 1661/2000. Number of steps to finish: 20. Loss: 20.809951782226562 Reward: -14.0\nEpsilon = 0.0445605927487933\nEpsilon = 0.044556136689518425\nEpsilon = 0.044551681075849475\nEpsilon = 0.04454722590774189\nEpsilon = 0.04454277118515112\nEpsilon = 0.0445383169080326\nEpsilon = 0.0445338630763418\nEpsilon = 0.04452940969003417\nEpsilon = 0.04452495674906516\nEpsilon = 0.044520504253390256\nEpsilon = 0.044516052202964916\nEpsilon = 0.04451160059774462\nEpsilon = 0.04450714943768485\nEpsilon = 0.04450269872274108\nEpsilon = 0.04449824845286881\nEpsilon = 0.04449379862802352\nEpsilon = 0.04448934924816072\nEpsilon = 0.0444849003132359\nEpsilon = 0.04448045182320458\nEpsilon = 0.04447600377802226\nAgent: ddqn_agent . Episode 1662/2000. Number of steps to finish: 20. Loss: 21.70408058166504 Reward: -16.0\nEpsilon = 0.04447155617764446\nEpsilon = 0.04446710902202669\nEpsilon = 0.04446266231112449\nEpsilon = 0.044458216044893376\nEpsilon = 0.044453770223288884\nEpsilon = 0.044449324846266555\nEpsilon = 0.04444487991378193\nEpsilon = 0.044440435425790556\nEpsilon = 0.04443599138224798\nEpsilon = 0.04443154778310975\nEpsilon = 0.044427104628331444\nEpsilon = 0.04442266191786861\nEpsilon = 0.044418219651676825\nEpsilon = 0.044413777829711655\nEpsilon = 0.04440933645192868\nEpsilon = 0.04440489551828349\nEpsilon = 0.04440045502873166\nEpsilon = 0.04439601498322879\nEpsilon = 0.04439157538173046\nEpsilon = 0.04438713622419229\nAgent: ddqn_agent . Episode 1663/2000. Number of steps to finish: 20. Loss: 19.126903533935547 Reward: -10.0\nEpsilon = 0.04438269751056987\nEpsilon = 0.04437825924081881\nEpsilon = 0.04437382141489473\nEpsilon = 0.04436938403275324\nEpsilon = 0.04436494709434997\nEpsilon = 0.04436051059964053\nEpsilon = 0.044356074548580565\nEpsilon = 0.04435163894112571\nEpsilon = 0.0443472037772316\nEpsilon = 0.04434276905685388\nEpsilon = 0.044338334779948196\nEpsilon = 0.0443339009464702\nEpsilon = 0.044329467556375554\nEpsilon = 0.044325034609619916\nEpsilon = 0.04432060210615896\nEpsilon = 0.044316170045948344\nEpsilon = 0.04431173842894375\nEpsilon = 0.04430730725510086\nEpsilon = 0.044302876524375354\nEpsilon = 0.04429844623672292\nAgent: ddqn_agent . Episode 1664/2000. Number of steps to finish: 20. Loss: 17.95986557006836 Reward: -20.0\nEpsilon = 0.04429401639209925\nEpsilon = 0.04428958699046004\nEpsilon = 0.044285158031760995\nEpsilon = 0.044280729515957816\nEpsilon = 0.04427630144300622\nEpsilon = 0.04427187381286192\nEpsilon = 0.044267446625480635\nEpsilon = 0.044263019880818086\nEpsilon = 0.044258593578830004\nAgent: ddqn_agent . Episode 1665/2000. Number of steps to finish: 9. Loss: 10.707256317138672 Reward: 3.0\nEpsilon = 0.04425416771947212\nEpsilon = 0.04424974230270017\nEpsilon = 0.0442453173284699\nEpsilon = 0.044240892796737055\nEpsilon = 0.044236468707457385\nEpsilon = 0.04423204506058664\nEpsilon = 0.044227621856080576\nEpsilon = 0.044223199093894966\nEpsilon = 0.04421877677398558\nEpsilon = 0.04421435489630818\nEpsilon = 0.04420993346081855\nEpsilon = 0.044205512467472466\nEpsilon = 0.04420109191622572\nEpsilon = 0.0441966718070341\nEpsilon = 0.044192252139853394\nEpsilon = 0.04418783291463941\nEpsilon = 0.044183414131347944\nEpsilon = 0.04417899578993481\nEpsilon = 0.044174577890355816\nEpsilon = 0.04417016043256678\nAgent: ddqn_agent . Episode 1666/2000. Number of steps to finish: 20. Loss: 19.3339786529541 Reward: -16.0\nEpsilon = 0.044165743416523524\nEpsilon = 0.044161326842181874\nEpsilon = 0.044156910709497656\nEpsilon = 0.044152495018426705\nEpsilon = 0.04414807976892486\nEpsilon = 0.044143664960947966\nEpsilon = 0.044139250594451875\nEpsilon = 0.04413483666939243\nEpsilon = 0.044130423185725486\nEpsilon = 0.04412601014340691\nEpsilon = 0.044121597542392574\nEpsilon = 0.044117185382638334\nEpsilon = 0.04411277366410007\nEpsilon = 0.04410836238673366\nEpsilon = 0.044103951550494985\nEpsilon = 0.044099541155339934\nEpsilon = 0.0440951312012244\nEpsilon = 0.04409072168810428\nEpsilon = 0.04408631261593547\nEpsilon = 0.044081903984673876\nAgent: ddqn_agent . Episode 1667/2000. Number of steps to finish: 20. Loss: 19.362272262573242 Reward: -12.0\nEpsilon = 0.04407749579427541\nEpsilon = 0.044073088044695986\nEpsilon = 0.04406868073589152\nEpsilon = 0.04406427386781793\nEpsilon = 0.044059867440431146\nEpsilon = 0.044055461453687106\nEpsilon = 0.044051055907541735\nEpsilon = 0.044046650801950984\nEpsilon = 0.04404224613687079\nEpsilon = 0.044037841912257104\nEpsilon = 0.04403343812806588\nEpsilon = 0.044029034784253074\nEpsilon = 0.04402463188077465\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.04402022941758657\nEpsilon = 0.04401582739464481\nEpsilon = 0.04401142581190535\nEpsilon = 0.04400702466932416\nEpsilon = 0.044002623966857224\nEpsilon = 0.04399822370446054\nEpsilon = 0.0439938238820901\nAgent: ddqn_agent . Episode 1668/2000. Number of steps to finish: 20. Loss: 19.551963806152344 Reward: -12.0\nEpsilon = 0.04398942449970189\nEpsilon = 0.04398502555725192\nEpsilon = 0.0439806270546962\nEpsilon = 0.04397622899199073\nEpsilon = 0.04397183136909153\nEpsilon = 0.04396743418595462\nEpsilon = 0.043963037442536025\nEpsilon = 0.04395864113879177\nEpsilon = 0.04395424527467789\nEpsilon = 0.04394984985015042\nEpsilon = 0.043945454865165406\nEpsilon = 0.04394106031967889\nEpsilon = 0.043936666213646926\nEpsilon = 0.04393227254702556\nEpsilon = 0.04392787931977086\nEpsilon = 0.04392348653183888\nEpsilon = 0.0439190941831857\nEpsilon = 0.043914702273767384\nEpsilon = 0.043910310803540006\nEpsilon = 0.04390591977245965\nAgent: ddqn_agent . Episode 1669/2000. Number of steps to finish: 20. Loss: 19.913734436035156 Reward: -14.0\nEpsilon = 0.04390152918048241\nEpsilon = 0.04389713902756436\nEpsilon = 0.043892749313661605\nEpsilon = 0.04388836003873024\nEpsilon = 0.04388397120272637\nEpsilon = 0.043879582805606096\nEpsilon = 0.043875194847325535\nEpsilon = 0.0438708073278408\nEpsilon = 0.04386642024710802\nEpsilon = 0.043862033605083305\nEpsilon = 0.043857647401722794\nEpsilon = 0.043853261636982624\nEpsilon = 0.04384887631081893\nEpsilon = 0.043844491423187845\nEpsilon = 0.04384010697404553\nEpsilon = 0.043835722963348124\nEpsilon = 0.04383133939105179\nEpsilon = 0.043826956257112686\nEpsilon = 0.04382257356148697\nEpsilon = 0.043818191304130824\nAgent: ddqn_agent . Episode 1670/2000. Number of steps to finish: 20. Loss: 22.949750900268555 Reward: -14.0\nEpsilon = 0.04381380948500041\nEpsilon = 0.04380942810405191\nEpsilon = 0.043805047161241506\nEpsilon = 0.043800666656525385\nEpsilon = 0.043796286589859734\nEpsilon = 0.04379190696120075\nEpsilon = 0.04378752777050463\nEpsilon = 0.043783149017727584\nEpsilon = 0.04377877070282581\nEpsilon = 0.04377439282575553\nEpsilon = 0.04377001538647295\nEpsilon = 0.04376563838493431\nEpsilon = 0.04376126182109582\nEpsilon = 0.04375688569491371\nEpsilon = 0.04375251000634422\nEpsilon = 0.04374813475534359\nEpsilon = 0.043743759941868056\nEpsilon = 0.04373938556587387\nEpsilon = 0.04373501162731728\nEpsilon = 0.04373063812615455\nAgent: ddqn_agent . Episode 1671/2000. Number of steps to finish: 20. Loss: 21.04388999938965 Reward: -14.0\nEpsilon = 0.04372626506234193\nEpsilon = 0.0437218924358357\nEpsilon = 0.04371752024659212\nEpsilon = 0.04371314849456746\nEpsilon = 0.043708777179718\nEpsilon = 0.04370440630200003\nEpsilon = 0.04370003586136983\nEpsilon = 0.04369566585778369\nEpsilon = 0.04369129629119792\nEpsilon = 0.0436869271615688\nEpsilon = 0.04368255846885264\nEpsilon = 0.04367819021300576\nEpsilon = 0.04367382239398446\nEpsilon = 0.04366945501174506\nEpsilon = 0.04366508806624388\nEpsilon = 0.04366072155743726\nEpsilon = 0.04365635548528152\nEpsilon = 0.04365198984973299\nEpsilon = 0.043647624650748014\nEpsilon = 0.04364325988828294\nAgent: ddqn_agent . Episode 1672/2000. Number of steps to finish: 20. Loss: 25.412240982055664 Reward: -10.0\nEpsilon = 0.04363889556229411\nEpsilon = 0.04363453167273788\nEpsilon = 0.04363016821957061\nEpsilon = 0.043625805202748655\nEpsilon = 0.04362144262222838\nEpsilon = 0.04361708047796616\nEpsilon = 0.04361271876991837\nEpsilon = 0.04360835749804137\nEpsilon = 0.04360399666229157\nEpsilon = 0.04359963626262534\nEpsilon = 0.04359527629899908\nEpsilon = 0.04359091677136918\nEpsilon = 0.043586557679692044\nEpsilon = 0.04358219902392407\nEpsilon = 0.04357784080402168\nEpsilon = 0.04357348301994128\nEpsilon = 0.043569125671639286\nEpsilon = 0.04356476875907212\nEpsilon = 0.04356041228219622\nEpsilon = 0.043556056240968\nAgent: ddqn_agent . Episode 1673/2000. Number of steps to finish: 20. Loss: 20.13574981689453 Reward: -12.0\nEpsilon = 0.0435517006353439\nEpsilon = 0.043547345465280364\nEpsilon = 0.04354299073073384\nEpsilon = 0.04353863643166077\nEpsilon = 0.043534282568017606\nEpsilon = 0.0435299291397608\nEpsilon = 0.04352557614684683\nEpsilon = 0.04352122358923215\nEpsilon = 0.043516871466873226\nEpsilon = 0.04351251977972654\nEpsilon = 0.04350816852774857\nEpsilon = 0.043503817710895794\nEpsilon = 0.04349946732912471\nEpsilon = 0.04349511738239179\nEpsilon = 0.043490767870653556\nEpsilon = 0.04348641879386649\nEpsilon = 0.04348207015198711\nEpsilon = 0.04347772194497191\nEpsilon = 0.04347337417277741\nEpsilon = 0.043469026835360136\nAgent: ddqn_agent . Episode 1674/2000. Number of steps to finish: 20. Loss: 18.348682403564453 Reward: -10.0\nEpsilon = 0.0434646799326766\nEpsilon = 0.04346033346468334\nEpsilon = 0.04345598743133687\nEpsilon = 0.043451641832593736\nEpsilon = 0.043447296668410476\nEpsilon = 0.04344295193874364\nEpsilon = 0.043438607643549766\nEpsilon = 0.043434263782785414\nEpsilon = 0.04342992035640714\nEpsilon = 0.0434255773643715\nEpsilon = 0.04342123480663506\nEpsilon = 0.0434168926831544\nEpsilon = 0.043412550993886086\nEpsilon = 0.043408209738786695\nEpsilon = 0.04340386891781282\nEpsilon = 0.043399528530921036\nEpsilon = 0.04339518857806794\nEpsilon = 0.04339084905921014\nEpsilon = 0.04338650997430422\nEpsilon = 0.04338217132330679\nAgent: ddqn_agent . Episode 1675/2000. Number of steps to finish: 20. Loss: 19.070301055908203 Reward: -14.0\nEpsilon = 0.043377833106174456\nEpsilon = 0.04337349532286384\nEpsilon = 0.04336915797333155\nEpsilon = 0.04336482105753422\nEpsilon = 0.043360484575428467\nEpsilon = 0.04335614852697092\nEpsilon = 0.043351812912118225\nEpsilon = 0.04334747773082701\nEpsilon = 0.04334314298305393\nEpsilon = 0.04333880866875563\nEpsilon = 0.04333447478788875\nEpsilon = 0.04333014134040997\nEpsilon = 0.04332580832627592\nEpsilon = 0.043321475745443296\nEpsilon = 0.04331714359786875\nEpsilon = 0.04331281188350896\nEpsilon = 0.043308480602320606\nEpsilon = 0.043304149754260376\nEpsilon = 0.04329981933928495\nEpsilon = 0.04329548935735102\nAgent: ddqn_agent . Episode 1676/2000. Number of steps to finish: 20. Loss: 21.287160873413086 Reward: -14.0\nEpsilon = 0.04329115980841529\nEpsilon = 0.04328683069243445\nEpsilon = 0.043282502009365204\nEpsilon = 0.04327817375916427\nEpsilon = 0.043273845941788354\nEpsilon = 0.04326951855719417\nEpsilon = 0.043265191605338454\nEpsilon = 0.04326086508617792\nEpsilon = 0.0432565389996693\nEpsilon = 0.043252213345769334\nEpsilon = 0.04324788812443476\nEpsilon = 0.04324356333562231\nEpsilon = 0.043239238979288754\nEpsilon = 0.04323491505539082\nEpsilon = 0.043230591563885284\nEpsilon = 0.043226268504728896\nEpsilon = 0.04322194587787842\nEpsilon = 0.043217623683290636\nEpsilon = 0.043213301920922306\nEpsilon = 0.043208980590730216\nAgent: ddqn_agent . Episode 1677/2000. Number of steps to finish: 20. Loss: 18.75282859802246 Reward: -12.0\nEpsilon = 0.04320465969267114\nEpsilon = 0.04320033922670188\nEpsilon = 0.043196019192779204\nEpsilon = 0.04319169959085993\nEpsilon = 0.043187380420900846\nEpsilon = 0.04318306168285876\nEpsilon = 0.04317874337669047\nEpsilon = 0.0431744255023528\nEpsilon = 0.043170108059802564\nEpsilon = 0.04316579104899659\nEpsilon = 0.043161474469891686\nEpsilon = 0.0431571583224447\nEpsilon = 0.04315284260661245\nEpsilon = 0.04314852732235179\nEpsilon = 0.04314421246961955\nEpsilon = 0.043139898048372594\nEpsilon = 0.04313558405856776\nEpsilon = 0.043131270500161904\nEpsilon = 0.04312695737311189\nEpsilon = 0.04312264467737458\nAgent: ddqn_agent . Episode 1678/2000. Number of steps to finish: 20. Loss: 18.984891891479492 Reward: -12.0\nEpsilon = 0.04311833241290684\nEpsilon = 0.043114020579665555\nEpsilon = 0.04310970917760759\nEpsilon = 0.04310539820668983\nEpsilon = 0.04310108766686916\nEpsilon = 0.04309677755810248\nEpsilon = 0.043092467880346666\nEpsilon = 0.04308815863355863\nEpsilon = 0.04308384981769527\nEpsilon = 0.0430795414327135\nEpsilon = 0.04307523347857023\nEpsilon = 0.043070925955222376\nEpsilon = 0.04306661886262685\nEpsilon = 0.04306231220074059\nEpsilon = 0.043058005969520516\nEpsilon = 0.04305370016892356\nEpsilon = 0.04304939479890667\nEpsilon = 0.043045089859426774\nEpsilon = 0.043040785350440834\nEpsilon = 0.04303648127190579\nAgent: ddqn_agent . Episode 1679/2000. Number of steps to finish: 20. Loss: 21.03368377685547 Reward: -14.0\nEpsilon = 0.0430321776237786\nEpsilon = 0.04302787440601623\nEpsilon = 0.043023571618575625\nEpsilon = 0.043019269261413765\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.043014967334487626\nEpsilon = 0.043010665837754175\nEpsilon = 0.0430063647711704\nEpsilon = 0.04300206413469328\nEpsilon = 0.04299776392827981\nEpsilon = 0.042993464151886984\nEpsilon = 0.0429891648054718\nEpsilon = 0.04298486588899125\nEpsilon = 0.04298056740240235\nEpsilon = 0.042976269345662114\nEpsilon = 0.04297197171872755\nEpsilon = 0.04296767452155568\nEpsilon = 0.04296337775410352\nEpsilon = 0.04295908141632811\nEpsilon = 0.042954785508186474\nEpsilon = 0.042950490029635656\nAgent: ddqn_agent . Episode 1680/2000. Number of steps to finish: 20. Loss: 20.227638244628906 Reward: -12.0\nEpsilon = 0.04294619498063269\nEpsilon = 0.04294190036113463\nEpsilon = 0.042937606171098514\nEpsilon = 0.04293331241048141\nEpsilon = 0.042929019079240356\nEpsilon = 0.04292472617733243\nEpsilon = 0.0429204337047147\nEpsilon = 0.04291614166134423\nEpsilon = 0.042911850047178096\nEpsilon = 0.04290755886217338\nEpsilon = 0.04290326810628716\nEpsilon = 0.04289897777947653\nEpsilon = 0.04289468788169858\nEpsilon = 0.04289039841291041\nEpsilon = 0.04288610937306912\nEpsilon = 0.04288182076213182\nEpsilon = 0.042877532580055604\nEpsilon = 0.0428732448267976\nEpsilon = 0.04286895750231492\nEpsilon = 0.04286467060656469\nAgent: ddqn_agent . Episode 1681/2000. Number of steps to finish: 20. Loss: 17.277658462524414 Reward: -18.0\nEpsilon = 0.042860384139504035\nEpsilon = 0.04285609810109008\nEpsilon = 0.04285181249127997\nEpsilon = 0.042847527310030845\nEpsilon = 0.04284324255729984\nEpsilon = 0.042838958233044115\nEpsilon = 0.04283467433722081\nEpsilon = 0.042830390869787086\nEpsilon = 0.04282610783070011\nEpsilon = 0.04282182521991704\nEpsilon = 0.042817543037395045\nEpsilon = 0.042813261283091306\nEpsilon = 0.042808979956963\nEpsilon = 0.042804699058967305\nEpsilon = 0.04280041858906141\nEpsilon = 0.0427961385472025\nEpsilon = 0.04279185893334778\nEpsilon = 0.04278757974745445\nEpsilon = 0.04278330098947971\nEpsilon = 0.042779022659380765\nAgent: ddqn_agent . Episode 1682/2000. Number of steps to finish: 20. Loss: 22.902843475341797 Reward: -12.0\nEpsilon = 0.042774744757114826\nEpsilon = 0.04277046728263911\nEpsilon = 0.04276619023591085\nEpsilon = 0.042761913616887255\nEpsilon = 0.04275763742552557\nEpsilon = 0.042753361661783014\nEpsilon = 0.04274908632561684\nEpsilon = 0.042744811416984274\nEpsilon = 0.04274053693584257\nEpsilon = 0.04273626288214899\nEpsilon = 0.042731989255860775\nEpsilon = 0.04272771605693519\nEpsilon = 0.042723443285329496\nEpsilon = 0.042719170941000965\nEpsilon = 0.04271489902390686\nEpsilon = 0.042710627534004474\nEpsilon = 0.042706356471251074\nEpsilon = 0.04270208583560395\nEpsilon = 0.04269781562702039\nEpsilon = 0.04269354584545768\nAgent: ddqn_agent . Episode 1683/2000. Number of steps to finish: 20. Loss: 19.662492752075195 Reward: -14.0\nEpsilon = 0.04268927649087314\nEpsilon = 0.042685007563224055\nEpsilon = 0.042680739062467735\nEpsilon = 0.04267647098856149\nEpsilon = 0.042672203341462635\nEpsilon = 0.042667936121128486\nEpsilon = 0.042663669327516376\nAgent: ddqn_agent . Episode 1684/2000. Number of steps to finish: 7. Loss: 7.106942176818848 Reward: 5.0\nEpsilon = 0.042659402960583626\nEpsilon = 0.042655137020287566\nEpsilon = 0.04265087150658554\nEpsilon = 0.04264660641943488\nEpsilon = 0.04264234175879294\nEpsilon = 0.04263807752461706\nEpsilon = 0.0426338137168646\nEpsilon = 0.04262955033549291\nEpsilon = 0.04262528738045936\nEpsilon = 0.04262102485172132\nEpsilon = 0.04261676274923615\nEpsilon = 0.042612501072961224\nEpsilon = 0.04260823982285393\nEpsilon = 0.042603978998871646\nEpsilon = 0.04259971860097176\nEpsilon = 0.04259545862911166\nEpsilon = 0.04259119908324875\nEpsilon = 0.042586939963340426\nEpsilon = 0.04258268126934409\nEpsilon = 0.042578423001217156\nAgent: ddqn_agent . Episode 1685/2000. Number of steps to finish: 20. Loss: 18.589290618896484 Reward: -12.0\nEpsilon = 0.042574165158917036\nEpsilon = 0.042569907742401145\nEpsilon = 0.0425656507516269\nEpsilon = 0.04256139418655174\nEpsilon = 0.04255713804713309\nEpsilon = 0.04255288233332838\nEpsilon = 0.042548627045095044\nEpsilon = 0.04254437218239054\nEpsilon = 0.0425401177451723\nEpsilon = 0.04253586373339779\nEpsilon = 0.042531610147024446\nEpsilon = 0.04252735698600974\nEpsilon = 0.04252310425031114\nEpsilon = 0.04251885193988611\nEpsilon = 0.04251460005469212\nEpsilon = 0.04251034859468665\nEpsilon = 0.042506097559827184\nEpsilon = 0.042501846950071205\nEpsilon = 0.0424975967653762\nEpsilon = 0.042493347005699664\nAgent: ddqn_agent . Episode 1686/2000. Number of steps to finish: 20. Loss: 21.866230010986328 Reward: -12.0\nEpsilon = 0.04248909767099909\nEpsilon = 0.04248484876123199\nEpsilon = 0.04248060027635587\nEpsilon = 0.042476352216328235\nEpsilon = 0.042472104581106604\nEpsilon = 0.042467857370648496\nEpsilon = 0.04246361058491143\nEpsilon = 0.04245936422385294\nEpsilon = 0.04245511828743056\nAgent: ddqn_agent . Episode 1687/2000. Number of steps to finish: 9. Loss: 9.02205753326416 Reward: 3.0\nEpsilon = 0.04245087277560181\nEpsilon = 0.04244662768832425\nEpsilon = 0.04244238302555542\nEpsilon = 0.04243813878725287\nEpsilon = 0.042433894973374145\nEpsilon = 0.04242965158387681\nEpsilon = 0.04242540861871842\nEpsilon = 0.04242116607785655\nEpsilon = 0.04241692396124876\nAgent: ddqn_agent . Episode 1688/2000. Number of steps to finish: 9. Loss: 10.765670776367188 Reward: 3.0\nEpsilon = 0.04241268226885264\nEpsilon = 0.042408441000625755\nEpsilon = 0.04240420015652569\nEpsilon = 0.04239995973651004\nEpsilon = 0.04239571974053639\nEpsilon = 0.04239148016856233\nEpsilon = 0.042387241020545474\nEpsilon = 0.04238300229644342\nEpsilon = 0.04237876399621378\nEpsilon = 0.042374526119814154\nEpsilon = 0.042370288667202174\nEpsilon = 0.042366051638335456\nEpsilon = 0.04236181503317162\nEpsilon = 0.042357578851668305\nEpsilon = 0.042353343093783136\nEpsilon = 0.04234910775947376\nEpsilon = 0.04234487284869781\nEpsilon = 0.04234063836141294\nEpsilon = 0.0423364042975768\nEpsilon = 0.04233217065714704\nAgent: ddqn_agent . Episode 1689/2000. Number of steps to finish: 20. Loss: 20.327510833740234 Reward: -20.0\nEpsilon = 0.04232793744008133\nEpsilon = 0.04232370464633732\nEpsilon = 0.04231947227587269\nEpsilon = 0.0423152403286451\nEpsilon = 0.042311008804612235\nEpsilon = 0.04230677770373177\nEpsilon = 0.042302547025961396\nEpsilon = 0.0422983167712588\nEpsilon = 0.04229408693958168\nEpsilon = 0.04228985753088772\nEpsilon = 0.04228562854513464\nEpsilon = 0.04228139998228012\nEpsilon = 0.04227717184228189\nEpsilon = 0.04227294412509766\nEpsilon = 0.04226871683068515\nEpsilon = 0.04226448995900208\nEpsilon = 0.04226026351000618\nEpsilon = 0.04225603748365518\nEpsilon = 0.042251811879906814\nEpsilon = 0.04224758669871882\nAgent: ddqn_agent . Episode 1690/2000. Number of steps to finish: 20. Loss: 20.4049129486084 Reward: -14.0\nEpsilon = 0.04224336194004895\nEpsilon = 0.04223913760385494\nEpsilon = 0.04223491369009456\nEpsilon = 0.04223069019872555\nEpsilon = 0.04222646712970568\nEpsilon = 0.04222224448299271\nEpsilon = 0.04221802225854441\nEpsilon = 0.042213800456318555\nEpsilon = 0.04220957907627292\nEpsilon = 0.042205358118365295\nEpsilon = 0.04220113758255346\nEpsilon = 0.042196917468795204\nEpsilon = 0.04219269777704832\nEpsilon = 0.042188478507270616\nEpsilon = 0.04218425965941989\nEpsilon = 0.042180041233453945\nEpsilon = 0.0421758232293306\nEpsilon = 0.042171605647007664\nEpsilon = 0.04216738848644296\nEpsilon = 0.04216317174759432\nAgent: ddqn_agent . Episode 1691/2000. Number of steps to finish: 20. Loss: 21.912137985229492 Reward: -12.0\nEpsilon = 0.04215895543041956\nEpsilon = 0.042154739534876515\nEpsilon = 0.042150524060923025\nEpsilon = 0.04214630900851693\nEpsilon = 0.04214209437761608\nEpsilon = 0.04213788016817832\nEpsilon = 0.04213366638016151\nEpsilon = 0.04212945301352349\nEpsilon = 0.04212524006822214\nEpsilon = 0.04212102754421532\nEpsilon = 0.0421168154414609\nEpsilon = 0.04211260375991675\nEpsilon = 0.042108392499540764\nEpsilon = 0.04210418166029081\nEpsilon = 0.042099971242124785\nEpsilon = 0.04209576124500057\nEpsilon = 0.04209155166887607\nEpsilon = 0.042087342513709186\nEpsilon = 0.04208313377945781\nEpsilon = 0.042078925466079865\nAgent: ddqn_agent . Episode 1692/2000. Number of steps to finish: 20. Loss: 21.744356155395508 Reward: -12.0\nEpsilon = 0.04207471757353326\nEpsilon = 0.042070510101775906\nEpsilon = 0.04206630305076573\nEpsilon = 0.04206209642046065\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.042057890210818606\nEpsilon = 0.04205368442179752\nEpsilon = 0.04204947905335534\nEpsilon = 0.04204527410545\nEpsilon = 0.04204106957803946\nEpsilon = 0.04203686547108165\nEpsilon = 0.04203266178453455\nEpsilon = 0.04202845851835609\nEpsilon = 0.04202425567250426\nEpsilon = 0.04202005324693701\nEpsilon = 0.04201585124161231\nEpsilon = 0.042011649656488154\nEpsilon = 0.042007448491522506\nEpsilon = 0.042003247746673356\nEpsilon = 0.04199904742189869\nEpsilon = 0.0419948475171565\nAgent: ddqn_agent . Episode 1693/2000. Number of steps to finish: 20. Loss: 18.477041244506836 Reward: -10.0\nEpsilon = 0.041990648032404786\nEpsilon = 0.04198644896760154\nEpsilon = 0.041982250322704785\nEpsilon = 0.04197805209767252\nEpsilon = 0.04197385429246275\nEpsilon = 0.041969656907033505\nEpsilon = 0.0419654599413428\nEpsilon = 0.04196126339534866\nEpsilon = 0.04195706726900913\nEpsilon = 0.04195287156228223\nAgent: ddqn_agent . Episode 1694/2000. Number of steps to finish: 10. Loss: 10.012075424194336 Reward: 2.0\nEpsilon = 0.041948676275126\nEpsilon = 0.04194448140749849\nEpsilon = 0.04194028695935774\nEpsilon = 0.041936092930661806\nEpsilon = 0.04193189932136874\nEpsilon = 0.0419277061314366\nEpsilon = 0.04192351336082346\nEpsilon = 0.04191932100948737\nEpsilon = 0.041915129077386426\nEpsilon = 0.04191093756447869\nEpsilon = 0.04190674647072224\nEpsilon = 0.04190255579607517\nEpsilon = 0.041898365540495565\nEpsilon = 0.041894175703941515\nEpsilon = 0.04188998628637112\nEpsilon = 0.041885797287742485\nEpsilon = 0.04188160870801371\nEpsilon = 0.04187742054714291\nEpsilon = 0.0418732328050882\nEpsilon = 0.04186904548180769\nAgent: ddqn_agent . Episode 1695/2000. Number of steps to finish: 20. Loss: 19.785968780517578 Reward: -18.0\nEpsilon = 0.04186485857725951\nEpsilon = 0.041860672091401784\nEpsilon = 0.041856486024192646\nEpsilon = 0.04185230037559023\nEpsilon = 0.04184811514555267\nEpsilon = 0.04184393033403812\nEpsilon = 0.041839745941004716\nEpsilon = 0.04183556196641062\nEpsilon = 0.041831378410213976\nEpsilon = 0.041827195272372955\nEpsilon = 0.04182301255284572\nEpsilon = 0.04181883025159044\nEpsilon = 0.04181464836856528\nEpsilon = 0.04181046690372842\nEpsilon = 0.04180628585703805\nEpsilon = 0.041802105228452346\nEpsilon = 0.041797925017929505\nEpsilon = 0.04179374522542771\nEpsilon = 0.041789565850905167\nEpsilon = 0.04178538689432008\nAgent: ddqn_agent . Episode 1696/2000. Number of steps to finish: 20. Loss: 21.348960876464844 Reward: -16.0\nEpsilon = 0.04178120835563064\nEpsilon = 0.04177703023479508\nEpsilon = 0.0417728525317716\nEpsilon = 0.04176867524651842\nEpsilon = 0.04176449837899377\nEpsilon = 0.04176032192915587\nEpsilon = 0.04175614589696296\nEpsilon = 0.04175197028237326\nEpsilon = 0.04174779508534503\nEpsilon = 0.04174362030583649\nEpsilon = 0.04173944594380591\nEpsilon = 0.04173527199921153\nEpsilon = 0.04173109847201161\nEpsilon = 0.04172692536216441\nEpsilon = 0.041722752669628195\nEpsilon = 0.04171858039436123\nEpsilon = 0.041714408536321794\nEpsilon = 0.041710237095468165\nEpsilon = 0.04170606607175862\nEpsilon = 0.04170189546515145\nAgent: ddqn_agent . Episode 1697/2000. Number of steps to finish: 20. Loss: 19.586002349853516 Reward: -14.0\nEpsilon = 0.04169772527560493\nEpsilon = 0.04169355550307737\nEpsilon = 0.04168938614752706\nEpsilon = 0.04168521720891231\nEpsilon = 0.04168104868719142\nEpsilon = 0.0416768805823227\nEpsilon = 0.04167271289426447\nEpsilon = 0.04166854562297504\nEpsilon = 0.041664378768412745\nEpsilon = 0.041660212330535906\nEpsilon = 0.041656046309302856\nEpsilon = 0.04165188070467193\nEpsilon = 0.04164771551660146\nEpsilon = 0.0416435507450498\nEpsilon = 0.041639386389975296\nEpsilon = 0.0416352224513363\nEpsilon = 0.04163105892909117\nEpsilon = 0.041626895823198264\nEpsilon = 0.04162273313361595\nEpsilon = 0.04161857086030259\nAgent: ddqn_agent . Episode 1698/2000. Number of steps to finish: 20. Loss: 19.50174903869629 Reward: -18.0\nEpsilon = 0.04161440900321656\nEpsilon = 0.04161024756231624\nEpsilon = 0.041606086537560004\nEpsilon = 0.04160192592890625\nEpsilon = 0.04159776573631336\nEpsilon = 0.04159360595973973\nEpsilon = 0.04158944659914376\nEpsilon = 0.04158528765448384\nEpsilon = 0.041581129125718395\nEpsilon = 0.04157697101280582\nEpsilon = 0.041572813315704546\nEpsilon = 0.04156865603437298\nEpsilon = 0.041564499168769545\nEpsilon = 0.041560342718852665\nEpsilon = 0.04155618668458078\nEpsilon = 0.04155203106591233\nEpsilon = 0.041547875862805735\nEpsilon = 0.041543721075219454\nEpsilon = 0.04153956670311193\nEpsilon = 0.04153541274644162\nAgent: ddqn_agent . Episode 1699/2000. Number of steps to finish: 20. Loss: 19.756654739379883 Reward: -10.0\nEpsilon = 0.041531259205166975\nEpsilon = 0.04152710607924646\nEpsilon = 0.04152295336863853\nEpsilon = 0.041518801073301664\nEpsilon = 0.041514649193194336\nEpsilon = 0.04151049772827502\nEpsilon = 0.04150634667850219\nEpsilon = 0.041502196043834344\nEpsilon = 0.04149804582422996\nEpsilon = 0.041493896019647536\nEpsilon = 0.04148974663004557\nEpsilon = 0.04148559765538257\nEpsilon = 0.04148144909561703\nEpsilon = 0.04147730095070747\nEpsilon = 0.0414731532206124\nEpsilon = 0.04146900590529034\nEpsilon = 0.04146485900469981\nEpsilon = 0.04146071251879934\nEpsilon = 0.04145656644754746\nEpsilon = 0.0414524207909027\nAgent: ddqn_agent . Episode 1700/2000. Number of steps to finish: 20. Loss: 21.38673973083496 Reward: -10.0\nEpsilon = 0.04144827554882361\nEpsilon = 0.04144413072126873\nEpsilon = 0.0414399863081966\nEpsilon = 0.041435842309565776\nEpsilon = 0.04143169872533482\nEpsilon = 0.04142755555546229\nEpsilon = 0.041423412799906745\nEpsilon = 0.041419270458626756\nEpsilon = 0.04141512853158089\nEpsilon = 0.04141098701872774\nEpsilon = 0.041406845920025866\nEpsilon = 0.041402705235433865\nEpsilon = 0.041398564964910324\nEpsilon = 0.04139442510841383\nEpsilon = 0.04139028566590299\nEpsilon = 0.041386146637336405\nEpsilon = 0.041382008022672674\nEpsilon = 0.04137786982187041\nEpsilon = 0.04137373203488822\nEpsilon = 0.04136959466168473\nAgent: ddqn_agent . Episode 1701/2000. Number of steps to finish: 20. Loss: 23.79071044921875 Reward: -16.0\nEpsilon = 0.04136545770221856\nEpsilon = 0.04136132115644834\nEpsilon = 0.04135718502433269\nEpsilon = 0.04135304930583026\nEpsilon = 0.04134891400089968\nEpsilon = 0.04134477910949959\nEpsilon = 0.04134064463158864\nEpsilon = 0.04133651056712548\nEpsilon = 0.041332376916068767\nAgent: ddqn_agent . Episode 1702/2000. Number of steps to finish: 9. Loss: 9.92951488494873 Reward: 3.0\nEpsilon = 0.04132824367837716\nEpsilon = 0.041324110854009326\nEpsilon = 0.04131997844292393\nEpsilon = 0.041315846445079636\nEpsilon = 0.041311714860435125\nEpsilon = 0.04130758368894908\nEpsilon = 0.04130345293058019\nEpsilon = 0.04129932258528713\nEpsilon = 0.041295192653028605\nEpsilon = 0.0412910631337633\nEpsilon = 0.04128693402744993\nEpsilon = 0.041282805334047185\nEpsilon = 0.04127867705351378\nEpsilon = 0.04127454918580843\nEpsilon = 0.04127042173088985\nEpsilon = 0.04126629468871676\nEpsilon = 0.041262168059247886\nEpsilon = 0.04125804184244196\nEpsilon = 0.04125391603825772\nEpsilon = 0.04124979064665389\nAgent: ddqn_agent . Episode 1703/2000. Number of steps to finish: 20. Loss: 23.177398681640625 Reward: -14.0\nEpsilon = 0.04124566566758923\nEpsilon = 0.04124154110102247\nEpsilon = 0.04123741694691237\nEpsilon = 0.04123329320521767\nEpsilon = 0.04122916987589715\nEpsilon = 0.04122504695890956\nEpsilon = 0.041220924454213666\nEpsilon = 0.041216802361768244\nEpsilon = 0.04121268068153207\nEpsilon = 0.041208559413463915\nEpsilon = 0.041204438557522566\nEpsilon = 0.04120031811366681\nEpsilon = 0.04119619808185544\nEpsilon = 0.041192078462047255\nEpsilon = 0.04118795925420105\nEpsilon = 0.04118384045827563\nEpsilon = 0.041179722074229805\nEpsilon = 0.04117560410202238\nEpsilon = 0.04117148654161218\nEpsilon = 0.041167369392958016\nAgent: ddqn_agent . Episode 1704/2000. Number of steps to finish: 20. Loss: 22.435874938964844 Reward: -12.0\nEpsilon = 0.04116325265601872\nEpsilon = 0.04115913633075312\nEpsilon = 0.04115502041712004\nEpsilon = 0.04115090491507833\nEpsilon = 0.04114678982458682\nEpsilon = 0.04114267514560436\nEpsilon = 0.0411385608780898\nEpsilon = 0.04113444702200199\nEpsilon = 0.04113033357729979\nEpsilon = 0.04112622054394206\nEpsilon = 0.04112210792188767\nEpsilon = 0.04111799571109548\nEpsilon = 0.04111388391152437\nEpsilon = 0.04110977252313321\nEpsilon = 0.0411056615458809\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.04110155097972631\nEpsilon = 0.04109744082462834\nEpsilon = 0.04109333108054587\nEpsilon = 0.041089221747437815\nEpsilon = 0.04108511282526307\nAgent: ddqn_agent . Episode 1705/2000. Number of steps to finish: 20. Loss: 20.633529663085938 Reward: -12.0\nEpsilon = 0.04108100431398055\nEpsilon = 0.04107689621354915\nEpsilon = 0.041072788523927796\nEpsilon = 0.0410686812450754\nEpsilon = 0.041064574376950894\nEpsilon = 0.0410604679195132\nEpsilon = 0.04105636187272125\nEpsilon = 0.041052256236533975\nEpsilon = 0.04104815101091032\nEpsilon = 0.04104404619580923\nEpsilon = 0.04103994179118965\nEpsilon = 0.04103583779701053\nEpsilon = 0.04103173421323083\nEpsilon = 0.041027631039809505\nEpsilon = 0.04102352827670552\nEpsilon = 0.041019425923877854\nEpsilon = 0.04101532398128547\nEpsilon = 0.041011222448887344\nEpsilon = 0.04100712132664246\nEpsilon = 0.04100302061450979\nAgent: ddqn_agent . Episode 1706/2000. Number of steps to finish: 20. Loss: 22.49425506591797 Reward: -18.0\nEpsilon = 0.04099892031244834\nEpsilon = 0.0409948204204171\nEpsilon = 0.04099072093837506\nEpsilon = 0.04098662186628122\nEpsilon = 0.040982523204094595\nEpsilon = 0.04097842495177419\nEpsilon = 0.04097432710927901\nEpsilon = 0.04097022967656808\nEpsilon = 0.040966132653600426\nEpsilon = 0.04096203604033507\nEpsilon = 0.04095793983673104\nEpsilon = 0.040953844042747364\nEpsilon = 0.04094974865834309\nEpsilon = 0.04094565368347725\nEpsilon = 0.04094155911810891\nEpsilon = 0.0409374649621971\nEpsilon = 0.04093337121570088\nEpsilon = 0.04092927787857931\nEpsilon = 0.04092518495079145\nEpsilon = 0.04092109243229637\nAgent: ddqn_agent . Episode 1707/2000. Number of steps to finish: 20. Loss: 19.884504318237305 Reward: -14.0\nEpsilon = 0.04091700032305314\nEpsilon = 0.040912908623020834\nEpsilon = 0.04090881733215853\nEpsilon = 0.04090472645042532\nEpsilon = 0.04090063597778028\nEpsilon = 0.0408965459141825\nEpsilon = 0.040892456259591085\nEpsilon = 0.04088836701396513\nEpsilon = 0.040884278177263735\nAgent: ddqn_agent . Episode 1708/2000. Number of steps to finish: 9. Loss: 7.509411811828613 Reward: 3.0\nEpsilon = 0.04088018974944601\nEpsilon = 0.040876101730471064\nEpsilon = 0.04087201412029802\nEpsilon = 0.04086792691888599\nEpsilon = 0.0408638401261941\nEpsilon = 0.04085975374218148\nEpsilon = 0.04085566776680726\nEpsilon = 0.04085158220003058\nEpsilon = 0.04084749704181058\nEpsilon = 0.0408434122921064\nEpsilon = 0.040839327950877184\nEpsilon = 0.0408352440180821\nEpsilon = 0.04083116049368029\nEpsilon = 0.04082707737763092\nEpsilon = 0.04082299466989316\nEpsilon = 0.040818912370426175\nEpsilon = 0.04081483047918913\nEpsilon = 0.04081074899614121\nEpsilon = 0.0408066679212416\nEpsilon = 0.04080258725444948\nAgent: ddqn_agent . Episode 1709/2000. Number of steps to finish: 20. Loss: 23.174007415771484 Reward: -10.0\nEpsilon = 0.04079850699572403\nEpsilon = 0.04079442714502446\nEpsilon = 0.04079034770230996\nEpsilon = 0.040786268667539725\nEpsilon = 0.040782190040672975\nEpsilon = 0.040778111821668905\nEpsilon = 0.04077403401048674\nEpsilon = 0.04076995660708569\nEpsilon = 0.040765879611424985\nEpsilon = 0.04076180302346384\nEpsilon = 0.04075772684316149\nEpsilon = 0.040753651070477175\nEpsilon = 0.04074957570537013\nEpsilon = 0.04074550074779959\nEpsilon = 0.04074142619772481\nEpsilon = 0.04073735205510504\nEpsilon = 0.040733278319899535\nEpsilon = 0.040729204992067544\nEpsilon = 0.04072513207156834\nEpsilon = 0.04072105955836118\nAgent: ddqn_agent . Episode 1710/2000. Number of steps to finish: 20. Loss: 23.221261978149414 Reward: -12.0\nEpsilon = 0.040716987452405345\nEpsilon = 0.0407129157536601\nEpsilon = 0.04070884446208474\nEpsilon = 0.040704773577638534\nEpsilon = 0.04070070310028077\nEpsilon = 0.04069663302997074\nEpsilon = 0.04069256336666774\nEpsilon = 0.04068849411033107\nEpsilon = 0.04068442526092004\nEpsilon = 0.04068035681839395\nEpsilon = 0.04067628878271211\nEpsilon = 0.040672221153833836\nEpsilon = 0.04066815393171845\nEpsilon = 0.04066408711632528\nEpsilon = 0.040660020707613646\nEpsilon = 0.04065595470554288\nEpsilon = 0.04065188911007233\nEpsilon = 0.04064782392116132\nEpsilon = 0.040643759138769206\nEpsilon = 0.04063969476285533\nAgent: ddqn_agent . Episode 1711/2000. Number of steps to finish: 20. Loss: 21.67366600036621 Reward: -14.0\nEpsilon = 0.04063563079337905\nEpsilon = 0.04063156723029971\nEpsilon = 0.04062750407357668\nEpsilon = 0.04062344132316932\nEpsilon = 0.040619378979037\nEpsilon = 0.0406153170411391\nEpsilon = 0.040611255509434986\nEpsilon = 0.04060719438388404\nEpsilon = 0.04060313366444565\nEpsilon = 0.04059907335107921\nEpsilon = 0.040595013443744096\nEpsilon = 0.04059095394239972\nEpsilon = 0.04058689484700548\nEpsilon = 0.04058283615752078\nEpsilon = 0.04057877787390503\nEpsilon = 0.040574719996117635\nEpsilon = 0.04057066252411802\nEpsilon = 0.04056660545786561\nEpsilon = 0.040562548797319825\nEpsilon = 0.040558492542440094\nAgent: ddqn_agent . Episode 1712/2000. Number of steps to finish: 20. Loss: 21.5262393951416 Reward: -16.0\nEpsilon = 0.04055443669318585\nEpsilon = 0.04055038124951653\nEpsilon = 0.04054632621139158\nEpsilon = 0.04054227157877044\nEpsilon = 0.04053821735161256\nEpsilon = 0.0405341635298774\nEpsilon = 0.040530110113524406\nEpsilon = 0.04052605710251305\nEpsilon = 0.0405220044968028\nEpsilon = 0.04051795229635312\nEpsilon = 0.04051390050112348\nEpsilon = 0.040509849111073366\nEpsilon = 0.04050579812616226\nEpsilon = 0.040501747546349644\nEpsilon = 0.04049769737159501\nEpsilon = 0.04049364760185785\nEpsilon = 0.04048959823709766\nEpsilon = 0.040485549277273956\nEpsilon = 0.04048150072234623\nEpsilon = 0.040477452572273996\nAgent: ddqn_agent . Episode 1713/2000. Number of steps to finish: 20. Loss: 24.01262092590332 Reward: -16.0\nEpsilon = 0.04047340482701677\nEpsilon = 0.04046935748653407\nEpsilon = 0.040465310550785415\nEpsilon = 0.04046126401973034\nEpsilon = 0.040457217893328366\nEpsilon = 0.040453172171539035\nEpsilon = 0.04044912685432188\nEpsilon = 0.04044508194163645\nEpsilon = 0.04044103743344229\nEpsilon = 0.040436993329698946\nEpsilon = 0.04043294963036598\nEpsilon = 0.04042890633540294\nEpsilon = 0.0404248634447694\nEpsilon = 0.040420820958424926\nEpsilon = 0.040416778876329086\nEpsilon = 0.040412737198441455\nEpsilon = 0.04040869592472161\nEpsilon = 0.04040465505512914\nEpsilon = 0.04040061458962363\nEpsilon = 0.04039657452816467\nAgent: ddqn_agent . Episode 1714/2000. Number of steps to finish: 20. Loss: 21.79863929748535 Reward: -12.0\nEpsilon = 0.04039253487071185\nEpsilon = 0.04038849561722478\nEpsilon = 0.04038445676766306\nEpsilon = 0.04038041832198629\nEpsilon = 0.0403763802801541\nEpsilon = 0.04037234264212608\nEpsilon = 0.04036830540786187\nEpsilon = 0.04036426857732108\nEpsilon = 0.04036023215046335\nEpsilon = 0.040356196127248306\nEpsilon = 0.040352160507635584\nEpsilon = 0.040348125291584824\nEpsilon = 0.04034409047905567\nEpsilon = 0.04034005607000776\nEpsilon = 0.04033602206440076\nEpsilon = 0.040331988462194324\nEpsilon = 0.040327955263348106\nEpsilon = 0.04032392246782177\nEpsilon = 0.04031989007557499\nEpsilon = 0.04031585808656743\nAgent: ddqn_agent . Episode 1715/2000. Number of steps to finish: 20. Loss: 20.781291961669922 Reward: -14.0\nEpsilon = 0.04031182650075878\nEpsilon = 0.0403077953181087\nEpsilon = 0.04030376453857689\nEpsilon = 0.040299734162123034\nEpsilon = 0.04029570418870682\nEpsilon = 0.040291674618287955\nEpsilon = 0.04028764545082612\nEpsilon = 0.04028361668628104\nEpsilon = 0.04027958832461241\nEpsilon = 0.04027556036577995\nEpsilon = 0.040271532809743375\nEpsilon = 0.0402675056564624\nEpsilon = 0.04026347890589675\nEpsilon = 0.04025945255800616\nEpsilon = 0.04025542661275036\nEpsilon = 0.040251401070089086\nEpsilon = 0.04024737592998208\nEpsilon = 0.04024335119238908\nEpsilon = 0.040239326857269844\nEpsilon = 0.04023530292458412\nAgent: ddqn_agent . Episode 1716/2000. Number of steps to finish: 20. Loss: 19.328065872192383 Reward: -12.0\nEpsilon = 0.040231279394291664\nEpsilon = 0.040227256266352235\nEpsilon = 0.0402232335407256\nEpsilon = 0.04021921121737153\nEpsilon = 0.040215189296249794\nEpsilon = 0.04021116777732017\nEpsilon = 0.04020714666054244\nEpsilon = 0.04020312594587638\nEpsilon = 0.040199105633281794\nEpsilon = 0.04019508572271847\nEpsilon = 0.0401910662141462\nEpsilon = 0.040187047107524784\nEpsilon = 0.040183028402814035\nEpsilon = 0.04017901009997375\nEpsilon = 0.040174992198963755\nEpsilon = 0.04017097469974386\nEpsilon = 0.04016695760227389\nEpsilon = 0.04016294090651366\nEpsilon = 0.04015892461242301\nEpsilon = 0.04015490871996177\nAgent: ddqn_agent . Episode 1717/2000. Number of steps to finish: 20. Loss: 22.267526626586914 Reward: -14.0\nEpsilon = 0.04015089322908977\nEpsilon = 0.04014687813976686\nEpsilon = 0.040142863451952886\nEpsilon = 0.04013884916560769\nEpsilon = 0.04013483528069113\nEpsilon = 0.04013082179716306\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.04012680871498334\nEpsilon = 0.04012279603411185\nEpsilon = 0.04011878375450844\nEpsilon = 0.04011477187613299\nEpsilon = 0.04011076039894538\nEpsilon = 0.040106749322905484\nEpsilon = 0.040102738647973195\nEpsilon = 0.0400987283741084\nEpsilon = 0.040094718501270984\nEpsilon = 0.04009070902942086\nEpsilon = 0.040086699958517914\nEpsilon = 0.040082691288522064\nEpsilon = 0.04007868301939321\nEpsilon = 0.040074675151091275\nAgent: ddqn_agent . Episode 1718/2000. Number of steps to finish: 20. Loss: 17.943830490112305 Reward: -14.0\nEpsilon = 0.040070667683576165\nEpsilon = 0.04006666061680781\nEpsilon = 0.04006265395074613\nEpsilon = 0.040058647685351056\nEpsilon = 0.04005464182058252\nEpsilon = 0.040050636356400465\nEpsilon = 0.040046631292764825\nEpsilon = 0.04004262662963555\nEpsilon = 0.04003862236697259\nEpsilon = 0.04003461850473589\nEpsilon = 0.04003061504288542\nEpsilon = 0.04002661198138113\nEpsilon = 0.04002260932018299\nEpsilon = 0.04001860705925098\nEpsilon = 0.04001460519854505\nEpsilon = 0.040010603738025195\nEpsilon = 0.040006602677651396\nEpsilon = 0.04000260201738363\nEpsilon = 0.03999860175718189\nEpsilon = 0.03999460189700617\nAgent: ddqn_agent . Episode 1719/2000. Number of steps to finish: 20. Loss: 20.586172103881836 Reward: -14.0\nEpsilon = 0.039990602436816465\nEpsilon = 0.03998660337657278\nEpsilon = 0.039982604716235125\nEpsilon = 0.0399786064557635\nEpsilon = 0.03997460859511793\nEpsilon = 0.03997061113425842\nEpsilon = 0.039966614073144995\nEpsilon = 0.03996261741173768\nEpsilon = 0.0399586211499965\nEpsilon = 0.039954625287881504\nEpsilon = 0.03995062982535272\nEpsilon = 0.03994663476237018\nEpsilon = 0.03994264009889394\nEpsilon = 0.03993864583488405\nEpsilon = 0.039934651970300564\nEpsilon = 0.039930658505103536\nEpsilon = 0.03992666543925302\nEpsilon = 0.0399226727727091\nEpsilon = 0.039918680505431826\nEpsilon = 0.03991468863738128\nAgent: ddqn_agent . Episode 1720/2000. Number of steps to finish: 20. Loss: 19.871498107910156 Reward: -12.0\nEpsilon = 0.03991069716851755\nEpsilon = 0.0399067060988007\nEpsilon = 0.039902715428190814\nEpsilon = 0.039898725156647996\nEpsilon = 0.03989473528413233\nEpsilon = 0.039890745810603916\nEpsilon = 0.039886756736022856\nEpsilon = 0.03988276806034925\nEpsilon = 0.03987877978354322\nEpsilon = 0.039874791905564866\nEpsilon = 0.03987080442637431\nEpsilon = 0.03986681734593167\nEpsilon = 0.03986283066419708\nEpsilon = 0.03985884438113066\nEpsilon = 0.03985485849669255\nEpsilon = 0.03985087301084288\nEpsilon = 0.039846887923541796\nEpsilon = 0.03984290323474944\nEpsilon = 0.03983891894442597\nEpsilon = 0.03983493505253153\nAgent: ddqn_agent . Episode 1721/2000. Number of steps to finish: 20. Loss: 21.82489013671875 Reward: -12.0\nEpsilon = 0.03983095155902628\nEpsilon = 0.03982696846387038\nEpsilon = 0.03982298576702399\nEpsilon = 0.03981900346844729\nEpsilon = 0.03981502156810044\nEpsilon = 0.03981104006594363\nEpsilon = 0.039807058961937036\nEpsilon = 0.03980307825604084\nEpsilon = 0.039799097948215234\nEpsilon = 0.03979511803842041\nEpsilon = 0.039791138526616567\nEpsilon = 0.0397871594127639\nEpsilon = 0.03978318069682263\nEpsilon = 0.039779202378752945\nEpsilon = 0.03977522445851507\nEpsilon = 0.03977124693606922\nEpsilon = 0.03976726981137561\nEpsilon = 0.039763293084394476\nEpsilon = 0.03975931675508604\nEpsilon = 0.03975534082341053\nAgent: ddqn_agent . Episode 1722/2000. Number of steps to finish: 20. Loss: 21.396345138549805 Reward: -18.0\nEpsilon = 0.03975136528932819\nEpsilon = 0.03974739015279926\nEpsilon = 0.03974341541378398\nEpsilon = 0.03973944107224261\nEpsilon = 0.03973546712813538\nEpsilon = 0.03973149358142257\nEpsilon = 0.03972752043206443\nEpsilon = 0.039723547680021225\nEpsilon = 0.03971957532525322\nEpsilon = 0.039715603367720696\nEpsilon = 0.03971163180738393\nEpsilon = 0.03970766064420319\nEpsilon = 0.03970368987813877\nEpsilon = 0.03969971950915096\nEpsilon = 0.039695749537200044\nEpsilon = 0.03969177996224632\nEpsilon = 0.0396878107842501\nEpsilon = 0.039683842003171675\nEpsilon = 0.03967987361897136\nEpsilon = 0.039675905631609464\nAgent: ddqn_agent . Episode 1723/2000. Number of steps to finish: 20. Loss: 19.909561157226562 Reward: -14.0\nEpsilon = 0.0396719380410463\nEpsilon = 0.0396679708472422\nEpsilon = 0.03966400405015747\nEpsilon = 0.039660037649752455\nEpsilon = 0.03965607164598748\nEpsilon = 0.03965210603882288\nEpsilon = 0.039648140828219\nEpsilon = 0.039644176014136175\nEpsilon = 0.03964021159653476\nEpsilon = 0.03963624757537511\nEpsilon = 0.03963228395061757\nEpsilon = 0.039628320722222506\nEpsilon = 0.039624357890150286\nEpsilon = 0.039620395454361274\nEpsilon = 0.03961643341481584\nEpsilon = 0.03961247177147436\nEpsilon = 0.039608510524297216\nEpsilon = 0.03960454967324479\nEpsilon = 0.039600589218277464\nEpsilon = 0.039596629159355634\nAgent: ddqn_agent . Episode 1724/2000. Number of steps to finish: 20. Loss: 19.383525848388672 Reward: -12.0\nEpsilon = 0.0395926694964397\nEpsilon = 0.03958871022949006\nEpsilon = 0.03958475135846711\nEpsilon = 0.039580792883331266\nEpsilon = 0.039576834804042935\nEpsilon = 0.03957287712056253\nEpsilon = 0.03956891983285048\nEpsilon = 0.03956496294086719\nEpsilon = 0.0395610064445731\nEpsilon = 0.039557050343928646\nEpsilon = 0.039553094638894254\nEpsilon = 0.03954913932943036\nEpsilon = 0.03954518441549742\nEpsilon = 0.03954122989705587\nEpsilon = 0.03953727577406617\nEpsilon = 0.03953332204648876\nEpsilon = 0.03952936871428411\nEpsilon = 0.039525415777412684\nEpsilon = 0.03952146323583494\nEpsilon = 0.03951751108951136\nAgent: ddqn_agent . Episode 1725/2000. Number of steps to finish: 20. Loss: 20.616044998168945 Reward: -10.0\nEpsilon = 0.039513559338402414\nEpsilon = 0.039509607982468574\nEpsilon = 0.039505657021670325\nEpsilon = 0.03950170645596816\nEpsilon = 0.03949775628532256\nEpsilon = 0.03949380650969403\nEpsilon = 0.03948985712904306\nEpsilon = 0.03948590814333015\nEpsilon = 0.03948195955251582\nEpsilon = 0.03947801135656057\nEpsilon = 0.03947406355542492\nEpsilon = 0.039470116149069374\nEpsilon = 0.03946616913745447\nEpsilon = 0.03946222252054072\nEpsilon = 0.039458276298288665\nEpsilon = 0.039454330470658834\nEpsilon = 0.039450385037611765\nEpsilon = 0.039446439999108004\nEpsilon = 0.0394424953551081\nEpsilon = 0.03943855110557259\nAgent: ddqn_agent . Episode 1726/2000. Number of steps to finish: 20. Loss: 21.568750381469727 Reward: -14.0\nEpsilon = 0.03943460725046203\nEpsilon = 0.03943066378973698\nEpsilon = 0.03942672072335801\nEpsilon = 0.039422778051285676\nEpsilon = 0.03941883577348055\nEpsilon = 0.0394148938899032\nEpsilon = 0.03941095240051421\nEpsilon = 0.03940701130527416\nEpsilon = 0.03940307060414363\nEpsilon = 0.03939913029708322\nEpsilon = 0.03939519038405351\nEpsilon = 0.039391250865015104\nEpsilon = 0.039387311739928606\nEpsilon = 0.039383373008754614\nEpsilon = 0.03937943467145374\nEpsilon = 0.03937549672798659\nEpsilon = 0.039371559178313795\nEpsilon = 0.039367622022395965\nEpsilon = 0.03936368526019372\nEpsilon = 0.039359748891667705\nAgent: ddqn_agent . Episode 1727/2000. Number of steps to finish: 20. Loss: 19.735937118530273 Reward: -10.0\nEpsilon = 0.03935581291677854\nEpsilon = 0.03935187733548686\nEpsilon = 0.03934794214775331\nEpsilon = 0.03934400735353854\nEpsilon = 0.03934007295280319\nEpsilon = 0.03933613894550791\nAgent: ddqn_agent . Episode 1728/2000. Number of steps to finish: 6. Loss: 4.892009735107422 Reward: 6.0\nEpsilon = 0.03933220533161336\nEpsilon = 0.0393282721110802\nEpsilon = 0.03932433928386909\nEpsilon = 0.0393204068499407\nEpsilon = 0.03931647480925571\nEpsilon = 0.039312543161774786\nEpsilon = 0.03930861190745861\nEpsilon = 0.039304681046267866\nEpsilon = 0.03930075057816324\nEpsilon = 0.03929682050310543\nEpsilon = 0.03929289082105512\nEpsilon = 0.03928896153197301\nEpsilon = 0.03928503263581982\nEpsilon = 0.039281104132556234\nEpsilon = 0.03927717602214298\nEpsilon = 0.039273248304540764\nEpsilon = 0.03926932097971031\nEpsilon = 0.03926539404761234\nEpsilon = 0.03926146750820758\nEpsilon = 0.03925754136145676\nAgent: ddqn_agent . Episode 1729/2000. Number of steps to finish: 20. Loss: 21.678722381591797 Reward: -16.0\nEpsilon = 0.039253615607320616\nEpsilon = 0.03924969024575988\nEpsilon = 0.0392457652767353\nEpsilon = 0.03924184070020763\nEpsilon = 0.03923791651613761\nEpsilon = 0.039233992724485994\nEpsilon = 0.03923006932521354\nEpsilon = 0.03922614631828102\nEpsilon = 0.039222223703649195\nEpsilon = 0.03921830148127883\nEpsilon = 0.039214379651130706\nEpsilon = 0.03921045821316559\nEpsilon = 0.03920653716734428\nEpsilon = 0.039202616513627546\nEpsilon = 0.03919869625197618\nEpsilon = 0.039194776382350985\nEpsilon = 0.03919085690471275\nEpsilon = 0.03918693781902228\nEpsilon = 0.03918301912524038\nEpsilon = 0.03917910082332786\nAgent: ddqn_agent . Episode 1730/2000. Number of steps to finish: 20. Loss: 20.85097885131836 Reward: -12.0\nEpsilon = 0.039175182913245524\nEpsilon = 0.0391712653949542\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.039167348268414705\nEpsilon = 0.03916343153358787\nEpsilon = 0.03915951519043451\nEpsilon = 0.03915559923891546\nEpsilon = 0.03915168367899157\nEpsilon = 0.039147768510623675\nEpsilon = 0.03914385373377261\nEpsilon = 0.03913993934839923\nEpsilon = 0.03913602535446439\nEpsilon = 0.03913211175192895\nEpsilon = 0.03912819854075376\nEpsilon = 0.03912428572089968\nEpsilon = 0.03912037329232759\nEpsilon = 0.03911646125499836\nEpsilon = 0.03911254960887286\nEpsilon = 0.03910863835391197\nEpsilon = 0.03910472749007658\nEpsilon = 0.03910081701732757\nAgent: ddqn_agent . Episode 1731/2000. Number of steps to finish: 20. Loss: 23.85445213317871 Reward: -18.0\nEpsilon = 0.03909690693562584\nEpsilon = 0.03909299724493228\nEpsilon = 0.03908908794520779\nEpsilon = 0.039085179036413266\nEpsilon = 0.039081270518509625\nEpsilon = 0.03907736239145777\nEpsilon = 0.03907345465521863\nEpsilon = 0.03906954730975311\nEpsilon = 0.039065640355022135\nEpsilon = 0.03906173379098663\nEpsilon = 0.03905782761760753\nEpsilon = 0.03905392183484577\nEpsilon = 0.03905001644266228\nEpsilon = 0.039046111441018015\nEpsilon = 0.039042206829873916\nEpsilon = 0.03903830260919093\nEpsilon = 0.03903439877893001\nEpsilon = 0.03903049533905212\nEpsilon = 0.039026592289518214\nEpsilon = 0.039022689630289265\nAgent: ddqn_agent . Episode 1732/2000. Number of steps to finish: 20. Loss: 23.758609771728516 Reward: -18.0\nEpsilon = 0.03901878736132624\nEpsilon = 0.039014885482590106\nEpsilon = 0.03901098399404185\nEpsilon = 0.03900708289564245\nEpsilon = 0.03900318218735288\nEpsilon = 0.03899928186913414\nEpsilon = 0.03899538194094723\nEpsilon = 0.03899148240275314\nEpsilon = 0.03898758325451286\nEpsilon = 0.03898368449618741\nEpsilon = 0.03897978612773779\nEpsilon = 0.03897588814912502\nEpsilon = 0.03897199056031011\nEpsilon = 0.03896809336125408\nEpsilon = 0.038964196551917955\nEpsilon = 0.038960300132262767\nEpsilon = 0.03895640410224954\nEpsilon = 0.03895250846183932\nEpsilon = 0.03894861321099314\nEpsilon = 0.03894471834967204\nAgent: ddqn_agent . Episode 1733/2000. Number of steps to finish: 20. Loss: 20.91605567932129 Reward: -12.0\nEpsilon = 0.03894082387783707\nEpsilon = 0.03893692979544929\nEpsilon = 0.038933036102469745\nEpsilon = 0.0389291427988595\nEpsilon = 0.03892524988457961\nEpsilon = 0.03892135735959115\nEpsilon = 0.0389174652238552\nEpsilon = 0.03891357347733281\nEpsilon = 0.03890968211998508\nEpsilon = 0.03890579115177308\nEpsilon = 0.0389019005726579\nEpsilon = 0.038898010382600635\nEpsilon = 0.03889412058156238\nEpsilon = 0.03889023116950422\nEpsilon = 0.03888634214638727\nEpsilon = 0.03888245351217263\nEpsilon = 0.03887856526682141\nAgent: ddqn_agent . Episode 1734/2000. Number of steps to finish: 17. Loss: 19.032766342163086 Reward: -5.0\nEpsilon = 0.038874677410294725\nEpsilon = 0.0388707899425537\nEpsilon = 0.03886690286355944\nEpsilon = 0.038863016173273084\nEpsilon = 0.038859129871655754\nEpsilon = 0.03885524395866859\nEpsilon = 0.03885135843427272\nEpsilon = 0.03884747329842929\nEpsilon = 0.03884358855109945\nEpsilon = 0.03883970419224434\nEpsilon = 0.038835820221825114\nEpsilon = 0.03883193663980293\nEpsilon = 0.03882805344613895\nEpsilon = 0.03882417064079434\nEpsilon = 0.03882028822373026\nEpsilon = 0.038816406194907886\nEpsilon = 0.038812524554288394\nEpsilon = 0.038808643301832965\nEpsilon = 0.038804762437502784\nEpsilon = 0.03880088196125903\nAgent: ddqn_agent . Episode 1735/2000. Number of steps to finish: 20. Loss: 23.14735984802246 Reward: -10.0\nEpsilon = 0.038797001873062904\nEpsilon = 0.038793122172875595\nEpsilon = 0.03878924286065831\nEpsilon = 0.03878536393637225\nEpsilon = 0.03878148539997861\nEpsilon = 0.03877760725143861\nEpsilon = 0.03877372949071347\nEpsilon = 0.038769852117764396\nEpsilon = 0.03876597513255262\nEpsilon = 0.03876209853503937\nEpsilon = 0.038758222325185866\nEpsilon = 0.03875434650295335\nEpsilon = 0.03875047106830305\nEpsilon = 0.03874659602119622\nEpsilon = 0.0387427213615941\nEpsilon = 0.03873884708945794\nEpsilon = 0.038734973204748994\nEpsilon = 0.03873109970742852\nEpsilon = 0.038727226597457774\nEpsilon = 0.03872335387479803\nAgent: ddqn_agent . Episode 1736/2000. Number of steps to finish: 20. Loss: 23.1874942779541 Reward: -14.0\nEpsilon = 0.03871948153941055\nEpsilon = 0.03871560959125661\nEpsilon = 0.03871173803029749\nEpsilon = 0.03870786685649446\nEpsilon = 0.03870399606980881\nEpsilon = 0.038700125670201826\nEpsilon = 0.03869625565763481\nEpsilon = 0.038692386032069044\nEpsilon = 0.03868851679346584\nEpsilon = 0.03868464794178649\nEpsilon = 0.03868077947699231\nEpsilon = 0.03867691139904461\nEpsilon = 0.038673043707904704\nEpsilon = 0.038669176403533914\nEpsilon = 0.03866530948589356\nEpsilon = 0.038661442954944975\nEpsilon = 0.03865757681064948\nEpsilon = 0.038653711052968415\nEpsilon = 0.03864984568186312\nEpsilon = 0.038645980697294934\nAgent: ddqn_agent . Episode 1737/2000. Number of steps to finish: 20. Loss: 21.85289764404297 Reward: -14.0\nEpsilon = 0.0386421160992252\nEpsilon = 0.03863825188761528\nEpsilon = 0.038634388062426514\nEpsilon = 0.03863052462362027\nEpsilon = 0.03862666157115791\nEpsilon = 0.038622798905000796\nEpsilon = 0.038618936625110296\nEpsilon = 0.038615074731447784\nEpsilon = 0.038611213223974636\nEpsilon = 0.03860735210265224\nEpsilon = 0.038603491367441976\nEpsilon = 0.038599631018305235\nEpsilon = 0.038595771055203405\nEpsilon = 0.03859191147809789\nEpsilon = 0.03858805228695008\nEpsilon = 0.03858419348172139\nEpsilon = 0.03858033506237322\nEpsilon = 0.03857647702886698\nEpsilon = 0.038572619381164096\nEpsilon = 0.03856876211922598\nAgent: ddqn_agent . Episode 1738/2000. Number of steps to finish: 20. Loss: 21.852275848388672 Reward: -18.0\nEpsilon = 0.038564905243014064\nEpsilon = 0.038561048752489765\nEpsilon = 0.038557192647614515\nEpsilon = 0.03855333692834975\nEpsilon = 0.03854948159465692\nEpsilon = 0.03854562664649745\nEpsilon = 0.038541772083832804\nEpsilon = 0.03853791790662442\nEpsilon = 0.03853406411483376\nEpsilon = 0.038530210708422274\nEpsilon = 0.03852635768735143\nEpsilon = 0.038522505051582695\nEpsilon = 0.03851865280107754\nEpsilon = 0.03851480093579743\nEpsilon = 0.038510949455703845\nEpsilon = 0.038507098360758274\nEpsilon = 0.038503247650922195\nEpsilon = 0.038499397326157106\nEpsilon = 0.03849554738642449\nEpsilon = 0.038491697831685844\nAgent: ddqn_agent . Episode 1739/2000. Number of steps to finish: 20. Loss: 21.919719696044922 Reward: -10.0\nEpsilon = 0.03848784866190268\nEpsilon = 0.03848399987703649\nEpsilon = 0.038480151477048787\nEpsilon = 0.03847630346190108\nEpsilon = 0.0384724558315549\nEpsilon = 0.038468608585971745\nEpsilon = 0.03846476172511315\nEpsilon = 0.03846091524894064\nEpsilon = 0.038457069157415744\nEpsilon = 0.0384532234505\nEpsilon = 0.03844937812815495\nEpsilon = 0.038445533190342135\nEpsilon = 0.0384416886370231\nEpsilon = 0.0384378444681594\nEpsilon = 0.03843400068371258\nEpsilon = 0.03843015728364421\nEpsilon = 0.038426314267915845\nEpsilon = 0.038422471636489056\nEpsilon = 0.03841862938932541\nEpsilon = 0.038414787526386475\nAgent: ddqn_agent . Episode 1740/2000. Number of steps to finish: 20. Loss: 20.912464141845703 Reward: -8.0\nEpsilon = 0.038410946047633836\nEpsilon = 0.03840710495302907\nEpsilon = 0.03840326424253377\nEpsilon = 0.03839942391610952\nEpsilon = 0.03839558397371791\nEpsilon = 0.03839174441532054\nEpsilon = 0.03838790524087901\nAgent: ddqn_agent . Episode 1741/2000. Number of steps to finish: 7. Loss: 7.680168151855469 Reward: 5.0\nEpsilon = 0.03838406645035492\nEpsilon = 0.038380228043709885\nEpsilon = 0.03837639002090552\nEpsilon = 0.03837255238190343\nEpsilon = 0.03836871512666524\nEpsilon = 0.03836487825515257\nEpsilon = 0.038361041767327055\nEpsilon = 0.038357205663150325\nEpsilon = 0.03835336994258401\nEpsilon = 0.03834953460558975\nEpsilon = 0.03834569965212919\nEpsilon = 0.03834186508216397\nEpsilon = 0.03833803089565576\nEpsilon = 0.0383341970925662\nEpsilon = 0.03833036367285694\nEpsilon = 0.03832653063648966\nEpsilon = 0.03832269798342601\nEpsilon = 0.038318865713627664\nEpsilon = 0.038315033827056304\nEpsilon = 0.0383112023236736\nAgent: ddqn_agent . Episode 1742/2000. Number of steps to finish: 20. Loss: 23.3836669921875 Reward: -10.0\nEpsilon = 0.03830737120344123\nEpsilon = 0.03830354046632089\nEpsilon = 0.03829971011227426\nEpsilon = 0.03829588014126303\nEpsilon = 0.038292050553248906\nEpsilon = 0.03828822134819358\nEpsilon = 0.038284392526058766\nEpsilon = 0.03828056408680616\nEpsilon = 0.03827673603039748\nEpsilon = 0.03827290835679444\nEpsilon = 0.03826908106595876\nEpsilon = 0.03826525415785217\nEpsilon = 0.03826142763243638\nEpsilon = 0.03825760148967314\nEpsilon = 0.038253775729524174\nEpsilon = 0.03824995035195122\nEpsilon = 0.03824612535691603\nEpsilon = 0.03824230074438034\nEpsilon = 0.038238476514305904\nEpsilon = 0.038234652666654476\nAgent: ddqn_agent . Episode 1743/2000. Number of steps to finish: 20. Loss: 23.778926849365234 Reward: -12.0\nEpsilon = 0.03823082920138781\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.038227006118467674\nEpsilon = 0.03822318341785583\nEpsilon = 0.03821936109951404\nEpsilon = 0.038215539163404096\nEpsilon = 0.03821171760948776\nEpsilon = 0.03820789643772681\nEpsilon = 0.038204075648083036\nEpsilon = 0.038200255240518226\nEpsilon = 0.038196435214994176\nEpsilon = 0.038192615571472674\nEpsilon = 0.03818879630991553\nEpsilon = 0.03818497743028454\nEpsilon = 0.03818115893254151\nEpsilon = 0.03817734081664825\nEpsilon = 0.038173523082566586\nEpsilon = 0.03816970573025833\nEpsilon = 0.0381658887596853\nEpsilon = 0.03816207217080934\nAgent: ddqn_agent . Episode 1744/2000. Number of steps to finish: 19. Loss: 17.703210830688477 Reward: -7.0\nEpsilon = 0.03815825596359226\nEpsilon = 0.0381544401379959\nEpsilon = 0.038150624693982095\nEpsilon = 0.0381468096315127\nEpsilon = 0.03814299495054955\nEpsilon = 0.0381391806510545\nEpsilon = 0.03813536673298939\nEpsilon = 0.03813155319631609\nEpsilon = 0.038127740040996456\nEpsilon = 0.03812392726699236\nEpsilon = 0.03812011487426566\nEpsilon = 0.038116302862778235\nEpsilon = 0.03811249123249196\nEpsilon = 0.03810867998336871\nEpsilon = 0.03810486911537037\nEpsilon = 0.038101058628458834\nEpsilon = 0.03809724852259599\nEpsilon = 0.038093438797743726\nEpsilon = 0.03808962945386395\nEpsilon = 0.03808582049091857\nAgent: ddqn_agent . Episode 1745/2000. Number of steps to finish: 20. Loss: 23.047983169555664 Reward: -16.0\nEpsilon = 0.03808201190886948\nEpsilon = 0.038078203707678594\nEpsilon = 0.03807439588730783\nEpsilon = 0.0380705884477191\nEpsilon = 0.03806678138887433\nEpsilon = 0.03806297471073544\nEpsilon = 0.03805916841326437\nEpsilon = 0.038055362496423044\nEpsilon = 0.0380515569601734\nEpsilon = 0.038047751804477385\nEpsilon = 0.038043947029296935\nEpsilon = 0.03804014263459401\nEpsilon = 0.03803633862033055\nEpsilon = 0.03803253498646851\nEpsilon = 0.03802873173296986\nEpsilon = 0.03802492885979657\nEpsilon = 0.03802112636691059\nEpsilon = 0.0380173242542739\nEpsilon = 0.038013522521848475\nEpsilon = 0.03800972116959629\nAgent: ddqn_agent . Episode 1746/2000. Number of steps to finish: 20. Loss: 22.49049186706543 Reward: -10.0\nEpsilon = 0.03800592019747933\nEpsilon = 0.03800211960545958\nEpsilon = 0.037998319393499035\nEpsilon = 0.037994519561559685\nEpsilon = 0.037990720109603526\nEpsilon = 0.03798692103759257\nEpsilon = 0.03798312234548881\nEpsilon = 0.03797932403325426\nEpsilon = 0.037975526100850934\nEpsilon = 0.03797172854824085\nEpsilon = 0.03796793137538603\nEpsilon = 0.037964134582248496\nEpsilon = 0.03796033816879027\nEpsilon = 0.03795654213497339\nEpsilon = 0.03795274648075989\nEpsilon = 0.03794895120611182\nEpsilon = 0.037945156310991206\nEpsilon = 0.037941361795360104\nEpsilon = 0.03793756765918057\nEpsilon = 0.037933773902414654\nAgent: ddqn_agent . Episode 1747/2000. Number of steps to finish: 20. Loss: 20.93991470336914 Reward: -12.0\nEpsilon = 0.037929980525024415\nEpsilon = 0.03792618752697191\nEpsilon = 0.03792239490821921\nEpsilon = 0.037918602668728395\nEpsilon = 0.03791481080846152\nEpsilon = 0.037911019327380675\nEpsilon = 0.03790722822544794\nEpsilon = 0.0379034375026254\nEpsilon = 0.03789964715887514\nEpsilon = 0.03789585719415925\nEpsilon = 0.037892067608439835\nEpsilon = 0.03788827840167899\nEpsilon = 0.037884489573838824\nEpsilon = 0.03788070112488144\nEpsilon = 0.03787691305476895\nEpsilon = 0.037873125363463475\nEpsilon = 0.037869338050927126\nEpsilon = 0.03786555111712203\nEpsilon = 0.03786176456201032\nEpsilon = 0.03785797838555412\nAgent: ddqn_agent . Episode 1748/2000. Number of steps to finish: 20. Loss: 22.12990379333496 Reward: -8.0\nEpsilon = 0.03785419258771556\nEpsilon = 0.037850407168456786\nEpsilon = 0.03784662212773994\nEpsilon = 0.03784283746552717\nEpsilon = 0.037839053181780616\nEpsilon = 0.03783526927646244\nEpsilon = 0.03783148574953479\nEpsilon = 0.03782770260095984\nEpsilon = 0.03782391983069975\nEpsilon = 0.03782013743871668\nEpsilon = 0.03781635542497281\nEpsilon = 0.037812573789430315\nEpsilon = 0.03780879253205137\nEpsilon = 0.03780501165279816\nEpsilon = 0.037801231151632884\nEpsilon = 0.03779745102851772\nEpsilon = 0.03779367128341487\nAgent: ddqn_agent . Episode 1749/2000. Number of steps to finish: 17. Loss: 16.537261962890625 Reward: -5.0\nEpsilon = 0.037789891916286525\nEpsilon = 0.0377861129270949\nEpsilon = 0.03778233431580219\nEpsilon = 0.03777855608237061\nEpsilon = 0.037774778226762375\nEpsilon = 0.0377710007489397\nEpsilon = 0.037767223648864805\nEpsilon = 0.03776344692649992\nEpsilon = 0.03775967058180727\nEpsilon = 0.03775589461474909\nEpsilon = 0.03775211902528761\nEpsilon = 0.037748343813385085\nEpsilon = 0.037744568979003745\nEpsilon = 0.037740794522105846\nEpsilon = 0.03773702044265364\nEpsilon = 0.03773324674060938\nEpsilon = 0.03772947341593532\nEpsilon = 0.03772570046859373\nEpsilon = 0.03772192789854687\nEpsilon = 0.03771815570575702\nAgent: ddqn_agent . Episode 1750/2000. Number of steps to finish: 20. Loss: 22.72323226928711 Reward: -10.0\nEpsilon = 0.03771438389018644\nEpsilon = 0.03771061245179742\nEpsilon = 0.03770684139055224\nEpsilon = 0.03770307070641318\nEpsilon = 0.03769930039934254\nEpsilon = 0.03769553046930261\nEpsilon = 0.03769176091625568\nEpsilon = 0.03768799174016405\nEpsilon = 0.03768422294099004\nEpsilon = 0.03768045451869594\nEpsilon = 0.03767668647324407\nEpsilon = 0.03767291880459674\nEpsilon = 0.037669151512716284\nEpsilon = 0.037665384597565015\nEpsilon = 0.037661618059105256\nEpsilon = 0.037657851897299345\nEpsilon = 0.03765408611210962\nEpsilon = 0.037650320703498406\nEpsilon = 0.03764655567142806\nEpsilon = 0.03764279101586091\nAgent: ddqn_agent . Episode 1751/2000. Number of steps to finish: 20. Loss: 22.004867553710938 Reward: -10.0\nEpsilon = 0.037639026736759326\nEpsilon = 0.03763526283408565\nEpsilon = 0.03763149930780224\nEpsilon = 0.03762773615787146\nEpsilon = 0.037623973384255674\nEpsilon = 0.03762021098691725\nEpsilon = 0.03761644896581856\nEpsilon = 0.03761268732092198\nEpsilon = 0.03760892605218989\nEpsilon = 0.037605165159584675\nEpsilon = 0.037601404643068714\nEpsilon = 0.03759764450260441\nEpsilon = 0.03759388473815415\nEpsilon = 0.037590125349680335\nEpsilon = 0.03758636633714537\nEpsilon = 0.03758260770051165\nEpsilon = 0.0375788494397416\nEpsilon = 0.03757509155479763\nEpsilon = 0.03757133404564215\nEpsilon = 0.03756757691223758\nAgent: ddqn_agent . Episode 1752/2000. Number of steps to finish: 20. Loss: 25.060346603393555 Reward: -16.0\nEpsilon = 0.03756382015454636\nEpsilon = 0.037560063772530905\nEpsilon = 0.03755630776615365\nEpsilon = 0.03755255213537704\nEpsilon = 0.0375487968801635\nEpsilon = 0.03754504200047548\nEpsilon = 0.03754128749627544\nEpsilon = 0.03753753336752581\nEpsilon = 0.037533779614189064\nEpsilon = 0.03753002623622764\nEpsilon = 0.03752627323360402\nEpsilon = 0.037522520606280654\nEpsilon = 0.03751876835422003\nEpsilon = 0.03751501647738461\nEpsilon = 0.03751126497573687\nEpsilon = 0.037507513849239296\nEpsilon = 0.03750376309785437\nEpsilon = 0.03750001272154459\nEpsilon = 0.037496262720272434\nEpsilon = 0.03749251309400041\nAgent: ddqn_agent . Episode 1753/2000. Number of steps to finish: 20. Loss: 21.47427749633789 Reward: -18.0\nEpsilon = 0.03748876384269101\nEpsilon = 0.03748501496630674\nEpsilon = 0.03748126646481011\nEpsilon = 0.03747751833816363\nEpsilon = 0.037473770586329815\nEpsilon = 0.03747002320927118\nEpsilon = 0.037466276206950255\nEpsilon = 0.03746252957932956\nEpsilon = 0.03745878332637163\nEpsilon = 0.03745503744803899\nEpsilon = 0.03745129194429419\nEpsilon = 0.03744754681509976\nEpsilon = 0.037443802060418245\nEpsilon = 0.037440057680212205\nEpsilon = 0.03743631367444419\nEpsilon = 0.03743257004307674\nEpsilon = 0.037428826786072435\nEpsilon = 0.03742508390339383\nEpsilon = 0.03742134139500349\nEpsilon = 0.03741759926086399\nAgent: ddqn_agent . Episode 1754/2000. Number of steps to finish: 20. Loss: 20.187658309936523 Reward: -14.0\nEpsilon = 0.037413857500937904\nEpsilon = 0.03741011611518781\nEpsilon = 0.03740637510357629\nEpsilon = 0.03740263446606593\nEpsilon = 0.037398894202619326\nEpsilon = 0.037395154313199064\nEpsilon = 0.037391414797767746\nEpsilon = 0.03738767565628797\nEpsilon = 0.037383936888722345\nEpsilon = 0.037380198495033476\nEpsilon = 0.037376460475183976\nEpsilon = 0.03737272282913646\nEpsilon = 0.03736898555685354\nEpsilon = 0.03736524865829786\nEpsilon = 0.03736151213343203\nEpsilon = 0.037357775982218684\nEpsilon = 0.037354040204620464\nEpsilon = 0.037350304800600004\nEpsilon = 0.03734656977011994\nEpsilon = 0.03734283511314293\nAgent: ddqn_agent . Episode 1755/2000. Number of steps to finish: 20. Loss: 20.329517364501953 Reward: -14.0\nEpsilon = 0.037339100829631613\nEpsilon = 0.03733536691954865\nEpsilon = 0.037331633382856694\nEpsilon = 0.03732790021951841\nEpsilon = 0.03732416742949646\nEpsilon = 0.037320435012753506\nEpsilon = 0.03731670296925223\nEpsilon = 0.037312971298955305\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.03730924000182541\nEpsilon = 0.037305509077825226\nEpsilon = 0.03730177852691744\nEpsilon = 0.03729804834906475\nEpsilon = 0.03729431854422984\nEpsilon = 0.03729058911237542\nEpsilon = 0.03728686005346418\nEpsilon = 0.03728313136745884\nEpsilon = 0.03727940305432209\nEpsilon = 0.03727567511401666\nEpsilon = 0.03727194754650526\nEpsilon = 0.037268220351750606\nAgent: ddqn_agent . Episode 1756/2000. Number of steps to finish: 20. Loss: 23.14905548095703 Reward: -18.0\nEpsilon = 0.037264493529715434\nEpsilon = 0.03726076708036246\nEpsilon = 0.03725704100365442\nEpsilon = 0.03725331529955406\nEpsilon = 0.037249589968024105\nEpsilon = 0.0372458650090273\nEpsilon = 0.0372421404225264\nEpsilon = 0.03723841620848415\nEpsilon = 0.0372346923668633\nEpsilon = 0.037230968897626615\nEpsilon = 0.03722724580073685\nEpsilon = 0.03722352307615678\nEpsilon = 0.03721980072384916\nEpsilon = 0.037216078743776776\nEpsilon = 0.0372123571359024\nEpsilon = 0.03720863590018881\nEpsilon = 0.03720491503659879\nEpsilon = 0.03720119454509513\nEpsilon = 0.03719747442564062\nEpsilon = 0.037193754678198056\nAgent: ddqn_agent . Episode 1757/2000. Number of steps to finish: 20. Loss: 24.463930130004883 Reward: -14.0\nEpsilon = 0.03719003530273023\nEpsilon = 0.03718631629919996\nEpsilon = 0.03718259766757004\nEpsilon = 0.03717887940780328\nEpsilon = 0.037175161519862505\nEpsilon = 0.03717144400371052\nEpsilon = 0.03716772685931015\nAgent: ddqn_agent . Episode 1758/2000. Number of steps to finish: 7. Loss: 8.846506118774414 Reward: 5.0\nEpsilon = 0.03716401008662422\nEpsilon = 0.037160293685615554\nEpsilon = 0.03715657765624699\nEpsilon = 0.03715286199848137\nEpsilon = 0.03714914671228152\nEpsilon = 0.03714543179761029\nEpsilon = 0.03714171725443053\nEpsilon = 0.03713800308270509\nEpsilon = 0.037134289282396815\nEpsilon = 0.03713057585346857\nEpsilon = 0.037126862795883224\nEpsilon = 0.03712315010960364\nEpsilon = 0.03711943779459268\nEpsilon = 0.03711572585081322\nEpsilon = 0.03711201427822814\nEpsilon = 0.03710830307680032\nEpsilon = 0.03710459224649264\nEpsilon = 0.03710088178726799\nEpsilon = 0.037097171699089264\nEpsilon = 0.03709346198191935\nAgent: ddqn_agent . Episode 1759/2000. Number of steps to finish: 20. Loss: 23.51331329345703 Reward: -10.0\nEpsilon = 0.03708975263572116\nEpsilon = 0.03708604366045759\nEpsilon = 0.03708233505609154\nEpsilon = 0.03707862682258593\nEpsilon = 0.037074918959903676\nEpsilon = 0.03707121146800769\nEpsilon = 0.03706750434686089\nEpsilon = 0.0370637975964262\nEpsilon = 0.03706009121666656\nEpsilon = 0.037056385207544895\nEpsilon = 0.03705267956902414\nEpsilon = 0.03704897430106724\nEpsilon = 0.03704526940363713\nEpsilon = 0.03704156487669676\nEpsilon = 0.03703786072020909\nEpsilon = 0.03703415693413707\nEpsilon = 0.03703045351844366\nEpsilon = 0.03702675047309182\nEpsilon = 0.03702304779804451\nEpsilon = 0.0370193454932647\nAgent: ddqn_agent . Episode 1760/2000. Number of steps to finish: 20. Loss: 21.05713653564453 Reward: -12.0\nEpsilon = 0.03701564355871537\nEpsilon = 0.037011941994359504\nEpsilon = 0.03700824080016007\nEpsilon = 0.03700453997608005\nEpsilon = 0.03700083952208245\nEpsilon = 0.03699713943813024\nEpsilon = 0.03699343972418643\nEpsilon = 0.03698974038021401\nEpsilon = 0.03698604140617599\nEpsilon = 0.03698234280203538\nEpsilon = 0.036978644567755174\nEpsilon = 0.0369749467032984\nEpsilon = 0.03697124920862807\nEpsilon = 0.03696755208370721\nEpsilon = 0.036963855328498836\nEpsilon = 0.03696015894296599\nEpsilon = 0.03695646292707169\nEpsilon = 0.03695276728077899\nEpsilon = 0.03694907200405091\nEpsilon = 0.03694537709685051\nAgent: ddqn_agent . Episode 1761/2000. Number of steps to finish: 20. Loss: 24.72484588623047 Reward: -14.0\nEpsilon = 0.03694168255914082\nEpsilon = 0.036937988390884906\nEpsilon = 0.036934294592045815\nEpsilon = 0.03693060116258661\nEpsilon = 0.03692690810247035\nEpsilon = 0.036923215411660104\nEpsilon = 0.036919523090118936\nEpsilon = 0.036915831137809926\nEpsilon = 0.03691213955469615\nEpsilon = 0.036908448340740675\nEpsilon = 0.036904757495906604\nEpsilon = 0.03690106702015701\nEpsilon = 0.036897376913454995\nEpsilon = 0.03689368717576365\nEpsilon = 0.03688999780704607\nEpsilon = 0.036886308807265365\nEpsilon = 0.03688262017638464\nEpsilon = 0.036878931914367\nEpsilon = 0.03687524402117556\nEpsilon = 0.03687155649677345\nAgent: ddqn_agent . Episode 1762/2000. Number of steps to finish: 20. Loss: 21.422290802001953 Reward: -14.0\nEpsilon = 0.03686786934112377\nEpsilon = 0.03686418255418966\nEpsilon = 0.03686049613593424\nEpsilon = 0.03685681008632065\nEpsilon = 0.036853124405312014\nEpsilon = 0.036849439092871486\nEpsilon = 0.0368457541489622\nEpsilon = 0.036842069573547305\nAgent: ddqn_agent . Episode 1763/2000. Number of steps to finish: 8. Loss: 8.177846908569336 Reward: 4.0\nEpsilon = 0.036838385366589954\nEpsilon = 0.03683470152805329\nEpsilon = 0.03683101805790049\nEpsilon = 0.0368273349560947\nEpsilon = 0.03682365222259909\nEpsilon = 0.03681996985737683\nEpsilon = 0.036816287860391095\nEpsilon = 0.03681260623160506\nEpsilon = 0.0368089249709819\nEpsilon = 0.0368052440784848\nEpsilon = 0.03680156355407695\nEpsilon = 0.03679788339772155\nEpsilon = 0.03679420360938177\nEpsilon = 0.03679052418902083\nEpsilon = 0.03678684513660193\nEpsilon = 0.03678316645208827\nEpsilon = 0.03677948813544306\nEpsilon = 0.036775810186629516\nEpsilon = 0.03677213260561085\nEpsilon = 0.03676845539235029\nAgent: ddqn_agent . Episode 1764/2000. Number of steps to finish: 20. Loss: 24.609922409057617 Reward: -12.0\nEpsilon = 0.03676477854681105\nEpsilon = 0.03676110206895637\nEpsilon = 0.03675742595874948\nEpsilon = 0.036753750216153604\nEpsilon = 0.03675007484113199\nEpsilon = 0.03674639983364788\nEpsilon = 0.03674272519366451\nEpsilon = 0.03673905092114515\nEpsilon = 0.036735377016053034\nEpsilon = 0.036731703478351425\nEpsilon = 0.03672803030800359\nEpsilon = 0.03672435750497279\nEpsilon = 0.0367206850692223\nEpsilon = 0.03671701300071538\nEpsilon = 0.03671334129941531\nEpsilon = 0.036709669965285366\nEpsilon = 0.03670599899828884\nEpsilon = 0.03670232839838901\nEpsilon = 0.03669865816554917\nEpsilon = 0.03669498829973261\nAgent: ddqn_agent . Episode 1765/2000. Number of steps to finish: 20. Loss: 21.80196762084961 Reward: -16.0\nEpsilon = 0.036691318800902636\nEpsilon = 0.03668764966902255\nEpsilon = 0.03668398090405565\nEpsilon = 0.03668031250596524\nEpsilon = 0.036676644474714644\nEpsilon = 0.036672976810267176\nEpsilon = 0.03666930951258615\nEpsilon = 0.03666564258163489\nEpsilon = 0.03666197601737673\nEpsilon = 0.03665830981977499\nEpsilon = 0.03665464398879301\nEpsilon = 0.03665097852439413\nEpsilon = 0.03664731342654169\nEpsilon = 0.03664364869519904\nEpsilon = 0.03663998433032952\nEpsilon = 0.03663632033189649\nEpsilon = 0.0366326566998633\nEpsilon = 0.03662899343419331\nEpsilon = 0.03662533053484989\nEpsilon = 0.036621668001796406\nAgent: ddqn_agent . Episode 1766/2000. Number of steps to finish: 20. Loss: 21.493398666381836 Reward: -18.0\nEpsilon = 0.03661800583499623\nEpsilon = 0.03661434403441273\nEpsilon = 0.03661068260000929\nEpsilon = 0.03660702153174929\nEpsilon = 0.036603360829596115\nEpsilon = 0.03659970049351315\nEpsilon = 0.0365960405234638\nEpsilon = 0.03659238091941146\nEpsilon = 0.03658872168131952\nEpsilon = 0.036585062809151386\nAgent: ddqn_agent . Episode 1767/2000. Number of steps to finish: 10. Loss: 12.312812805175781 Reward: 2.0\nEpsilon = 0.03658140430287047\nEpsilon = 0.03657774616244018\nEpsilon = 0.03657408838782394\nEpsilon = 0.03657043097898516\nEpsilon = 0.03656677393588726\nEpsilon = 0.03656311725849367\nEpsilon = 0.03655946094676782\nEpsilon = 0.036555805000673146\nEpsilon = 0.036552149420173076\nEpsilon = 0.03654849420523106\nEpsilon = 0.036544839355810536\nEpsilon = 0.03654118487187496\nEpsilon = 0.036537530753387774\nEpsilon = 0.03653387700031244\nEpsilon = 0.036530223612612406\nEpsilon = 0.03652657059025115\nEpsilon = 0.03652291793319212\nEpsilon = 0.036519265641398804\nEpsilon = 0.036515613714834666\nEpsilon = 0.03651196215346318\nAgent: ddqn_agent . Episode 1768/2000. Number of steps to finish: 20. Loss: 21.051128387451172 Reward: -14.0\nEpsilon = 0.03650831095724784\nEpsilon = 0.03650466012615211\nEpsilon = 0.03650100966013949\nEpsilon = 0.03649735955917348\nEpsilon = 0.03649370982321756\nEpsilon = 0.03649006045223524\nEpsilon = 0.03648641144619002\nEpsilon = 0.0364827628050454\nEpsilon = 0.03647911452876489\nEpsilon = 0.03647546661731202\nEpsilon = 0.03647181907065029\nEpsilon = 0.036468171888743225\nEpsilon = 0.03646452507155435\nEpsilon = 0.0364608786190472\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.036457232531185296\nEpsilon = 0.036453586807932176\nEpsilon = 0.036449941449251386\nEpsilon = 0.03644629645510646\nEpsilon = 0.03644265182546095\nEpsilon = 0.0364390075602784\nAgent: ddqn_agent . Episode 1769/2000. Number of steps to finish: 20. Loss: 19.46957015991211 Reward: -12.0\nEpsilon = 0.03643536365952237\nEpsilon = 0.03643172012315642\nEpsilon = 0.036428076951144105\nEpsilon = 0.03642443414344899\nEpsilon = 0.03642079170003464\nEpsilon = 0.03641714962086464\nEpsilon = 0.03641350790590255\nEpsilon = 0.03640986655511196\nEpsilon = 0.03640622556845645\nEpsilon = 0.0364025849458996\nEpsilon = 0.03639894468740501\nEpsilon = 0.03639530479293627\nEpsilon = 0.036391665262456976\nEpsilon = 0.03638802609593073\nEpsilon = 0.03638438729332114\nEpsilon = 0.03638074885459181\nEpsilon = 0.03637711077970635\nEpsilon = 0.036373473068628384\nEpsilon = 0.03636983572132152\nEpsilon = 0.036366198737749385\nAgent: ddqn_agent . Episode 1770/2000. Number of steps to finish: 20. Loss: 21.69247817993164 Reward: -16.0\nEpsilon = 0.03636256211787561\nEpsilon = 0.036358925861663824\nEpsilon = 0.03635528996907766\nEpsilon = 0.03635165444008075\nEpsilon = 0.03634801927463674\nEpsilon = 0.036344384472709275\nEpsilon = 0.036340750034262\nEpsilon = 0.03633711595925858\nEpsilon = 0.036333482247662655\nEpsilon = 0.03632984889943789\nEpsilon = 0.036326215914547946\nEpsilon = 0.03632258329295649\nEpsilon = 0.0363189510346272\nEpsilon = 0.03631531913952374\nEpsilon = 0.036311687607609784\nEpsilon = 0.036308056438849025\nEpsilon = 0.03630442563320514\nEpsilon = 0.036300795190641826\nEpsilon = 0.03629716511112276\nEpsilon = 0.03629353539461165\nAgent: ddqn_agent . Episode 1771/2000. Number of steps to finish: 20. Loss: 23.36737060546875 Reward: -12.0\nEpsilon = 0.03628990604107219\nEpsilon = 0.03628627705046809\nEpsilon = 0.03628264842276304\nEpsilon = 0.03627902015792076\nEpsilon = 0.03627539225590497\nEpsilon = 0.03627176471667938\nEpsilon = 0.03626813754020771\nEpsilon = 0.03626451072645369\nEpsilon = 0.036260884275381045\nEpsilon = 0.03625725818695351\nEpsilon = 0.03625363246113481\nEpsilon = 0.0362500070978887\nEpsilon = 0.03624638209717891\nEpsilon = 0.036242757458969196\nEpsilon = 0.0362391331832233\nEpsilon = 0.03623550926990497\nEpsilon = 0.03623188571897798\nEpsilon = 0.036228262530406084\nEpsilon = 0.036224639704153044\nEpsilon = 0.03622101724018263\nAgent: ddqn_agent . Episode 1772/2000. Number of steps to finish: 20. Loss: 22.02552032470703 Reward: -10.0\nEpsilon = 0.03621739513845861\nEpsilon = 0.036213773398944764\nEpsilon = 0.03621015202160487\nEpsilon = 0.03620653100640271\nEpsilon = 0.03620291035330207\nEpsilon = 0.03619929006226674\nEpsilon = 0.03619567013326051\nEpsilon = 0.036192050566247184\nEpsilon = 0.03618843136119056\nEpsilon = 0.036184812518054436\nEpsilon = 0.03618119403680263\nEpsilon = 0.03617757591739895\nEpsilon = 0.036173958159807215\nEpsilon = 0.036170340763991234\nEpsilon = 0.036166723729914835\nEpsilon = 0.036163107057541846\nEpsilon = 0.036159490746836095\nEpsilon = 0.03615587479776141\nEpsilon = 0.03615225921028163\nEpsilon = 0.0361486439843606\nAgent: ddqn_agent . Episode 1773/2000. Number of steps to finish: 20. Loss: 23.537233352661133 Reward: -18.0\nEpsilon = 0.03614502911996217\nEpsilon = 0.03614141461705017\nEpsilon = 0.03613780047558847\nEpsilon = 0.03613418669554091\nEpsilon = 0.03613057327687135\nEpsilon = 0.03612696021954367\nEpsilon = 0.03612334752352171\nEpsilon = 0.03611973518876936\nEpsilon = 0.036116123215250484\nEpsilon = 0.03611251160292896\nEpsilon = 0.03610890035176866\nEpsilon = 0.036105289461733484\nEpsilon = 0.03610167893278731\nEpsilon = 0.036098068764894034\nEpsilon = 0.03609445895801754\nEpsilon = 0.03609084951212174\nEpsilon = 0.03608724042717053\nEpsilon = 0.03608363170312781\nEpsilon = 0.036080023339957495\nEpsilon = 0.0360764153376235\nAgent: ddqn_agent . Episode 1774/2000. Number of steps to finish: 20. Loss: 23.489730834960938 Reward: -20.0\nEpsilon = 0.03607280769608974\nEpsilon = 0.03606920041532013\nEpsilon = 0.036065593495278596\nEpsilon = 0.03606198693592907\nEpsilon = 0.036058380737235475\nEpsilon = 0.03605477489916175\nEpsilon = 0.03605116942167184\nEpsilon = 0.03604756430472967\nEpsilon = 0.0360439595482992\nEpsilon = 0.03604035515234437\nEpsilon = 0.036036751116829135\nEpsilon = 0.036033147441717456\nEpsilon = 0.03602954412697328\nEpsilon = 0.03602594117256058\nEpsilon = 0.03602233857844333\nEpsilon = 0.03601873634458549\nEpsilon = 0.03601513447095103\nEpsilon = 0.036011532957503936\nEpsilon = 0.036007931804208186\nEpsilon = 0.03600433101102776\nAgent: ddqn_agent . Episode 1775/2000. Number of steps to finish: 20. Loss: 19.554256439208984 Reward: -18.0\nEpsilon = 0.03600073057792666\nEpsilon = 0.035997130504868864\nEpsilon = 0.03599353079181838\nEpsilon = 0.035989931438739194\nEpsilon = 0.03598633244559532\nEpsilon = 0.03598273381235076\nEpsilon = 0.035979135538969524\nEpsilon = 0.03597553762541563\nEpsilon = 0.03597194007165309\nEpsilon = 0.03596834287764592\nEpsilon = 0.035964746043358156\nEpsilon = 0.03596114956875382\nEpsilon = 0.03595755345379695\nEpsilon = 0.03595395769845157\nEpsilon = 0.035950362302681727\nEpsilon = 0.035946767266451456\nEpsilon = 0.03594317258972481\nEpsilon = 0.03593957827246584\nEpsilon = 0.0359359843146386\nEpsilon = 0.03593239071620714\nAgent: ddqn_agent . Episode 1776/2000. Number of steps to finish: 20. Loss: 25.30529022216797 Reward: -12.0\nEpsilon = 0.03592879747713552\nEpsilon = 0.0359252045973878\nEpsilon = 0.03592161207692806\nEpsilon = 0.03591801991572037\nEpsilon = 0.035914428113728795\nEpsilon = 0.03591083667091742\nEpsilon = 0.03590724558725033\nEpsilon = 0.035903654862691606\nEpsilon = 0.035900064497205335\nEpsilon = 0.035896474490755614\nEpsilon = 0.03589288484330654\nEpsilon = 0.03588929555482221\nEpsilon = 0.03588570662526673\nEpsilon = 0.03588211805460421\nEpsilon = 0.03587852984279875\nEpsilon = 0.03587494198981447\nEpsilon = 0.03587135449561549\nEpsilon = 0.03586776736016593\nEpsilon = 0.03586418058342991\nEpsilon = 0.035860594165371566\nAgent: ddqn_agent . Episode 1777/2000. Number of steps to finish: 20. Loss: 22.544572830200195 Reward: -12.0\nEpsilon = 0.03585700810595503\nEpsilon = 0.035853422405144435\nEpsilon = 0.03584983706290392\nEpsilon = 0.03584625207919763\nEpsilon = 0.03584266745398971\nEpsilon = 0.03583908318724431\nEpsilon = 0.035835499278925584\nEpsilon = 0.035831915728997694\nEpsilon = 0.0358283325374248\nEpsilon = 0.03582474970417106\nAgent: ddqn_agent . Episode 1778/2000. Number of steps to finish: 10. Loss: 11.809696197509766 Reward: 2.0\nEpsilon = 0.03582116722920064\nEpsilon = 0.03581758511247772\nEpsilon = 0.03581400335396647\nEpsilon = 0.03581042195363107\nEpsilon = 0.03580684091143571\nEpsilon = 0.03580326022734457\nEpsilon = 0.03579967990132184\nEpsilon = 0.03579609993333171\nEpsilon = 0.03579252032333838\nEpsilon = 0.03578894107130604\nEpsilon = 0.03578536217719891\nEpsilon = 0.03578178364098119\nEpsilon = 0.03577820546261709\nEpsilon = 0.03577462764207083\nEpsilon = 0.035771050179306624\nEpsilon = 0.035767473074288694\nEpsilon = 0.035763896326981265\nEpsilon = 0.035760319937348566\nEpsilon = 0.03575674390535483\nEpsilon = 0.0357531682309643\nAgent: ddqn_agent . Episode 1779/2000. Number of steps to finish: 20. Loss: 21.039161682128906 Reward: -14.0\nEpsilon = 0.035749592914141204\nEpsilon = 0.03574601795484979\nEpsilon = 0.0357424433530543\nEpsilon = 0.03573886910871899\nEpsilon = 0.03573529522180812\nEpsilon = 0.03573172169228594\nEpsilon = 0.03572814852011671\nEpsilon = 0.035724575705264695\nEpsilon = 0.03572100324769417\nEpsilon = 0.0357174311473694\nEpsilon = 0.035713859404254665\nEpsilon = 0.03571028801831424\nEpsilon = 0.03570671698951241\nEpsilon = 0.03570314631781346\nEpsilon = 0.03569957600318168\nEpsilon = 0.03569600604558136\nEpsilon = 0.0356924364449768\nEpsilon = 0.035688867201332304\nEpsilon = 0.035685298314612174\nEpsilon = 0.035681729784780715\nAgent: ddqn_agent . Episode 1780/2000. Number of steps to finish: 20. Loss: 23.781118392944336 Reward: -14.0\nEpsilon = 0.03567816161180224\nEpsilon = 0.03567459379564106\nEpsilon = 0.03567102633626149\nEpsilon = 0.035667459233627866\nEpsilon = 0.035663892487704504\nEpsilon = 0.035660326098455736\nEpsilon = 0.035656760065845894\nEpsilon = 0.03565319438983931\nEpsilon = 0.03564962907040033\nEpsilon = 0.03564606410749329\nEpsilon = 0.035642499501082545\nEpsilon = 0.03563893525113244\nEpsilon = 0.035635371357607325\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.03563180782047157\nEpsilon = 0.03562824463968952\nEpsilon = 0.03562468181522555\nEpsilon = 0.03562111934704403\nEpsilon = 0.03561755723510933\nEpsilon = 0.03561399547938582\nEpsilon = 0.03561043407983788\nAgent: ddqn_agent . Episode 1781/2000. Number of steps to finish: 20. Loss: 23.2872314453125 Reward: -14.0\nEpsilon = 0.0356068730364299\nEpsilon = 0.035603312349126254\nEpsilon = 0.03559975201789134\nEpsilon = 0.03559619204268955\nEpsilon = 0.03559263242348528\nEpsilon = 0.035589073160242936\nEpsilon = 0.035585514252926914\nEpsilon = 0.03558195570150162\nEpsilon = 0.03557839750593147\nEpsilon = 0.03557483966618088\nEpsilon = 0.03557128218221426\nEpsilon = 0.03556772505399604\nEpsilon = 0.035564168281490637\nEpsilon = 0.035560611864662486\nEpsilon = 0.03555705580347602\nEpsilon = 0.03555350009789567\nEpsilon = 0.035549944747885885\nEpsilon = 0.035546389753411095\nEpsilon = 0.035542835114435754\nEpsilon = 0.03553928083092431\nAgent: ddqn_agent . Episode 1782/2000. Number of steps to finish: 20. Loss: 23.303821563720703 Reward: -14.0\nEpsilon = 0.035535726902841215\nEpsilon = 0.035532173330150935\nEpsilon = 0.03552862011281792\nEpsilon = 0.03552506725080664\nEpsilon = 0.03552151474408156\nEpsilon = 0.03551796259260715\nEpsilon = 0.035514410796347894\nEpsilon = 0.03551085935526826\nEpsilon = 0.03550730826933273\nEpsilon = 0.0355037575385058\nEpsilon = 0.03550020716275195\nEpsilon = 0.035496657142035676\nEpsilon = 0.03549310747632147\nEpsilon = 0.03548955816557384\nEpsilon = 0.03548600920975728\nEpsilon = 0.0354824606088363\nEpsilon = 0.035478912362775415\nEpsilon = 0.035475364471539136\nEpsilon = 0.035471816935091986\nEpsilon = 0.03546826975339848\nAgent: ddqn_agent . Episode 1783/2000. Number of steps to finish: 20. Loss: 22.04900550842285 Reward: -18.0\nEpsilon = 0.03546472292642314\nEpsilon = 0.035461176454130494\nEpsilon = 0.03545763033648508\nEpsilon = 0.03545408457345143\nEpsilon = 0.035450539164994084\nEpsilon = 0.03544699411107759\nEpsilon = 0.03544344941166648\nEpsilon = 0.035439905066725313\nEpsilon = 0.03543636107621864\nEpsilon = 0.03543281744011102\nEpsilon = 0.03542927415836701\nEpsilon = 0.03542573123095118\nEpsilon = 0.035422188657828084\nEpsilon = 0.0354186464389623\nEpsilon = 0.03541510457431841\nEpsilon = 0.03541156306386098\nEpsilon = 0.03540802190755459\nEpsilon = 0.03540448110536384\nEpsilon = 0.0354009406572533\nEpsilon = 0.035397400563187575\nAgent: ddqn_agent . Episode 1784/2000. Number of steps to finish: 20. Loss: 21.063844680786133 Reward: -16.0\nEpsilon = 0.03539386082313126\nEpsilon = 0.03539032143704894\nEpsilon = 0.03538678240490524\nEpsilon = 0.03538324372666475\nEpsilon = 0.035379705402292086\nEpsilon = 0.035376167431751857\nEpsilon = 0.03537262981500868\nEpsilon = 0.03536909255202718\nEpsilon = 0.035365555642771974\nEpsilon = 0.0353620190872077\nEpsilon = 0.035358482885298984\nEpsilon = 0.035354947037010455\nEpsilon = 0.035351411542306754\nEpsilon = 0.03534787640115252\nEpsilon = 0.03534434161351241\nEpsilon = 0.03534080717935106\nEpsilon = 0.035337273098633125\nEpsilon = 0.03533373937132326\nEpsilon = 0.03533020599738613\nEpsilon = 0.03532667297678639\nAgent: ddqn_agent . Episode 1785/2000. Number of steps to finish: 20. Loss: 19.564682006835938 Reward: -16.0\nEpsilon = 0.035323140309488715\nEpsilon = 0.03531960799545777\nEpsilon = 0.03531607603465822\nEpsilon = 0.035312544427054754\nEpsilon = 0.03530901317261205\nEpsilon = 0.03530548227129479\nEpsilon = 0.03530195172306766\nEpsilon = 0.035298421527895356\nEpsilon = 0.03529489168574257\nEpsilon = 0.03529136219657399\nEpsilon = 0.03528783306035434\nEpsilon = 0.0352843042770483\nEpsilon = 0.0352807758466206\nEpsilon = 0.03527724776903594\nEpsilon = 0.035273720044259035\nEpsilon = 0.03527019267225461\nEpsilon = 0.03526666565298739\nEpsilon = 0.03526313898642209\nEpsilon = 0.03525961267252345\nEpsilon = 0.0352560867112562\nAgent: ddqn_agent . Episode 1786/2000. Number of steps to finish: 20. Loss: 26.05276870727539 Reward: -14.0\nEpsilon = 0.035252561102585074\nEpsilon = 0.03524903584647482\nEpsilon = 0.03524551094289017\nEpsilon = 0.03524198639179588\nEpsilon = 0.035238462193156704\nEpsilon = 0.03523493834693739\nEpsilon = 0.035231414853102695\nEpsilon = 0.03522789171161739\nEpsilon = 0.03522436892244622\nEpsilon = 0.03522084648555398\nEpsilon = 0.03521732440090542\nEpsilon = 0.035213802668465334\nEpsilon = 0.03521028128819849\nEpsilon = 0.03520676026006967\nEpsilon = 0.035203239584043665\nEpsilon = 0.03519971926008526\nEpsilon = 0.03519619928815926\nEpsilon = 0.03519267966823044\nEpsilon = 0.03518916040026362\nEpsilon = 0.035185641484223595\nAgent: ddqn_agent . Episode 1787/2000. Number of steps to finish: 20. Loss: 22.019746780395508 Reward: -20.0\nEpsilon = 0.03518212292007517\nEpsilon = 0.03517860470778317\nEpsilon = 0.03517508684731239\nEpsilon = 0.03517156933862766\nEpsilon = 0.0351680521816938\nEpsilon = 0.035164535376475625\nEpsilon = 0.03516101892293798\nEpsilon = 0.03515750282104568\nEpsilon = 0.03515398707076358\nEpsilon = 0.0351504716720565\nEpsilon = 0.0351469566248893\nEpsilon = 0.03514344192922681\nEpsilon = 0.03513992758503389\nEpsilon = 0.03513641359227539\nEpsilon = 0.03513289995091616\nEpsilon = 0.035129386660921066\nEpsilon = 0.03512587372225497\nEpsilon = 0.03512236113488275\nEpsilon = 0.03511884889876926\nEpsilon = 0.03511533701387939\nAgent: ddqn_agent . Episode 1788/2000. Number of steps to finish: 20. Loss: 21.533315658569336 Reward: -12.0\nEpsilon = 0.035111825480178\nEpsilon = 0.035108314297629985\nEpsilon = 0.03510480346620022\nEpsilon = 0.0351012929858536\nEpsilon = 0.035097782856555015\nEpsilon = 0.03509427307826936\nEpsilon = 0.03509076365096153\nEpsilon = 0.03508725457459644\nEpsilon = 0.03508374584913898\nEpsilon = 0.03508023747455407\nEpsilon = 0.035076729450806615\nEpsilon = 0.035073221777861534\nEpsilon = 0.03506971445568375\nEpsilon = 0.03506620748423818\nEpsilon = 0.035062700863489754\nEpsilon = 0.035059194593403405\nEpsilon = 0.03505568867394406\nEpsilon = 0.03505218310507667\nEpsilon = 0.03504867788676616\nEpsilon = 0.035045173018977485\nAgent: ddqn_agent . Episode 1789/2000. Number of steps to finish: 20. Loss: 21.307573318481445 Reward: -12.0\nEpsilon = 0.03504166850167559\nEpsilon = 0.03503816433482542\nEpsilon = 0.035034660518391936\nEpsilon = 0.0350311570523401\nEpsilon = 0.035027653936634866\nEpsilon = 0.035024151171241205\nEpsilon = 0.03502064875612408\nEpsilon = 0.03501714669124847\nEpsilon = 0.03501364497657934\nAgent: ddqn_agent . Episode 1790/2000. Number of steps to finish: 9. Loss: 9.725024223327637 Reward: 3.0\nEpsilon = 0.035010143612081684\nEpsilon = 0.035006642597720475\nEpsilon = 0.035003141933460706\nEpsilon = 0.03499964161926736\nEpsilon = 0.03499614165510543\nEpsilon = 0.03499264204093992\nEpsilon = 0.03498914277673583\nEpsilon = 0.03498564386245816\nEpsilon = 0.034982145298071914\nEpsilon = 0.034978647083542105\nEpsilon = 0.034975149218833754\nEpsilon = 0.03497165170391187\nEpsilon = 0.03496815453874148\nEpsilon = 0.03496465772328761\nEpsilon = 0.03496116125751528\nEpsilon = 0.034957665141389524\nEpsilon = 0.03495416937487539\nEpsilon = 0.034950673957937904\nEpsilon = 0.03494717889054211\nEpsilon = 0.034943684172653054\nAgent: ddqn_agent . Episode 1791/2000. Number of steps to finish: 20. Loss: 23.166444778442383 Reward: -14.0\nEpsilon = 0.03494018980423579\nEpsilon = 0.03493669578525537\nEpsilon = 0.03493320211567685\nEpsilon = 0.03492970879546528\nEpsilon = 0.03492621582458574\nEpsilon = 0.03492272320300328\nEpsilon = 0.03491923093068298\nEpsilon = 0.03491573900758991\nEpsilon = 0.03491224743368915\nEpsilon = 0.034908756208945776\nEpsilon = 0.03490526533332488\nEpsilon = 0.03490177480679155\nEpsilon = 0.03489828462931087\nEpsilon = 0.034894794800847936\nEpsilon = 0.03489130532136785\nEpsilon = 0.03488781619083572\nEpsilon = 0.03488432740921663\nEpsilon = 0.03488083897647571\nEpsilon = 0.03487735089257806\nEpsilon = 0.034873863157488805\nAgent: ddqn_agent . Episode 1792/2000. Number of steps to finish: 20. Loss: 22.07235336303711 Reward: -14.0\nEpsilon = 0.03487037577117306\nEpsilon = 0.03486688873359594\nEpsilon = 0.03486340204472258\nEpsilon = 0.03485991570451811\nEpsilon = 0.03485642971294766\nEpsilon = 0.03485294406997637\nEpsilon = 0.03484945877556937\nEpsilon = 0.03484597382969182\nEpsilon = 0.03484248923230885\nAgent: ddqn_agent . Episode 1793/2000. Number of steps to finish: 9. Loss: 11.48028564453125 Reward: 3.0\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.03483900498338562\nEpsilon = 0.03483552108288728\nEpsilon = 0.03483203753077899\nEpsilon = 0.03482855432702592\nEpsilon = 0.034825071471593215\nEpsilon = 0.03482158896444606\nEpsilon = 0.034818106805549615\nEpsilon = 0.03481462499486906\nEpsilon = 0.03481114353236957\nAgent: ddqn_agent . Episode 1794/2000. Number of steps to finish: 9. Loss: 10.950360298156738 Reward: 3.0\nEpsilon = 0.03480766241801633\nEpsilon = 0.034804181651774534\nEpsilon = 0.03480070123360936\nEpsilon = 0.034797221163486\nEpsilon = 0.03479374144136965\nEpsilon = 0.03479026206722551\nEpsilon = 0.03478678304101879\nEpsilon = 0.034783304362714686\nEpsilon = 0.034779826032278416\nEpsilon = 0.034776348049675186\nEpsilon = 0.03477287041487022\nEpsilon = 0.03476939312782873\nEpsilon = 0.03476591618851595\nEpsilon = 0.03476243959689709\nEpsilon = 0.034758963352937405\nEpsilon = 0.03475548745660211\nEpsilon = 0.03475201190785645\nEpsilon = 0.03474853670666567\nEpsilon = 0.034745061852995\nEpsilon = 0.0347415873468097\nAgent: ddqn_agent . Episode 1795/2000. Number of steps to finish: 20. Loss: 22.16551399230957 Reward: -16.0\nEpsilon = 0.03473811318807502\nEpsilon = 0.034734639376756214\nEpsilon = 0.03473116591281854\nEpsilon = 0.034727692796227255\nEpsilon = 0.03472422002694763\nEpsilon = 0.03472074760494494\nEpsilon = 0.034717275530184444\nEpsilon = 0.03471380380263143\nAgent: ddqn_agent . Episode 1796/2000. Number of steps to finish: 8. Loss: 8.50928783416748 Reward: 4.0\nEpsilon = 0.03471033242225117\nEpsilon = 0.03470686138900894\nEpsilon = 0.03470339070287004\nEpsilon = 0.034699920363799756\nEpsilon = 0.034696450371763374\nEpsilon = 0.0346929807267262\nEpsilon = 0.03468951142865353\nEpsilon = 0.03468604247751066\nEpsilon = 0.034682573873262915\nEpsilon = 0.03467910561587559\nAgent: ddqn_agent . Episode 1797/2000. Number of steps to finish: 10. Loss: 12.665969848632812 Reward: 2.0\nEpsilon = 0.034675637705314004\nEpsilon = 0.034672170141543474\nEpsilon = 0.03466870292452932\nEpsilon = 0.03466523605423687\nEpsilon = 0.034661769530631444\nEpsilon = 0.03465830335367838\nEpsilon = 0.034654837523343016\nEpsilon = 0.034651372039590685\nEpsilon = 0.03464790690238673\nEpsilon = 0.03464444211169649\nEpsilon = 0.03464097766748532\nEpsilon = 0.03463751356971857\nEpsilon = 0.0346340498183616\nEpsilon = 0.034630586413379766\nEpsilon = 0.03462712335473843\nEpsilon = 0.03462366064240296\nEpsilon = 0.03462019827633872\nEpsilon = 0.034616736256511085\nEpsilon = 0.034613274582885434\nEpsilon = 0.03460981325542715\nAgent: ddqn_agent . Episode 1798/2000. Number of steps to finish: 20. Loss: 23.689558029174805 Reward: -18.0\nEpsilon = 0.0346063522741016\nEpsilon = 0.03460289163887419\nEpsilon = 0.03459943134971031\nEpsilon = 0.03459597140657534\nEpsilon = 0.034592511809434685\nEpsilon = 0.034589052558253744\nEpsilon = 0.03458559365299792\nAgent: ddqn_agent . Episode 1799/2000. Number of steps to finish: 7. Loss: 6.533777713775635 Reward: 5.0\nEpsilon = 0.034582135093632616\nEpsilon = 0.034578676880123255\nEpsilon = 0.034575219012435245\nEpsilon = 0.034571761490534\nEpsilon = 0.03456830431438495\nEpsilon = 0.03456484748395351\nEpsilon = 0.03456139099920512\nEpsilon = 0.0345579348601052\nEpsilon = 0.03455447906661919\nEpsilon = 0.034551023618712526\nEpsilon = 0.03454756851635066\nEpsilon = 0.03454411375949902\nEpsilon = 0.03454065934812307\nEpsilon = 0.03453720528218826\nEpsilon = 0.03453375156166004\nEpsilon = 0.034530298186503876\nEpsilon = 0.03452684515668523\nEpsilon = 0.03452339247216956\nEpsilon = 0.034519940132922346\nEpsilon = 0.034516488138909056\nAgent: ddqn_agent . Episode 1800/2000. Number of steps to finish: 20. Loss: 22.867961883544922 Reward: -16.0\nEpsilon = 0.03451303649009516\nEpsilon = 0.03450958518644615\nEpsilon = 0.03450613422792751\nEpsilon = 0.03450268361450472\nEpsilon = 0.034499233346143264\nEpsilon = 0.03449578342280865\nEpsilon = 0.03449233384446637\nEpsilon = 0.034488884611081926\nEpsilon = 0.03448543572262082\nEpsilon = 0.03448198717904856\nEpsilon = 0.03447853898033065\nEpsilon = 0.03447509112643262\nEpsilon = 0.034471643617319976\nEpsilon = 0.03446819645295825\nEpsilon = 0.034464749633312954\nEpsilon = 0.034461303158349624\nEpsilon = 0.03445785702803379\nEpsilon = 0.034454411242330986\nEpsilon = 0.034450965801206754\nEpsilon = 0.03444752070462663\nAgent: ddqn_agent . Episode 1801/2000. Number of steps to finish: 20. Loss: 21.575157165527344 Reward: -20.0\nEpsilon = 0.03444407595255617\nEpsilon = 0.034440631544960916\nEpsilon = 0.03443718748180642\nEpsilon = 0.034433743763058244\nEpsilon = 0.03443030038868194\nEpsilon = 0.03442685735864307\nAgent: ddqn_agent . Episode 1802/2000. Number of steps to finish: 6. Loss: 6.921950340270996 Reward: 6.0\nEpsilon = 0.03442341467290721\nEpsilon = 0.03441997233143992\nEpsilon = 0.034416530334206774\nEpsilon = 0.034413088681173355\nEpsilon = 0.03440964737230524\nEpsilon = 0.03440620640756801\nEpsilon = 0.034402765786927254\nEpsilon = 0.03439932551034856\nEpsilon = 0.034395885577797526\nEpsilon = 0.034392445989239746\nEpsilon = 0.03438900674464082\nEpsilon = 0.034385567843966354\nEpsilon = 0.034382129287181956\nEpsilon = 0.03437869107425324\nEpsilon = 0.03437525320514581\nEpsilon = 0.0343718156798253\nEpsilon = 0.034368378498257315\nEpsilon = 0.03436494166040749\nEpsilon = 0.03436150516624145\nEpsilon = 0.03435806901572483\nAgent: ddqn_agent . Episode 1803/2000. Number of steps to finish: 20. Loss: 20.457529067993164 Reward: -10.0\nEpsilon = 0.03435463320882326\nEpsilon = 0.034351197745502375\nEpsilon = 0.03434776262572783\nEpsilon = 0.034344327849465256\nEpsilon = 0.03434089341668031\nEpsilon = 0.03433745932733864\nEpsilon = 0.03433402558140591\nEpsilon = 0.03433059217884776\nEpsilon = 0.03432715911962988\nEpsilon = 0.03432372640371792\nEpsilon = 0.034320294031077545\nEpsilon = 0.034316862001674435\nEpsilon = 0.03431343031547427\nEpsilon = 0.034309998972442725\nEpsilon = 0.03430656797254548\nEpsilon = 0.034303137315748224\nEpsilon = 0.03429970700201665\nEpsilon = 0.03429627703131645\nEpsilon = 0.03429284740361332\nEpsilon = 0.034289418118872955\nAgent: ddqn_agent . Episode 1804/2000. Number of steps to finish: 20. Loss: 24.877822875976562 Reward: -16.0\nEpsilon = 0.03428598917706107\nEpsilon = 0.03428256057814336\nEpsilon = 0.03427913232208555\nEpsilon = 0.034275704408853344\nEpsilon = 0.03427227683841246\nEpsilon = 0.03426884961072862\nEpsilon = 0.03426542272576755\nEpsilon = 0.03426199618349497\nEpsilon = 0.03425856998387662\nEpsilon = 0.03425514412687823\nEpsilon = 0.034251718612465544\nEpsilon = 0.034248293440604295\nEpsilon = 0.034244868611260235\nEpsilon = 0.03424144412439911\nEpsilon = 0.03423801997998667\nEpsilon = 0.03423459617798867\nEpsilon = 0.034231172718370874\nEpsilon = 0.03422774960109904\nEpsilon = 0.03422432682613893\nEpsilon = 0.03422090439345631\nAgent: ddqn_agent . Episode 1805/2000. Number of steps to finish: 20. Loss: 24.87865447998047 Reward: -14.0\nEpsilon = 0.034217482303016966\nEpsilon = 0.034214060554786666\nEpsilon = 0.03421063914873119\nEpsilon = 0.03420721808481632\nEpsilon = 0.03420379736300784\nEpsilon = 0.03420037698327154\nEpsilon = 0.03419695694557321\nEpsilon = 0.034193537249878656\nEpsilon = 0.03419011789615367\nEpsilon = 0.034186698884364056\nEpsilon = 0.03418328021447562\nEpsilon = 0.03417986188645417\nEpsilon = 0.03417644390026553\nEpsilon = 0.034173026255875504\nEpsilon = 0.034169608953249914\nEpsilon = 0.034166191992354586\nEpsilon = 0.03416277537315535\nEpsilon = 0.03415935909561804\nEpsilon = 0.03415594315970848\nEpsilon = 0.034152527565392506\nAgent: ddqn_agent . Episode 1806/2000. Number of steps to finish: 20. Loss: 19.06685447692871 Reward: -12.0\nEpsilon = 0.03414911231263597\nEpsilon = 0.034145697401404704\nEpsilon = 0.03414228283166457\nEpsilon = 0.0341388686033814\nEpsilon = 0.03413545471652107\nEpsilon = 0.03413204117104941\nEpsilon = 0.03412862796693231\nEpsilon = 0.03412521510413562\nEpsilon = 0.034121802582625205\nEpsilon = 0.034118390402366944\nEpsilon = 0.03411497856332671\nEpsilon = 0.034111567065470375\nEpsilon = 0.03410815590876383\nEpsilon = 0.034104745093172954\nEpsilon = 0.03410133461866364\nEpsilon = 0.03409792448520177\nEpsilon = 0.03409451469275325\nEpsilon = 0.034091105241283975\nEpsilon = 0.03408769613075985\nEpsilon = 0.03408428736114677\nAgent: ddqn_agent . Episode 1807/2000. Number of steps to finish: 20. Loss: 23.82571792602539 Reward: -18.0\nEpsilon = 0.03408087893241066\nEpsilon = 0.03407747084451742\nEpsilon = 0.034074063097432966\nEpsilon = 0.034070655691123226\nEpsilon = 0.03406724862555412\nEpsilon = 0.03406384190069156\nEpsilon = 0.034060435516501494\nEpsilon = 0.03405702947294984\nEpsilon = 0.034053623770002546\nAgent: ddqn_agent . Episode 1808/2000. Number of steps to finish: 9. Loss: 10.674142837524414 Reward: 3.0\nEpsilon = 0.034050218407625545\nEpsilon = 0.03404681338578478\nEpsilon = 0.0340434087044462\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.03404000436357576\nEpsilon = 0.0340366003631394\nEpsilon = 0.034033196703103084\nEpsilon = 0.034029793383432774\nEpsilon = 0.03402639040409443\nEpsilon = 0.03402298776505402\nEpsilon = 0.034019585466277515\nEpsilon = 0.03401618350773089\nEpsilon = 0.03401278188938012\nEpsilon = 0.03400938061119118\nEpsilon = 0.034005979673130064\nEpsilon = 0.03400257907516275\nEpsilon = 0.033999178817255234\nEpsilon = 0.03399577889937351\nEpsilon = 0.03399237932148357\nEpsilon = 0.033988980083551425\nEpsilon = 0.03398558118554307\nAgent: ddqn_agent . Episode 1809/2000. Number of steps to finish: 20. Loss: 22.8505859375 Reward: -12.0\nEpsilon = 0.033982182627424515\nEpsilon = 0.03397878440916177\nEpsilon = 0.03397538653072086\nEpsilon = 0.03397198899206779\nEpsilon = 0.033968591793168586\nEpsilon = 0.03396519493398927\nEpsilon = 0.033961798414495876\nEpsilon = 0.033958402234654426\nEpsilon = 0.03395500639443096\nEpsilon = 0.03395161089379152\nEpsilon = 0.03394821573270214\nEpsilon = 0.03394482091112887\nEpsilon = 0.033941426429037753\nEpsilon = 0.03393803228639485\nEpsilon = 0.03393463848316621\nEpsilon = 0.03393124501931789\nEpsilon = 0.03392785189481596\nEpsilon = 0.03392445910962648\nEpsilon = 0.03392106666371552\nEpsilon = 0.03391767455704915\nAgent: ddqn_agent . Episode 1810/2000. Number of steps to finish: 20. Loss: 24.440330505371094 Reward: -12.0\nEpsilon = 0.03391428278959345\nEpsilon = 0.03391089136131449\nEpsilon = 0.033907500272178356\nEpsilon = 0.03390410952215114\nEpsilon = 0.03390071911119892\nEpsilon = 0.0338973290392878\nEpsilon = 0.03389393930638387\nEpsilon = 0.03389054991245323\nEpsilon = 0.03388716085746198\nEpsilon = 0.03388377214137624\nEpsilon = 0.0338803837641621\nEpsilon = 0.03387699572578569\nEpsilon = 0.03387360802621311\nEpsilon = 0.03387022066541049\nEpsilon = 0.033866833643343947\nEpsilon = 0.033863446959979615\nEpsilon = 0.03386006061528362\nEpsilon = 0.03385667460922209\nEpsilon = 0.03385328894176117\nEpsilon = 0.033849903612866994\nAgent: ddqn_agent . Episode 1811/2000. Number of steps to finish: 20. Loss: 21.58924674987793 Reward: -18.0\nEpsilon = 0.033846518622505706\nEpsilon = 0.03384313397064346\nEpsilon = 0.03383974965724639\nEpsilon = 0.03383636568228067\nEpsilon = 0.033832982045712445\nEpsilon = 0.03382959874750788\nEpsilon = 0.033826215787633125\nEpsilon = 0.03382283316605436\nEpsilon = 0.03381945088273776\nEpsilon = 0.033816068937649484\nEpsilon = 0.03381268733075572\nEpsilon = 0.03380930606202265\nEpsilon = 0.03380592513141645\nEpsilon = 0.03380254453890331\nEpsilon = 0.033799164284449415\nEpsilon = 0.03379578436802097\nEpsilon = 0.03379240478958417\nEpsilon = 0.033789025549105206\nEpsilon = 0.033785646646550295\nEpsilon = 0.03378226808188564\nAgent: ddqn_agent . Episode 1812/2000. Number of steps to finish: 20. Loss: 23.619182586669922 Reward: -16.0\nEpsilon = 0.03377888985507745\nEpsilon = 0.033775511966091944\nEpsilon = 0.033772134414895336\nEpsilon = 0.03376875720145384\nEpsilon = 0.0337653803257337\nEpsilon = 0.033762003787701125\nEpsilon = 0.03375862758732236\nEpsilon = 0.033755251724563624\nEpsilon = 0.03375187619939117\nEpsilon = 0.03374850101177123\nEpsilon = 0.03374512616167005\nEpsilon = 0.03374175164905389\nEpsilon = 0.033738377473888984\nEpsilon = 0.0337350036361416\nEpsilon = 0.033731630135777985\nEpsilon = 0.03372825697276441\nEpsilon = 0.03372488414706713\nEpsilon = 0.03372151165865243\nEpsilon = 0.03371813950748656\nEpsilon = 0.03371476769353581\nAgent: ddqn_agent . Episode 1813/2000. Number of steps to finish: 20. Loss: 27.735069274902344 Reward: -12.0\nEpsilon = 0.03371139621676646\nEpsilon = 0.03370802507714478\nEpsilon = 0.033704654274637064\nEpsilon = 0.0337012838092096\nEpsilon = 0.03369791368082868\nEpsilon = 0.033694543889460596\nEpsilon = 0.03369117443507165\nEpsilon = 0.03368780531762814\nEpsilon = 0.03368443653709638\nEpsilon = 0.03368106809344267\nEpsilon = 0.03367769998663333\nEpsilon = 0.03367433221663466\nEpsilon = 0.033670964783413\nEpsilon = 0.033667597686934655\nEpsilon = 0.033664230927165965\nEpsilon = 0.03366086450407325\nEpsilon = 0.03365749841762284\nEpsilon = 0.03365413266778108\nEpsilon = 0.0336507672545143\nEpsilon = 0.033647402177788846\nAgent: ddqn_agent . Episode 1814/2000. Number of steps to finish: 20. Loss: 23.270967483520508 Reward: -10.0\nEpsilon = 0.033644037437571064\nEpsilon = 0.033640673033827305\nEpsilon = 0.03363730896652392\nEpsilon = 0.033633945235627266\nEpsilon = 0.03363058184110371\nEpsilon = 0.0336272187829196\nEpsilon = 0.033623856061041305\nEpsilon = 0.0336204936754352\nEpsilon = 0.03361713162606766\nEpsilon = 0.03361376991290505\nEpsilon = 0.03361040853591376\nAgent: ddqn_agent . Episode 1815/2000. Number of steps to finish: 11. Loss: 13.661511421203613 Reward: 1.0\nEpsilon = 0.03360704749506017\nEpsilon = 0.033603686790310665\nEpsilon = 0.03360032642163163\nEpsilon = 0.03359696638898947\nEpsilon = 0.03359360669235057\nEpsilon = 0.03359024733168134\nEpsilon = 0.03358688830694817\nEpsilon = 0.033583529618117475\nEpsilon = 0.03358017126515566\nEpsilon = 0.03357681324802914\nEpsilon = 0.03357345556670434\nEpsilon = 0.03357009822114767\nEpsilon = 0.03356674121132555\nEpsilon = 0.03356338453720442\nEpsilon = 0.0335600281987507\nEpsilon = 0.03355667219593083\nEpsilon = 0.033553316528711234\nEpsilon = 0.033549961197058366\nEpsilon = 0.03354660620093866\nEpsilon = 0.03354325154031857\nAgent: ddqn_agent . Episode 1816/2000. Number of steps to finish: 20. Loss: 22.35363006591797 Reward: -12.0\nEpsilon = 0.03353989721516454\nEpsilon = 0.03353654322544302\nEpsilon = 0.033533189571120475\nEpsilon = 0.03352983625216337\nEpsilon = 0.03352648326853815\nEpsilon = 0.0335231306202113\nEpsilon = 0.03351977830714928\nEpsilon = 0.03351642632931856\nEpsilon = 0.03351307468668563\nEpsilon = 0.03350972337921696\nEpsilon = 0.03350637240687904\nEpsilon = 0.03350302176963835\nEpsilon = 0.033499671467461384\nEpsilon = 0.03349632150031464\nEpsilon = 0.033492971868164606\nEpsilon = 0.03348962257097779\nEpsilon = 0.03348627360872069\nEpsilon = 0.03348292498135982\nEpsilon = 0.03347957668886168\nEpsilon = 0.0334762287311928\nAgent: ddqn_agent . Episode 1817/2000. Number of steps to finish: 20. Loss: 26.273611068725586 Reward: -20.0\nEpsilon = 0.033472881108319676\nEpsilon = 0.03346953382020884\nEpsilon = 0.03346618686682682\nEpsilon = 0.03346284024814014\nEpsilon = 0.03345949396411533\nEpsilon = 0.03345614801471892\nEpsilon = 0.03345280239991745\nEpsilon = 0.03344945711967746\nEpsilon = 0.03344611217396549\nEpsilon = 0.03344276756274809\nEpsilon = 0.033439423285991816\nEpsilon = 0.033436079343663216\nEpsilon = 0.03343273573572885\nEpsilon = 0.03342939246215528\nEpsilon = 0.03342604952290907\nEpsilon = 0.033422706917956777\nEpsilon = 0.03341936464726498\nEpsilon = 0.03341602271080025\nEpsilon = 0.03341268110852917\nEpsilon = 0.03340933984041832\nAgent: ddqn_agent . Episode 1818/2000. Number of steps to finish: 20. Loss: 22.081817626953125 Reward: -12.0\nEpsilon = 0.03340599890643428\nEpsilon = 0.03340265830654364\nEpsilon = 0.033399318040712986\nEpsilon = 0.033395978108908916\nEpsilon = 0.03339263851109803\nEpsilon = 0.033389299247246915\nEpsilon = 0.03338596031732219\nEpsilon = 0.03338262172129046\nEpsilon = 0.03337928345911833\nEpsilon = 0.033375945530772416\nEpsilon = 0.03337260793621934\nEpsilon = 0.033369270675425715\nEpsilon = 0.033365933748358174\nEpsilon = 0.03336259715498334\nEpsilon = 0.03335926089526784\nEpsilon = 0.033355924969178316\nEpsilon = 0.033352589376681396\nEpsilon = 0.033349254117743726\nEpsilon = 0.03334591919233195\nEpsilon = 0.03334258460041272\nAgent: ddqn_agent . Episode 1819/2000. Number of steps to finish: 20. Loss: 24.89645767211914 Reward: -14.0\nEpsilon = 0.03333925034195268\nEpsilon = 0.033335916416918486\nEpsilon = 0.0333325828252768\nEpsilon = 0.03332924956699427\nEpsilon = 0.03332591664203757\nEpsilon = 0.03332258405037337\nEpsilon = 0.03331925179196833\nEpsilon = 0.03331591986678913\nEpsilon = 0.033312588274802456\nEpsilon = 0.03330925701597497\nEpsilon = 0.03330592609027338\nEpsilon = 0.03330259549766435\nEpsilon = 0.033299265238114585\nEpsilon = 0.03329593531159077\nEpsilon = 0.033292605718059615\nEpsilon = 0.03328927645748781\nEpsilon = 0.03328594752984206\nEpsilon = 0.033282618935089076\nEpsilon = 0.033279290673195565\nEpsilon = 0.033275962744128244\nAgent: ddqn_agent . Episode 1820/2000. Number of steps to finish: 20. Loss: 21.866207122802734 Reward: -10.0\nEpsilon = 0.033272635147853835\nEpsilon = 0.03326930788433905\nEpsilon = 0.03326598095355062\nEpsilon = 0.033262654355455265\nEpsilon = 0.03325932809001972\nEpsilon = 0.033256002157210716\nEpsilon = 0.033252676556994994\nEpsilon = 0.03324935128933929\nEpsilon = 0.03324602635421036\nEpsilon = 0.03324270175157494\nEpsilon = 0.03323937748139978\nEpsilon = 0.03323605354365164\nEpsilon = 0.033232729938297276\nEpsilon = 0.03322940666530345\nEpsilon = 0.033226083724636916\nEpsilon = 0.03322276111626445\nEpsilon = 0.033219438840152826\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.03321611689626881\nEpsilon = 0.03321279528457919\nEpsilon = 0.03320947400505073\nAgent: ddqn_agent . Episode 1821/2000. Number of steps to finish: 20. Loss: 20.886306762695312 Reward: -14.0\nEpsilon = 0.03320615305765023\nEpsilon = 0.03320283244234446\nEpsilon = 0.033199512159100227\nEpsilon = 0.03319619220788432\nEpsilon = 0.03319287258866353\nEpsilon = 0.03318955330140467\nEpsilon = 0.033186234346074525\nEpsilon = 0.03318291572263992\nEpsilon = 0.03317959743106766\nEpsilon = 0.03317627947132455\nEpsilon = 0.03317296184337742\nEpsilon = 0.03316964454719308\nEpsilon = 0.03316632758273836\nEpsilon = 0.03316301094998009\nEpsilon = 0.03315969464888509\nEpsilon = 0.0331563786794202\nEpsilon = 0.03315306304155226\nEpsilon = 0.033149747735248106\nEpsilon = 0.03314643276047458\nEpsilon = 0.033143118117198535\nAgent: ddqn_agent . Episode 1822/2000. Number of steps to finish: 20. Loss: 23.815383911132812 Reward: -14.0\nEpsilon = 0.033139803805386815\nEpsilon = 0.03313648982500628\nEpsilon = 0.03313317617602378\nEpsilon = 0.033129862858406176\nEpsilon = 0.03312654987212033\nEpsilon = 0.033123237217133124\nEpsilon = 0.03311992489341141\nAgent: ddqn_agent . Episode 1823/2000. Number of steps to finish: 7. Loss: 7.644240379333496 Reward: 5.0\nEpsilon = 0.03311661290092207\nEpsilon = 0.03311330123963198\nEpsilon = 0.03310998990950802\nEpsilon = 0.033106678910517065\nEpsilon = 0.03310336824262601\nEpsilon = 0.03310005790580175\nEpsilon = 0.033096747900011166\nEpsilon = 0.033093438225221164\nEpsilon = 0.033090128881398645\nEpsilon = 0.0330868198685105\nEpsilon = 0.033083511186523654\nEpsilon = 0.033080202835405\nEpsilon = 0.03307689481512146\nEpsilon = 0.033073587125639944\nEpsilon = 0.033070279766927384\nEpsilon = 0.03306697273895069\nEpsilon = 0.0330636660416768\nEpsilon = 0.033060359675072634\nEpsilon = 0.033057053639105126\nEpsilon = 0.03305374793374122\nAgent: ddqn_agent . Episode 1824/2000. Number of steps to finish: 20. Loss: 24.606069564819336 Reward: -14.0\nEpsilon = 0.033050442558947846\nEpsilon = 0.03304713751469195\nEpsilon = 0.03304383280094048\nEpsilon = 0.03304052841766039\nEpsilon = 0.033037224364818624\nEpsilon = 0.03303392064238214\nEpsilon = 0.033030617250317904\nEpsilon = 0.03302731418859287\nEpsilon = 0.03302401145717401\nEpsilon = 0.033020709056028295\nEpsilon = 0.03301740698512269\nEpsilon = 0.03301410524442418\nAgent: ddqn_agent . Episode 1825/2000. Number of steps to finish: 12. Loss: 14.44758415222168 Reward: 0.0\nEpsilon = 0.03301080383389973\nEpsilon = 0.03300750275351634\nEpsilon = 0.03300420200324099\nEpsilon = 0.033000901583040664\nEpsilon = 0.03299760149288236\nEpsilon = 0.03299430173273307\nEpsilon = 0.0329910023025598\nEpsilon = 0.03298770320232955\nEpsilon = 0.03298440443200932\nEpsilon = 0.03298110599156612\nEpsilon = 0.032977807880966964\nEpsilon = 0.03297451010017887\nEpsilon = 0.03297121264916885\nEpsilon = 0.032967915527903936\nEpsilon = 0.03296461873635115\nEpsilon = 0.03296132227447751\nEpsilon = 0.03295802614225007\nEpsilon = 0.03295473033963584\nEpsilon = 0.032951434866601875\nEpsilon = 0.032948139723115215\nAgent: ddqn_agent . Episode 1826/2000. Number of steps to finish: 20. Loss: 22.866518020629883 Reward: -14.0\nEpsilon = 0.0329448449091429\nEpsilon = 0.03294155042465199\nEpsilon = 0.03293825626960952\nEpsilon = 0.032934962443982564\nEpsilon = 0.03293166894773816\nEpsilon = 0.03292837578084339\nEpsilon = 0.03292508294326531\nEpsilon = 0.03292179043497098\nEpsilon = 0.032918498255927485\nAgent: ddqn_agent . Episode 1827/2000. Number of steps to finish: 9. Loss: 9.524250030517578 Reward: 3.0\nEpsilon = 0.03291520640610189\nEpsilon = 0.03291191488546128\nEpsilon = 0.03290862369397274\nEpsilon = 0.032905332831603344\nEpsilon = 0.03290204229832019\nEpsilon = 0.03289875209409036\nEpsilon = 0.03289546221888095\nEpsilon = 0.03289217267265906\nEpsilon = 0.032888883455391794\nEpsilon = 0.032885594567046254\nEpsilon = 0.03288230600758955\nEpsilon = 0.03287901777698879\nEpsilon = 0.03287572987521109\nEpsilon = 0.03287244230222357\nEpsilon = 0.03286915505799334\nEpsilon = 0.03286586814248754\nEpsilon = 0.032862581555673294\nEpsilon = 0.03285929529751773\nEpsilon = 0.03285600936798798\nEpsilon = 0.03285272376705118\nAgent: ddqn_agent . Episode 1828/2000. Number of steps to finish: 20. Loss: 22.57776641845703 Reward: -18.0\nEpsilon = 0.03284943849467448\nEpsilon = 0.03284615355082501\nEpsilon = 0.03284286893546992\nEpsilon = 0.03283958464857637\nEpsilon = 0.032836300690111515\nEpsilon = 0.032833017060042505\nEpsilon = 0.0328297337583365\nEpsilon = 0.032826450784960666\nEpsilon = 0.03282316813988217\nEpsilon = 0.03281988582306818\nEpsilon = 0.032816603834485876\nEpsilon = 0.032813322174102426\nEpsilon = 0.03281004084188502\nEpsilon = 0.032806759837800833\nEpsilon = 0.032803479161817056\nEpsilon = 0.03280019881390087\nEpsilon = 0.03279691879401948\nEpsilon = 0.03279363910214008\nEpsilon = 0.032790359738229864\nEpsilon = 0.03278708070225604\nAgent: ddqn_agent . Episode 1829/2000. Number of steps to finish: 20. Loss: 23.203697204589844 Reward: -10.0\nEpsilon = 0.03278380199418582\nEpsilon = 0.0327805236139864\nEpsilon = 0.03277724556162501\nEpsilon = 0.032773967837068846\nEpsilon = 0.03277069044028514\nEpsilon = 0.03276741337124111\nEpsilon = 0.03276413662990398\nEpsilon = 0.03276086021624099\nEpsilon = 0.03275758413021937\nEpsilon = 0.032754308371806345\nEpsilon = 0.03275103294096916\nEpsilon = 0.032747757837675064\nEpsilon = 0.032744483061891295\nEpsilon = 0.03274120861358511\nEpsilon = 0.03273793449272375\nEpsilon = 0.03273466069927448\nAgent: ddqn_agent . Episode 1830/2000. Number of steps to finish: 16. Loss: 17.381650924682617 Reward: -4.0\nEpsilon = 0.032731387233204554\nEpsilon = 0.03272811409448123\nEpsilon = 0.032724841283071786\nEpsilon = 0.03272156879894348\nEpsilon = 0.032718296642063584\nEpsilon = 0.03271502481239938\nEpsilon = 0.03271175330991814\nEpsilon = 0.03270848213458715\nEpsilon = 0.032705211286373694\nEpsilon = 0.032701940765245054\nEpsilon = 0.03269867057116853\nEpsilon = 0.032695400704111414\nEpsilon = 0.032692131164041\nEpsilon = 0.032688861950924596\nEpsilon = 0.032685593064729505\nEpsilon = 0.032682324505423035\nEpsilon = 0.0326790562729725\nEpsilon = 0.0326757883673452\nEpsilon = 0.03267252078850846\nEpsilon = 0.03266925353642961\nAgent: ddqn_agent . Episode 1831/2000. Number of steps to finish: 20. Loss: 20.839580535888672 Reward: -10.0\nEpsilon = 0.03266598661107597\nEpsilon = 0.03266272001241486\nEpsilon = 0.03265945374041362\nEpsilon = 0.03265618779503958\nEpsilon = 0.03265292217626008\nEpsilon = 0.03264965688404245\nEpsilon = 0.03264639191835404\nEpsilon = 0.03264312727916221\nEpsilon = 0.03263986296643429\nEpsilon = 0.03263659898013765\nEpsilon = 0.03263333532023963\nEpsilon = 0.03263007198670761\nEpsilon = 0.03262680897950894\nEpsilon = 0.032623546298610986\nEpsilon = 0.03262028394398112\nEpsilon = 0.03261702191558673\nEpsilon = 0.03261376021339517\nEpsilon = 0.03261049883737383\nEpsilon = 0.03260723778749009\nEpsilon = 0.03260397706371134\nAgent: ddqn_agent . Episode 1832/2000. Number of steps to finish: 20. Loss: 20.077312469482422 Reward: -16.0\nEpsilon = 0.03260071666600497\nEpsilon = 0.03259745659433837\nEpsilon = 0.03259419684867893\nEpsilon = 0.03259093742899406\nEpsilon = 0.03258767833525116\nEpsilon = 0.03258441956741764\nEpsilon = 0.032581161125460896\nEpsilon = 0.03257790300934835\nEpsilon = 0.03257464521904741\nEpsilon = 0.032571387754525505\nEpsilon = 0.032568130615750056\nEpsilon = 0.03256487380268848\nEpsilon = 0.03256161731530821\nEpsilon = 0.03255836115357668\nEpsilon = 0.03255510531746132\nEpsilon = 0.03255184980692958\nEpsilon = 0.03254859462194889\nEpsilon = 0.03254533976248669\nEpsilon = 0.032542085228510445\nEpsilon = 0.03253883101998759\nAgent: ddqn_agent . Episode 1833/2000. Number of steps to finish: 20. Loss: 25.32243537902832 Reward: -10.0\nEpsilon = 0.032535577136885596\nEpsilon = 0.03253232357917191\nEpsilon = 0.03252907034681399\nEpsilon = 0.03252581743977931\nEpsilon = 0.032522564858035334\nEpsilon = 0.032519312601549534\nEpsilon = 0.03251606067028938\nEpsilon = 0.03251280906422235\nEpsilon = 0.032509557783315934\nEpsilon = 0.03250630682753761\nEpsilon = 0.032503056196854854\nEpsilon = 0.03249980589123517\nEpsilon = 0.03249655591064605\nEpsilon = 0.032493306255054984\nEpsilon = 0.03249005692442948\nEpsilon = 0.03248680791873704\nEpsilon = 0.032483559237945166\nEpsilon = 0.03248031088202137\nEpsilon = 0.03247706285093317\nEpsilon = 0.03247381514464808\nAgent: ddqn_agent . Episode 1834/2000. Number of steps to finish: 20. Loss: 24.06327247619629 Reward: -16.0\nEpsilon = 0.03247056776313362\nEpsilon = 0.03246732070635731\nEpsilon = 0.03246407397428667\nEpsilon = 0.03246082756688924\nEpsilon = 0.03245758148413255\nEpsilon = 0.03245433572598414\nEpsilon = 0.03245109029241154\nEpsilon = 0.0324478451833823\nEpsilon = 0.03244460039886396\nEpsilon = 0.03244135593882407\nEpsilon = 0.03243811180323019\nEpsilon = 0.032434867992049865\nEpsilon = 0.03243162450525066\nEpsilon = 0.032428381342800135\nEpsilon = 0.03242513850466586\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.03242189599081539\nEpsilon = 0.03241865380121631\nEpsilon = 0.032415411935836184\nEpsilon = 0.0324121703946426\nEpsilon = 0.03240892917760314\nAgent: ddqn_agent . Episode 1835/2000. Number of steps to finish: 20. Loss: 19.459020614624023 Reward: -20.0\nEpsilon = 0.032405688284685374\nEpsilon = 0.03240244771585691\nEpsilon = 0.03239920747108532\nEpsilon = 0.03239596755033822\nEpsilon = 0.03239272795358318\nEpsilon = 0.03238948868078782\nEpsilon = 0.032386249731919746\nEpsilon = 0.03238301110694655\nEpsilon = 0.03237977280583586\nEpsilon = 0.03237653482855527\nEpsilon = 0.03237329717507242\nEpsilon = 0.03237005984535491\nEpsilon = 0.032366822839370375\nEpsilon = 0.03236358615708644\nEpsilon = 0.03236034979847073\nEpsilon = 0.03235711376349088\nEpsilon = 0.03235387805211453\nEpsilon = 0.03235064266430932\nEpsilon = 0.03234740760004289\nEpsilon = 0.032344172859282885\nAgent: ddqn_agent . Episode 1836/2000. Number of steps to finish: 20. Loss: 27.463993072509766 Reward: -10.0\nEpsilon = 0.03234093844199696\nEpsilon = 0.03233770434815276\nEpsilon = 0.03233447057771795\nEpsilon = 0.032331237130660176\nEpsilon = 0.03232800400694711\nEpsilon = 0.032324771206546414\nEpsilon = 0.03232153872942576\nEpsilon = 0.03231830657555282\nEpsilon = 0.03231507474489526\nEpsilon = 0.03231184323742077\nEpsilon = 0.03230861205309703\nEpsilon = 0.03230538119189172\nEpsilon = 0.03230215065377253\nEpsilon = 0.032298920438707156\nEpsilon = 0.032295690546663286\nEpsilon = 0.03229246097760862\nEpsilon = 0.03228923173151086\nEpsilon = 0.03228600280833771\nEpsilon = 0.03228277420805688\nEpsilon = 0.032279545930636075\nAgent: ddqn_agent . Episode 1837/2000. Number of steps to finish: 20. Loss: 25.859607696533203 Reward: -14.0\nEpsilon = 0.03227631797604301\nEpsilon = 0.03227309034424541\nEpsilon = 0.03226986303521098\nEpsilon = 0.03226663604890746\nEpsilon = 0.032263409385302574\nEpsilon = 0.032260183044364045\nEpsilon = 0.03225695702605961\nEpsilon = 0.032253731330357006\nEpsilon = 0.03225050595722397\nEpsilon = 0.03224728090662825\nEpsilon = 0.03224405617853759\nEpsilon = 0.03224083177291973\nEpsilon = 0.03223760768974244\nEpsilon = 0.032234383928973466\nEpsilon = 0.03223116049058057\nEpsilon = 0.032227937374531515\nEpsilon = 0.03222471458079406\nEpsilon = 0.03222149210933598\nEpsilon = 0.03221826996012505\nEpsilon = 0.03221504813312904\nAgent: ddqn_agent . Episode 1838/2000. Number of steps to finish: 20. Loss: 22.620080947875977 Reward: -12.0\nEpsilon = 0.032211826628315725\nEpsilon = 0.03220860544565289\nEpsilon = 0.032205384585108326\nEpsilon = 0.032202164046649814\nEpsilon = 0.03219894383024515\nEpsilon = 0.032195723935862126\nEpsilon = 0.032192504363468544\nEpsilon = 0.0321892851130322\nEpsilon = 0.0321860661845209\nEpsilon = 0.03218284757790245\nEpsilon = 0.032179629293144656\nEpsilon = 0.03217641133021534\nEpsilon = 0.03217319368908232\nEpsilon = 0.032169976369713416\nEpsilon = 0.03216675937207644\nEpsilon = 0.03216354269613923\nEpsilon = 0.03216032634186962\nEpsilon = 0.03215711030923543\nEpsilon = 0.03215389459820451\nEpsilon = 0.03215067920874469\nAgent: ddqn_agent . Episode 1839/2000. Number of steps to finish: 20. Loss: 21.895933151245117 Reward: -10.0\nEpsilon = 0.03214746414082382\nEpsilon = 0.032144249394409734\nEpsilon = 0.032141034969470295\nEpsilon = 0.032137820865973346\nEpsilon = 0.03213460708388675\nEpsilon = 0.03213139362317836\nEpsilon = 0.03212818048381604\nEpsilon = 0.03212496766576766\nEpsilon = 0.032121755169001086\nEpsilon = 0.032118542993484184\nEpsilon = 0.032115331139184834\nEpsilon = 0.03211211960607092\nEpsilon = 0.03210890839411031\nEpsilon = 0.0321056975032709\nEpsilon = 0.03210248693352057\nEpsilon = 0.03209927668482722\nEpsilon = 0.03209606675715874\nEpsilon = 0.032092857150483024\nEpsilon = 0.03208964786476798\nEpsilon = 0.0320864388999815\nAgent: ddqn_agent . Episode 1840/2000. Number of steps to finish: 20. Loss: 23.45745277404785 Reward: -10.0\nEpsilon = 0.032083230256091506\nEpsilon = 0.032080021933065896\nEpsilon = 0.03207681393087259\nEpsilon = 0.032073606249479504\nEpsilon = 0.03207039888885456\nEpsilon = 0.03206719184896568\nEpsilon = 0.03206398512978078\nEpsilon = 0.0320607787312678\nEpsilon = 0.03205757265339468\nEpsilon = 0.03205436689612934\nEpsilon = 0.032051161459439724\nEpsilon = 0.03204795634329378\nEpsilon = 0.03204475154765945\nEpsilon = 0.032041547072504686\nEpsilon = 0.03203834291779744\nEpsilon = 0.03203513908350566\nEpsilon = 0.032031935569597304\nEpsilon = 0.03202873237604034\nEpsilon = 0.03202552950280274\nEpsilon = 0.032022326949852464\nAgent: ddqn_agent . Episode 1841/2000. Number of steps to finish: 20. Loss: 20.42664909362793 Reward: -18.0\nEpsilon = 0.03201912471715748\nEpsilon = 0.032015922804685765\nEpsilon = 0.032012721212405296\nEpsilon = 0.03200951994028405\nEpsilon = 0.032006318988290025\nEpsilon = 0.0320031183563912\nEpsilon = 0.03199991804455556\nEpsilon = 0.03199671805275111\nEpsilon = 0.031993518380945836\nEpsilon = 0.03199031902910774\nEpsilon = 0.03198711999720483\nEpsilon = 0.03198392128520511\nEpsilon = 0.03198072289307659\nEpsilon = 0.03197752482078728\nEpsilon = 0.0319743270683052\nEpsilon = 0.031971129635598367\nEpsilon = 0.03196793252263481\nEpsilon = 0.03196473572938254\nEpsilon = 0.03196153925580961\nEpsilon = 0.03195834310188403\nAgent: ddqn_agent . Episode 1842/2000. Number of steps to finish: 20. Loss: 20.447933197021484 Reward: -16.0\nEpsilon = 0.03195514726757384\nEpsilon = 0.03195195175284708\nEpsilon = 0.0319487565576718\nEpsilon = 0.03194556168201603\nEpsilon = 0.03194236712584783\nEpsilon = 0.031939172889135246\nEpsilon = 0.031935978971846336\nEpsilon = 0.031932785373949155\nEpsilon = 0.03192959209541176\nAgent: ddqn_agent . Episode 1843/2000. Number of steps to finish: 9. Loss: 11.20326042175293 Reward: 3.0\nEpsilon = 0.03192639913620222\nEpsilon = 0.031923206496288596\nEpsilon = 0.031920014175638965\nEpsilon = 0.0319168221742214\nEpsilon = 0.03191363049200398\nEpsilon = 0.031910439128954776\nEpsilon = 0.03190724808504188\nAgent: ddqn_agent . Episode 1844/2000. Number of steps to finish: 7. Loss: 7.0796589851379395 Reward: 5.0\nEpsilon = 0.03190405736023338\nEpsilon = 0.031900866954497356\nEpsilon = 0.03189767686780191\nEpsilon = 0.031894487100115124\nEpsilon = 0.03189129765140511\nEpsilon = 0.03188810852163997\nEpsilon = 0.03188491971078781\nEpsilon = 0.03188173121881673\nEpsilon = 0.03187854304569485\nEpsilon = 0.03187535519139029\nEpsilon = 0.03187216765587115\nEpsilon = 0.03186898043910556\nAgent: ddqn_agent . Episode 1845/2000. Number of steps to finish: 12. Loss: 14.31104850769043 Reward: 0.0\nEpsilon = 0.03186579354106165\nEpsilon = 0.03186260696170754\nEpsilon = 0.03185942070101137\nEpsilon = 0.031856234758941275\nEpsilon = 0.03185304913546538\nEpsilon = 0.031849863830551835\nEpsilon = 0.03184667884416878\nEpsilon = 0.03184349417628436\nEpsilon = 0.03184030982686673\nEpsilon = 0.03183712579588405\nEpsilon = 0.03183394208330446\nEpsilon = 0.03183075868909613\nEpsilon = 0.031827575613227216\nEpsilon = 0.03182439285566589\nEpsilon = 0.03182121041638033\nEpsilon = 0.03181802829533869\nEpsilon = 0.03181484649250915\nEpsilon = 0.0318116650078599\nEpsilon = 0.03180848384135912\nEpsilon = 0.03180530299297499\nAgent: ddqn_agent . Episode 1846/2000. Number of steps to finish: 20. Loss: 19.58707618713379 Reward: -12.0\nEpsilon = 0.03180212246267569\nEpsilon = 0.031798942250429424\nEpsilon = 0.031795762356204384\nEpsilon = 0.03179258277996876\nEpsilon = 0.03178940352169077\nEpsilon = 0.0317862245813386\nEpsilon = 0.031783045958880464\nEpsilon = 0.031779867654284574\nEpsilon = 0.03177668966751915\nEpsilon = 0.0317735119985524\nEpsilon = 0.031770334647352544\nEpsilon = 0.031767157613887806\nEpsilon = 0.03176398089812642\nEpsilon = 0.031760804500036606\nEpsilon = 0.031757628419586605\nEpsilon = 0.03175445265674465\nEpsilon = 0.031751277211478975\nEpsilon = 0.03174810208375783\nEpsilon = 0.03174492727354945\nEpsilon = 0.0317417527808221\nAgent: ddqn_agent . Episode 1847/2000. Number of steps to finish: 20. Loss: 26.662330627441406 Reward: -10.0\nEpsilon = 0.03173857860554402\nEpsilon = 0.031735404747683466\nEpsilon = 0.031732231207208694\nEpsilon = 0.031729057984087976\nEpsilon = 0.031725885078289565\nEpsilon = 0.031722712489781736\nEpsilon = 0.03171954021853276\nEpsilon = 0.03171636826451091\nEpsilon = 0.03171319662768446\nEpsilon = 0.031710025308021694\nEpsilon = 0.03170685430549089\nEpsilon = 0.03170368362006034\nEpsilon = 0.031700513251698335\nEpsilon = 0.031697343200373165\nEpsilon = 0.031694173466053126\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.03169100404870652\nEpsilon = 0.03168783494830165\nEpsilon = 0.03168466616480682\nEpsilon = 0.03168149769819034\nEpsilon = 0.03167832954842052\nAgent: ddqn_agent . Episode 1848/2000. Number of steps to finish: 20. Loss: 21.9969482421875 Reward: -12.0\nEpsilon = 0.03167516171546568\nEpsilon = 0.03167199419929413\nEpsilon = 0.0316688269998742\nEpsilon = 0.03166566011717421\nEpsilon = 0.0316624935511625\nEpsilon = 0.03165932730180738\nEpsilon = 0.0316561613690772\nEpsilon = 0.0316529957529403\nEpsilon = 0.031649830453365\nEpsilon = 0.03164666547031966\nEpsilon = 0.03164350080377263\nEpsilon = 0.031640336453692255\nEpsilon = 0.031637172420046884\nEpsilon = 0.03163400870280488\nEpsilon = 0.0316308453019346\nEpsilon = 0.031627682217404404\nEpsilon = 0.03162451944918267\nEpsilon = 0.03162135699723775\nEpsilon = 0.031618194861538024\nEpsilon = 0.03161503304205187\nAgent: ddqn_agent . Episode 1849/2000. Number of steps to finish: 20. Loss: 24.487363815307617 Reward: -20.0\nEpsilon = 0.031611871538747664\nEpsilon = 0.03160871035159379\nEpsilon = 0.03160554948055863\nEpsilon = 0.031602388925610576\nEpsilon = 0.031599228686718016\nEpsilon = 0.03159606876384934\nEpsilon = 0.03159290915697296\nEpsilon = 0.03158974986605726\nEpsilon = 0.03158659089107065\nEpsilon = 0.03158343223198155\nEpsilon = 0.03158027388875835\nEpsilon = 0.03157711586136948\nEpsilon = 0.031573958149783345\nEpsilon = 0.03157080075396837\nEpsilon = 0.031567643673892976\nEpsilon = 0.03156448690952559\nEpsilon = 0.03156133046083464\nEpsilon = 0.031558174327788555\nEpsilon = 0.031555018510355776\nEpsilon = 0.03155186300850474\nAgent: ddqn_agent . Episode 1850/2000. Number of steps to finish: 20. Loss: 23.37359619140625 Reward: -16.0\nEpsilon = 0.03154870782220389\nEpsilon = 0.03154555295142167\nEpsilon = 0.03154239839612653\nEpsilon = 0.031539244156286915\nEpsilon = 0.03153609023187129\nEpsilon = 0.031532936622848104\nEpsilon = 0.031529783329185816\nEpsilon = 0.031526630350852895\nEpsilon = 0.03152347768781781\nEpsilon = 0.03152032534004903\nEpsilon = 0.03151717330751503\nEpsilon = 0.03151402159018428\nEpsilon = 0.03151087018802526\nEpsilon = 0.03150771910100646\nEpsilon = 0.03150456832909636\nEpsilon = 0.031501417872263446\nEpsilon = 0.03149826773047622\nEpsilon = 0.03149511790370317\nEpsilon = 0.0314919683919128\nEpsilon = 0.03148881919507361\nAgent: ddqn_agent . Episode 1851/2000. Number of steps to finish: 20. Loss: 25.241193771362305 Reward: -14.0\nEpsilon = 0.03148567031315411\nEpsilon = 0.031482521746122795\nEpsilon = 0.031479373493948186\nEpsilon = 0.03147622555659879\nEpsilon = 0.03147307793404313\nEpsilon = 0.031469930626249726\nEpsilon = 0.0314667836331871\nEpsilon = 0.03146363695482378\nEpsilon = 0.0314604905911283\nEpsilon = 0.031457344542069186\nEpsilon = 0.03145419880761498\nEpsilon = 0.03145105338773422\nEpsilon = 0.031447908282395444\nEpsilon = 0.031444763491567204\nEpsilon = 0.031441619015218046\nEpsilon = 0.03143847485331652\nEpsilon = 0.031435331005831194\nEpsilon = 0.03143218747273061\nEpsilon = 0.03142904425398334\nEpsilon = 0.03142590134955794\nAgent: ddqn_agent . Episode 1852/2000. Number of steps to finish: 20. Loss: 23.316213607788086 Reward: -12.0\nEpsilon = 0.031422758759422985\nEpsilon = 0.03141961648354704\nEpsilon = 0.031416474521898684\nEpsilon = 0.031413332874446497\nEpsilon = 0.03141019154115905\nEpsilon = 0.03140705052200494\nEpsilon = 0.031403909816952735\nEpsilon = 0.03140076942597104\nEpsilon = 0.03139762934902844\nEpsilon = 0.03139448958609354\nEpsilon = 0.031391350137134934\nEpsilon = 0.03138821100212122\nEpsilon = 0.03138507218102101\nEpsilon = 0.03138193367380291\nEpsilon = 0.03137879548043553\nEpsilon = 0.03137565760088749\nEpsilon = 0.031372520035127395\nEpsilon = 0.03136938278312388\nEpsilon = 0.03136624584484557\nEpsilon = 0.03136310922026109\nAgent: ddqn_agent . Episode 1853/2000. Number of steps to finish: 20. Loss: 24.403425216674805 Reward: -18.0\nEpsilon = 0.03135997290933906\nEpsilon = 0.03135683691204813\nEpsilon = 0.03135370122835693\nEpsilon = 0.03135056585823409\nEpsilon = 0.03134743080164827\nEpsilon = 0.03134429605856811\nAgent: ddqn_agent . Episode 1854/2000. Number of steps to finish: 6. Loss: 5.444340705871582 Reward: 6.0\nEpsilon = 0.03134116162896225\nEpsilon = 0.031338027512799356\nEpsilon = 0.03133489371004808\nEpsilon = 0.031331760220677074\nEpsilon = 0.03132862704465501\nEpsilon = 0.03132549418195054\nEpsilon = 0.031322361632532346\nEpsilon = 0.031319229396369094\nEpsilon = 0.031316097473429456\nEpsilon = 0.03131296586368212\nEpsilon = 0.03130983456709575\nEpsilon = 0.03130670358363904\nEpsilon = 0.031303572913280675\nEpsilon = 0.03130044255598935\nEpsilon = 0.03129731251173375\nEpsilon = 0.03129418278048258\nEpsilon = 0.03129105336220453\nEpsilon = 0.031287924256868306\nEpsilon = 0.03128479546444262\nEpsilon = 0.03128166698489618\nAgent: ddqn_agent . Episode 1855/2000. Number of steps to finish: 20. Loss: 24.226051330566406 Reward: -16.0\nEpsilon = 0.03127853881819769\nEpsilon = 0.031275410964315874\nEpsilon = 0.031272283423219444\nEpsilon = 0.03126915619487712\nEpsilon = 0.03126602927925763\nEpsilon = 0.03126290267632971\nEpsilon = 0.031259776386062074\nEpsilon = 0.031256650408423466\nEpsilon = 0.031253524743382624\nEpsilon = 0.03125039939090828\nEpsilon = 0.031247274350969193\nEpsilon = 0.031244149623534098\nEpsilon = 0.031241025208571745\nEpsilon = 0.03123790110605089\nEpsilon = 0.031234777315940283\nEpsilon = 0.031231653838208688\nEpsilon = 0.031228530672824866\nEpsilon = 0.031225407819757585\nEpsilon = 0.03122228527897561\nEpsilon = 0.031219163050447713\nAgent: ddqn_agent . Episode 1856/2000. Number of steps to finish: 20. Loss: 24.379549026489258 Reward: -16.0\nEpsilon = 0.03121604113414267\nEpsilon = 0.031212919530029255\nEpsilon = 0.031209798238076254\nEpsilon = 0.031206677258252446\nEpsilon = 0.03120355659052662\nEpsilon = 0.031200436234867568\nEpsilon = 0.031197316191244082\nEpsilon = 0.031194196459624957\nEpsilon = 0.031191077039978993\nEpsilon = 0.031187957932274996\nEpsilon = 0.03118483913648177\nEpsilon = 0.03118172065256812\nEpsilon = 0.031178602480502863\nEpsilon = 0.031175484620254813\nEpsilon = 0.031172367071792786\nEpsilon = 0.031169249835085608\nEpsilon = 0.0311661329101021\nEpsilon = 0.03116301629681109\nEpsilon = 0.03115989999518141\nEpsilon = 0.031156784005181893\nAgent: ddqn_agent . Episode 1857/2000. Number of steps to finish: 20. Loss: 20.701784133911133 Reward: -10.0\nEpsilon = 0.031153668326781375\nEpsilon = 0.0311505529599487\nEpsilon = 0.031147437904652703\nEpsilon = 0.031144323160862238\nEpsilon = 0.031141208728546154\nEpsilon = 0.0311380946076733\nEpsilon = 0.03113498079821253\nEpsilon = 0.03113186730013271\nEpsilon = 0.0311287541134027\nEpsilon = 0.03112564123799136\nEpsilon = 0.03112252867386756\nEpsilon = 0.031119416421000173\nEpsilon = 0.03111630447935807\nEpsilon = 0.031113192848910134\nEpsilon = 0.031110081529625244\nEpsilon = 0.031106970521472283\nEpsilon = 0.031103859824420137\nEpsilon = 0.031100749438437696\nEpsilon = 0.031097639363493853\nEpsilon = 0.031094529599557504\nAgent: ddqn_agent . Episode 1858/2000. Number of steps to finish: 20. Loss: 21.465696334838867 Reward: -12.0\nEpsilon = 0.031091420146597548\nEpsilon = 0.03108831100458289\nEpsilon = 0.031085202173482433\nEpsilon = 0.031082093653265086\nEpsilon = 0.03107898544389976\nEpsilon = 0.03107587754535537\nEpsilon = 0.031072769957600836\nEpsilon = 0.031069662680605077\nEpsilon = 0.031066555714337016\nEpsilon = 0.031063449058765583\nEpsilon = 0.031060342713859705\nEpsilon = 0.031057236679588318\nEpsilon = 0.03105413095592036\nEpsilon = 0.031051025542824767\nEpsilon = 0.031047920440270486\nEpsilon = 0.03104481564822646\nEpsilon = 0.031041711166661635\nEpsilon = 0.03103860699554497\nEpsilon = 0.031035503134845415\nEpsilon = 0.031032399584531932\nAgent: ddqn_agent . Episode 1859/2000. Number of steps to finish: 20. Loss: 25.983983993530273 Reward: -10.0\nEpsilon = 0.03102929634457348\nEpsilon = 0.03102619341493902\nEpsilon = 0.03102309079559753\nEpsilon = 0.03101998848651797\nEpsilon = 0.03101688648766932\nEpsilon = 0.031013784799020554\nEpsilon = 0.03101068342054065\nEpsilon = 0.0310075823521986\nEpsilon = 0.03100448159396338\nEpsilon = 0.03100138114580398\nEpsilon = 0.0309982810076894\nEpsilon = 0.030995181179588632\nEpsilon = 0.030992081661470675\nEpsilon = 0.030988982453304528\nEpsilon = 0.0309858835550592\nEpsilon = 0.030982784966703692\nEpsilon = 0.030979686688207022\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.030976588719538203\nEpsilon = 0.03097349106066625\nEpsilon = 0.03097039371156018\nAgent: ddqn_agent . Episode 1860/2000. Number of steps to finish: 20. Loss: 20.836856842041016 Reward: -10.0\nEpsilon = 0.030967296672189025\nEpsilon = 0.030964199942521805\nEpsilon = 0.030961103522527553\nEpsilon = 0.0309580074121753\nEpsilon = 0.030954911611434082\nEpsilon = 0.030951816120272937\nEpsilon = 0.030948720938660912\nEpsilon = 0.030945626066567047\nEpsilon = 0.03094253150396039\nEpsilon = 0.030939437250809994\nEpsilon = 0.030936343307084915\nEpsilon = 0.030933249672754206\nEpsilon = 0.03093015634778693\nEpsilon = 0.030927063332152154\nEpsilon = 0.03092397062581894\nEpsilon = 0.03092087822875636\nEpsilon = 0.030917786140933486\nEpsilon = 0.030914694362319393\nEpsilon = 0.03091160289288316\nEpsilon = 0.03090851173259387\nAgent: ddqn_agent . Episode 1861/2000. Number of steps to finish: 20. Loss: 22.818105697631836 Reward: -14.0\nEpsilon = 0.030905420881420613\nEpsilon = 0.03090233033933247\nEpsilon = 0.030899240106298537\nEpsilon = 0.030896150182287908\nEpsilon = 0.03089306056726968\nEpsilon = 0.030889971261212953\nEpsilon = 0.030886882264086833\nEpsilon = 0.030883793575860425\nEpsilon = 0.03088070519650284\nEpsilon = 0.03087761712598319\nEpsilon = 0.03087452936427059\nEpsilon = 0.030871441911334164\nEpsilon = 0.03086835476714303\nEpsilon = 0.030865267931666315\nEpsilon = 0.030862181404873148\nEpsilon = 0.03085909518673266\nEpsilon = 0.030856009277213988\nEpsilon = 0.030852923676286267\nEpsilon = 0.030849838383918637\nEpsilon = 0.030846753400080244\nAgent: ddqn_agent . Episode 1862/2000. Number of steps to finish: 20. Loss: 25.596342086791992 Reward: -10.0\nEpsilon = 0.030843668724740238\nEpsilon = 0.030840584357867764\nEpsilon = 0.030837500299431976\nEpsilon = 0.030834416549402034\nEpsilon = 0.030831333107747095\nEpsilon = 0.030828249974436322\nEpsilon = 0.03082516714943888\nEpsilon = 0.030822084632723935\nAgent: ddqn_agent . Episode 1863/2000. Number of steps to finish: 8. Loss: 8.392857551574707 Reward: 4.0\nEpsilon = 0.030819002424260663\nEpsilon = 0.03081592052401824\nEpsilon = 0.030812838931965836\nEpsilon = 0.03080975764807264\nEpsilon = 0.030806676672307833\nEpsilon = 0.030803596004640603\nEpsilon = 0.03080051564504014\nEpsilon = 0.030797435593475635\nEpsilon = 0.03079435584991629\nAgent: ddqn_agent . Episode 1864/2000. Number of steps to finish: 9. Loss: 8.252060890197754 Reward: 3.0\nEpsilon = 0.030791276414331298\nEpsilon = 0.030788197286689866\nEpsilon = 0.0307851184669612\nEpsilon = 0.030782039955114502\nEpsilon = 0.03077896175111899\nEpsilon = 0.03077588385494388\nEpsilon = 0.030772806266558386\nEpsilon = 0.030769728985931732\nEpsilon = 0.03076665201303314\nEpsilon = 0.030763575347831838\nEpsilon = 0.030760498990297053\nEpsilon = 0.030757422940398023\nEpsilon = 0.030754347198103982\nEpsilon = 0.030751271763384172\nEpsilon = 0.030748196636207835\nEpsilon = 0.030745121816544214\nEpsilon = 0.03074204730436256\nEpsilon = 0.030738973099632125\nEpsilon = 0.030735899202322163\nEpsilon = 0.03073282561240193\nAgent: ddqn_agent . Episode 1865/2000. Number of steps to finish: 20. Loss: 24.5498104095459 Reward: -14.0\nEpsilon = 0.03072975232984069\nEpsilon = 0.030726679354607704\nEpsilon = 0.030723606686672244\nEpsilon = 0.030720534326003576\nEpsilon = 0.030717462272570977\nEpsilon = 0.03071439052634372\nEpsilon = 0.030711319087291085\nEpsilon = 0.030708247955382358\nEpsilon = 0.03070517713058682\nEpsilon = 0.030702106612873762\nEpsilon = 0.030699036402212474\nEpsilon = 0.030695966498572255\nEpsilon = 0.0306928969019224\nEpsilon = 0.030689827612232205\nEpsilon = 0.030686758629470984\nEpsilon = 0.030683689953608036\nEpsilon = 0.030680621584612676\nEpsilon = 0.030677553522454215\nEpsilon = 0.03067448576710197\nEpsilon = 0.03067141831852526\nAgent: ddqn_agent . Episode 1866/2000. Number of steps to finish: 20. Loss: 25.91208267211914 Reward: -16.0\nEpsilon = 0.03066835117669341\nEpsilon = 0.03066528434157574\nEpsilon = 0.030662217813141584\nEpsilon = 0.03065915159136027\nEpsilon = 0.030656085676201133\nEpsilon = 0.030653020067633513\nEpsilon = 0.03064995476562675\nEpsilon = 0.030646889770150187\nEpsilon = 0.030643825081173172\nEpsilon = 0.030640760698665056\nEpsilon = 0.03063769662259519\nEpsilon = 0.03063463285293293\nEpsilon = 0.030631569389647638\nEpsilon = 0.030628506232708675\nEpsilon = 0.030625443382085404\nEpsilon = 0.030622380837747196\nEpsilon = 0.030619318599663423\nEpsilon = 0.030616256667803456\nEpsilon = 0.030613195042136675\nEpsilon = 0.030610133722632462\nAgent: ddqn_agent . Episode 1867/2000. Number of steps to finish: 20. Loss: 26.126419067382812 Reward: -12.0\nEpsilon = 0.030607072709260198\nEpsilon = 0.030604012001989273\nEpsilon = 0.030600951600789076\nEpsilon = 0.030597891505629\nEpsilon = 0.030594831716478435\nEpsilon = 0.030591772233306787\nEpsilon = 0.030588713056083457\nEpsilon = 0.03058565418477785\nAgent: ddqn_agent . Episode 1868/2000. Number of steps to finish: 8. Loss: 10.069806098937988 Reward: 4.0\nEpsilon = 0.030582595619359373\nEpsilon = 0.03057953735979744\nEpsilon = 0.03057647940606146\nEpsilon = 0.030573421758120854\nEpsilon = 0.03057036441594504\nEpsilon = 0.030567307379503448\nEpsilon = 0.030564250648765497\nEpsilon = 0.03056119422370062\nEpsilon = 0.03055813810427825\nEpsilon = 0.030555082290467824\nEpsilon = 0.030552026782238777\nEpsilon = 0.030548971579560553\nEpsilon = 0.0305459166824026\nEpsilon = 0.030542862090734358\nEpsilon = 0.030539807804525285\nEpsilon = 0.030536753823744833\nEpsilon = 0.030533700148362458\nEpsilon = 0.03053064677834762\nEpsilon = 0.030527593713669785\nEpsilon = 0.030524540954298417\nAgent: ddqn_agent . Episode 1869/2000. Number of steps to finish: 20. Loss: 27.756711959838867 Reward: -18.0\nEpsilon = 0.03052148850020299\nEpsilon = 0.03051843635135297\nEpsilon = 0.030515384507717836\nEpsilon = 0.030512332969267066\nEpsilon = 0.030509281735970138\nEpsilon = 0.03050623080779654\nEpsilon = 0.03050318018471576\nEpsilon = 0.03050012986669729\nEpsilon = 0.030497079853710623\nEpsilon = 0.030494030145725253\nEpsilon = 0.030490980742710682\nEpsilon = 0.03048793164463641\nEpsilon = 0.030484882851471946\nEpsilon = 0.0304818343631868\nEpsilon = 0.030478786179750483\nEpsilon = 0.030475738301132507\nEpsilon = 0.030472690727302392\nEpsilon = 0.030469643458229664\nEpsilon = 0.030466596493883842\nEpsilon = 0.030463549834234455\nAgent: ddqn_agent . Episode 1870/2000. Number of steps to finish: 20. Loss: 18.039669036865234 Reward: -12.0\nEpsilon = 0.03046050347925103\nEpsilon = 0.030457457428903108\nEpsilon = 0.030454411683160217\nEpsilon = 0.0304513662419919\nEpsilon = 0.0304483211053677\nEpsilon = 0.030445276273257164\nEpsilon = 0.03044223174562984\nEpsilon = 0.030439187522455276\nEpsilon = 0.03043614360370303\nEpsilon = 0.03043309998934266\nEpsilon = 0.030430056679343728\nEpsilon = 0.030427013673675795\nEpsilon = 0.030423970972308426\nEpsilon = 0.030420928575211195\nEpsilon = 0.030417886482353673\nEpsilon = 0.030414844693705437\nEpsilon = 0.030411803209236067\nEpsilon = 0.030408762028915143\nEpsilon = 0.030405721152712253\nEpsilon = 0.03040268058059698\nAgent: ddqn_agent . Episode 1871/2000. Number of steps to finish: 20. Loss: 23.451383590698242 Reward: -12.0\nEpsilon = 0.030399640312538924\nEpsilon = 0.03039660034850767\nEpsilon = 0.03039356068847282\nEpsilon = 0.030390521332403973\nEpsilon = 0.030387482280270734\nEpsilon = 0.03038444353204271\nEpsilon = 0.030381405087689505\nEpsilon = 0.030378366947180737\nEpsilon = 0.03037532911048602\nEpsilon = 0.03037229157757497\nEpsilon = 0.030369254348417214\nEpsilon = 0.030366217422982372\nEpsilon = 0.030363180801240074\nEpsilon = 0.03036014448315995\nEpsilon = 0.030357108468711636\nEpsilon = 0.030354072757864767\nEpsilon = 0.030351037350588982\nEpsilon = 0.030348002246853925\nEpsilon = 0.03034496744662924\nEpsilon = 0.03034193294988458\nAgent: ddqn_agent . Episode 1872/2000. Number of steps to finish: 20. Loss: 22.691492080688477 Reward: -14.0\nEpsilon = 0.030338898756589593\nEpsilon = 0.030335864866713934\nEpsilon = 0.030332831280227263\nEpsilon = 0.03032979799709924\nEpsilon = 0.03032676501729953\nEpsilon = 0.0303237323407978\nEpsilon = 0.03032069996756372\nEpsilon = 0.030317667897566965\nEpsilon = 0.030314636130777207\nEpsilon = 0.03031160466716413\nEpsilon = 0.03030857350669741\nEpsilon = 0.03030554264934674\nEpsilon = 0.030302512095081807\nEpsilon = 0.030299481843872298\nEpsilon = 0.03029645189568791\nEpsilon = 0.03029342225049834\nEpsilon = 0.030290392908273293\nEpsilon = 0.030287363868982467\nEpsilon = 0.03028433513259557\nEpsilon = 0.03028130669908231\nAgent: ddqn_agent . Episode 1873/2000. Number of steps to finish: 20. Loss: 26.554040908813477 Reward: -12.0\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.030278278568412403\nEpsilon = 0.03027525074055556\nEpsilon = 0.030272223215481505\nEpsilon = 0.030269195993159956\nEpsilon = 0.03026616907356064\nEpsilon = 0.030263142456653284\nEpsilon = 0.03026011614240762\nEpsilon = 0.03025709013079338\nEpsilon = 0.030254064421780302\nEpsilon = 0.030251039015338124\nEpsilon = 0.03024801391143659\nEpsilon = 0.03024498911004545\nEpsilon = 0.030241964611134445\nEpsilon = 0.030238940414673333\nEpsilon = 0.030235916520631866\nEpsilon = 0.030232892928979804\nEpsilon = 0.030229869639686907\nEpsilon = 0.03022684665272294\nEpsilon = 0.03022382396805767\nEpsilon = 0.030220801585660862\nAgent: ddqn_agent . Episode 1874/2000. Number of steps to finish: 20. Loss: 24.189306259155273 Reward: -12.0\nEpsilon = 0.030217779505502296\nEpsilon = 0.030214757727551746\nEpsilon = 0.030211736251778993\nEpsilon = 0.030208715078153814\nEpsilon = 0.030205694206645997\nEpsilon = 0.030202673637225334\nEpsilon = 0.030199653369861613\nEpsilon = 0.030196633404524626\nEpsilon = 0.030193613741184174\nEpsilon = 0.030190594379810056\nEpsilon = 0.030187575320372074\nEpsilon = 0.030184556562840036\nEpsilon = 0.030181538107183752\nEpsilon = 0.030178519953373034\nEpsilon = 0.030175502101377698\nEpsilon = 0.03017248455116756\nEpsilon = 0.030169467302712444\nEpsilon = 0.03016645035598217\nEpsilon = 0.030163433710946572\nEpsilon = 0.03016041736757548\nAgent: ddqn_agent . Episode 1875/2000. Number of steps to finish: 20. Loss: 22.645606994628906 Reward: -18.0\nEpsilon = 0.03015740132583872\nEpsilon = 0.03015438558570614\nEpsilon = 0.03015137014714757\nEpsilon = 0.030148355010132855\nEpsilon = 0.030145340174631843\nEpsilon = 0.030142325640614382\nEpsilon = 0.03013931140805032\nEpsilon = 0.030136297476909518\nEpsilon = 0.030133283847161827\nEpsilon = 0.030130270518777112\nEpsilon = 0.030127257491725234\nEpsilon = 0.030124244765976062\nEpsilon = 0.030121232341499465\nEpsilon = 0.030118220218265317\nEpsilon = 0.03011520839624349\nEpsilon = 0.030112196875403865\nEpsilon = 0.030109185655716325\nEpsilon = 0.030106174737150753\nEpsilon = 0.030103164119677037\nEpsilon = 0.03010015380326507\nAgent: ddqn_agent . Episode 1876/2000. Number of steps to finish: 20. Loss: 20.90756607055664 Reward: -14.0\nEpsilon = 0.030097143787884743\nEpsilon = 0.030094134073505956\nEpsilon = 0.030091124660098605\nEpsilon = 0.030088115547632594\nEpsilon = 0.03008510673607783\nEpsilon = 0.030082098225404225\nEpsilon = 0.030079090015581685\nEpsilon = 0.030076082106580127\nAgent: ddqn_agent . Episode 1877/2000. Number of steps to finish: 8. Loss: 8.309820175170898 Reward: 4.0\nEpsilon = 0.03007307449836947\nEpsilon = 0.030070067190919633\nEpsilon = 0.03006706018420054\nEpsilon = 0.03006405347818212\nEpsilon = 0.0300610470728343\nEpsilon = 0.030058040968127017\nEpsilon = 0.030055035164030203\nAgent: ddqn_agent . Episode 1878/2000. Number of steps to finish: 7. Loss: 11.105629920959473 Reward: 5.0\nEpsilon = 0.0300520296605138\nEpsilon = 0.03004902445754775\nEpsilon = 0.030046019555101996\nEpsilon = 0.030043014953146485\nEpsilon = 0.03004001065165117\nEpsilon = 0.030037006650586003\nEpsilon = 0.030034002949920945\nEpsilon = 0.03003099954962595\nEpsilon = 0.03002799644967099\nEpsilon = 0.030024993650026022\nEpsilon = 0.03002199115066102\nEpsilon = 0.030018988951545953\nEpsilon = 0.030015987052650798\nEpsilon = 0.030012985453945533\nEpsilon = 0.030009984155400137\nEpsilon = 0.030006983156984596\nEpsilon = 0.0300039824586689\nEpsilon = 0.030000982060423034\nEpsilon = 0.02999798196221699\nEpsilon = 0.02999498216402077\nAgent: ddqn_agent . Episode 1879/2000. Number of steps to finish: 20. Loss: 24.573781967163086 Reward: -12.0\nEpsilon = 0.029991982665804367\nEpsilon = 0.029988983467537787\nEpsilon = 0.029985984569191035\nEpsilon = 0.029982985970734115\nEpsilon = 0.02997998767213704\nEpsilon = 0.029976989673369828\nEpsilon = 0.02997399197440249\nEpsilon = 0.02997099457520505\nAgent: ddqn_agent . Episode 1880/2000. Number of steps to finish: 8. Loss: 8.127812385559082 Reward: 4.0\nEpsilon = 0.02996799747574753\nEpsilon = 0.029965000675999956\nEpsilon = 0.029962004175932356\nEpsilon = 0.029959007975514762\nEpsilon = 0.029956012074717212\nEpsilon = 0.02995301647350974\nEpsilon = 0.02995002117186239\nEpsilon = 0.029947026169745202\nEpsilon = 0.029944031467128228\nEpsilon = 0.029941037063981515\nEpsilon = 0.029938042960275118\nEpsilon = 0.02993504915597909\nEpsilon = 0.02993205565106349\nEpsilon = 0.029929062445498384\nEpsilon = 0.029926069539253836\nEpsilon = 0.02992307693229991\nEpsilon = 0.029920084624606682\nEpsilon = 0.029917092616144222\nEpsilon = 0.029914100906882606\nEpsilon = 0.02991110949679192\nAgent: ddqn_agent . Episode 1881/2000. Number of steps to finish: 20. Loss: 23.18990707397461 Reward: -14.0\nEpsilon = 0.02990811838584224\nEpsilon = 0.029905127574003657\nEpsilon = 0.029902137061246257\nEpsilon = 0.029899146847540133\nEpsilon = 0.02989615693285538\nEpsilon = 0.029893167317162093\nEpsilon = 0.029890178000430378\nEpsilon = 0.029887188982630337\nEpsilon = 0.029884200263732074\nEpsilon = 0.0298812118437057\nEpsilon = 0.02987822372252133\nEpsilon = 0.029875235900149077\nEpsilon = 0.029872248376559062\nEpsilon = 0.029869261151721407\nEpsilon = 0.029866274225606237\nEpsilon = 0.029863287598183675\nEpsilon = 0.029860301269423857\nEpsilon = 0.029857315239296914\nEpsilon = 0.029854329507772986\nEpsilon = 0.02985134407482221\nAgent: ddqn_agent . Episode 1882/2000. Number of steps to finish: 20. Loss: 28.509803771972656 Reward: -10.0\nEpsilon = 0.029848358940414727\nEpsilon = 0.029845374104520685\nEpsilon = 0.029842389567110234\nEpsilon = 0.029839405328153525\nEpsilon = 0.029836421387620708\nEpsilon = 0.029833437745481948\nEpsilon = 0.0298304544017074\nEpsilon = 0.02982747135626723\nEpsilon = 0.029824488609131602\nEpsilon = 0.02982150616027069\nEpsilon = 0.029818524009654665\nEpsilon = 0.0298155421572537\nEpsilon = 0.029812560603037976\nEpsilon = 0.02980957934697767\nEpsilon = 0.029806598389042975\nEpsilon = 0.02980361772920407\nEpsilon = 0.02980063736743115\nEpsilon = 0.029797657303694408\nEpsilon = 0.02979467753796404\nEpsilon = 0.029791698070210242\nAgent: ddqn_agent . Episode 1883/2000. Number of steps to finish: 20. Loss: 21.314044952392578 Reward: -14.0\nEpsilon = 0.029788718900403223\nEpsilon = 0.029785740028513183\nEpsilon = 0.029782761454510333\nEpsilon = 0.029779783178364883\nEpsilon = 0.029776805200047045\nEpsilon = 0.029773827519527042\nEpsilon = 0.02977085013677509\nEpsilon = 0.02976787305176141\nEpsilon = 0.029764896264456237\nEpsilon = 0.029761919774829792\nEpsilon = 0.029758943582852308\nEpsilon = 0.029755967688494023\nEpsilon = 0.029752992091725175\nEpsilon = 0.029750016792516\nEpsilon = 0.02974704179083675\nEpsilon = 0.029744067086657666\nEpsilon = 0.029741092679949\nEpsilon = 0.029738118570681007\nEpsilon = 0.02973514475882394\nEpsilon = 0.029732171244348057\nAgent: ddqn_agent . Episode 1884/2000. Number of steps to finish: 20. Loss: 24.57246971130371 Reward: -14.0\nEpsilon = 0.029729198027223624\nEpsilon = 0.029726225107420904\nEpsilon = 0.029723252484910163\nEpsilon = 0.02972028015966167\nEpsilon = 0.029717308131645706\nEpsilon = 0.02971433640083254\nEpsilon = 0.02971136496719246\nEpsilon = 0.02970839383069574\nEpsilon = 0.029705422991312673\nEpsilon = 0.029702452449013542\nEpsilon = 0.02969948220376864\nEpsilon = 0.029696512255548264\nEpsilon = 0.02969354260432271\nEpsilon = 0.029690573250062278\nEpsilon = 0.029687604192737273\nEpsilon = 0.029684635432318\nEpsilon = 0.02968166696877477\nEpsilon = 0.029678698802077894\nEpsilon = 0.029675730932197684\nEpsilon = 0.029672763359104466\nAgent: ddqn_agent . Episode 1885/2000. Number of steps to finish: 20. Loss: 23.84916114807129 Reward: -12.0\nEpsilon = 0.029669796082768558\nEpsilon = 0.02966682910316028\nEpsilon = 0.029663862420249966\nEpsilon = 0.02966089603400794\nEpsilon = 0.02965792994440454\nEpsilon = 0.0296549641514101\nEpsilon = 0.029651998654994956\nEpsilon = 0.029649033455129457\nEpsilon = 0.029646068551783943\nEpsilon = 0.029643103944928764\nEpsilon = 0.02964013963453427\nEpsilon = 0.02963717562057082\nEpsilon = 0.029634211903008762\nEpsilon = 0.029631248481818463\nEpsilon = 0.02962828535697028\nEpsilon = 0.029625322528434583\nEpsilon = 0.02962235999618174\nEpsilon = 0.02961939776018212\nEpsilon = 0.029616435820406104\nEpsilon = 0.029613474176824063\nAgent: ddqn_agent . Episode 1886/2000. Number of steps to finish: 20. Loss: 18.76535987854004 Reward: -16.0\nEpsilon = 0.029610512829406382\nEpsilon = 0.029607551778123443\nEpsilon = 0.02960459102294563\nEpsilon = 0.029601630563843337\nEpsilon = 0.029598670400786953\nEpsilon = 0.029595710533746874\nEpsilon = 0.0295927509626935\nEpsilon = 0.02958979168759723\nEpsilon = 0.02958683270842847\nEpsilon = 0.029583874025157626\nEpsilon = 0.02958091563775511\nEpsilon = 0.029577957546191334\nEpsilon = 0.029574999750436714\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.029572042250461672\nEpsilon = 0.029569085046236628\nEpsilon = 0.029566128137732003\nEpsilon = 0.02956317152491823\nEpsilon = 0.02956021520776574\nEpsilon = 0.029557259186244963\nEpsilon = 0.02955430346032634\nAgent: ddqn_agent . Episode 1887/2000. Number of steps to finish: 20. Loss: 25.352283477783203 Reward: -16.0\nEpsilon = 0.029551348029980308\nEpsilon = 0.02954839289517731\nEpsilon = 0.029545438055887795\nEpsilon = 0.029542483512082208\nEpsilon = 0.029539529263731\nEpsilon = 0.029536575310804628\nEpsilon = 0.029533621653273546\nEpsilon = 0.02953066829110822\nEpsilon = 0.02952771522427911\nEpsilon = 0.029524762452756682\nEpsilon = 0.029521809976511406\nEpsilon = 0.029518857795513755\nEpsilon = 0.029515905909734203\nEpsilon = 0.02951295431914323\nEpsilon = 0.029510003023711318\nEpsilon = 0.029507052023408947\nEpsilon = 0.029504101318206608\nEpsilon = 0.02950115090807479\nEpsilon = 0.029498200792983983\nEpsilon = 0.029495250972904685\nAgent: ddqn_agent . Episode 1888/2000. Number of steps to finish: 20. Loss: 23.963973999023438 Reward: -14.0\nEpsilon = 0.029492301447807394\nEpsilon = 0.029489352217662614\nEpsilon = 0.029486403282440846\nEpsilon = 0.029483454642112602\nEpsilon = 0.02948050629664839\nEpsilon = 0.029477558246018727\nEpsilon = 0.029474610490194127\nEpsilon = 0.02947166302914511\nEpsilon = 0.029468715862842193\nEpsilon = 0.02946576899125591\nEpsilon = 0.029462822414356785\nAgent: ddqn_agent . Episode 1889/2000. Number of steps to finish: 11. Loss: 12.253159523010254 Reward: 1.0\nEpsilon = 0.02945987613211535\nEpsilon = 0.02945693014450214\nEpsilon = 0.02945398445148769\nEpsilon = 0.02945103905304254\nEpsilon = 0.029448093949137236\nEpsilon = 0.029445149139742322\nEpsilon = 0.02944220462482835\nEpsilon = 0.029439260404365866\nEpsilon = 0.02943631647832543\nAgent: ddqn_agent . Episode 1890/2000. Number of steps to finish: 9. Loss: 10.903766632080078 Reward: 3.0\nEpsilon = 0.029433372846677595\nEpsilon = 0.029430429509392928\nEpsilon = 0.02942748646644199\nEpsilon = 0.029424543717795345\nEpsilon = 0.029421601263423566\nEpsilon = 0.029418659103297225\nEpsilon = 0.029415717237386896\nEpsilon = 0.02941277566566316\nEpsilon = 0.029409834388096592\nEpsilon = 0.029406893404657783\nEpsilon = 0.029403952715317316\nEpsilon = 0.029401012320045786\nEpsilon = 0.02939807221881378\nEpsilon = 0.0293951324115919\nEpsilon = 0.029392192898350742\nEpsilon = 0.02938925367906091\nEpsilon = 0.029386314753693004\nEpsilon = 0.029383376122217635\nEpsilon = 0.029380437784605414\nEpsilon = 0.029377499740826953\nAgent: ddqn_agent . Episode 1891/2000. Number of steps to finish: 20. Loss: 24.487205505371094 Reward: -10.0\nEpsilon = 0.029374561990852872\nEpsilon = 0.02937162453465379\nEpsilon = 0.029368687372200324\nEpsilon = 0.029365750503463104\nEpsilon = 0.029362813928412758\nEpsilon = 0.029359877647019916\nEpsilon = 0.029356941659255214\nEpsilon = 0.02935400596508929\nEpsilon = 0.02935107056449278\nEpsilon = 0.02934813545743633\nEpsilon = 0.029345200643890585\nEpsilon = 0.029342266123826195\nEpsilon = 0.029339331897213813\nEpsilon = 0.029336397964024093\nEpsilon = 0.02933346432422769\nEpsilon = 0.02933053097779527\nEpsilon = 0.02932759792469749\nEpsilon = 0.02932466516490502\nEpsilon = 0.02932173269838853\nEpsilon = 0.02931880052511869\nAgent: ddqn_agent . Episode 1892/2000. Number of steps to finish: 20. Loss: 25.202943801879883 Reward: -14.0\nEpsilon = 0.02931586864506618\nEpsilon = 0.029312937058201673\nEpsilon = 0.029310005764495852\nEpsilon = 0.029307074763919402\nEpsilon = 0.02930414405644301\nEpsilon = 0.029301213642037367\nEpsilon = 0.029298283520673162\nEpsilon = 0.029295353692321096\nEpsilon = 0.029292424156951866\nEpsilon = 0.029289494914536172\nEpsilon = 0.029286565965044718\nEpsilon = 0.029283637308448213\nEpsilon = 0.02928070894471737\nEpsilon = 0.0292777808738229\nEpsilon = 0.029274853095735517\nEpsilon = 0.029271925610425943\nEpsilon = 0.0292689984178649\nEpsilon = 0.029266071518023115\nEpsilon = 0.029263144910871314\nEpsilon = 0.029260218596380227\nAgent: ddqn_agent . Episode 1893/2000. Number of steps to finish: 20. Loss: 21.89309310913086 Reward: -14.0\nEpsilon = 0.029257292574520588\nEpsilon = 0.029254366845263138\nEpsilon = 0.02925144140857861\nEpsilon = 0.029248516264437755\nEpsilon = 0.02924559141281131\nEpsilon = 0.02924266685367003\nEpsilon = 0.029239742586984665\nEpsilon = 0.02923681861272597\nEpsilon = 0.029233894930864698\nEpsilon = 0.029230971541371612\nEpsilon = 0.029228048444217475\nEpsilon = 0.029225125639373052\nEpsilon = 0.029222203126809114\nEpsilon = 0.029219280906496434\nEpsilon = 0.029216358978405785\nEpsilon = 0.029213437342507944\nEpsilon = 0.029210515998773695\nEpsilon = 0.02920759494717382\nEpsilon = 0.0292046741876791\nEpsilon = 0.029201753720260334\nAgent: ddqn_agent . Episode 1894/2000. Number of steps to finish: 20. Loss: 25.48612403869629 Reward: -16.0\nEpsilon = 0.02919883354488831\nEpsilon = 0.02919591366153382\nEpsilon = 0.029192994070167667\nEpsilon = 0.02919007477076065\nEpsilon = 0.029187155763283575\nEpsilon = 0.029184237047707246\nEpsilon = 0.029181318624002476\nEpsilon = 0.029178400492140075\nEpsilon = 0.029175482652090863\nEpsilon = 0.029172565103825654\nEpsilon = 0.02916964784731527\nEpsilon = 0.02916673088253054\nEpsilon = 0.029163814209442286\nEpsilon = 0.02916089782802134\nEpsilon = 0.02915798173823854\nEpsilon = 0.029155065940064714\nEpsilon = 0.02915215043347071\nEpsilon = 0.029149235218427362\nEpsilon = 0.02914632029490552\nEpsilon = 0.02914340566287603\nAgent: ddqn_agent . Episode 1895/2000. Number of steps to finish: 20. Loss: 26.42448616027832 Reward: -18.0\nEpsilon = 0.029140491322309742\nEpsilon = 0.029137577273177513\nEpsilon = 0.029134663515450195\nEpsilon = 0.02913175004909865\nEpsilon = 0.02912883687409374\nEpsilon = 0.02912592399040633\nEpsilon = 0.02912301139800729\nEpsilon = 0.02912009909686749\nEpsilon = 0.029117187086957805\nEpsilon = 0.02911427536824911\nEpsilon = 0.029111363940712287\nEpsilon = 0.029108452804318216\nEpsilon = 0.029105541959037785\nEpsilon = 0.029102631404841883\nEpsilon = 0.0290997211417014\nEpsilon = 0.02909681116958723\nEpsilon = 0.029093901488470273\nEpsilon = 0.029090992098321427\nEpsilon = 0.029088082999111597\nEpsilon = 0.029085174190811684\nAgent: ddqn_agent . Episode 1896/2000. Number of steps to finish: 20. Loss: 23.36795997619629 Reward: -16.0\nEpsilon = 0.029082265673392605\nEpsilon = 0.029079357446825264\nEpsilon = 0.02907644951108058\nEpsilon = 0.029073541866129475\nEpsilon = 0.02907063451194286\nEpsilon = 0.029067727448491665\nEpsilon = 0.029064820675746818\nEpsilon = 0.029061914193679244\nEpsilon = 0.029059008002259877\nEpsilon = 0.02905610210145965\nEpsilon = 0.029053196491249503\nEpsilon = 0.02905029117160038\nEpsilon = 0.029047386142483218\nEpsilon = 0.02904448140386897\nEpsilon = 0.029041576955728583\nEpsilon = 0.02903867279803301\nEpsilon = 0.029035768930753207\nEpsilon = 0.029032865353860134\nEpsilon = 0.02902996206732475\nEpsilon = 0.02902705907111802\nAgent: ddqn_agent . Episode 1897/2000. Number of steps to finish: 20. Loss: 26.296907424926758 Reward: -20.0\nEpsilon = 0.029024156365210906\nEpsilon = 0.029021253949574385\nEpsilon = 0.029018351824179427\nEpsilon = 0.029015449988997008\nEpsilon = 0.029012548443998108\nEpsilon = 0.02900964718915371\nEpsilon = 0.029006746224434794\nEpsilon = 0.02900384554981235\nEpsilon = 0.029000945165257367\nEpsilon = 0.02899804507074084\nEpsilon = 0.028995145266233768\nEpsilon = 0.028992245751707144\nEpsilon = 0.028989346527131973\nEpsilon = 0.02898644759247926\nEpsilon = 0.02898354894772001\nEpsilon = 0.02898065059282524\nEpsilon = 0.02897775252776596\nEpsilon = 0.02897485475251318\nEpsilon = 0.02897195726703793\nEpsilon = 0.028969060071311226\nAgent: ddqn_agent . Episode 1898/2000. Number of steps to finish: 20. Loss: 22.17322540283203 Reward: -14.0\nEpsilon = 0.028966163165304096\nEpsilon = 0.028963266548987565\nEpsilon = 0.028960370222332665\nEpsilon = 0.028957474185310434\nEpsilon = 0.028954578437891905\nEpsilon = 0.028951682980048115\nEpsilon = 0.02894878781175011\nEpsilon = 0.028945892932968936\nEpsilon = 0.028942998343675638\nEpsilon = 0.02894010404384127\nEpsilon = 0.028937210033436883\nEpsilon = 0.02893431631243354\nEpsilon = 0.028931422880802297\nEpsilon = 0.028928529738514216\nEpsilon = 0.028925636885540364\nEpsilon = 0.02892274432185181\nEpsilon = 0.028919852047419623\nEpsilon = 0.02891696006221488\nEpsilon = 0.028914068366208658\nEpsilon = 0.028911176959372037\nAgent: ddqn_agent . Episode 1899/2000. Number of steps to finish: 20. Loss: 21.050193786621094 Reward: -14.0\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.0289082858416761\nEpsilon = 0.02890539501309193\nEpsilon = 0.02890250447359062\nEpsilon = 0.02889961422314326\nEpsilon = 0.028896724261720946\nEpsilon = 0.028893834589294773\nEpsilon = 0.028890945205835843\nEpsilon = 0.028888056111315258\nEpsilon = 0.028885167305704125\nEpsilon = 0.028882278788973554\nEpsilon = 0.028879390561094658\nEpsilon = 0.028876502622038547\nEpsilon = 0.028873614971776342\nEpsilon = 0.028870727610279166\nEpsilon = 0.02886784053751814\nEpsilon = 0.028864953753464387\nEpsilon = 0.02886206725808904\nEpsilon = 0.028859181051363234\nEpsilon = 0.0288562951332581\nEpsilon = 0.028853409503744775\nAgent: ddqn_agent . Episode 1900/2000. Number of steps to finish: 20. Loss: 29.341632843017578 Reward: -14.0\nEpsilon = 0.028850524162794402\nEpsilon = 0.02884763911037812\nEpsilon = 0.028844754346467084\nEpsilon = 0.028841869871032436\nEpsilon = 0.028838985684045332\nEpsilon = 0.028836101785476926\nEpsilon = 0.028833218175298377\nEpsilon = 0.028830334853480848\nEpsilon = 0.0288274518199955\nEpsilon = 0.0288245690748135\nEpsilon = 0.02882168661790602\nEpsilon = 0.028818804449244232\nEpsilon = 0.028815922568799307\nEpsilon = 0.02881304097654243\nEpsilon = 0.028810159672444775\nEpsilon = 0.02880727865647753\nEpsilon = 0.028804397928611884\nEpsilon = 0.028801517488819022\nEpsilon = 0.02879863733707014\nEpsilon = 0.028795757473336434\nAgent: ddqn_agent . Episode 1901/2000. Number of steps to finish: 20. Loss: 24.140779495239258 Reward: -16.0\nEpsilon = 0.0287928778975891\nEpsilon = 0.028789998609799344\nEpsilon = 0.028787119609938364\nEpsilon = 0.02878424089797737\nEpsilon = 0.028781362473887573\nEpsilon = 0.028778484337640183\nEpsilon = 0.02877560648920642\nEpsilon = 0.0287727289285575\nEpsilon = 0.028769851655664643\nEpsilon = 0.028766974670499077\nEpsilon = 0.02876409797303203\nEpsilon = 0.028761221563234726\nEpsilon = 0.028758345441078404\nEpsilon = 0.028755469606534297\nEpsilon = 0.028752594059573644\nEpsilon = 0.028749718800167686\nEpsilon = 0.02874684382828767\nEpsilon = 0.02874396914390484\nEpsilon = 0.02874109474699045\nEpsilon = 0.02873822063751575\nAgent: ddqn_agent . Episode 1902/2000. Number of steps to finish: 20. Loss: 26.288043975830078 Reward: -12.0\nEpsilon = 0.028735346815452\nEpsilon = 0.028732473280770453\nEpsilon = 0.02872960003344238\nEpsilon = 0.028726727073439033\nEpsilon = 0.02872385440073169\nEpsilon = 0.028720982015291618\nEpsilon = 0.02871810991709009\nEpsilon = 0.02871523810609838\nEpsilon = 0.02871236658228777\nEpsilon = 0.02870949534562954\nEpsilon = 0.028706624396094976\nEpsilon = 0.028703753733655368\nEpsilon = 0.028700883358282\nEpsilon = 0.028698013269946174\nEpsilon = 0.02869514346861918\nEpsilon = 0.028692273954272318\nEpsilon = 0.02868940472687689\nEpsilon = 0.0286865357864042\nEpsilon = 0.028683667132825563\nEpsilon = 0.02868079876611228\nAgent: ddqn_agent . Episode 1903/2000. Number of steps to finish: 20. Loss: 22.076173782348633 Reward: -20.0\nEpsilon = 0.02867793068623567\nEpsilon = 0.028675062893167045\nEpsilon = 0.02867219538687773\nEpsilon = 0.028669328167339043\nEpsilon = 0.02866646123452231\nEpsilon = 0.028663594588398858\nEpsilon = 0.02866072822894002\nEpsilon = 0.028657862156117125\nEpsilon = 0.028654996369901514\nEpsilon = 0.028652130870264525\nEpsilon = 0.0286492656571775\nEpsilon = 0.028646400730611782\nEpsilon = 0.02864353609053872\nEpsilon = 0.028640671736929666\nEpsilon = 0.028637807669755974\nEpsilon = 0.028634943888989\nEpsilon = 0.0286320803946001\nEpsilon = 0.02862921718656064\nEpsilon = 0.028626354264841985\nEpsilon = 0.0286234916294155\nAgent: ddqn_agent . Episode 1904/2000. Number of steps to finish: 20. Loss: 21.809955596923828 Reward: -18.0\nEpsilon = 0.02862062928025256\nEpsilon = 0.028617767217324536\nEpsilon = 0.028614905440602804\nEpsilon = 0.028612043950058744\nEpsilon = 0.02860918274566374\nEpsilon = 0.028606321827389175\nEpsilon = 0.028603461195206438\nAgent: ddqn_agent . Episode 1905/2000. Number of steps to finish: 7. Loss: 8.2237548828125 Reward: 5.0\nEpsilon = 0.02860060084908692\nEpsilon = 0.02859774078900201\nEpsilon = 0.02859488101492311\nEpsilon = 0.02859202152682162\nEpsilon = 0.02858916232466894\nEpsilon = 0.028586303408436473\nEpsilon = 0.02858344477809563\nEpsilon = 0.02858058643361782\nEpsilon = 0.028577728374974458\nEpsilon = 0.02857487060213696\nEpsilon = 0.028572013115076747\nEpsilon = 0.02856915591376524\nEpsilon = 0.028566298998173863\nEpsilon = 0.028563442368274045\nEpsilon = 0.02856058602403722\nEpsilon = 0.028557729965434816\nEpsilon = 0.02855487419243827\nEpsilon = 0.028552018705019028\nEpsilon = 0.028549163503148525\nEpsilon = 0.02854630858679821\nAgent: ddqn_agent . Episode 1906/2000. Number of steps to finish: 20. Loss: 22.66758918762207 Reward: -12.0\nEpsilon = 0.028543453955939532\nEpsilon = 0.028540599610543938\nEpsilon = 0.028537745550582885\nEpsilon = 0.028534891776027825\nEpsilon = 0.028532038286850223\nEpsilon = 0.02852918508302154\nEpsilon = 0.028526332164513238\nEpsilon = 0.028523479531296785\nEpsilon = 0.028520627183343656\nEpsilon = 0.02851777512062532\nEpsilon = 0.028514923343113258\nEpsilon = 0.028512071850778947\nEpsilon = 0.02850922064359387\nEpsilon = 0.028506369721529513\nEpsilon = 0.02850351908455736\nEpsilon = 0.028500668732648903\nEpsilon = 0.028497818665775637\nEpsilon = 0.02849496888390906\nEpsilon = 0.02849211938702067\nEpsilon = 0.028489270175081967\nAgent: ddqn_agent . Episode 1907/2000. Number of steps to finish: 20. Loss: 23.11580467224121 Reward: -18.0\nEpsilon = 0.02848642124806446\nEpsilon = 0.028483572605939655\nEpsilon = 0.028480724248679062\nEpsilon = 0.028477876176254194\nEpsilon = 0.02847502838863657\nEpsilon = 0.028472180885797706\nEpsilon = 0.028469333667709127\nEpsilon = 0.028466486734342357\nEpsilon = 0.02846364008566892\nEpsilon = 0.028460793721660355\nEpsilon = 0.02845794764228819\nEpsilon = 0.02845510184752396\nEpsilon = 0.028452256337339207\nEpsilon = 0.028449411111705472\nEpsilon = 0.028446566170594302\nEpsilon = 0.028443721513977244\nEpsilon = 0.028440877141825846\nEpsilon = 0.028438033054111665\nEpsilon = 0.028435189250806254\nEpsilon = 0.028432345731881175\nAgent: ddqn_agent . Episode 1908/2000. Number of steps to finish: 20. Loss: 26.12492561340332 Reward: -20.0\nEpsilon = 0.02842950249730799\nEpsilon = 0.02842665954705826\nEpsilon = 0.028423816881103556\nEpsilon = 0.028420974499415445\nEpsilon = 0.028418132401965505\nEpsilon = 0.028415290588725307\nEpsilon = 0.028412449059666434\nEpsilon = 0.028409607814760467\nEpsilon = 0.02840676685397899\nEpsilon = 0.028403926177293592\nEpsilon = 0.028401085784675862\nEpsilon = 0.028398245676097396\nEpsilon = 0.028395405851529786\nEpsilon = 0.028392566310944632\nEpsilon = 0.028389727054313536\nEpsilon = 0.028386888081608105\nEpsilon = 0.028384049392799943\nEpsilon = 0.028381210987860665\nEpsilon = 0.02837837286676188\nEpsilon = 0.028375535029475204\nAgent: ddqn_agent . Episode 1909/2000. Number of steps to finish: 20. Loss: 25.528568267822266 Reward: -14.0\nEpsilon = 0.028372697475972257\nEpsilon = 0.02836986020622466\nEpsilon = 0.028367023220204037\nEpsilon = 0.028364186517882018\nEpsilon = 0.02836135009923023\nEpsilon = 0.028358513964220306\nEpsilon = 0.028355678112823884\nEpsilon = 0.0283528425450126\nEpsilon = 0.0283500072607581\nEpsilon = 0.028347172260032024\nEpsilon = 0.02834433754280602\nEpsilon = 0.02834150310905174\nEpsilon = 0.028338668958740834\nEpsilon = 0.02833583509184496\nEpsilon = 0.028333001508335776\nEpsilon = 0.028330168208184944\nEpsilon = 0.028327335191364125\nEpsilon = 0.02832450245784499\nEpsilon = 0.028321670007599206\nEpsilon = 0.028318837840598448\nAgent: ddqn_agent . Episode 1910/2000. Number of steps to finish: 20. Loss: 26.792407989501953 Reward: -14.0\nEpsilon = 0.02831600595681439\nEpsilon = 0.028313174356218707\nEpsilon = 0.028310343038783086\nEpsilon = 0.02830751200447921\nEpsilon = 0.02830468125327876\nEpsilon = 0.028301850785153432\nEpsilon = 0.028299020600074918\nEpsilon = 0.028296190698014912\nEpsilon = 0.02829336107894511\nEpsilon = 0.028290531742837216\nEpsilon = 0.028287702689662932\nEpsilon = 0.028284873919393966\nEpsilon = 0.028282045432002027\nEpsilon = 0.02827921722745883\nEpsilon = 0.02827638930573608\nEpsilon = 0.02827356166680551\nEpsilon = 0.02827073431063883\nEpsilon = 0.028267907237207766\nEpsilon = 0.028265080446484046\nEpsilon = 0.028262253938439397\nAgent: ddqn_agent . Episode 1911/2000. Number of steps to finish: 20. Loss: 22.106327056884766 Reward: -12.0\nEpsilon = 0.028259427713045553\nEpsilon = 0.02825660177027425\nEpsilon = 0.028253776110097224\nEpsilon = 0.028250950732486214\nEpsilon = 0.028248125637412965\nEpsilon = 0.028245300824849225\nEpsilon = 0.02824247629476674\nEpsilon = 0.028239652047137263\nEpsilon = 0.02823682808193255\nEpsilon = 0.028234004399124355\nEpsilon = 0.028231180998684442\nEpsilon = 0.028228357880584573\nEpsilon = 0.028225535044796513\nEpsilon = 0.028222712491292035\nEpsilon = 0.028219890220042905\nEpsilon = 0.0282170682310209\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.0282142465241978\nEpsilon = 0.02821142509954538\nEpsilon = 0.028208603957035425\nEpsilon = 0.028205783096639722\nAgent: ddqn_agent . Episode 1912/2000. Number of steps to finish: 20. Loss: 25.427343368530273 Reward: -8.0\nEpsilon = 0.028202962518330058\nEpsilon = 0.028200142222078225\nEpsilon = 0.028197322207856017\nEpsilon = 0.028194502475635232\nEpsilon = 0.02819168302538767\nEpsilon = 0.02818886385708513\nEpsilon = 0.02818604497069942\nEpsilon = 0.028183226366202352\nEpsilon = 0.02818040804356573\nEpsilon = 0.028177590002761377\nEpsilon = 0.028174772243761102\nEpsilon = 0.028171954766536726\nEpsilon = 0.02816913757106007\nEpsilon = 0.028166320657302964\nEpsilon = 0.028163504025237235\nEpsilon = 0.028160687674834713\nEpsilon = 0.02815787160606723\nEpsilon = 0.02815505581890662\nEpsilon = 0.02815224031332473\nEpsilon = 0.028149425089293398\nAgent: ddqn_agent . Episode 1913/2000. Number of steps to finish: 20. Loss: 28.65726089477539 Reward: -14.0\nEpsilon = 0.02814661014678447\nEpsilon = 0.02814379548576979\nEpsilon = 0.028140981106221215\nEpsilon = 0.028138167008110594\nEpsilon = 0.028135353191409782\nEpsilon = 0.02813253965609064\nEpsilon = 0.028129726402125032\nEpsilon = 0.02812691342948482\nEpsilon = 0.02812410073814187\nEpsilon = 0.028121288328068057\nEpsilon = 0.02811847619923525\nEpsilon = 0.028115664351615326\nEpsilon = 0.028112852785180165\nEpsilon = 0.028110041499901647\nEpsilon = 0.028107230495751655\nEpsilon = 0.02810441977270208\nEpsilon = 0.028101609330724812\nEpsilon = 0.02809879916979174\nEpsilon = 0.02809598928987476\nAgent: ddqn_agent . Episode 1914/2000. Number of steps to finish: 19. Loss: 27.01066017150879 Reward: -7.0\nEpsilon = 0.028093179690945776\nEpsilon = 0.02809037037297668\nEpsilon = 0.028087561335939384\nEpsilon = 0.02808475257980579\nEpsilon = 0.028081944104547812\nEpsilon = 0.02807913591013736\nEpsilon = 0.028076327996546347\nEpsilon = 0.028073520363746694\nEpsilon = 0.02807071301171032\nEpsilon = 0.02806790594040915\nEpsilon = 0.02806509914981511\nEpsilon = 0.02806229263990013\nEpsilon = 0.028059486410636138\nEpsilon = 0.028056680461995075\nEpsilon = 0.028053874793948875\nEpsilon = 0.02805106940646948\nEpsilon = 0.028048264299528836\nEpsilon = 0.028045459473098885\nEpsilon = 0.028042654927151574\nEpsilon = 0.02803985066165886\nAgent: ddqn_agent . Episode 1915/2000. Number of steps to finish: 20. Loss: 24.504898071289062 Reward: -14.0\nEpsilon = 0.028037046676592695\nEpsilon = 0.028034242971925034\nEpsilon = 0.02803143954762784\nEpsilon = 0.028028636403673077\nEpsilon = 0.02802583354003271\nEpsilon = 0.028023030956678706\nEpsilon = 0.02802022865358304\nEpsilon = 0.02801742663071768\nEpsilon = 0.02801462488805461\nEpsilon = 0.028011823425565806\nEpsilon = 0.02800902224322325\nEpsilon = 0.028006221340998925\nEpsilon = 0.028003420718864826\nEpsilon = 0.02800062037679294\nEpsilon = 0.027997820314755263\nEpsilon = 0.027995020532723788\nEpsilon = 0.027992221030670516\nEpsilon = 0.02798942180856745\nEpsilon = 0.027986622866386594\nEpsilon = 0.027983824204099957\nAgent: ddqn_agent . Episode 1916/2000. Number of steps to finish: 20. Loss: 27.80006217956543 Reward: -12.0\nEpsilon = 0.027981025821679547\nEpsilon = 0.02797822771909738\nEpsilon = 0.027975429896325472\nEpsilon = 0.02797263235333584\nEpsilon = 0.02796983509010051\nEpsilon = 0.0279670381065915\nEpsilon = 0.02796424140278084\nEpsilon = 0.027961444978640563\nEpsilon = 0.0279586488341427\nEpsilon = 0.027955852969259284\nEpsilon = 0.02795305738396236\nEpsilon = 0.027950262078223962\nEpsilon = 0.02794746705201614\nEpsilon = 0.02794467230531094\nEpsilon = 0.027941877838080408\nEpsilon = 0.0279390836502966\nEpsilon = 0.02793628974193157\nEpsilon = 0.027933496112957378\nEpsilon = 0.027930702763346082\nEpsilon = 0.027927909693069748\nAgent: ddqn_agent . Episode 1917/2000. Number of steps to finish: 20. Loss: 21.463315963745117 Reward: -14.0\nEpsilon = 0.027925116902100443\nEpsilon = 0.027922324390410234\nEpsilon = 0.027919532157971192\nEpsilon = 0.027916740204755396\nEpsilon = 0.02791394853073492\nEpsilon = 0.027911157135881846\nEpsilon = 0.02790836602016826\nEpsilon = 0.027905575183566244\nEpsilon = 0.027902784626047887\nEpsilon = 0.02789999434758528\nAgent: ddqn_agent . Episode 1918/2000. Number of steps to finish: 10. Loss: 9.61478328704834 Reward: 2.0\nEpsilon = 0.027897204348150524\nEpsilon = 0.02789441462771571\nEpsilon = 0.027891625186252936\nEpsilon = 0.02788883602373431\nEpsilon = 0.027886047140131938\nEpsilon = 0.027883258535417926\nEpsilon = 0.027880470209564384\nEpsilon = 0.027877682162543427\nEpsilon = 0.027874894394327172\nEpsilon = 0.02787210690488774\nAgent: ddqn_agent . Episode 1919/2000. Number of steps to finish: 10. Loss: 12.621990203857422 Reward: 2.0\nEpsilon = 0.02786931969419725\nEpsilon = 0.02786653276222783\nEpsilon = 0.027863746108951606\nEpsilon = 0.02786095973434071\nEpsilon = 0.027858173638367276\nEpsilon = 0.027855387821003438\nEpsilon = 0.027852602282221337\nEpsilon = 0.027849817021993115\nEpsilon = 0.027847032040290917\nEpsilon = 0.02784424733708689\nEpsilon = 0.02784146291235318\nEpsilon = 0.027838678766061945\nEpsilon = 0.02783589489818534\nEpsilon = 0.02783311130869552\nEpsilon = 0.02783032799756465\nEpsilon = 0.027827544964764896\nEpsilon = 0.02782476221026842\nEpsilon = 0.027821979734047392\nEpsilon = 0.027819197536073988\nEpsilon = 0.02781641561632038\nAgent: ddqn_agent . Episode 1920/2000. Number of steps to finish: 20. Loss: 25.600482940673828 Reward: -14.0\nEpsilon = 0.02781363397475875\nEpsilon = 0.027810852611361275\nEpsilon = 0.02780807152610014\nEpsilon = 0.02780529071894753\nEpsilon = 0.027802510189875635\nEpsilon = 0.027799729938856647\nEpsilon = 0.027796949965862763\nEpsilon = 0.027794170270866177\nEpsilon = 0.02779139085383909\nEpsilon = 0.027788611714753705\nEpsilon = 0.02778583285358223\nEpsilon = 0.02778305427029687\nEpsilon = 0.02778027596486984\nEpsilon = 0.027777497937273354\nEpsilon = 0.02777472018747963\nEpsilon = 0.027771942715460882\nEpsilon = 0.027769165521189337\nEpsilon = 0.027766388604637218\nEpsilon = 0.027763611965776754\nEpsilon = 0.027760835604580175\nAgent: ddqn_agent . Episode 1921/2000. Number of steps to finish: 20. Loss: 24.636240005493164 Reward: -14.0\nEpsilon = 0.02775805952101972\nEpsilon = 0.02775528371506762\nEpsilon = 0.027752508186696112\nEpsilon = 0.027749732935877444\nEpsilon = 0.027746957962583856\nEpsilon = 0.027744183266787598\nEpsilon = 0.027741408848460918\nEpsilon = 0.027738634707576072\nEpsilon = 0.027735860844105314\nEpsilon = 0.027733087258020903\nEpsilon = 0.0277303139492951\nEpsilon = 0.02772754091790017\nEpsilon = 0.02772476816380838\nEpsilon = 0.027721995686992\nEpsilon = 0.0277192234874233\nEpsilon = 0.02771645156507456\nEpsilon = 0.027713679919918053\nEpsilon = 0.02771090855192606\nEpsilon = 0.027708137461070866\nEpsilon = 0.02770536664732476\nAgent: ddqn_agent . Episode 1922/2000. Number of steps to finish: 20. Loss: 23.596935272216797 Reward: -10.0\nEpsilon = 0.027702596110660026\nEpsilon = 0.02769982585104896\nEpsilon = 0.027697055868463858\nEpsilon = 0.027694286162877012\nEpsilon = 0.027691516734260724\nEpsilon = 0.027688747582587298\nEpsilon = 0.02768597870782904\nEpsilon = 0.027683210109958258\nEpsilon = 0.027680441788947262\nEpsilon = 0.02767767374476837\nAgent: ddqn_agent . Episode 1923/2000. Number of steps to finish: 10. Loss: 11.829684257507324 Reward: 2.0\nEpsilon = 0.02767490597739389\nEpsilon = 0.02767213848679615\nEpsilon = 0.027669371272947472\nEpsilon = 0.027666604335820178\nEpsilon = 0.027663837675386597\nEpsilon = 0.027661071291619058\nEpsilon = 0.027658305184489897\nEpsilon = 0.02765553935397145\nEpsilon = 0.027652773800036053\nAgent: ddqn_agent . Episode 1924/2000. Number of steps to finish: 9. Loss: 9.646960258483887 Reward: 3.0\nEpsilon = 0.02765000852265605\nEpsilon = 0.027647243521803786\nEpsilon = 0.027644478797451605\nEpsilon = 0.02764171434957186\nEpsilon = 0.027638950178136903\nEpsilon = 0.02763618628311909\nEpsilon = 0.02763342266449078\nEpsilon = 0.027630659322224332\nEpsilon = 0.02762789625629211\nEpsilon = 0.027625133466666483\nEpsilon = 0.027622370953319816\nEpsilon = 0.027619608716224483\nEpsilon = 0.02761684675535286\nEpsilon = 0.027614085070677325\nEpsilon = 0.027611323662170256\nEpsilon = 0.02760856252980404\nEpsilon = 0.02760580167355106\nEpsilon = 0.027603041093383705\nEpsilon = 0.027600280789274366\nEpsilon = 0.02759752076119544\nAgent: ddqn_agent . Episode 1925/2000. Number of steps to finish: 20. Loss: 29.04473876953125 Reward: -12.0\nEpsilon = 0.027594761009119322\nEpsilon = 0.027592001533018412\nEpsilon = 0.02758924233286511\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.027586483408631825\nEpsilon = 0.027583724760290963\nEpsilon = 0.027580966387814935\nEpsilon = 0.027578208291176153\nEpsilon = 0.027575450470347034\nEpsilon = 0.0275726929253\nEpsilon = 0.02756993565600747\nEpsilon = 0.02756717866244187\nEpsilon = 0.027564421944575625\nEpsilon = 0.027561665502381167\nEpsilon = 0.02755890933583093\nEpsilon = 0.027556153444897345\nEpsilon = 0.027553397829552854\nEpsilon = 0.0275506424897699\nEpsilon = 0.02754788742552092\nEpsilon = 0.02754513263677837\nEpsilon = 0.02754237812351469\nAgent: ddqn_agent . Episode 1926/2000. Number of steps to finish: 20. Loss: 25.094886779785156 Reward: -20.0\nEpsilon = 0.02753962388570234\nEpsilon = 0.02753686992331377\nEpsilon = 0.02753411623632144\nEpsilon = 0.027531362824697807\nEpsilon = 0.027528609688415338\nEpsilon = 0.027525856827446498\nEpsilon = 0.027523104241763754\nEpsilon = 0.027520351931339577\nEpsilon = 0.027517599896146442\nEpsilon = 0.027514848136156827\nEpsilon = 0.027512096651343212\nEpsilon = 0.027509345441678077\nEpsilon = 0.02750659450713391\nEpsilon = 0.027503843847683198\nEpsilon = 0.02750109346329843\nEpsilon = 0.027498343353952102\nEpsilon = 0.027495593519616706\nEpsilon = 0.027492843960264746\nEpsilon = 0.02749009467586872\nEpsilon = 0.027487345666401132\nAgent: ddqn_agent . Episode 1927/2000. Number of steps to finish: 20. Loss: 27.253223419189453 Reward: -18.0\nEpsilon = 0.02748459693183449\nEpsilon = 0.02748184847214131\nEpsilon = 0.027479100287294094\nEpsilon = 0.027476352377265364\nEpsilon = 0.027473604742027636\nEpsilon = 0.027470857381553433\nEpsilon = 0.02746811029581528\nEpsilon = 0.027465363484785697\nEpsilon = 0.02746261694843722\nEpsilon = 0.027459870686742376\nEpsilon = 0.0274571246996737\nEpsilon = 0.027454378987203733\nEpsilon = 0.027451633549305014\nEpsilon = 0.027448888385950086\nEpsilon = 0.02744614349711149\nEpsilon = 0.027443398882761778\nEpsilon = 0.0274406545428735\nEpsilon = 0.027437910477419215\nEpsilon = 0.027435166686371474\nEpsilon = 0.027432423169702836\nAgent: ddqn_agent . Episode 1928/2000. Number of steps to finish: 20. Loss: 21.61334991455078 Reward: -14.0\nEpsilon = 0.027429679927385864\nEpsilon = 0.027426936959393126\nEpsilon = 0.02742419426569719\nEpsilon = 0.02742145184627062\nEpsilon = 0.02741870970108599\nEpsilon = 0.02741596783011588\nEpsilon = 0.02741322623333287\nEpsilon = 0.027410484910709538\nEpsilon = 0.02740774386221847\nEpsilon = 0.027405003087832246\nEpsilon = 0.027402262587523464\nEpsilon = 0.02739952236126471\nEpsilon = 0.027396782409028587\nEpsilon = 0.027394042730787684\nEpsilon = 0.027391303326514606\nEpsilon = 0.027388564196181953\nEpsilon = 0.027385825339762335\nEpsilon = 0.02738308675722836\nEpsilon = 0.02738034844855264\nEpsilon = 0.027377610413707783\nAgent: ddqn_agent . Episode 1929/2000. Number of steps to finish: 20. Loss: 27.528804779052734 Reward: -10.0\nEpsilon = 0.027374872652666412\nEpsilon = 0.027372135165401146\nEpsilon = 0.027369397951884606\nEpsilon = 0.027366661012089417\nEpsilon = 0.027363924345988208\nEpsilon = 0.02736118795355361\nEpsilon = 0.027358451834758254\nEpsilon = 0.02735571598957478\nEpsilon = 0.027352980417975823\nEpsilon = 0.027350245119934027\nEpsilon = 0.027347510095422033\nEpsilon = 0.02734477534441249\nEpsilon = 0.02734204086687805\nEpsilon = 0.027339306662791364\nEpsilon = 0.027336572732125086\nEpsilon = 0.027333839074851873\nEpsilon = 0.02733110569094439\nEpsilon = 0.027328372580375297\nEpsilon = 0.02732563974311726\nEpsilon = 0.02732290717914295\nAgent: ddqn_agent . Episode 1930/2000. Number of steps to finish: 20. Loss: 25.321016311645508 Reward: -14.0\nEpsilon = 0.027320174888425038\nEpsilon = 0.027317442870936195\nEpsilon = 0.027314711126649103\nEpsilon = 0.027311979655536438\nEpsilon = 0.027309248457570885\nEpsilon = 0.027306517532725127\nEpsilon = 0.027303786880971855\nEpsilon = 0.027301056502283757\nEpsilon = 0.02729832639663353\nEpsilon = 0.027295596563993867\nEpsilon = 0.02729286700433747\nEpsilon = 0.027290137717637036\nEpsilon = 0.02728740870386527\nEpsilon = 0.027284679962994884\nEpsilon = 0.027281951494998586\nEpsilon = 0.027279223299849085\nEpsilon = 0.0272764953775191\nEpsilon = 0.02727376772798135\nEpsilon = 0.02727104035120855\nEpsilon = 0.02726831324717343\nAgent: ddqn_agent . Episode 1931/2000. Number of steps to finish: 20. Loss: 20.056344985961914 Reward: -14.0\nEpsilon = 0.027265586415848714\nEpsilon = 0.02726285985720713\nEpsilon = 0.02726013357122141\nEpsilon = 0.027257407557864287\nEpsilon = 0.027254681817108502\nEpsilon = 0.02725195634892679\nEpsilon = 0.0272492311532919\nEpsilon = 0.02724650623017657\nEpsilon = 0.027243781579553553\nEpsilon = 0.027241057201395597\nEpsilon = 0.027238333095675457\nEpsilon = 0.02723560926236589\nEpsilon = 0.027232885701439655\nEpsilon = 0.02723016241286951\nEpsilon = 0.027227439396628226\nEpsilon = 0.027224716652688562\nEpsilon = 0.027221994181023293\nEpsilon = 0.02721927198160519\nEpsilon = 0.02721655005440703\nEpsilon = 0.02721382839940159\nAgent: ddqn_agent . Episode 1932/2000. Number of steps to finish: 20. Loss: 27.727270126342773 Reward: -16.0\nEpsilon = 0.02721110701656165\nEpsilon = 0.027208385905859993\nEpsilon = 0.027205665067269406\nEpsilon = 0.027202944500762678\nEpsilon = 0.0272002242063126\nEpsilon = 0.02719750418389197\nEpsilon = 0.02719478443347358\nEpsilon = 0.027192064955030234\nEpsilon = 0.02718934574853473\nEpsilon = 0.027186626813959877\nEpsilon = 0.027183908151278482\nEpsilon = 0.027181189760463353\nEpsilon = 0.027178471641487307\nEpsilon = 0.02717575379432316\nEpsilon = 0.02717303621894373\nEpsilon = 0.027170318915321835\nEpsilon = 0.027167601883430304\nEpsilon = 0.02716488512324196\nEpsilon = 0.027162168634729637\nEpsilon = 0.027159452417866166\nAgent: ddqn_agent . Episode 1933/2000. Number of steps to finish: 20. Loss: 28.301822662353516 Reward: -12.0\nEpsilon = 0.027156736472624378\nEpsilon = 0.027154020798977115\nEpsilon = 0.027151305396897217\nEpsilon = 0.027148590266357527\nEpsilon = 0.02714587540733089\nEpsilon = 0.02714316081979016\nEpsilon = 0.02714044650370818\nEpsilon = 0.02713773245905781\nEpsilon = 0.027135018685811904\nEpsilon = 0.027132305183943325\nEpsilon = 0.02712959195342493\nEpsilon = 0.027126878994229588\nEpsilon = 0.027124166306330164\nEpsilon = 0.027121453889699533\nEpsilon = 0.027118741744310563\nEpsilon = 0.027116029870136133\nEpsilon = 0.02711331826714912\nEpsilon = 0.027110606935322405\nEpsilon = 0.027107895874628873\nEpsilon = 0.027105185085041412\nAgent: ddqn_agent . Episode 1934/2000. Number of steps to finish: 20. Loss: 25.325592041015625 Reward: -10.0\nEpsilon = 0.02710247456653291\nEpsilon = 0.027099764319076257\nEpsilon = 0.02709705434264435\nEpsilon = 0.027094344637210086\nEpsilon = 0.027091635202746364\nEpsilon = 0.02708892603922609\nEpsilon = 0.02708621714662217\nEpsilon = 0.027083508524907506\nEpsilon = 0.027080800174055016\nEpsilon = 0.02707809209403761\nAgent: ddqn_agent . Episode 1935/2000. Number of steps to finish: 10. Loss: 15.57352066040039 Reward: 2.0\nEpsilon = 0.027075384284828207\nEpsilon = 0.027072676746399724\nEpsilon = 0.027069969478725083\nEpsilon = 0.02706726248177721\nEpsilon = 0.027064555755529032\nEpsilon = 0.02706184929995348\nEpsilon = 0.027059143115023485\nEpsilon = 0.02705643720071198\nEpsilon = 0.02705373155699191\nEpsilon = 0.02705102618383621\nEpsilon = 0.027048321081217824\nEpsilon = 0.027045616249109703\nEpsilon = 0.027042911687484794\nEpsilon = 0.027040207396316046\nEpsilon = 0.027037503375576415\nEpsilon = 0.02703479962523886\nEpsilon = 0.027032096145276335\nEpsilon = 0.027029392935661808\nEpsilon = 0.02702668999636824\nEpsilon = 0.027023987327368604\nAgent: ddqn_agent . Episode 1936/2000. Number of steps to finish: 20. Loss: 24.443113327026367 Reward: -20.0\nEpsilon = 0.027021284928635866\nEpsilon = 0.027018582800143003\nEpsilon = 0.02701588094186299\nEpsilon = 0.027013179353768804\nEpsilon = 0.027010478035833427\nEpsilon = 0.027007776988029846\nEpsilon = 0.027005076210331044\nEpsilon = 0.02700237570271001\nEpsilon = 0.02699967546513974\nEpsilon = 0.026996975497593226\nEpsilon = 0.026994275800043467\nEpsilon = 0.026991576372463463\nEpsilon = 0.026988877214826217\nEpsilon = 0.026986178327104735\nEpsilon = 0.026983479709272024\nEpsilon = 0.0269807813613011\nEpsilon = 0.02697808328316497\nEpsilon = 0.026975385474836654\nEpsilon = 0.02697268793628917\nEpsilon = 0.02696999066749554\nAgent: ddqn_agent . Episode 1937/2000. Number of steps to finish: 20. Loss: 24.902334213256836 Reward: -18.0\nEpsilon = 0.02696729366842879\nEpsilon = 0.02696459693906195\nEpsilon = 0.026961900479368046\nEpsilon = 0.02695920428932011\nEpsilon = 0.02695650836889118\nEpsilon = 0.02695381271805429\nEpsilon = 0.026951117336782484\nEpsilon = 0.026948422225048806\nEpsilon = 0.0269457273828263\nEpsilon = 0.02694303281008802\nEpsilon = 0.02694033850680701\nEpsilon = 0.02693764447295633\nEpsilon = 0.026934950708509035\nEpsilon = 0.026932257213438185\nEpsilon = 0.026929563987716842\nEpsilon = 0.02692687103131807\nEpsilon = 0.02692417834421494\nEpsilon = 0.02692148592638052\nEpsilon = 0.026918793777787883\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.026916101898410105\nAgent: ddqn_agent . Episode 1938/2000. Number of steps to finish: 20. Loss: 24.503942489624023 Reward: -14.0\nEpsilon = 0.026913410288220262\nEpsilon = 0.02691071894719144\nEpsilon = 0.02690802787529672\nEpsilon = 0.02690533707250919\nEpsilon = 0.02690264653880194\nEpsilon = 0.02689995627414806\nEpsilon = 0.026897266278520647\nEpsilon = 0.026894576551892795\nAgent: ddqn_agent . Episode 1939/2000. Number of steps to finish: 8. Loss: 8.299812316894531 Reward: 4.0\nEpsilon = 0.026891887094237607\nEpsilon = 0.026889197905528184\nEpsilon = 0.02688650898573763\nEpsilon = 0.02688382033483906\nEpsilon = 0.026881131952805577\nEpsilon = 0.026878443839610296\nEpsilon = 0.026875755995226337\nEpsilon = 0.026873068419626816\nAgent: ddqn_agent . Episode 1940/2000. Number of steps to finish: 8. Loss: 8.299798965454102 Reward: 4.0\nEpsilon = 0.026870381112784853\nEpsilon = 0.026867694074673574\nEpsilon = 0.026865007305266107\nEpsilon = 0.026862320804535583\nEpsilon = 0.02685963457245513\nEpsilon = 0.026856948608997882\nEpsilon = 0.02685426291413698\nEpsilon = 0.026851577487845567\nEpsilon = 0.026848892330096782\nEpsilon = 0.026846207440863773\nEpsilon = 0.026843522820119686\nEpsilon = 0.026840838467837674\nEpsilon = 0.02683815438399089\nEpsilon = 0.026835470568552493\nEpsilon = 0.02683278702149564\nEpsilon = 0.02683010374279349\nEpsilon = 0.026827420732419213\nEpsilon = 0.026824737990345973\nEpsilon = 0.026822055516546938\nEpsilon = 0.026819373310995282\nAgent: ddqn_agent . Episode 1941/2000. Number of steps to finish: 20. Loss: 24.06372833251953 Reward: -12.0\nEpsilon = 0.026816691373664183\nEpsilon = 0.02681400970452682\nEpsilon = 0.026811328303556366\nEpsilon = 0.02680864717072601\nEpsilon = 0.026805966306008937\nEpsilon = 0.026803285709378337\nEpsilon = 0.026800605380807398\nEpsilon = 0.026797925320269318\nEpsilon = 0.02679524552773729\nEpsilon = 0.026792566003184515\nEpsilon = 0.026789886746584196\nEpsilon = 0.026787207757909538\nEpsilon = 0.026784529037133745\nEpsilon = 0.026781850584230032\nEpsilon = 0.02677917239917161\nEpsilon = 0.026776494481931694\nEpsilon = 0.0267738168324835\nEpsilon = 0.026771139450800254\nEpsilon = 0.026768462336855176\nEpsilon = 0.02676578549062149\nAgent: ddqn_agent . Episode 1942/2000. Number of steps to finish: 20. Loss: 22.11551856994629 Reward: -12.0\nEpsilon = 0.02676310891207243\nEpsilon = 0.02676043260118122\nEpsilon = 0.026757756557921102\nEpsilon = 0.026755080782265312\nEpsilon = 0.026752405274187086\nEpsilon = 0.026749730033659668\nEpsilon = 0.0267470550606563\nEpsilon = 0.026744380355150237\nEpsilon = 0.026741705917114722\nEpsilon = 0.02673903174652301\nEpsilon = 0.02673635784334836\nEpsilon = 0.026733684207564026\nEpsilon = 0.026731010839143268\nEpsilon = 0.026728337738059355\nAgent: ddqn_agent . Episode 1943/2000. Number of steps to finish: 14. Loss: 17.992717742919922 Reward: -2.0\nEpsilon = 0.02672566490428555\nEpsilon = 0.02672299233779512\nEpsilon = 0.02672032003856134\nEpsilon = 0.026717648006557484\nEpsilon = 0.026714976241756828\nEpsilon = 0.026712304744132654\nEpsilon = 0.02670963351365824\nAgent: ddqn_agent . Episode 1944/2000. Number of steps to finish: 7. Loss: 9.015642166137695 Reward: 5.0\nEpsilon = 0.026706962550306874\nEpsilon = 0.026704291854051845\nEpsilon = 0.02670162142486644\nEpsilon = 0.026698951262723952\nEpsilon = 0.02669628136759768\nEpsilon = 0.026693611739460922\nEpsilon = 0.026690942378286977\nEpsilon = 0.026688273284049147\nEpsilon = 0.026685604456720743\nEpsilon = 0.02668293589627507\nEpsilon = 0.026680267602685444\nEpsilon = 0.026677599575925175\nEpsilon = 0.026674931815967584\nEpsilon = 0.026672264322785987\nEpsilon = 0.02666959709635371\nEpsilon = 0.026666930136644073\nEpsilon = 0.026664263443630407\nEpsilon = 0.026661597017286043\nEpsilon = 0.026658930857584313\nEpsilon = 0.026656264964498556\nAgent: ddqn_agent . Episode 1945/2000. Number of steps to finish: 20. Loss: 22.111881256103516 Reward: -14.0\nEpsilon = 0.026653599338002106\nEpsilon = 0.026650933978068305\nEpsilon = 0.0266482688846705\nEpsilon = 0.026645604057782032\nEpsilon = 0.026642939497376256\nEpsilon = 0.026640275203426517\nEpsilon = 0.026637611175906174\nEpsilon = 0.026634947414788585\nEpsilon = 0.026632283920047108\nEpsilon = 0.026629620691655104\nEpsilon = 0.02662695772958594\nEpsilon = 0.026624295033812983\nEpsilon = 0.0266216326043096\nEpsilon = 0.02661897044104917\nEpsilon = 0.02661630854400507\nEpsilon = 0.026613646913150667\nEpsilon = 0.02661098554845935\nEpsilon = 0.026608324449904505\nEpsilon = 0.026605663617459516\nEpsilon = 0.02660300305109777\nAgent: ddqn_agent . Episode 1946/2000. Number of steps to finish: 20. Loss: 24.283212661743164 Reward: -14.0\nEpsilon = 0.02660034275079266\nEpsilon = 0.026597682716517582\nEpsilon = 0.026595022948245933\nEpsilon = 0.026592363445951107\nEpsilon = 0.02658970420960651\nEpsilon = 0.02658704523918555\nEpsilon = 0.026584386534661632\nEpsilon = 0.026581728096008166\nEpsilon = 0.026579069923198567\nEpsilon = 0.026576412016206245\nEpsilon = 0.026573754375004626\nEpsilon = 0.026571096999567126\nEpsilon = 0.02656843988986717\nEpsilon = 0.026565783045878182\nEpsilon = 0.026563126467573594\nEpsilon = 0.026560470154926838\nEpsilon = 0.026557814107911346\nEpsilon = 0.026555158326500555\nEpsilon = 0.026552502810667906\nEpsilon = 0.02654984756038684\nAgent: ddqn_agent . Episode 1947/2000. Number of steps to finish: 20. Loss: 22.80467987060547 Reward: -16.0\nEpsilon = 0.0265471925756308\nEpsilon = 0.026544537856373236\nEpsilon = 0.0265418834025876\nEpsilon = 0.026539229214247343\nEpsilon = 0.026536575291325918\nEpsilon = 0.026533921633796784\nEpsilon = 0.026531268241633404\nEpsilon = 0.02652861511480924\nEpsilon = 0.02652596225329776\nEpsilon = 0.026523309657072433\nEpsilon = 0.026520657326106725\nEpsilon = 0.026518005260374115\nEpsilon = 0.026515353459848078\nEpsilon = 0.026512701924502093\nEpsilon = 0.02651005065430964\nEpsilon = 0.02650739964924421\nEpsilon = 0.02650474890927929\nEpsilon = 0.02650209843438836\nEpsilon = 0.026499448224544923\nEpsilon = 0.02649679827972247\nAgent: ddqn_agent . Episode 1948/2000. Number of steps to finish: 20. Loss: 22.15416717529297 Reward: -10.0\nEpsilon = 0.026494148599894497\nEpsilon = 0.026491499185034507\nEpsilon = 0.026488850035116005\nEpsilon = 0.026486201150112493\nEpsilon = 0.02648355252999748\nEpsilon = 0.026480904174744482\nEpsilon = 0.02647825608432701\nAgent: ddqn_agent . Episode 1949/2000. Number of steps to finish: 7. Loss: 8.487671852111816 Reward: 5.0\nEpsilon = 0.026475608258718576\nEpsilon = 0.026472960697892706\nEpsilon = 0.026470313401822917\nEpsilon = 0.026467666370482736\nEpsilon = 0.026465019603845688\nEpsilon = 0.026462373101885303\nEpsilon = 0.026459726864575116\nEpsilon = 0.026457080891888658\nEpsilon = 0.02645443518379947\nEpsilon = 0.02645178974028109\nEpsilon = 0.02644914456130706\nEpsilon = 0.02644649964685093\nEpsilon = 0.026443854996886243\nEpsilon = 0.026441210611386556\nEpsilon = 0.026438566490325416\nEpsilon = 0.026435922633676383\nEpsilon = 0.026433279041413016\nEpsilon = 0.026430635713508874\nEpsilon = 0.026427992649937525\nEpsilon = 0.02642534985067253\nAgent: ddqn_agent . Episode 1950/2000. Number of steps to finish: 20. Loss: 24.818967819213867 Reward: -10.0\nEpsilon = 0.026422707315687462\nEpsilon = 0.026420065044955893\nEpsilon = 0.026417423038451397\nEpsilon = 0.026414781296147553\nEpsilon = 0.02641213981801794\nEpsilon = 0.026409498604036137\nEpsilon = 0.026406857654175733\nEpsilon = 0.026404216968410315\nEpsilon = 0.026401576546713473\nEpsilon = 0.0263989363890588\nEpsilon = 0.026396296495419896\nEpsilon = 0.026393656865770353\nEpsilon = 0.026391017500083776\nEpsilon = 0.02638837839833377\nEpsilon = 0.026385739560493938\nEpsilon = 0.02638310098653789\nEpsilon = 0.026380462676439235\nEpsilon = 0.02637782463017159\nEpsilon = 0.026375186847708572\nEpsilon = 0.0263725493290238\nAgent: ddqn_agent . Episode 1951/2000. Number of steps to finish: 20. Loss: 23.565427780151367 Reward: -12.0\nEpsilon = 0.0263699120740909\nEpsilon = 0.02636727508288349\nEpsilon = 0.026364638355375204\nEpsilon = 0.026362001891539668\nEpsilon = 0.026359365691350514\nEpsilon = 0.02635672975478138\nEpsilon = 0.0263540940818059\nEpsilon = 0.02635145867239772\nEpsilon = 0.02634882352653048\nEpsilon = 0.026346188644177825\nEpsilon = 0.02634355402531341\nEpsilon = 0.026340919669910878\nEpsilon = 0.026338285577943887\nEpsilon = 0.026335651749386093\nEpsilon = 0.026333018184211153\nEpsilon = 0.026330384882392734\nEpsilon = 0.026327751843904495\nEpsilon = 0.026325119068720104\nEpsilon = 0.026322486556813234\nAgent: ddqn_agent . Episode 1952/2000. Number of steps to finish: 19. Loss: 22.707387924194336 Reward: -7.0\nEpsilon = 0.02631985430815755\nEpsilon = 0.026317222322726735\nEpsilon = 0.026314590600494462\nEpsilon = 0.026311959141434412\nEpsilon = 0.02630932794552027\nEpsilon = 0.026306697012725717\nEpsilon = 0.026304066343024443\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.02630143593639014\nEpsilon = 0.0262988057927965\nEpsilon = 0.02629617591221722\nEpsilon = 0.026293546294625998\nEpsilon = 0.026290916939996536\nEpsilon = 0.026288287848302536\nEpsilon = 0.026285659019517706\nEpsilon = 0.026283030453615755\nEpsilon = 0.026280402150570394\nEpsilon = 0.026277774110355336\nEpsilon = 0.0262751463329443\nEpsilon = 0.026272518818311007\nEpsilon = 0.026269891566429178\nAgent: ddqn_agent . Episode 1953/2000. Number of steps to finish: 20. Loss: 29.01590919494629 Reward: -12.0\nEpsilon = 0.026267264577272534\nEpsilon = 0.026264637850814808\nEpsilon = 0.026262011387029726\nEpsilon = 0.026259385185891024\nEpsilon = 0.026256759247372436\nEpsilon = 0.0262541335714477\nAgent: ddqn_agent . Episode 1954/2000. Number of steps to finish: 6. Loss: 7.831246376037598 Reward: 6.0\nEpsilon = 0.026251508158090558\nEpsilon = 0.026248883007274747\nEpsilon = 0.02624625811897402\nEpsilon = 0.02624363349316212\nEpsilon = 0.026241009129812806\nEpsilon = 0.026238385028899823\nEpsilon = 0.026235761190396934\nEpsilon = 0.026233137614277896\nEpsilon = 0.02623051430051647\nEpsilon = 0.026227891249086418\nEpsilon = 0.02622526845996151\nEpsilon = 0.026222645933115515\nEpsilon = 0.026220023668522205\nEpsilon = 0.026217401666155354\nEpsilon = 0.026214779925988738\nEpsilon = 0.02621215844799614\nEpsilon = 0.02620953723215134\nEpsilon = 0.026206916278428128\nEpsilon = 0.026204295586800284\nEpsilon = 0.026201675157241606\nAgent: ddqn_agent . Episode 1955/2000. Number of steps to finish: 20. Loss: 21.46999740600586 Reward: -12.0\nEpsilon = 0.02619905498972588\nEpsilon = 0.026196435084226907\nEpsilon = 0.026193815440718485\nEpsilon = 0.026191196059174412\nEpsilon = 0.026188576939568494\nEpsilon = 0.026185958081874537\nEpsilon = 0.02618333948606635\nEpsilon = 0.026180721152117743\nEpsilon = 0.02617810308000253\nEpsilon = 0.02617548526969453\nEpsilon = 0.02617286772116756\nEpsilon = 0.026170250434395442\nEpsilon = 0.026167633409352003\nEpsilon = 0.026165016646011067\nEpsilon = 0.026162400144346468\nEpsilon = 0.026159783904332032\nEpsilon = 0.0261571679259416\nEpsilon = 0.026154552209149005\nEpsilon = 0.02615193675392809\nEpsilon = 0.026149321560252698\nAgent: ddqn_agent . Episode 1956/2000. Number of steps to finish: 20. Loss: 24.243165969848633 Reward: -14.0\nEpsilon = 0.026146706628096674\nEpsilon = 0.026144091957433866\nEpsilon = 0.02614147754823812\nEpsilon = 0.026138863400483298\nEpsilon = 0.02613624951414325\nEpsilon = 0.026133635889191834\nEpsilon = 0.026131022525602916\nEpsilon = 0.026128409423350357\nEpsilon = 0.02612579658240802\nEpsilon = 0.02612318400274978\nEpsilon = 0.026120571684349506\nEpsilon = 0.02611795962718107\nEpsilon = 0.02611534783121835\nEpsilon = 0.02611273629643523\nEpsilon = 0.02611012502280559\nEpsilon = 0.02610751401030331\nEpsilon = 0.02610490325890228\nEpsilon = 0.02610229276857639\nEpsilon = 0.02609968253929953\nEpsilon = 0.026097072571045602\nAgent: ddqn_agent . Episode 1957/2000. Number of steps to finish: 20. Loss: 26.698970794677734 Reward: -14.0\nEpsilon = 0.026094462863788498\nEpsilon = 0.026091853417502118\nEpsilon = 0.02608924423216037\nEpsilon = 0.026086635307737152\nEpsilon = 0.02608402664420638\nEpsilon = 0.02608141824154196\nEpsilon = 0.026078810099717804\nEpsilon = 0.026076202218707833\nEpsilon = 0.02607359459848596\nEpsilon = 0.02607098723902611\nEpsilon = 0.02606838014030221\nEpsilon = 0.02606577330228818\nEpsilon = 0.026063166724957954\nEpsilon = 0.026060560408285458\nEpsilon = 0.02605795435224463\nEpsilon = 0.026055348556809407\nEpsilon = 0.026052743021953727\nEpsilon = 0.026050137747651532\nEpsilon = 0.026047532733876767\nEpsilon = 0.02604492798060338\nAgent: ddqn_agent . Episode 1958/2000. Number of steps to finish: 20. Loss: 27.097204208374023 Reward: -16.0\nEpsilon = 0.02604232348780532\nEpsilon = 0.02603971925545654\nEpsilon = 0.026037115283530997\nEpsilon = 0.026034511572002643\nEpsilon = 0.026031908120845444\nEpsilon = 0.02602930493003336\nEpsilon = 0.026026701999540356\nEpsilon = 0.0260240993293404\nEpsilon = 0.02602149691940747\nEpsilon = 0.026018894769715528\nEpsilon = 0.026016292880238556\nEpsilon = 0.026013691250950533\nEpsilon = 0.026011089881825437\nEpsilon = 0.026008488772837256\nEpsilon = 0.026005887923959973\nEpsilon = 0.026003287335167578\nEpsilon = 0.02600068700643406\nEpsilon = 0.025998086937733415\nEpsilon = 0.02599548712903964\nEpsilon = 0.025992887580326737\nAgent: ddqn_agent . Episode 1959/2000. Number of steps to finish: 20. Loss: 26.202756881713867 Reward: -18.0\nEpsilon = 0.025990288291568703\nEpsilon = 0.025987689262739547\nEpsilon = 0.025985090493813274\nEpsilon = 0.025982491984763892\nEpsilon = 0.025979893735565415\nEpsilon = 0.02597729574619186\nEpsilon = 0.02597469801661724\nEpsilon = 0.025972100546815577\nEpsilon = 0.025969503336760895\nEpsilon = 0.025966906386427218\nAgent: ddqn_agent . Episode 1960/2000. Number of steps to finish: 10. Loss: 14.388833045959473 Reward: 2.0\nEpsilon = 0.025964309695788574\nEpsilon = 0.025961713264818994\nEpsilon = 0.025959117093492513\nEpsilon = 0.025956521181783165\nEpsilon = 0.025953925529664988\nEpsilon = 0.02595133013711202\nEpsilon = 0.02594873500409831\nEpsilon = 0.0259461401305979\nEpsilon = 0.02594354551658484\nEpsilon = 0.025940951162033182\nEpsilon = 0.025938357066916978\nEpsilon = 0.025935763231210288\nEpsilon = 0.025933169654887168\nEpsilon = 0.02593057633792168\nEpsilon = 0.02592798328028789\nEpsilon = 0.02592539048195986\nEpsilon = 0.025922797942911666\nEpsilon = 0.025920205663117375\nEpsilon = 0.025917613642551064\nEpsilon = 0.025915021881186807\nAgent: ddqn_agent . Episode 1961/2000. Number of steps to finish: 20. Loss: 21.591201782226562 Reward: -14.0\nEpsilon = 0.02591243037899869\nEpsilon = 0.02590983913596079\nEpsilon = 0.025907248152047195\nEpsilon = 0.02590465742723199\nEpsilon = 0.025902066961489268\nEpsilon = 0.025899476754793118\nEpsilon = 0.025896886807117638\nEpsilon = 0.025894297118436927\nEpsilon = 0.025891707688725084\nEpsilon = 0.025889118517956212\nEpsilon = 0.025886529606104415\nEpsilon = 0.025883940953143805\nEpsilon = 0.02588135255904849\nEpsilon = 0.025878764423792586\nEpsilon = 0.025876176547350208\nEpsilon = 0.025873588929695473\nEpsilon = 0.025871001570802504\nEpsilon = 0.025868414470645425\nEpsilon = 0.02586582762919836\nEpsilon = 0.02586324104643544\nAgent: ddqn_agent . Episode 1962/2000. Number of steps to finish: 20. Loss: 22.630624771118164 Reward: -10.0\nEpsilon = 0.025860654722330796\nEpsilon = 0.025858068656858562\nEpsilon = 0.025855482849992877\nEpsilon = 0.025852897301707877\nEpsilon = 0.025850312011977707\nEpsilon = 0.02584772698077651\nEpsilon = 0.02584514220807843\nEpsilon = 0.025842557693857623\nEpsilon = 0.025839973438088238\nEpsilon = 0.02583738944074443\nEpsilon = 0.025834805701800356\nEpsilon = 0.025832222221230175\nEpsilon = 0.025829638999008052\nEpsilon = 0.02582705603510815\nEpsilon = 0.02582447332950464\nEpsilon = 0.02582189088217169\nEpsilon = 0.02581930869308347\nEpsilon = 0.025816726762214165\nEpsilon = 0.025814145089537943\nEpsilon = 0.02581156367502899\nAgent: ddqn_agent . Episode 1963/2000. Number of steps to finish: 20. Loss: 26.48285484313965 Reward: -12.0\nEpsilon = 0.025808982518661486\nEpsilon = 0.02580640162040962\nEpsilon = 0.02580382098024758\nEpsilon = 0.025801240598149554\nEpsilon = 0.02579866047408974\nEpsilon = 0.02579608060804233\nEpsilon = 0.025793500999981525\nEpsilon = 0.02579092164988153\nEpsilon = 0.025788342557716542\nEpsilon = 0.02578576372346077\nEpsilon = 0.025783185147088425\nEpsilon = 0.025780606828573718\nEpsilon = 0.02577802876789086\nEpsilon = 0.02577545096501407\nEpsilon = 0.025772873419917568\nEpsilon = 0.025770296132575577\nEpsilon = 0.02576771910296232\nEpsilon = 0.025765142331052024\nEpsilon = 0.02576256581681892\nEpsilon = 0.02575998956023724\nAgent: ddqn_agent . Episode 1964/2000. Number of steps to finish: 20. Loss: 26.086454391479492 Reward: -14.0\nEpsilon = 0.025757413561281216\nEpsilon = 0.02575483781992509\nEpsilon = 0.025752262336143097\nEpsilon = 0.025749687109909485\nEpsilon = 0.025747112141198493\nEpsilon = 0.025744537429984372\nEpsilon = 0.025741962976241373\nEpsilon = 0.02573938877994375\nEpsilon = 0.025736814841065755\nEpsilon = 0.025734241159581648\nEpsilon = 0.02573166773546569\nEpsilon = 0.025729094568692146\nEpsilon = 0.025726521659235276\nEpsilon = 0.02572394900706935\nEpsilon = 0.025721376612168646\nEpsilon = 0.02571880447450743\nEpsilon = 0.02571623259405998\nEpsilon = 0.025713660970800575\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.025711089604703495\nEpsilon = 0.025708518495743026\nAgent: ddqn_agent . Episode 1965/2000. Number of steps to finish: 20. Loss: 26.711641311645508 Reward: -14.0\nEpsilon = 0.025705947643893452\nEpsilon = 0.025703377049129065\nEpsilon = 0.025700806711424152\nEpsilon = 0.025698236630753008\nEpsilon = 0.025695666807089932\nEpsilon = 0.025693097240409222\nEpsilon = 0.025690527930685183\nEpsilon = 0.025687958877892113\nEpsilon = 0.025685390082004326\nEpsilon = 0.025682821542996125\nEpsilon = 0.025680253260841827\nEpsilon = 0.025677685235515744\nEpsilon = 0.025675117466992192\nEpsilon = 0.025672549955245493\nEpsilon = 0.02566998270024997\nEpsilon = 0.025667415701979945\nEpsilon = 0.02566484896040975\nEpsilon = 0.02566228247551371\nEpsilon = 0.025659716247266158\nEpsilon = 0.025657150275641433\nAgent: ddqn_agent . Episode 1966/2000. Number of steps to finish: 20. Loss: 25.230998992919922 Reward: -18.0\nEpsilon = 0.02565458456061387\nEpsilon = 0.025652019102157808\nEpsilon = 0.02564945390024759\nEpsilon = 0.025646888954857568\nEpsilon = 0.025644324265962082\nEpsilon = 0.025641759833535487\nEpsilon = 0.025639195657552134\nAgent: ddqn_agent . Episode 1967/2000. Number of steps to finish: 7. Loss: 9.97674560546875 Reward: 5.0\nEpsilon = 0.02563663173798638\nEpsilon = 0.025634068074812583\nEpsilon = 0.025631504668005102\nEpsilon = 0.025628941517538303\nEpsilon = 0.02562637862338655\nEpsilon = 0.025623815985524212\nEpsilon = 0.02562125360392566\nEpsilon = 0.025618691478565266\nEpsilon = 0.02561612960941741\nEpsilon = 0.02561356799645647\nEpsilon = 0.025611006639656823\nEpsilon = 0.025608445538992858\nEpsilon = 0.02560588469443896\nEpsilon = 0.025603324105969518\nEpsilon = 0.02560076377355892\nEpsilon = 0.025598203697181566\nEpsilon = 0.025595643876811847\nEpsilon = 0.025593084312424165\nEpsilon = 0.025590525003992923\nEpsilon = 0.025587965951492524\nAgent: ddqn_agent . Episode 1968/2000. Number of steps to finish: 20. Loss: 26.73722267150879 Reward: -14.0\nEpsilon = 0.025585407154897374\nEpsilon = 0.025582848614181884\nEpsilon = 0.025580290329320465\nEpsilon = 0.025577732300287533\nEpsilon = 0.025575174527057504\nEpsilon = 0.0255726170096048\nEpsilon = 0.02557005974790384\nEpsilon = 0.02556750274192905\nEpsilon = 0.025564945991654854\nAgent: ddqn_agent . Episode 1969/2000. Number of steps to finish: 9. Loss: 10.071216583251953 Reward: 3.0\nEpsilon = 0.02556238949705569\nEpsilon = 0.025559833258105985\nEpsilon = 0.025557277274780174\nEpsilon = 0.025554721547052697\nEpsilon = 0.025552166074897992\nEpsilon = 0.0255496108582905\nEpsilon = 0.025547055897204674\nEpsilon = 0.025544501191614954\nEpsilon = 0.02554194674149579\nEpsilon = 0.02553939254682164\nEpsilon = 0.025536838607566958\nEpsilon = 0.025534284923706203\nEpsilon = 0.025531731495213834\nEpsilon = 0.025529178322064312\nEpsilon = 0.025526625404232107\nEpsilon = 0.025524072741691686\nEpsilon = 0.025521520334417517\nEpsilon = 0.025518968182384076\nEpsilon = 0.02551641628556584\nEpsilon = 0.025513864643937283\nAgent: ddqn_agent . Episode 1970/2000. Number of steps to finish: 20. Loss: 27.08287239074707 Reward: -14.0\nEpsilon = 0.025511313257472888\nEpsilon = 0.025508762126147142\nEpsilon = 0.02550621124993453\nEpsilon = 0.025503660628809537\nEpsilon = 0.025501110262746655\nEpsilon = 0.02549856015172038\nEpsilon = 0.025496010295705206\nEpsilon = 0.025493460694675635\nEpsilon = 0.02549091134860617\nEpsilon = 0.02548836225747131\nEpsilon = 0.025485813421245562\nEpsilon = 0.025483264839903437\nAgent: ddqn_agent . Episode 1971/2000. Number of steps to finish: 12. Loss: 14.278509140014648 Reward: 0.0\nEpsilon = 0.025480716513419446\nEpsilon = 0.025478168441768104\nEpsilon = 0.025475620624923926\nEpsilon = 0.025473073062861434\nEpsilon = 0.025470525755555147\nEpsilon = 0.025467978702979593\nEpsilon = 0.025465431905109297\nEpsilon = 0.025462885361918788\nEpsilon = 0.025460339073382597\nEpsilon = 0.02545779303947526\nEpsilon = 0.025455247260171312\nEpsilon = 0.025452701735445296\nEpsilon = 0.025450156465271753\nEpsilon = 0.025447611449625228\nEpsilon = 0.025445066688480265\nEpsilon = 0.025442522181811415\nEpsilon = 0.025439977929593235\nEpsilon = 0.025437433931800275\nEpsilon = 0.025434890188407094\nEpsilon = 0.025432346699388254\nAgent: ddqn_agent . Episode 1972/2000. Number of steps to finish: 20. Loss: 25.285329818725586 Reward: -18.0\nEpsilon = 0.025429803464718317\nEpsilon = 0.025427260484371845\nEpsilon = 0.025424717758323407\nEpsilon = 0.025422175286547576\nEpsilon = 0.02541963306901892\nEpsilon = 0.02541709110571202\nEpsilon = 0.025414549396601448\nEpsilon = 0.025412007941661787\nEpsilon = 0.025409466740867623\nEpsilon = 0.025406925794193538\nEpsilon = 0.025404385101614118\nEpsilon = 0.025401844663103957\nEpsilon = 0.025399304478637648\nEpsilon = 0.025396764548189784\nEpsilon = 0.025394224871734965\nEpsilon = 0.025391685449247792\nEpsilon = 0.02538914628070287\nEpsilon = 0.025386607366074797\nEpsilon = 0.02538406870533819\nEpsilon = 0.025381530298467656\nAgent: ddqn_agent . Episode 1973/2000. Number of steps to finish: 20. Loss: 31.278182983398438 Reward: -14.0\nEpsilon = 0.02537899214543781\nEpsilon = 0.025376454246223267\nEpsilon = 0.025373916600798647\nEpsilon = 0.025371379209138567\nEpsilon = 0.025368842071217652\nEpsilon = 0.02536630518701053\nEpsilon = 0.02536376855649183\nEpsilon = 0.025361232179636182\nAgent: ddqn_agent . Episode 1974/2000. Number of steps to finish: 8. Loss: 9.596074104309082 Reward: 4.0\nEpsilon = 0.025358696056418217\nEpsilon = 0.025356160186812575\nEpsilon = 0.025353624570793893\nEpsilon = 0.025351089208336813\nEpsilon = 0.02534855409941598\nEpsilon = 0.02534601924400604\nEpsilon = 0.025343484642081637\nEpsilon = 0.02534095029361743\nAgent: ddqn_agent . Episode 1975/2000. Number of steps to finish: 8. Loss: 8.155818939208984 Reward: 4.0\nEpsilon = 0.02533841619858807\nEpsilon = 0.02533588235696821\nEpsilon = 0.025333348768732514\nEpsilon = 0.025330815433855642\nEpsilon = 0.025328282352312256\nEpsilon = 0.025325749524077026\nEpsilon = 0.025323216949124618\nEpsilon = 0.025320684627429705\nEpsilon = 0.025318152558966963\nEpsilon = 0.025315620743711065\nEpsilon = 0.025313089181636696\nEpsilon = 0.025310557872718534\nEpsilon = 0.025308026816931263\nEpsilon = 0.02530549601424957\nEpsilon = 0.025302965464648147\nEpsilon = 0.02530043516810168\nEpsilon = 0.025297905124584873\nEpsilon = 0.025295375334072414\nEpsilon = 0.025292845796539006\nEpsilon = 0.025290316511959354\nAgent: ddqn_agent . Episode 1976/2000. Number of steps to finish: 20. Loss: 26.801685333251953 Reward: -14.0\nEpsilon = 0.025287787480308157\nEpsilon = 0.025285258701560127\nEpsilon = 0.025282730175689973\nEpsilon = 0.025280201902672404\nEpsilon = 0.025277673882482136\nEpsilon = 0.025275146115093888\nEpsilon = 0.02527261860048238\nEpsilon = 0.025270091338622333\nEpsilon = 0.02526756432948847\nEpsilon = 0.02526503757305552\nEpsilon = 0.025262511069298214\nEpsilon = 0.025259984818191285\nEpsilon = 0.025257458819709467\nEpsilon = 0.025254933073827496\nEpsilon = 0.025252407580520116\nEpsilon = 0.025249882339762064\nEpsilon = 0.025247357351528086\nEpsilon = 0.025244832615792932\nEpsilon = 0.025242308132531352\nEpsilon = 0.025239783901718098\nAgent: ddqn_agent . Episode 1977/2000. Number of steps to finish: 20. Loss: 25.46453857421875 Reward: -10.0\nEpsilon = 0.025237259923327926\nEpsilon = 0.025234736197335593\nEpsilon = 0.02523221272371586\nEpsilon = 0.025229689502443486\nEpsilon = 0.02522716653349324\nEpsilon = 0.025224643816839892\nEpsilon = 0.02522212135245821\nEpsilon = 0.025219599140322964\nEpsilon = 0.025217077180408932\nEpsilon = 0.02521455547269089\nEpsilon = 0.02521203401714362\nEpsilon = 0.025209512813741907\nEpsilon = 0.02520699186246053\nEpsilon = 0.025204471163274286\nEpsilon = 0.02520195071615796\nEpsilon = 0.025199430521086346\nEpsilon = 0.025196910578034238\nEpsilon = 0.025194390886976434\nEpsilon = 0.025191871447887735\nEpsilon = 0.025189352260742946\nAgent: ddqn_agent . Episode 1978/2000. Number of steps to finish: 20. Loss: 30.724689483642578 Reward: -20.0\nEpsilon = 0.025186833325516873\nEpsilon = 0.025184314642184322\nEpsilon = 0.025181796210720104\nEpsilon = 0.025179278031099032\nEpsilon = 0.025176760103295923\nEpsilon = 0.025174242427285595\nEpsilon = 0.025171725003042867\nEpsilon = 0.025169207830542564\nEpsilon = 0.02516669090975951\nEpsilon = 0.025164174240668535\nEpsilon = 0.02516165782324447\nEpsilon = 0.025159141657462147\nEpsilon = 0.025156625743296402\nEpsilon = 0.025154110080722072\nEpsilon = 0.025151594669714\nEpsilon = 0.025149079510247026\nEpsilon = 0.025146564602296\nEpsilon = 0.025144049945835772\nEpsilon = 0.02514153554084119\nEpsilon = 0.025139021387287106\nAgent: ddqn_agent . Episode 1979/2000. Number of steps to finish: 20. Loss: 26.025135040283203 Reward: -18.0\nEpsilon = 0.02513650748514838\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.025133993834399865\nEpsilon = 0.025131480435016427\nEpsilon = 0.025128967286972927\nEpsilon = 0.02512645439024423\nEpsilon = 0.025123941744805204\nEpsilon = 0.025121429350630723\nEpsilon = 0.02511891720769566\nEpsilon = 0.02511640531597489\nEpsilon = 0.025113893675443293\nEpsilon = 0.025111382286075748\nEpsilon = 0.025108871147847142\nEpsilon = 0.025106360260732357\nEpsilon = 0.025103849624706284\nEpsilon = 0.025101339239743815\nEpsilon = 0.025098829105819842\nEpsilon = 0.02509631922290926\nEpsilon = 0.02509380959098697\nEpsilon = 0.02509130021002787\nEpsilon = 0.02508879108000687\nAgent: ddqn_agent . Episode 1980/2000. Number of steps to finish: 20. Loss: 25.110254287719727 Reward: -16.0\nEpsilon = 0.02508628220089887\nEpsilon = 0.02508377357267878\nEpsilon = 0.02508126519532151\nEpsilon = 0.025078757068801978\nEpsilon = 0.025076249193095097\nEpsilon = 0.025073741568175788\nEpsilon = 0.02507123419401897\nEpsilon = 0.025068727070599567\nEpsilon = 0.025066220197892508\nEpsilon = 0.02506371357587272\nEpsilon = 0.025061207204515133\nEpsilon = 0.025058701083794682\nEpsilon = 0.025056195213686305\nEpsilon = 0.025053689594164937\nEpsilon = 0.02505118422520552\nEpsilon = 0.025048679106783\nEpsilon = 0.025046174238872322\nEpsilon = 0.025043669621448435\nEpsilon = 0.02504116525448629\nEpsilon = 0.02503866113796084\nAgent: ddqn_agent . Episode 1981/2000. Number of steps to finish: 20. Loss: 29.201231002807617 Reward: -10.0\nEpsilon = 0.025036157271847045\nEpsilon = 0.02503365365611986\nEpsilon = 0.02503115029075425\nEpsilon = 0.025028647175725175\nEpsilon = 0.025026144311007602\nEpsilon = 0.0250236416965765\nEpsilon = 0.025021139332406843\nEpsilon = 0.025018637218473604\nEpsilon = 0.025016135354751758\nEpsilon = 0.025013633741216283\nEpsilon = 0.025011132377842162\nEpsilon = 0.02500863126460438\nEpsilon = 0.02500613040147792\nEpsilon = 0.02500362978843777\nEpsilon = 0.025001129425458925\nEpsilon = 0.02499862931251638\nEpsilon = 0.02499612944958513\nEpsilon = 0.02499362983664017\nEpsilon = 0.024991130473656507\nEpsilon = 0.02498863136060914\nAgent: ddqn_agent . Episode 1982/2000. Number of steps to finish: 20. Loss: 24.900815963745117 Reward: -12.0\nEpsilon = 0.02498613249747308\nEpsilon = 0.024983633884223335\nEpsilon = 0.02498113552083491\nEpsilon = 0.024978637407282827\nEpsilon = 0.024976139543542098\nEpsilon = 0.024973641929587745\nEpsilon = 0.024971144565394787\nEpsilon = 0.02496864745093825\nEpsilon = 0.024966150586193156\nEpsilon = 0.024963653971134536\nEpsilon = 0.024961157605737423\nEpsilon = 0.02495866148997685\nEpsilon = 0.024956165623827853\nEpsilon = 0.02495367000726547\nEpsilon = 0.024951174640264744\nEpsilon = 0.024948679522800717\nEpsilon = 0.02494618465484844\nEpsilon = 0.024943690036382953\nEpsilon = 0.024941195667379314\nEpsilon = 0.024938701547812577\nAgent: ddqn_agent . Episode 1983/2000. Number of steps to finish: 20. Loss: 27.137901306152344 Reward: -14.0\nEpsilon = 0.024936207677657796\nEpsilon = 0.02493371405689003\nEpsilon = 0.02493122068548434\nEpsilon = 0.02492872756341579\nEpsilon = 0.02492623469065945\nEpsilon = 0.024923742067190384\nEpsilon = 0.024921249692983666\nEpsilon = 0.024918757568014368\nEpsilon = 0.024916265692257567\nEpsilon = 0.024913774065688343\nEpsilon = 0.024911282688281775\nEpsilon = 0.02490879156001295\nEpsilon = 0.024906300680856946\nEpsilon = 0.02490381005078886\nEpsilon = 0.024901319669783782\nEpsilon = 0.024898829537816802\nEpsilon = 0.02489633965486302\nEpsilon = 0.024893850020897536\nEpsilon = 0.024891360635895445\nEpsilon = 0.024888871499831856\nAgent: ddqn_agent . Episode 1984/2000. Number of steps to finish: 20. Loss: 27.867033004760742 Reward: -18.0\nEpsilon = 0.024886382612681873\nEpsilon = 0.024883893974420604\nEpsilon = 0.02488140558502316\nEpsilon = 0.02487891744446466\nEpsilon = 0.024876429552720215\nEpsilon = 0.024873941909764944\nEpsilon = 0.024871454515573967\nEpsilon = 0.02486896737012241\nEpsilon = 0.0248664804733854\nEpsilon = 0.024863993825338063\nEpsilon = 0.02486150742595553\nEpsilon = 0.024859021275212934\nEpsilon = 0.024856535373085412\nEpsilon = 0.024854049719548105\nEpsilon = 0.02485156431457615\nEpsilon = 0.02484907915814469\nEpsilon = 0.02484659425022888\nEpsilon = 0.024844109590803857\nEpsilon = 0.024841625179844777\nEpsilon = 0.02483914101732679\nAgent: ddqn_agent . Episode 1985/2000. Number of steps to finish: 20. Loss: 26.28966522216797 Reward: -14.0\nEpsilon = 0.02483665710322506\nEpsilon = 0.024834173437514737\nEpsilon = 0.024831690020170986\nEpsilon = 0.024829206851168968\nEpsilon = 0.02482672393048385\nEpsilon = 0.024824241258090803\nEpsilon = 0.024821758833964994\nEpsilon = 0.024819276658081597\nEpsilon = 0.02481679473041579\nEpsilon = 0.024814313050942748\nEpsilon = 0.024811831619637653\nEpsilon = 0.02480935043647569\nEpsilon = 0.024806869501432044\nEpsilon = 0.024804388814481902\nEpsilon = 0.024801908375600454\nEpsilon = 0.024799428184762894\nEpsilon = 0.02479694824194442\nEpsilon = 0.024794468547120224\nEpsilon = 0.02479198910026551\nEpsilon = 0.024789509901355485\nAgent: ddqn_agent . Episode 1986/2000. Number of steps to finish: 20. Loss: 24.314077377319336 Reward: -10.0\nEpsilon = 0.02478703095036535\nEpsilon = 0.024784552247270313\nEpsilon = 0.024782073792045587\nEpsilon = 0.024779595584666383\nEpsilon = 0.024777117625107918\nEpsilon = 0.024774639913345407\nEpsilon = 0.02477216244935407\nEpsilon = 0.024769685233109136\nEpsilon = 0.024767208264585824\nEpsilon = 0.024764731543759366\nEpsilon = 0.02476225507060499\nEpsilon = 0.024759778845097932\nEpsilon = 0.024757302867213424\nEpsilon = 0.024754827136926703\nEpsilon = 0.02475235165421301\nEpsilon = 0.02474987641904759\nEpsilon = 0.024747401431405684\nEpsilon = 0.024744926691262544\nEpsilon = 0.024742452198593418\nEpsilon = 0.024739977953373557\nAgent: ddqn_agent . Episode 1987/2000. Number of steps to finish: 20. Loss: 24.539064407348633 Reward: -10.0\nEpsilon = 0.024737503955578218\nEpsilon = 0.02473503020518266\nEpsilon = 0.024732556702162144\nEpsilon = 0.024730083446491927\nEpsilon = 0.02472761043814728\nEpsilon = 0.024725137677103465\nEpsilon = 0.024722665163335756\nEpsilon = 0.024720192896819423\nEpsilon = 0.024717720877529742\nEpsilon = 0.02471524910544199\nEpsilon = 0.024712777580531448\nEpsilon = 0.024710306302773394\nEpsilon = 0.024707835272143118\nEpsilon = 0.024705364488615905\nEpsilon = 0.024702893952167043\nEpsilon = 0.024700423662771826\nEpsilon = 0.024697953620405548\nEpsilon = 0.024695483825043506\nEpsilon = 0.024693014276661002\nEpsilon = 0.024690544975233337\nAgent: ddqn_agent . Episode 1988/2000. Number of steps to finish: 20. Loss: 23.763668060302734 Reward: -14.0\nEpsilon = 0.024688075920735814\nEpsilon = 0.024685607113143743\nEpsilon = 0.02468313855243243\nEpsilon = 0.024680670238577188\nEpsilon = 0.02467820217155333\nEpsilon = 0.024675734351336177\nEpsilon = 0.024673266777901045\nEpsilon = 0.024670799451223253\nEpsilon = 0.02466833237127813\nEpsilon = 0.024665865538041003\nEpsilon = 0.024663398951487198\nEpsilon = 0.024660932611592048\nEpsilon = 0.02465846651833089\nEpsilon = 0.024656000671679057\nEpsilon = 0.02465353507161189\nEpsilon = 0.02465106971810473\nEpsilon = 0.02464860461113292\nEpsilon = 0.024646139750671807\nEpsilon = 0.02464367513669674\nEpsilon = 0.02464121076918307\nAgent: ddqn_agent . Episode 1989/2000. Number of steps to finish: 20. Loss: 25.36830711364746 Reward: -14.0\nEpsilon = 0.024638746648106155\nEpsilon = 0.024636282773441345\nEpsilon = 0.024633819145164\nEpsilon = 0.024631355763249485\nEpsilon = 0.02462889262767316\nEpsilon = 0.024626429738410394\nAgent: ddqn_agent . Episode 1990/2000. Number of steps to finish: 6. Loss: 6.932364463806152 Reward: 6.0\nEpsilon = 0.024623967095436555\nEpsilon = 0.024621504698727013\nEpsilon = 0.024619042548257142\nEpsilon = 0.024616580644002316\nEpsilon = 0.024614118985937915\nEpsilon = 0.02461165757403932\nEpsilon = 0.02460919640828192\nEpsilon = 0.02460673548864109\nEpsilon = 0.024604274815092225\nEpsilon = 0.024601814387610717\nEpsilon = 0.02459935420617196\nEpsilon = 0.024596894270751343\nEpsilon = 0.02459443458132427\nEpsilon = 0.024591975137866137\nEpsilon = 0.02458951594035235\nEpsilon = 0.024587056988758314\nEpsilon = 0.02458459828305944\nEpsilon = 0.024582139823231135\nEpsilon = 0.024579681609248814\nEpsilon = 0.02457722364108789\nAgent: ddqn_agent . Episode 1991/2000. Number of steps to finish: 20. Loss: 26.809185028076172 Reward: -12.0\nEpsilon = 0.024574765918723783\nEpsilon = 0.024572308442131912\nEpsilon = 0.0245698512112877\nEpsilon = 0.02456739422616657\nEpsilon = 0.024564937486743956\nEpsilon = 0.02456248099299528\nEpsilon = 0.02456002474489598\nEpsilon = 0.02455756874242149\nEpsilon = 0.02455511298554725\nAgent: ddqn_agent . Episode 1992/2000. Number of steps to finish: 9. Loss: 12.108138084411621 Reward: 3.0\nEpsilon = 0.024552657474248694\nEpsilon = 0.02455020220850127\nEpsilon = 0.024547747188280417\nEpsilon = 0.02454529241356159\nEpsilon = 0.024542837884320236\nEpsilon = 0.024540383600531804\nEpsilon = 0.02453792956217175\nEpsilon = 0.024535475769215533\nEpsilon = 0.024533022221638612\nEpsilon = 0.02453056891941645\n","name":"stdout"},{"output_type":"stream","text":"Epsilon = 0.024528115862524506\nEpsilon = 0.024525663050938253\nEpsilon = 0.02452321048463316\nEpsilon = 0.024520758163584698\nEpsilon = 0.02451830608776834\nEpsilon = 0.024515854257159565\nEpsilon = 0.02451340267173385\nEpsilon = 0.024510951331466677\nEpsilon = 0.02450850023633353\nEpsilon = 0.0245060493863099\nAgent: ddqn_agent . Episode 1993/2000. Number of steps to finish: 20. Loss: 27.831933975219727 Reward: -16.0\nEpsilon = 0.024503598781371268\nEpsilon = 0.024501148421493132\nEpsilon = 0.02449869830665098\nEpsilon = 0.024496248436820318\nEpsilon = 0.024493798811976636\nEpsilon = 0.02449134943209544\nEpsilon = 0.024488900297152227\nEpsilon = 0.024486451407122512\nEpsilon = 0.024484002761981802\nEpsilon = 0.024481554361705606\nEpsilon = 0.024479106206269436\nEpsilon = 0.02447665829564881\nEpsilon = 0.024474210629819245\nEpsilon = 0.024471763208756263\nEpsilon = 0.02446931603243539\nEpsilon = 0.024466869100832147\nEpsilon = 0.024464422413922063\nEpsilon = 0.02446197597168067\nEpsilon = 0.024459529774083503\nEpsilon = 0.024457083821106094\nAgent: ddqn_agent . Episode 1994/2000. Number of steps to finish: 20. Loss: 24.2280330657959 Reward: -12.0\nEpsilon = 0.024454638112723984\nEpsilon = 0.024452192648912713\nEpsilon = 0.02444974742964782\nEpsilon = 0.024447302454904857\nEpsilon = 0.024444857724659368\nEpsilon = 0.024442413238886903\nEpsilon = 0.024439968997563014\nEpsilon = 0.02443752500066326\nEpsilon = 0.024435081248163192\nEpsilon = 0.024432637740038378\nEpsilon = 0.024430194476264374\nEpsilon = 0.02442775145681675\nEpsilon = 0.024425308681671067\nEpsilon = 0.0244228661508029\nEpsilon = 0.02442042386418782\nEpsilon = 0.0244179818218014\nEpsilon = 0.02441554002361922\nEpsilon = 0.02441309846961686\nEpsilon = 0.024410657159769898\nEpsilon = 0.02440821609405392\nAgent: ddqn_agent . Episode 1995/2000. Number of steps to finish: 20. Loss: 25.871055603027344 Reward: -12.0\nEpsilon = 0.024405775272444516\nEpsilon = 0.024403334694917272\nEpsilon = 0.02440089436144778\nEpsilon = 0.024398454272011635\nEpsilon = 0.024396014426584435\nEpsilon = 0.024393574825141776\nEpsilon = 0.024391135467659263\nEpsilon = 0.024388696354112498\nEpsilon = 0.02438625748447709\nEpsilon = 0.024383818858728642\nEpsilon = 0.024381380476842768\nEpsilon = 0.024378942338795083\nEpsilon = 0.024376504444561204\nEpsilon = 0.024374066794116748\nEpsilon = 0.024371629387437338\nEpsilon = 0.024369192224498595\nEpsilon = 0.024366755305276145\nEpsilon = 0.024364318629745616\nEpsilon = 0.024361882197882642\nEpsilon = 0.024359446009662854\nAgent: ddqn_agent . Episode 1996/2000. Number of steps to finish: 20. Loss: 27.556766510009766 Reward: -10.0\nEpsilon = 0.02435701006506189\nEpsilon = 0.024354574364055383\nEpsilon = 0.024352138906618977\nEpsilon = 0.024349703692728316\nEpsilon = 0.024347268722359042\nEpsilon = 0.024344833995486807\nEpsilon = 0.02434239951208726\nEpsilon = 0.02433996527213605\nEpsilon = 0.024337531275608837\nEpsilon = 0.024335097522481277\nEpsilon = 0.02433266401272903\nEpsilon = 0.024330230746327758\nEpsilon = 0.024327797723253126\nEpsilon = 0.0243253649434808\nEpsilon = 0.02432293240698645\nEpsilon = 0.02432050011374575\nEpsilon = 0.024318068063734377\nEpsilon = 0.024315636256928005\nEpsilon = 0.024313204693302313\nEpsilon = 0.024310773372832984\nAgent: ddqn_agent . Episode 1997/2000. Number of steps to finish: 20. Loss: 27.768301010131836 Reward: -10.0\nEpsilon = 0.0243083422954957\nEpsilon = 0.024305911461266153\nEpsilon = 0.024303480870120027\nEpsilon = 0.024301050522033015\nEpsilon = 0.02429862041698081\nEpsilon = 0.024296190554939114\nEpsilon = 0.02429376093588362\nEpsilon = 0.024291331559790033\nEpsilon = 0.024288902426634056\nEpsilon = 0.024286473536391393\nEpsilon = 0.024284044889037754\nEpsilon = 0.02428161648454885\nEpsilon = 0.024279188322900398\nEpsilon = 0.024276760404068107\nEpsilon = 0.0242743327280277\nEpsilon = 0.024271905294754897\nEpsilon = 0.02426947810422542\nEpsilon = 0.024267051156414997\nEpsilon = 0.024264624451299354\nEpsilon = 0.024262197988854223\nAgent: ddqn_agent . Episode 1998/2000. Number of steps to finish: 20. Loss: 25.69818878173828 Reward: -18.0\nEpsilon = 0.02425977176905534\nEpsilon = 0.024257345791878434\nEpsilon = 0.024254920057299248\nEpsilon = 0.02425249456529352\nEpsilon = 0.02425006931583699\nEpsilon = 0.024247644308905407\nEpsilon = 0.024245219544474516\nEpsilon = 0.02424279502252007\nEpsilon = 0.024240370743017818\nAgent: ddqn_agent . Episode 1999/2000. Number of steps to finish: 9. Loss: 12.73254108428955 Reward: 3.0\nEpsilon = 0.024237946705943515\nEpsilon = 0.02423552291127292\nEpsilon = 0.024233099358981793\nEpsilon = 0.024230676049045895\nEpsilon = 0.02422825298144099\nEpsilon = 0.024225830156142845\nEpsilon = 0.024223407573127232\nEpsilon = 0.02422098523236992\nEpsilon = 0.024218563133846682\nEpsilon = 0.0242161412775333\nEpsilon = 0.024213719663405544\nEpsilon = 0.024211298291439205\nEpsilon = 0.024208877161610062\nEpsilon = 0.024206456273893902\nEpsilon = 0.024204035628266514\nEpsilon = 0.02420161522470369\nEpsilon = 0.02419919506318122\nEpsilon = 0.024196775143674902\nEpsilon = 0.024194355466160535\nEpsilon = 0.02419193603061392\nAgent: ddqn_agent . Episode 2000/2000. Number of steps to finish: 20. Loss: 29.21013641357422 Reward: -18.0\n########## sacd_agent is running ##########\nAgent: sacd_agent . Episode 0/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: sacd_agent . Episode 1/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: sacd_agent . Episode 2/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: sacd_agent . Episode 3/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: sacd_agent . Episode 4/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: sacd_agent . Episode 5/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: sacd_agent . Episode 6/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: sacd_agent . Episode 7/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: sacd_agent . Episode 8/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: sacd_agent . Episode 9/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: sacd_agent . Episode 10/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: sacd_agent . Episode 11/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: sacd_agent . Episode 12/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: sacd_agent . Episode 13/2000. Number of steps to finish: 20. Loss: 51.810340881347656 Reward: -10.0\nAgent: sacd_agent . Episode 14/2000. Number of steps to finish: 20. Loss: 64.2427749633789 Reward: -10.0\nAgent: sacd_agent . Episode 15/2000. Number of steps to finish: 20. Loss: 63.834285736083984 Reward: -10.0\nAgent: sacd_agent . Episode 16/2000. Number of steps to finish: 20. Loss: 63.10712432861328 Reward: -14.0\nAgent: sacd_agent . Episode 17/2000. Number of steps to finish: 20. Loss: 61.86779022216797 Reward: -14.0\nAgent: sacd_agent . Episode 18/2000. Number of steps to finish: 20. Loss: 60.53727722167969 Reward: -12.0\nAgent: sacd_agent . Episode 19/2000. Number of steps to finish: 20. Loss: 58.51421356201172 Reward: -12.0\nAgent: sacd_agent . Episode 20/2000. Number of steps to finish: 20. Loss: 58.05320358276367 Reward: -12.0\nAgent: sacd_agent . Episode 21/2000. Number of steps to finish: 20. Loss: 56.37799072265625 Reward: -10.0\nAgent: sacd_agent . Episode 22/2000. Number of steps to finish: 20. Loss: 54.62895965576172 Reward: -10.0\nAgent: sacd_agent . Episode 23/2000. Number of steps to finish: 20. Loss: 52.3150634765625 Reward: -12.0\nAgent: sacd_agent . Episode 24/2000. Number of steps to finish: 20. Loss: 50.33676528930664 Reward: -18.0\nAgent: sacd_agent . Episode 25/2000. Number of steps to finish: 20. Loss: 46.4107666015625 Reward: -16.0\nAgent: sacd_agent . Episode 26/2000. Number of steps to finish: 20. Loss: 43.18149185180664 Reward: -14.0\nAgent: sacd_agent . Episode 27/2000. Number of steps to finish: 20. Loss: 40.428794860839844 Reward: -14.0\nAgent: sacd_agent . Episode 28/2000. Number of steps to finish: 20. Loss: 36.697566986083984 Reward: -16.0\nAgent: sacd_agent . Episode 29/2000. Number of steps to finish: 20. Loss: 34.23885726928711 Reward: -10.0\nAgent: sacd_agent . Episode 30/2000. Number of steps to finish: 20. Loss: 32.393646240234375 Reward: -16.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 31/2000. Number of steps to finish: 20. Loss: 30.35213279724121 Reward: -10.0\nAgent: sacd_agent . Episode 32/2000. Number of steps to finish: 20. Loss: 28.578969955444336 Reward: -14.0\nAgent: sacd_agent . Episode 33/2000. Number of steps to finish: 20. Loss: 26.88949966430664 Reward: -10.0\nAgent: sacd_agent . Episode 34/2000. Number of steps to finish: 13. Loss: 18.142993927001953 Reward: -1.0\nAgent: sacd_agent . Episode 35/2000. Number of steps to finish: 20. Loss: 25.633377075195312 Reward: -10.0\nAgent: sacd_agent . Episode 36/2000. Number of steps to finish: 20. Loss: 24.697778701782227 Reward: -16.0\nAgent: sacd_agent . Episode 37/2000. Number of steps to finish: 19. Loss: 22.39208221435547 Reward: -7.0\nAgent: sacd_agent . Episode 38/2000. Number of steps to finish: 20. Loss: 23.333158493041992 Reward: -12.0\nAgent: sacd_agent . Episode 39/2000. Number of steps to finish: 20. Loss: 21.51478385925293 Reward: -10.0\nAgent: sacd_agent . Episode 40/2000. Number of steps to finish: 17. Loss: 18.90526008605957 Reward: -5.0\nAgent: sacd_agent . Episode 41/2000. Number of steps to finish: 20. Loss: 21.940319061279297 Reward: -12.0\nAgent: sacd_agent . Episode 42/2000. Number of steps to finish: 20. Loss: 20.070167541503906 Reward: -14.0\nAgent: sacd_agent . Episode 43/2000. Number of steps to finish: 19. Loss: 19.614364624023438 Reward: -7.0\nAgent: sacd_agent . Episode 44/2000. Number of steps to finish: 20. Loss: 19.940326690673828 Reward: -12.0\nAgent: sacd_agent . Episode 45/2000. Number of steps to finish: 20. Loss: 19.898555755615234 Reward: -14.0\nAgent: sacd_agent . Episode 46/2000. Number of steps to finish: 20. Loss: 18.85881805419922 Reward: -14.0\nAgent: sacd_agent . Episode 47/2000. Number of steps to finish: 20. Loss: 18.473011016845703 Reward: -10.0\nAgent: sacd_agent . Episode 48/2000. Number of steps to finish: 20. Loss: 19.361881256103516 Reward: -14.0\nAgent: sacd_agent . Episode 49/2000. Number of steps to finish: 20. Loss: 17.947162628173828 Reward: -18.0\nAgent: sacd_agent . Episode 50/2000. Number of steps to finish: 20. Loss: 18.08387565612793 Reward: -18.0\nAgent: sacd_agent . Episode 51/2000. Number of steps to finish: 20. Loss: 18.041440963745117 Reward: -12.0\nAgent: sacd_agent . Episode 52/2000. Number of steps to finish: 20. Loss: 17.10383415222168 Reward: -12.0\nAgent: sacd_agent . Episode 53/2000. Number of steps to finish: 20. Loss: 16.5728702545166 Reward: -10.0\nAgent: sacd_agent . Episode 54/2000. Number of steps to finish: 20. Loss: 16.645370483398438 Reward: -12.0\nAgent: sacd_agent . Episode 55/2000. Number of steps to finish: 20. Loss: 16.308794021606445 Reward: -12.0\nAgent: sacd_agent . Episode 56/2000. Number of steps to finish: 20. Loss: 15.497505187988281 Reward: -8.0\nAgent: sacd_agent . Episode 57/2000. Number of steps to finish: 20. Loss: 15.765625 Reward: -12.0\nAgent: sacd_agent . Episode 58/2000. Number of steps to finish: 20. Loss: 15.39963150024414 Reward: -10.0\nAgent: sacd_agent . Episode 59/2000. Number of steps to finish: 20. Loss: 15.534029006958008 Reward: -12.0\nAgent: sacd_agent . Episode 60/2000. Number of steps to finish: 20. Loss: 14.909400939941406 Reward: -14.0\nAgent: sacd_agent . Episode 61/2000. Number of steps to finish: 20. Loss: 15.332744598388672 Reward: -10.0\nAgent: sacd_agent . Episode 62/2000. Number of steps to finish: 20. Loss: 14.925790786743164 Reward: -10.0\nAgent: sacd_agent . Episode 63/2000. Number of steps to finish: 20. Loss: 15.364931106567383 Reward: -10.0\nAgent: sacd_agent . Episode 64/2000. Number of steps to finish: 20. Loss: 14.916315078735352 Reward: -12.0\nAgent: sacd_agent . Episode 65/2000. Number of steps to finish: 20. Loss: 14.273469924926758 Reward: -10.0\nAgent: sacd_agent . Episode 66/2000. Number of steps to finish: 20. Loss: 14.854135513305664 Reward: -12.0\nAgent: sacd_agent . Episode 67/2000. Number of steps to finish: 18. Loss: 12.788935661315918 Reward: -6.0\nAgent: sacd_agent . Episode 68/2000. Number of steps to finish: 20. Loss: 14.244535446166992 Reward: -10.0\nAgent: sacd_agent . Episode 69/2000. Number of steps to finish: 20. Loss: 14.420523643493652 Reward: -14.0\nAgent: sacd_agent . Episode 70/2000. Number of steps to finish: 20. Loss: 14.155823707580566 Reward: -10.0\nAgent: sacd_agent . Episode 71/2000. Number of steps to finish: 9. Loss: 6.092888355255127 Reward: 3.0\nAgent: sacd_agent . Episode 72/2000. Number of steps to finish: 20. Loss: 13.84213638305664 Reward: -14.0\nAgent: sacd_agent . Episode 73/2000. Number of steps to finish: 20. Loss: 14.462299346923828 Reward: -10.0\nAgent: sacd_agent . Episode 74/2000. Number of steps to finish: 20. Loss: 14.271081924438477 Reward: -14.0\nAgent: sacd_agent . Episode 75/2000. Number of steps to finish: 20. Loss: 14.077009201049805 Reward: -14.0\nAgent: sacd_agent . Episode 76/2000. Number of steps to finish: 20. Loss: 13.188128471374512 Reward: -12.0\nAgent: sacd_agent . Episode 77/2000. Number of steps to finish: 20. Loss: 13.739846229553223 Reward: -14.0\nAgent: sacd_agent . Episode 78/2000. Number of steps to finish: 20. Loss: 13.050948143005371 Reward: -12.0\nAgent: sacd_agent . Episode 79/2000. Number of steps to finish: 20. Loss: 12.876914978027344 Reward: -12.0\nAgent: sacd_agent . Episode 80/2000. Number of steps to finish: 20. Loss: 12.427400588989258 Reward: -12.0\nAgent: sacd_agent . Episode 81/2000. Number of steps to finish: 20. Loss: 13.142598152160645 Reward: -12.0\nAgent: sacd_agent . Episode 82/2000. Number of steps to finish: 15. Loss: 9.811136245727539 Reward: -3.0\nAgent: sacd_agent . Episode 83/2000. Number of steps to finish: 20. Loss: 12.801265716552734 Reward: -12.0\nAgent: sacd_agent . Episode 84/2000. Number of steps to finish: 20. Loss: 12.36493968963623 Reward: -10.0\nAgent: sacd_agent . Episode 85/2000. Number of steps to finish: 20. Loss: 12.294190406799316 Reward: -14.0\nAgent: sacd_agent . Episode 86/2000. Number of steps to finish: 20. Loss: 12.630826950073242 Reward: -10.0\nAgent: sacd_agent . Episode 87/2000. Number of steps to finish: 20. Loss: 12.740318298339844 Reward: -10.0\nAgent: sacd_agent . Episode 88/2000. Number of steps to finish: 20. Loss: 12.9644193649292 Reward: -14.0\nAgent: sacd_agent . Episode 89/2000. Number of steps to finish: 20. Loss: 12.714988708496094 Reward: -14.0\nAgent: sacd_agent . Episode 90/2000. Number of steps to finish: 20. Loss: 12.344067573547363 Reward: -10.0\nAgent: sacd_agent . Episode 91/2000. Number of steps to finish: 20. Loss: 12.69592571258545 Reward: -12.0\nAgent: sacd_agent . Episode 92/2000. Number of steps to finish: 20. Loss: 12.455784797668457 Reward: -12.0\nAgent: sacd_agent . Episode 93/2000. Number of steps to finish: 20. Loss: 12.469181060791016 Reward: -14.0\nAgent: sacd_agent . Episode 94/2000. Number of steps to finish: 20. Loss: 12.291303634643555 Reward: -12.0\nAgent: sacd_agent . Episode 95/2000. Number of steps to finish: 9. Loss: 5.4242753982543945 Reward: 3.0\nAgent: sacd_agent . Episode 96/2000. Number of steps to finish: 20. Loss: 12.330456733703613 Reward: -10.0\nAgent: sacd_agent . Episode 97/2000. Number of steps to finish: 13. Loss: 7.9789347648620605 Reward: -1.0\nAgent: sacd_agent . Episode 98/2000. Number of steps to finish: 20. Loss: 12.281519889831543 Reward: -12.0\nAgent: sacd_agent . Episode 99/2000. Number of steps to finish: 20. Loss: 12.2725830078125 Reward: -12.0\nAgent: sacd_agent . Episode 100/2000. Number of steps to finish: 20. Loss: 12.679521560668945 Reward: -16.0\nAgent: sacd_agent . Episode 101/2000. Number of steps to finish: 20. Loss: 12.514482498168945 Reward: -10.0\nAgent: sacd_agent . Episode 102/2000. Number of steps to finish: 16. Loss: 9.606922149658203 Reward: -4.0\nAgent: sacd_agent . Episode 103/2000. Number of steps to finish: 20. Loss: 12.256217002868652 Reward: -14.0\nAgent: sacd_agent . Episode 104/2000. Number of steps to finish: 20. Loss: 11.90195369720459 Reward: -10.0\nAgent: sacd_agent . Episode 105/2000. Number of steps to finish: 20. Loss: 11.951691627502441 Reward: -14.0\nAgent: sacd_agent . Episode 106/2000. Number of steps to finish: 20. Loss: 11.965903282165527 Reward: -10.0\nAgent: sacd_agent . Episode 107/2000. Number of steps to finish: 20. Loss: 12.482511520385742 Reward: -10.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 108/2000. Number of steps to finish: 20. Loss: 12.806489944458008 Reward: -10.0\nAgent: sacd_agent . Episode 109/2000. Number of steps to finish: 20. Loss: 12.172017097473145 Reward: -8.0\nAgent: sacd_agent . Episode 110/2000. Number of steps to finish: 14. Loss: 8.892094612121582 Reward: -2.0\nAgent: sacd_agent . Episode 111/2000. Number of steps to finish: 20. Loss: 12.35410213470459 Reward: -10.0\nAgent: sacd_agent . Episode 112/2000. Number of steps to finish: 20. Loss: 12.408808708190918 Reward: -10.0\nAgent: sacd_agent . Episode 113/2000. Number of steps to finish: 20. Loss: 12.892888069152832 Reward: -14.0\nAgent: sacd_agent . Episode 114/2000. Number of steps to finish: 20. Loss: 12.511465072631836 Reward: -14.0\nAgent: sacd_agent . Episode 115/2000. Number of steps to finish: 20. Loss: 12.077269554138184 Reward: -14.0\nAgent: sacd_agent . Episode 116/2000. Number of steps to finish: 20. Loss: 12.339550971984863 Reward: -10.0\nAgent: sacd_agent . Episode 117/2000. Number of steps to finish: 15. Loss: 9.285475730895996 Reward: -3.0\nAgent: sacd_agent . Episode 118/2000. Number of steps to finish: 20. Loss: 12.566670417785645 Reward: -12.0\nAgent: sacd_agent . Episode 119/2000. Number of steps to finish: 20. Loss: 12.678146362304688 Reward: -12.0\nAgent: sacd_agent . Episode 120/2000. Number of steps to finish: 17. Loss: 10.934144973754883 Reward: -5.0\nAgent: sacd_agent . Episode 121/2000. Number of steps to finish: 20. Loss: 12.613308906555176 Reward: -8.0\nAgent: sacd_agent . Episode 122/2000. Number of steps to finish: 19. Loss: 11.923160552978516 Reward: -7.0\nAgent: sacd_agent . Episode 123/2000. Number of steps to finish: 20. Loss: 12.837174415588379 Reward: -18.0\nAgent: sacd_agent . Episode 124/2000. Number of steps to finish: 20. Loss: 12.755315780639648 Reward: -8.0\nAgent: sacd_agent . Episode 125/2000. Number of steps to finish: 20. Loss: 12.819993019104004 Reward: -12.0\nAgent: sacd_agent . Episode 126/2000. Number of steps to finish: 20. Loss: 12.511573791503906 Reward: -18.0\nAgent: sacd_agent . Episode 127/2000. Number of steps to finish: 20. Loss: 12.824387550354004 Reward: -10.0\nAgent: sacd_agent . Episode 128/2000. Number of steps to finish: 20. Loss: 12.337881088256836 Reward: -12.0\nAgent: sacd_agent . Episode 129/2000. Number of steps to finish: 20. Loss: 12.261690139770508 Reward: -10.0\nAgent: sacd_agent . Episode 130/2000. Number of steps to finish: 20. Loss: 12.566278457641602 Reward: -10.0\nAgent: sacd_agent . Episode 131/2000. Number of steps to finish: 20. Loss: 12.279959678649902 Reward: -12.0\nAgent: sacd_agent . Episode 132/2000. Number of steps to finish: 20. Loss: 12.89747428894043 Reward: -14.0\nAgent: sacd_agent . Episode 133/2000. Number of steps to finish: 20. Loss: 12.31713581085205 Reward: -10.0\nAgent: sacd_agent . Episode 134/2000. Number of steps to finish: 20. Loss: 12.524214744567871 Reward: -10.0\nAgent: sacd_agent . Episode 135/2000. Number of steps to finish: 20. Loss: 12.531949043273926 Reward: -14.0\nAgent: sacd_agent . Episode 136/2000. Number of steps to finish: 12. Loss: 7.27182674407959 Reward: 0.0\nAgent: sacd_agent . Episode 137/2000. Number of steps to finish: 20. Loss: 12.689777374267578 Reward: -12.0\nAgent: sacd_agent . Episode 138/2000. Number of steps to finish: 20. Loss: 13.122984886169434 Reward: -10.0\nAgent: sacd_agent . Episode 139/2000. Number of steps to finish: 20. Loss: 12.470059394836426 Reward: -12.0\nAgent: sacd_agent . Episode 140/2000. Number of steps to finish: 20. Loss: 13.036744117736816 Reward: -10.0\nAgent: sacd_agent . Episode 141/2000. Number of steps to finish: 20. Loss: 12.234317779541016 Reward: -10.0\nAgent: sacd_agent . Episode 142/2000. Number of steps to finish: 20. Loss: 12.690103530883789 Reward: -10.0\nAgent: sacd_agent . Episode 143/2000. Number of steps to finish: 20. Loss: 12.625511169433594 Reward: -10.0\nAgent: sacd_agent . Episode 144/2000. Number of steps to finish: 20. Loss: 13.18026065826416 Reward: -10.0\nAgent: sacd_agent . Episode 145/2000. Number of steps to finish: 20. Loss: 12.319746971130371 Reward: -12.0\nAgent: sacd_agent . Episode 146/2000. Number of steps to finish: 20. Loss: 12.622532844543457 Reward: -12.0\nAgent: sacd_agent . Episode 147/2000. Number of steps to finish: 12. Loss: 7.780326843261719 Reward: 0.0\nAgent: sacd_agent . Episode 148/2000. Number of steps to finish: 20. Loss: 12.595342636108398 Reward: -14.0\nAgent: sacd_agent . Episode 149/2000. Number of steps to finish: 20. Loss: 12.987839698791504 Reward: -12.0\nAgent: sacd_agent . Episode 150/2000. Number of steps to finish: 20. Loss: 12.715507507324219 Reward: -12.0\nAgent: sacd_agent . Episode 151/2000. Number of steps to finish: 20. Loss: 12.474306106567383 Reward: -10.0\nAgent: sacd_agent . Episode 152/2000. Number of steps to finish: 15. Loss: 9.533082008361816 Reward: -3.0\nAgent: sacd_agent . Episode 153/2000. Number of steps to finish: 10. Loss: 6.645478248596191 Reward: 2.0\nAgent: sacd_agent . Episode 154/2000. Number of steps to finish: 20. Loss: 12.91185188293457 Reward: -10.0\nAgent: sacd_agent . Episode 155/2000. Number of steps to finish: 20. Loss: 12.612951278686523 Reward: -10.0\nAgent: sacd_agent . Episode 156/2000. Number of steps to finish: 20. Loss: 12.808723449707031 Reward: -12.0\nAgent: sacd_agent . Episode 157/2000. Number of steps to finish: 15. Loss: 10.067060470581055 Reward: -3.0\nAgent: sacd_agent . Episode 158/2000. Number of steps to finish: 20. Loss: 13.358981132507324 Reward: -10.0\nAgent: sacd_agent . Episode 159/2000. Number of steps to finish: 19. Loss: 12.452873229980469 Reward: -7.0\nAgent: sacd_agent . Episode 160/2000. Number of steps to finish: 16. Loss: 10.641532897949219 Reward: -4.0\nAgent: sacd_agent . Episode 161/2000. Number of steps to finish: 20. Loss: 13.230732917785645 Reward: -10.0\nAgent: sacd_agent . Episode 162/2000. Number of steps to finish: 20. Loss: 13.506367683410645 Reward: -14.0\nAgent: sacd_agent . Episode 163/2000. Number of steps to finish: 20. Loss: 13.688739776611328 Reward: -10.0\nAgent: sacd_agent . Episode 164/2000. Number of steps to finish: 13. Loss: 8.719415664672852 Reward: -1.0\nAgent: sacd_agent . Episode 165/2000. Number of steps to finish: 17. Loss: 11.589940071105957 Reward: -5.0\nAgent: sacd_agent . Episode 166/2000. Number of steps to finish: 20. Loss: 13.361001014709473 Reward: -16.0\nAgent: sacd_agent . Episode 167/2000. Number of steps to finish: 20. Loss: 13.264021873474121 Reward: -10.0\nAgent: sacd_agent . Episode 168/2000. Number of steps to finish: 20. Loss: 13.629081726074219 Reward: -12.0\nAgent: sacd_agent . Episode 169/2000. Number of steps to finish: 20. Loss: 13.509087562561035 Reward: -12.0\nAgent: sacd_agent . Episode 170/2000. Number of steps to finish: 20. Loss: 13.200413703918457 Reward: -10.0\nAgent: sacd_agent . Episode 171/2000. Number of steps to finish: 20. Loss: 13.881599426269531 Reward: -10.0\nAgent: sacd_agent . Episode 172/2000. Number of steps to finish: 12. Loss: 8.065274238586426 Reward: 0.0\nAgent: sacd_agent . Episode 173/2000. Number of steps to finish: 18. Loss: 11.920921325683594 Reward: -6.0\nAgent: sacd_agent . Episode 174/2000. Number of steps to finish: 20. Loss: 13.614863395690918 Reward: -12.0\nAgent: sacd_agent . Episode 175/2000. Number of steps to finish: 20. Loss: 13.80984878540039 Reward: -12.0\nAgent: sacd_agent . Episode 176/2000. Number of steps to finish: 20. Loss: 13.530247688293457 Reward: -14.0\nAgent: sacd_agent . Episode 177/2000. Number of steps to finish: 20. Loss: 13.661843299865723 Reward: -16.0\nAgent: sacd_agent . Episode 178/2000. Number of steps to finish: 15. Loss: 10.500043869018555 Reward: -3.0\nAgent: sacd_agent . Episode 179/2000. Number of steps to finish: 20. Loss: 13.831563949584961 Reward: -10.0\nAgent: sacd_agent . Episode 180/2000. Number of steps to finish: 17. Loss: 11.608014106750488 Reward: -5.0\nAgent: sacd_agent . Episode 181/2000. Number of steps to finish: 20. Loss: 13.455832481384277 Reward: -14.0\nAgent: sacd_agent . Episode 182/2000. Number of steps to finish: 20. Loss: 13.894230842590332 Reward: -8.0\nAgent: sacd_agent . Episode 183/2000. Number of steps to finish: 20. Loss: 14.607405662536621 Reward: -12.0\nAgent: sacd_agent . Episode 184/2000. Number of steps to finish: 20. Loss: 14.349417686462402 Reward: -10.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 185/2000. Number of steps to finish: 20. Loss: 14.256874084472656 Reward: -12.0\nAgent: sacd_agent . Episode 186/2000. Number of steps to finish: 20. Loss: 14.183676719665527 Reward: -10.0\nAgent: sacd_agent . Episode 187/2000. Number of steps to finish: 20. Loss: 13.88140869140625 Reward: -10.0\nAgent: sacd_agent . Episode 188/2000. Number of steps to finish: 20. Loss: 14.280414581298828 Reward: -14.0\nAgent: sacd_agent . Episode 189/2000. Number of steps to finish: 10. Loss: 7.101800441741943 Reward: 2.0\nAgent: sacd_agent . Episode 190/2000. Number of steps to finish: 20. Loss: 14.295629501342773 Reward: -16.0\nAgent: sacd_agent . Episode 191/2000. Number of steps to finish: 20. Loss: 13.781113624572754 Reward: -12.0\nAgent: sacd_agent . Episode 192/2000. Number of steps to finish: 20. Loss: 13.877405166625977 Reward: -10.0\nAgent: sacd_agent . Episode 193/2000. Number of steps to finish: 20. Loss: 13.971219062805176 Reward: -14.0\nAgent: sacd_agent . Episode 194/2000. Number of steps to finish: 15. Loss: 11.058807373046875 Reward: -3.0\nAgent: sacd_agent . Episode 195/2000. Number of steps to finish: 20. Loss: 14.051295280456543 Reward: -8.0\nAgent: sacd_agent . Episode 196/2000. Number of steps to finish: 20. Loss: 13.53549861907959 Reward: -16.0\nAgent: sacd_agent . Episode 197/2000. Number of steps to finish: 19. Loss: 13.481986999511719 Reward: -7.0\nAgent: sacd_agent . Episode 198/2000. Number of steps to finish: 12. Loss: 8.707755088806152 Reward: 0.0\nAgent: sacd_agent . Episode 199/2000. Number of steps to finish: 20. Loss: 14.702399253845215 Reward: -12.0\nAgent: sacd_agent . Episode 200/2000. Number of steps to finish: 20. Loss: 14.050946235656738 Reward: -14.0\nAgent: sacd_agent . Episode 201/2000. Number of steps to finish: 14. Loss: 10.105707168579102 Reward: -2.0\nAgent: sacd_agent . Episode 202/2000. Number of steps to finish: 20. Loss: 14.312793731689453 Reward: -14.0\nAgent: sacd_agent . Episode 203/2000. Number of steps to finish: 20. Loss: 14.288785934448242 Reward: -10.0\nAgent: sacd_agent . Episode 204/2000. Number of steps to finish: 20. Loss: 14.783212661743164 Reward: -10.0\nAgent: sacd_agent . Episode 205/2000. Number of steps to finish: 20. Loss: 15.866926193237305 Reward: -14.0\nAgent: sacd_agent . Episode 206/2000. Number of steps to finish: 20. Loss: 15.357793807983398 Reward: -10.0\nAgent: sacd_agent . Episode 207/2000. Number of steps to finish: 20. Loss: 14.659833908081055 Reward: -10.0\nAgent: sacd_agent . Episode 208/2000. Number of steps to finish: 20. Loss: 14.06446361541748 Reward: -16.0\nAgent: sacd_agent . Episode 209/2000. Number of steps to finish: 20. Loss: 14.75229549407959 Reward: -12.0\nAgent: sacd_agent . Episode 210/2000. Number of steps to finish: 17. Loss: 12.104118347167969 Reward: -5.0\nAgent: sacd_agent . Episode 211/2000. Number of steps to finish: 10. Loss: 7.171035289764404 Reward: 2.0\nAgent: sacd_agent . Episode 212/2000. Number of steps to finish: 20. Loss: 15.15175724029541 Reward: -10.0\nAgent: sacd_agent . Episode 213/2000. Number of steps to finish: 20. Loss: 15.103540420532227 Reward: -12.0\nAgent: sacd_agent . Episode 214/2000. Number of steps to finish: 20. Loss: 15.76687240600586 Reward: -12.0\nAgent: sacd_agent . Episode 215/2000. Number of steps to finish: 12. Loss: 9.202580451965332 Reward: 0.0\nAgent: sacd_agent . Episode 216/2000. Number of steps to finish: 17. Loss: 12.627484321594238 Reward: -5.0\nAgent: sacd_agent . Episode 217/2000. Number of steps to finish: 20. Loss: 15.90650463104248 Reward: -10.0\nAgent: sacd_agent . Episode 218/2000. Number of steps to finish: 20. Loss: 14.905756950378418 Reward: -12.0\nAgent: sacd_agent . Episode 219/2000. Number of steps to finish: 20. Loss: 15.029884338378906 Reward: -10.0\nAgent: sacd_agent . Episode 220/2000. Number of steps to finish: 20. Loss: 15.683917045593262 Reward: -10.0\nAgent: sacd_agent . Episode 221/2000. Number of steps to finish: 20. Loss: 14.266873359680176 Reward: -10.0\nAgent: sacd_agent . Episode 222/2000. Number of steps to finish: 19. Loss: 14.760512351989746 Reward: -7.0\nAgent: sacd_agent . Episode 223/2000. Number of steps to finish: 20. Loss: 15.8485689163208 Reward: -10.0\nAgent: sacd_agent . Episode 224/2000. Number of steps to finish: 20. Loss: 14.865034103393555 Reward: -12.0\nAgent: sacd_agent . Episode 225/2000. Number of steps to finish: 20. Loss: 15.400270462036133 Reward: -16.0\nAgent: sacd_agent . Episode 226/2000. Number of steps to finish: 20. Loss: 15.500906944274902 Reward: -10.0\nAgent: sacd_agent . Episode 227/2000. Number of steps to finish: 20. Loss: 16.51411247253418 Reward: -12.0\nAgent: sacd_agent . Episode 228/2000. Number of steps to finish: 20. Loss: 15.039875030517578 Reward: -12.0\nAgent: sacd_agent . Episode 229/2000. Number of steps to finish: 18. Loss: 12.87412166595459 Reward: -6.0\nAgent: sacd_agent . Episode 230/2000. Number of steps to finish: 20. Loss: 15.176011085510254 Reward: -12.0\nAgent: sacd_agent . Episode 231/2000. Number of steps to finish: 20. Loss: 16.15478515625 Reward: -14.0\nAgent: sacd_agent . Episode 232/2000. Number of steps to finish: 19. Loss: 15.24048900604248 Reward: -7.0\nAgent: sacd_agent . Episode 233/2000. Number of steps to finish: 20. Loss: 15.433725357055664 Reward: -10.0\nAgent: sacd_agent . Episode 234/2000. Number of steps to finish: 16. Loss: 12.722341537475586 Reward: -4.0\nAgent: sacd_agent . Episode 235/2000. Number of steps to finish: 15. Loss: 11.394462585449219 Reward: -3.0\nAgent: sacd_agent . Episode 236/2000. Number of steps to finish: 17. Loss: 13.445137977600098 Reward: -5.0\nAgent: sacd_agent . Episode 237/2000. Number of steps to finish: 20. Loss: 15.963616371154785 Reward: -8.0\nAgent: sacd_agent . Episode 238/2000. Number of steps to finish: 20. Loss: 16.74406623840332 Reward: -10.0\nAgent: sacd_agent . Episode 239/2000. Number of steps to finish: 19. Loss: 15.410013198852539 Reward: -7.0\nAgent: sacd_agent . Episode 240/2000. Number of steps to finish: 20. Loss: 16.2782039642334 Reward: -10.0\nAgent: sacd_agent . Episode 241/2000. Number of steps to finish: 20. Loss: 16.389572143554688 Reward: -14.0\nAgent: sacd_agent . Episode 242/2000. Number of steps to finish: 20. Loss: 16.260936737060547 Reward: -10.0\nAgent: sacd_agent . Episode 243/2000. Number of steps to finish: 18. Loss: 15.216273307800293 Reward: -6.0\nAgent: sacd_agent . Episode 244/2000. Number of steps to finish: 20. Loss: 16.06389808654785 Reward: -12.0\nAgent: sacd_agent . Episode 245/2000. Number of steps to finish: 20. Loss: 16.45730209350586 Reward: -12.0\nAgent: sacd_agent . Episode 246/2000. Number of steps to finish: 20. Loss: 16.96055793762207 Reward: -10.0\nAgent: sacd_agent . Episode 247/2000. Number of steps to finish: 20. Loss: 17.2581729888916 Reward: -10.0\nAgent: sacd_agent . Episode 248/2000. Number of steps to finish: 20. Loss: 16.77266502380371 Reward: -10.0\nAgent: sacd_agent . Episode 249/2000. Number of steps to finish: 10. Loss: 8.689157485961914 Reward: 2.0\nAgent: sacd_agent . Episode 250/2000. Number of steps to finish: 20. Loss: 16.079204559326172 Reward: -12.0\nAgent: sacd_agent . Episode 251/2000. Number of steps to finish: 20. Loss: 17.492769241333008 Reward: -14.0\nAgent: sacd_agent . Episode 252/2000. Number of steps to finish: 20. Loss: 16.695558547973633 Reward: -10.0\nAgent: sacd_agent . Episode 253/2000. Number of steps to finish: 20. Loss: 16.741601943969727 Reward: -14.0\nAgent: sacd_agent . Episode 254/2000. Number of steps to finish: 15. Loss: 12.948697090148926 Reward: -3.0\nAgent: sacd_agent . Episode 255/2000. Number of steps to finish: 20. Loss: 18.418352127075195 Reward: -12.0\nAgent: sacd_agent . Episode 256/2000. Number of steps to finish: 18. Loss: 15.66409683227539 Reward: -6.0\nAgent: sacd_agent . Episode 257/2000. Number of steps to finish: 20. Loss: 17.84306526184082 Reward: -12.0\nAgent: sacd_agent . Episode 258/2000. Number of steps to finish: 20. Loss: 17.90129280090332 Reward: -14.0\nAgent: sacd_agent . Episode 259/2000. Number of steps to finish: 20. Loss: 17.769149780273438 Reward: -10.0\nAgent: sacd_agent . Episode 260/2000. Number of steps to finish: 16. Loss: 13.893257141113281 Reward: -4.0\nAgent: sacd_agent . Episode 261/2000. Number of steps to finish: 20. Loss: 17.099611282348633 Reward: -10.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 262/2000. Number of steps to finish: 20. Loss: 17.920360565185547 Reward: -12.0\nAgent: sacd_agent . Episode 263/2000. Number of steps to finish: 17. Loss: 14.46810531616211 Reward: -5.0\nAgent: sacd_agent . Episode 264/2000. Number of steps to finish: 18. Loss: 17.668272018432617 Reward: -6.0\nAgent: sacd_agent . Episode 265/2000. Number of steps to finish: 20. Loss: 17.792627334594727 Reward: -12.0\nAgent: sacd_agent . Episode 266/2000. Number of steps to finish: 16. Loss: 15.619619369506836 Reward: -4.0\nAgent: sacd_agent . Episode 267/2000. Number of steps to finish: 16. Loss: 13.42623519897461 Reward: -4.0\nAgent: sacd_agent . Episode 268/2000. Number of steps to finish: 20. Loss: 18.919084548950195 Reward: -12.0\nAgent: sacd_agent . Episode 269/2000. Number of steps to finish: 20. Loss: 17.822227478027344 Reward: -8.0\nAgent: sacd_agent . Episode 270/2000. Number of steps to finish: 16. Loss: 15.028655052185059 Reward: -4.0\nAgent: sacd_agent . Episode 271/2000. Number of steps to finish: 20. Loss: 17.486106872558594 Reward: -10.0\nAgent: sacd_agent . Episode 272/2000. Number of steps to finish: 20. Loss: 19.047252655029297 Reward: -12.0\nAgent: sacd_agent . Episode 273/2000. Number of steps to finish: 17. Loss: 15.014623641967773 Reward: -5.0\nAgent: sacd_agent . Episode 274/2000. Number of steps to finish: 20. Loss: 18.512826919555664 Reward: -14.0\nAgent: sacd_agent . Episode 275/2000. Number of steps to finish: 20. Loss: 17.80872917175293 Reward: -12.0\nAgent: sacd_agent . Episode 276/2000. Number of steps to finish: 16. Loss: 15.025346755981445 Reward: -4.0\nAgent: sacd_agent . Episode 277/2000. Number of steps to finish: 20. Loss: 18.904695510864258 Reward: -12.0\nAgent: sacd_agent . Episode 278/2000. Number of steps to finish: 20. Loss: 18.931699752807617 Reward: -10.0\nAgent: sacd_agent . Episode 279/2000. Number of steps to finish: 20. Loss: 19.19218635559082 Reward: -10.0\nAgent: sacd_agent . Episode 280/2000. Number of steps to finish: 20. Loss: 18.47030258178711 Reward: -10.0\nAgent: sacd_agent . Episode 281/2000. Number of steps to finish: 18. Loss: 17.748035430908203 Reward: -6.0\nAgent: sacd_agent . Episode 282/2000. Number of steps to finish: 18. Loss: 16.68069839477539 Reward: -6.0\nAgent: sacd_agent . Episode 283/2000. Number of steps to finish: 15. Loss: 14.109050750732422 Reward: -3.0\nAgent: sacd_agent . Episode 284/2000. Number of steps to finish: 15. Loss: 15.311810493469238 Reward: -3.0\nAgent: sacd_agent . Episode 285/2000. Number of steps to finish: 20. Loss: 18.234357833862305 Reward: -10.0\nAgent: sacd_agent . Episode 286/2000. Number of steps to finish: 20. Loss: 19.75258445739746 Reward: -12.0\nAgent: sacd_agent . Episode 287/2000. Number of steps to finish: 20. Loss: 19.467172622680664 Reward: -10.0\nAgent: sacd_agent . Episode 288/2000. Number of steps to finish: 20. Loss: 19.560392379760742 Reward: -12.0\nAgent: sacd_agent . Episode 289/2000. Number of steps to finish: 20. Loss: 19.607643127441406 Reward: -14.0\nAgent: sacd_agent . Episode 290/2000. Number of steps to finish: 20. Loss: 19.919509887695312 Reward: -10.0\nAgent: sacd_agent . Episode 291/2000. Number of steps to finish: 20. Loss: 20.275117874145508 Reward: -14.0\nAgent: sacd_agent . Episode 292/2000. Number of steps to finish: 20. Loss: 21.637033462524414 Reward: -10.0\nAgent: sacd_agent . Episode 293/2000. Number of steps to finish: 20. Loss: 20.027040481567383 Reward: -10.0\nAgent: sacd_agent . Episode 294/2000. Number of steps to finish: 19. Loss: 20.05218505859375 Reward: -7.0\nAgent: sacd_agent . Episode 295/2000. Number of steps to finish: 20. Loss: 20.58959197998047 Reward: -12.0\nAgent: sacd_agent . Episode 296/2000. Number of steps to finish: 20. Loss: 19.13962745666504 Reward: -12.0\nAgent: sacd_agent . Episode 297/2000. Number of steps to finish: 15. Loss: 15.336599349975586 Reward: -3.0\nAgent: sacd_agent . Episode 298/2000. Number of steps to finish: 20. Loss: 19.8973331451416 Reward: -10.0\nAgent: sacd_agent . Episode 299/2000. Number of steps to finish: 20. Loss: 22.688430786132812 Reward: -14.0\nAgent: sacd_agent . Episode 300/2000. Number of steps to finish: 20. Loss: 22.096881866455078 Reward: -10.0\nAgent: sacd_agent . Episode 301/2000. Number of steps to finish: 19. Loss: 18.527013778686523 Reward: -7.0\nAgent: sacd_agent . Episode 302/2000. Number of steps to finish: 20. Loss: 22.143329620361328 Reward: -8.0\nAgent: sacd_agent . Episode 303/2000. Number of steps to finish: 20. Loss: 22.20955467224121 Reward: -14.0\nAgent: sacd_agent . Episode 304/2000. Number of steps to finish: 20. Loss: 19.153675079345703 Reward: -12.0\nAgent: sacd_agent . Episode 305/2000. Number of steps to finish: 20. Loss: 21.840063095092773 Reward: -10.0\nAgent: sacd_agent . Episode 306/2000. Number of steps to finish: 20. Loss: 22.056713104248047 Reward: -12.0\nAgent: sacd_agent . Episode 307/2000. Number of steps to finish: 19. Loss: 18.67140769958496 Reward: -7.0\nAgent: sacd_agent . Episode 308/2000. Number of steps to finish: 11. Loss: 11.59970474243164 Reward: 1.0\nAgent: sacd_agent . Episode 309/2000. Number of steps to finish: 20. Loss: 20.174694061279297 Reward: -12.0\nAgent: sacd_agent . Episode 310/2000. Number of steps to finish: 18. Loss: 18.4454288482666 Reward: -6.0\nAgent: sacd_agent . Episode 311/2000. Number of steps to finish: 8. Loss: 7.4070868492126465 Reward: 4.0\nAgent: sacd_agent . Episode 312/2000. Number of steps to finish: 13. Loss: 14.269874572753906 Reward: -1.0\nAgent: sacd_agent . Episode 313/2000. Number of steps to finish: 15. Loss: 16.85687255859375 Reward: -3.0\nAgent: sacd_agent . Episode 314/2000. Number of steps to finish: 20. Loss: 26.099117279052734 Reward: -12.0\nAgent: sacd_agent . Episode 315/2000. Number of steps to finish: 17. Loss: 19.87053108215332 Reward: -5.0\nAgent: sacd_agent . Episode 316/2000. Number of steps to finish: 20. Loss: 24.459726333618164 Reward: -8.0\nAgent: sacd_agent . Episode 317/2000. Number of steps to finish: 20. Loss: 23.378215789794922 Reward: -14.0\nAgent: sacd_agent . Episode 318/2000. Number of steps to finish: 20. Loss: 23.147090911865234 Reward: -10.0\nAgent: sacd_agent . Episode 319/2000. Number of steps to finish: 20. Loss: 21.666486740112305 Reward: -14.0\nAgent: sacd_agent . Episode 320/2000. Number of steps to finish: 15. Loss: 15.85385513305664 Reward: -3.0\nAgent: sacd_agent . Episode 321/2000. Number of steps to finish: 20. Loss: 22.897201538085938 Reward: -12.0\nAgent: sacd_agent . Episode 322/2000. Number of steps to finish: 20. Loss: 22.884029388427734 Reward: -10.0\nAgent: sacd_agent . Episode 323/2000. Number of steps to finish: 20. Loss: 21.153717041015625 Reward: -12.0\nAgent: sacd_agent . Episode 324/2000. Number of steps to finish: 20. Loss: 23.310501098632812 Reward: -14.0\nAgent: sacd_agent . Episode 325/2000. Number of steps to finish: 19. Loss: 22.220497131347656 Reward: -7.0\nAgent: sacd_agent . Episode 326/2000. Number of steps to finish: 16. Loss: 18.89742088317871 Reward: -4.0\nAgent: sacd_agent . Episode 327/2000. Number of steps to finish: 20. Loss: 25.490854263305664 Reward: -14.0\nAgent: sacd_agent . Episode 328/2000. Number of steps to finish: 20. Loss: 24.418132781982422 Reward: -12.0\nAgent: sacd_agent . Episode 329/2000. Number of steps to finish: 16. Loss: 22.562061309814453 Reward: -4.0\nAgent: sacd_agent . Episode 330/2000. Number of steps to finish: 20. Loss: 23.349531173706055 Reward: -10.0\nAgent: sacd_agent . Episode 331/2000. Number of steps to finish: 19. Loss: 22.954633712768555 Reward: -7.0\nAgent: sacd_agent . Episode 332/2000. Number of steps to finish: 20. Loss: 23.47348403930664 Reward: -10.0\nAgent: sacd_agent . Episode 333/2000. Number of steps to finish: 20. Loss: 26.771381378173828 Reward: -14.0\nAgent: sacd_agent . Episode 334/2000. Number of steps to finish: 20. Loss: 27.17852020263672 Reward: -10.0\nAgent: sacd_agent . Episode 335/2000. Number of steps to finish: 20. Loss: 24.552867889404297 Reward: -14.0\nAgent: sacd_agent . Episode 336/2000. Number of steps to finish: 20. Loss: 25.525117874145508 Reward: -12.0\nAgent: sacd_agent . Episode 337/2000. Number of steps to finish: 11. Loss: 15.277787208557129 Reward: 1.0\nAgent: sacd_agent . Episode 338/2000. Number of steps to finish: 20. Loss: 23.982213973999023 Reward: -10.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 339/2000. Number of steps to finish: 20. Loss: 23.48145866394043 Reward: -10.0\nAgent: sacd_agent . Episode 340/2000. Number of steps to finish: 15. Loss: 18.407873153686523 Reward: -3.0\nAgent: sacd_agent . Episode 341/2000. Number of steps to finish: 12. Loss: 15.277338027954102 Reward: 0.0\nAgent: sacd_agent . Episode 342/2000. Number of steps to finish: 16. Loss: 20.401283264160156 Reward: -4.0\nAgent: sacd_agent . Episode 343/2000. Number of steps to finish: 20. Loss: 23.951988220214844 Reward: -12.0\nAgent: sacd_agent . Episode 344/2000. Number of steps to finish: 20. Loss: 25.04131317138672 Reward: -10.0\nAgent: sacd_agent . Episode 345/2000. Number of steps to finish: 14. Loss: 18.116907119750977 Reward: -2.0\nAgent: sacd_agent . Episode 346/2000. Number of steps to finish: 16. Loss: 19.840045928955078 Reward: -4.0\nAgent: sacd_agent . Episode 347/2000. Number of steps to finish: 20. Loss: 26.523170471191406 Reward: -14.0\nAgent: sacd_agent . Episode 348/2000. Number of steps to finish: 20. Loss: 25.09776496887207 Reward: -14.0\nAgent: sacd_agent . Episode 349/2000. Number of steps to finish: 19. Loss: 24.79657554626465 Reward: -7.0\nAgent: sacd_agent . Episode 350/2000. Number of steps to finish: 17. Loss: 22.390186309814453 Reward: -5.0\nAgent: sacd_agent . Episode 351/2000. Number of steps to finish: 20. Loss: 26.695695877075195 Reward: -10.0\nAgent: sacd_agent . Episode 352/2000. Number of steps to finish: 20. Loss: 26.423933029174805 Reward: -8.0\nAgent: sacd_agent . Episode 353/2000. Number of steps to finish: 20. Loss: 26.73897361755371 Reward: -12.0\nAgent: sacd_agent . Episode 354/2000. Number of steps to finish: 8. Loss: 10.308429718017578 Reward: 4.0\nAgent: sacd_agent . Episode 355/2000. Number of steps to finish: 20. Loss: 25.081146240234375 Reward: -8.0\nAgent: sacd_agent . Episode 356/2000. Number of steps to finish: 17. Loss: 22.06972885131836 Reward: -5.0\nAgent: sacd_agent . Episode 357/2000. Number of steps to finish: 18. Loss: 24.806331634521484 Reward: -6.0\nAgent: sacd_agent . Episode 358/2000. Number of steps to finish: 20. Loss: 26.878616333007812 Reward: -10.0\nAgent: sacd_agent . Episode 359/2000. Number of steps to finish: 20. Loss: 27.28885841369629 Reward: -10.0\nAgent: sacd_agent . Episode 360/2000. Number of steps to finish: 20. Loss: 26.83363151550293 Reward: -8.0\nAgent: sacd_agent . Episode 361/2000. Number of steps to finish: 20. Loss: 28.187976837158203 Reward: -10.0\nAgent: sacd_agent . Episode 362/2000. Number of steps to finish: 20. Loss: 29.6492919921875 Reward: -10.0\nAgent: sacd_agent . Episode 363/2000. Number of steps to finish: 20. Loss: 30.132484436035156 Reward: -10.0\nAgent: sacd_agent . Episode 364/2000. Number of steps to finish: 12. Loss: 15.129067420959473 Reward: 0.0\nAgent: sacd_agent . Episode 365/2000. Number of steps to finish: 12. Loss: 16.099390029907227 Reward: 0.0\nAgent: sacd_agent . Episode 366/2000. Number of steps to finish: 13. Loss: 21.5520076751709 Reward: -1.0\nAgent: sacd_agent . Episode 367/2000. Number of steps to finish: 20. Loss: 28.49506187438965 Reward: -10.0\nAgent: sacd_agent . Episode 368/2000. Number of steps to finish: 20. Loss: 30.665117263793945 Reward: -10.0\nAgent: sacd_agent . Episode 369/2000. Number of steps to finish: 20. Loss: 27.220279693603516 Reward: -8.0\nAgent: sacd_agent . Episode 370/2000. Number of steps to finish: 16. Loss: 25.01011085510254 Reward: -4.0\nAgent: sacd_agent . Episode 371/2000. Number of steps to finish: 20. Loss: 31.137983322143555 Reward: -12.0\nAgent: sacd_agent . Episode 372/2000. Number of steps to finish: 20. Loss: 31.86267852783203 Reward: -14.0\nAgent: sacd_agent . Episode 373/2000. Number of steps to finish: 20. Loss: 28.504552841186523 Reward: -12.0\nAgent: sacd_agent . Episode 374/2000. Number of steps to finish: 20. Loss: 28.481409072875977 Reward: -14.0\nAgent: sacd_agent . Episode 375/2000. Number of steps to finish: 16. Loss: 21.194259643554688 Reward: -4.0\nAgent: sacd_agent . Episode 376/2000. Number of steps to finish: 20. Loss: 29.799543380737305 Reward: -10.0\nAgent: sacd_agent . Episode 377/2000. Number of steps to finish: 20. Loss: 33.43199920654297 Reward: -12.0\nAgent: sacd_agent . Episode 378/2000. Number of steps to finish: 20. Loss: 26.19124412536621 Reward: -14.0\nAgent: sacd_agent . Episode 379/2000. Number of steps to finish: 16. Loss: 26.252723693847656 Reward: -4.0\nAgent: sacd_agent . Episode 380/2000. Number of steps to finish: 10. Loss: 13.61248779296875 Reward: 2.0\nAgent: sacd_agent . Episode 381/2000. Number of steps to finish: 10. Loss: 14.728273391723633 Reward: 2.0\nAgent: sacd_agent . Episode 382/2000. Number of steps to finish: 13. Loss: 19.824140548706055 Reward: -1.0\nAgent: sacd_agent . Episode 383/2000. Number of steps to finish: 20. Loss: 32.3204231262207 Reward: -14.0\nAgent: sacd_agent . Episode 384/2000. Number of steps to finish: 20. Loss: 33.165287017822266 Reward: -10.0\nAgent: sacd_agent . Episode 385/2000. Number of steps to finish: 13. Loss: 18.060007095336914 Reward: -1.0\nAgent: sacd_agent . Episode 386/2000. Number of steps to finish: 20. Loss: 31.545412063598633 Reward: -10.0\nAgent: sacd_agent . Episode 387/2000. Number of steps to finish: 13. Loss: 23.841793060302734 Reward: -1.0\nAgent: sacd_agent . Episode 388/2000. Number of steps to finish: 20. Loss: 30.45466423034668 Reward: -14.0\nAgent: sacd_agent . Episode 389/2000. Number of steps to finish: 11. Loss: 18.128440856933594 Reward: 1.0\nAgent: sacd_agent . Episode 390/2000. Number of steps to finish: 17. Loss: 27.327224731445312 Reward: -5.0\nAgent: sacd_agent . Episode 391/2000. Number of steps to finish: 20. Loss: 31.80756378173828 Reward: -12.0\nAgent: sacd_agent . Episode 392/2000. Number of steps to finish: 8. Loss: 14.59013843536377 Reward: 4.0\nAgent: sacd_agent . Episode 393/2000. Number of steps to finish: 11. Loss: 18.27381134033203 Reward: 1.0\nAgent: sacd_agent . Episode 394/2000. Number of steps to finish: 10. Loss: 15.83981704711914 Reward: 2.0\nAgent: sacd_agent . Episode 395/2000. Number of steps to finish: 12. Loss: 20.32955551147461 Reward: 0.0\nAgent: sacd_agent . Episode 396/2000. Number of steps to finish: 17. Loss: 26.496620178222656 Reward: -5.0\nAgent: sacd_agent . Episode 397/2000. Number of steps to finish: 18. Loss: 29.28772735595703 Reward: -6.0\nAgent: sacd_agent . Episode 398/2000. Number of steps to finish: 20. Loss: 33.94068145751953 Reward: -14.0\nAgent: sacd_agent . Episode 399/2000. Number of steps to finish: 20. Loss: 28.814311981201172 Reward: -10.0\nAgent: sacd_agent . Episode 400/2000. Number of steps to finish: 11. Loss: 18.851238250732422 Reward: 1.0\nAgent: sacd_agent . Episode 401/2000. Number of steps to finish: 19. Loss: 34.80125427246094 Reward: -7.0\nAgent: sacd_agent . Episode 402/2000. Number of steps to finish: 20. Loss: 38.119380950927734 Reward: -12.0\nAgent: sacd_agent . Episode 403/2000. Number of steps to finish: 18. Loss: 31.755207061767578 Reward: -6.0\nAgent: sacd_agent . Episode 404/2000. Number of steps to finish: 20. Loss: 33.14930725097656 Reward: -16.0\nAgent: sacd_agent . Episode 405/2000. Number of steps to finish: 20. Loss: 32.611392974853516 Reward: -12.0\nAgent: sacd_agent . Episode 406/2000. Number of steps to finish: 20. Loss: 35.46769714355469 Reward: -12.0\nAgent: sacd_agent . Episode 407/2000. Number of steps to finish: 20. Loss: 32.43269348144531 Reward: -10.0\nAgent: sacd_agent . Episode 408/2000. Number of steps to finish: 16. Loss: 28.799222946166992 Reward: -4.0\nAgent: sacd_agent . Episode 409/2000. Number of steps to finish: 20. Loss: 32.6135139465332 Reward: -14.0\nAgent: sacd_agent . Episode 410/2000. Number of steps to finish: 12. Loss: 21.425357818603516 Reward: 0.0\nAgent: sacd_agent . Episode 411/2000. Number of steps to finish: 20. Loss: 34.606868743896484 Reward: -10.0\nAgent: sacd_agent . Episode 412/2000. Number of steps to finish: 20. Loss: 34.190223693847656 Reward: -10.0\nAgent: sacd_agent . Episode 413/2000. Number of steps to finish: 20. Loss: 40.09379577636719 Reward: -10.0\nAgent: sacd_agent . Episode 414/2000. Number of steps to finish: 20. Loss: 36.01510238647461 Reward: -10.0\nAgent: sacd_agent . Episode 415/2000. Number of steps to finish: 11. Loss: 19.212390899658203 Reward: 1.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 416/2000. Number of steps to finish: 20. Loss: 35.122249603271484 Reward: -10.0\nAgent: sacd_agent . Episode 417/2000. Number of steps to finish: 15. Loss: 29.337480545043945 Reward: -3.0\nAgent: sacd_agent . Episode 418/2000. Number of steps to finish: 20. Loss: 36.19668960571289 Reward: -10.0\nAgent: sacd_agent . Episode 419/2000. Number of steps to finish: 20. Loss: 36.820159912109375 Reward: -10.0\nAgent: sacd_agent . Episode 420/2000. Number of steps to finish: 20. Loss: 34.979148864746094 Reward: -10.0\nAgent: sacd_agent . Episode 421/2000. Number of steps to finish: 19. Loss: 38.140899658203125 Reward: -7.0\nAgent: sacd_agent . Episode 422/2000. Number of steps to finish: 20. Loss: 32.381988525390625 Reward: -12.0\nAgent: sacd_agent . Episode 423/2000. Number of steps to finish: 20. Loss: 34.25740432739258 Reward: -14.0\nAgent: sacd_agent . Episode 424/2000. Number of steps to finish: 11. Loss: 18.38477325439453 Reward: 1.0\nAgent: sacd_agent . Episode 425/2000. Number of steps to finish: 20. Loss: 36.6021614074707 Reward: -10.0\nAgent: sacd_agent . Episode 426/2000. Number of steps to finish: 20. Loss: 39.48904037475586 Reward: -12.0\nAgent: sacd_agent . Episode 427/2000. Number of steps to finish: 14. Loss: 29.5019474029541 Reward: -2.0\nAgent: sacd_agent . Episode 428/2000. Number of steps to finish: 10. Loss: 18.925479888916016 Reward: 2.0\nAgent: sacd_agent . Episode 429/2000. Number of steps to finish: 16. Loss: 30.92427635192871 Reward: -4.0\nAgent: sacd_agent . Episode 430/2000. Number of steps to finish: 20. Loss: 36.15263748168945 Reward: -10.0\nAgent: sacd_agent . Episode 431/2000. Number of steps to finish: 20. Loss: 41.09088897705078 Reward: -14.0\nAgent: sacd_agent . Episode 432/2000. Number of steps to finish: 19. Loss: 36.447242736816406 Reward: -7.0\nAgent: sacd_agent . Episode 433/2000. Number of steps to finish: 13. Loss: 26.09084129333496 Reward: -1.0\nAgent: sacd_agent . Episode 434/2000. Number of steps to finish: 16. Loss: 24.734283447265625 Reward: -4.0\nAgent: sacd_agent . Episode 435/2000. Number of steps to finish: 20. Loss: 39.79720687866211 Reward: -16.0\nAgent: sacd_agent . Episode 436/2000. Number of steps to finish: 20. Loss: 34.5837516784668 Reward: -10.0\nAgent: sacd_agent . Episode 437/2000. Number of steps to finish: 20. Loss: 40.94831085205078 Reward: -12.0\nAgent: sacd_agent . Episode 438/2000. Number of steps to finish: 20. Loss: 37.67402648925781 Reward: -16.0\nAgent: sacd_agent . Episode 439/2000. Number of steps to finish: 14. Loss: 30.14650535583496 Reward: -2.0\nAgent: sacd_agent . Episode 440/2000. Number of steps to finish: 17. Loss: 35.413692474365234 Reward: -5.0\nAgent: sacd_agent . Episode 441/2000. Number of steps to finish: 20. Loss: 42.53300476074219 Reward: -12.0\nAgent: sacd_agent . Episode 442/2000. Number of steps to finish: 20. Loss: 42.787410736083984 Reward: -8.0\nAgent: sacd_agent . Episode 443/2000. Number of steps to finish: 20. Loss: 43.166595458984375 Reward: -12.0\nAgent: sacd_agent . Episode 444/2000. Number of steps to finish: 16. Loss: 30.831825256347656 Reward: -4.0\nAgent: sacd_agent . Episode 445/2000. Number of steps to finish: 9. Loss: 20.602252960205078 Reward: 3.0\nAgent: sacd_agent . Episode 446/2000. Number of steps to finish: 12. Loss: 27.315113067626953 Reward: 0.0\nAgent: sacd_agent . Episode 447/2000. Number of steps to finish: 20. Loss: 38.92502975463867 Reward: -10.0\nAgent: sacd_agent . Episode 448/2000. Number of steps to finish: 14. Loss: 27.93194007873535 Reward: -2.0\nAgent: sacd_agent . Episode 449/2000. Number of steps to finish: 17. Loss: 39.57678985595703 Reward: -5.0\nAgent: sacd_agent . Episode 450/2000. Number of steps to finish: 20. Loss: 38.12948989868164 Reward: -14.0\nAgent: sacd_agent . Episode 451/2000. Number of steps to finish: 20. Loss: 45.358436584472656 Reward: -10.0\nAgent: sacd_agent . Episode 452/2000. Number of steps to finish: 16. Loss: 35.620182037353516 Reward: -4.0\nAgent: sacd_agent . Episode 453/2000. Number of steps to finish: 15. Loss: 31.667837142944336 Reward: -3.0\nAgent: sacd_agent . Episode 454/2000. Number of steps to finish: 15. Loss: 32.00713348388672 Reward: -3.0\nAgent: sacd_agent . Episode 455/2000. Number of steps to finish: 17. Loss: 33.0805778503418 Reward: -5.0\nAgent: sacd_agent . Episode 456/2000. Number of steps to finish: 20. Loss: 43.171485900878906 Reward: -8.0\nAgent: sacd_agent . Episode 457/2000. Number of steps to finish: 20. Loss: 46.74441146850586 Reward: -10.0\nAgent: sacd_agent . Episode 458/2000. Number of steps to finish: 20. Loss: 45.62529754638672 Reward: -10.0\nAgent: sacd_agent . Episode 459/2000. Number of steps to finish: 20. Loss: 43.099754333496094 Reward: -10.0\nAgent: sacd_agent . Episode 460/2000. Number of steps to finish: 17. Loss: 41.53844451904297 Reward: -5.0\nAgent: sacd_agent . Episode 461/2000. Number of steps to finish: 13. Loss: 26.18971824645996 Reward: -1.0\nAgent: sacd_agent . Episode 462/2000. Number of steps to finish: 20. Loss: 46.307640075683594 Reward: -10.0\nAgent: sacd_agent . Episode 463/2000. Number of steps to finish: 20. Loss: 41.37086486816406 Reward: -10.0\nAgent: sacd_agent . Episode 464/2000. Number of steps to finish: 20. Loss: 40.743167877197266 Reward: -10.0\nAgent: sacd_agent . Episode 465/2000. Number of steps to finish: 15. Loss: 35.4891357421875 Reward: -3.0\nAgent: sacd_agent . Episode 466/2000. Number of steps to finish: 12. Loss: 27.510971069335938 Reward: 0.0\nAgent: sacd_agent . Episode 467/2000. Number of steps to finish: 20. Loss: 43.55173873901367 Reward: -12.0\nAgent: sacd_agent . Episode 468/2000. Number of steps to finish: 18. Loss: 42.499900817871094 Reward: -6.0\nAgent: sacd_agent . Episode 469/2000. Number of steps to finish: 20. Loss: 42.76433563232422 Reward: -10.0\nAgent: sacd_agent . Episode 470/2000. Number of steps to finish: 20. Loss: 45.67374038696289 Reward: -8.0\nAgent: sacd_agent . Episode 471/2000. Number of steps to finish: 20. Loss: 42.948081970214844 Reward: -10.0\nAgent: sacd_agent . Episode 472/2000. Number of steps to finish: 20. Loss: 45.280296325683594 Reward: -10.0\nAgent: sacd_agent . Episode 473/2000. Number of steps to finish: 18. Loss: 38.23916244506836 Reward: -6.0\nAgent: sacd_agent . Episode 474/2000. Number of steps to finish: 20. Loss: 50.05546951293945 Reward: -10.0\nAgent: sacd_agent . Episode 475/2000. Number of steps to finish: 11. Loss: 24.39873504638672 Reward: 1.0\nAgent: sacd_agent . Episode 476/2000. Number of steps to finish: 20. Loss: 50.3400993347168 Reward: -12.0\nAgent: sacd_agent . Episode 477/2000. Number of steps to finish: 12. Loss: 23.979415893554688 Reward: 0.0\nAgent: sacd_agent . Episode 478/2000. Number of steps to finish: 20. Loss: 44.97166061401367 Reward: -14.0\nAgent: sacd_agent . Episode 479/2000. Number of steps to finish: 18. Loss: 42.50638198852539 Reward: -6.0\nAgent: sacd_agent . Episode 480/2000. Number of steps to finish: 18. Loss: 44.49616622924805 Reward: -6.0\nAgent: sacd_agent . Episode 481/2000. Number of steps to finish: 20. Loss: 42.08100509643555 Reward: -12.0\nAgent: sacd_agent . Episode 482/2000. Number of steps to finish: 20. Loss: 50.26116180419922 Reward: -8.0\nAgent: sacd_agent . Episode 483/2000. Number of steps to finish: 14. Loss: 29.275957107543945 Reward: -2.0\nAgent: sacd_agent . Episode 484/2000. Number of steps to finish: 20. Loss: 50.56644058227539 Reward: -12.0\nAgent: sacd_agent . Episode 485/2000. Number of steps to finish: 15. Loss: 32.55387878417969 Reward: -3.0\nAgent: sacd_agent . Episode 486/2000. Number of steps to finish: 20. Loss: 59.64082336425781 Reward: -16.0\nAgent: sacd_agent . Episode 487/2000. Number of steps to finish: 20. Loss: 51.58965301513672 Reward: -10.0\nAgent: sacd_agent . Episode 488/2000. Number of steps to finish: 20. Loss: 48.95197296142578 Reward: -10.0\nAgent: sacd_agent . Episode 489/2000. Number of steps to finish: 20. Loss: 48.20948791503906 Reward: -10.0\nAgent: sacd_agent . Episode 490/2000. Number of steps to finish: 18. Loss: 44.1900749206543 Reward: -6.0\nAgent: sacd_agent . Episode 491/2000. Number of steps to finish: 20. Loss: 49.33757019042969 Reward: -10.0\nAgent: sacd_agent . Episode 492/2000. Number of steps to finish: 14. Loss: 31.721826553344727 Reward: -2.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 493/2000. Number of steps to finish: 10. Loss: 19.081161499023438 Reward: 2.0\nAgent: sacd_agent . Episode 494/2000. Number of steps to finish: 20. Loss: 48.48780059814453 Reward: -12.0\nAgent: sacd_agent . Episode 495/2000. Number of steps to finish: 20. Loss: 44.75620651245117 Reward: -8.0\nAgent: sacd_agent . Episode 496/2000. Number of steps to finish: 18. Loss: 44.09454345703125 Reward: -6.0\nAgent: sacd_agent . Episode 497/2000. Number of steps to finish: 20. Loss: 53.643943786621094 Reward: -14.0\nAgent: sacd_agent . Episode 498/2000. Number of steps to finish: 20. Loss: 47.18601608276367 Reward: -12.0\nAgent: sacd_agent . Episode 499/2000. Number of steps to finish: 20. Loss: 47.404747009277344 Reward: -10.0\nAgent: sacd_agent . Episode 500/2000. Number of steps to finish: 10. Loss: 23.930072784423828 Reward: 2.0\nAgent: sacd_agent . Episode 501/2000. Number of steps to finish: 20. Loss: 49.0836296081543 Reward: -10.0\nAgent: sacd_agent . Episode 502/2000. Number of steps to finish: 20. Loss: 52.72128677368164 Reward: -12.0\nAgent: sacd_agent . Episode 503/2000. Number of steps to finish: 16. Loss: 42.171180725097656 Reward: -4.0\nAgent: sacd_agent . Episode 504/2000. Number of steps to finish: 20. Loss: 56.956172943115234 Reward: -8.0\nAgent: sacd_agent . Episode 505/2000. Number of steps to finish: 13. Loss: 34.33033752441406 Reward: -1.0\nAgent: sacd_agent . Episode 506/2000. Number of steps to finish: 16. Loss: 41.570804595947266 Reward: -4.0\nAgent: sacd_agent . Episode 507/2000. Number of steps to finish: 20. Loss: 51.60450744628906 Reward: -8.0\nAgent: sacd_agent . Episode 508/2000. Number of steps to finish: 20. Loss: 53.36181640625 Reward: -10.0\nAgent: sacd_agent . Episode 509/2000. Number of steps to finish: 20. Loss: 52.89253616333008 Reward: -10.0\nAgent: sacd_agent . Episode 510/2000. Number of steps to finish: 20. Loss: 61.32601547241211 Reward: -10.0\nAgent: sacd_agent . Episode 511/2000. Number of steps to finish: 19. Loss: 45.8227653503418 Reward: -7.0\nAgent: sacd_agent . Episode 512/2000. Number of steps to finish: 15. Loss: 41.78947830200195 Reward: -3.0\nAgent: sacd_agent . Episode 513/2000. Number of steps to finish: 20. Loss: 50.647735595703125 Reward: -18.0\nAgent: sacd_agent . Episode 514/2000. Number of steps to finish: 17. Loss: 42.22972869873047 Reward: -5.0\nAgent: sacd_agent . Episode 515/2000. Number of steps to finish: 11. Loss: 28.780773162841797 Reward: 1.0\nAgent: sacd_agent . Episode 516/2000. Number of steps to finish: 20. Loss: 50.419795989990234 Reward: -10.0\nAgent: sacd_agent . Episode 517/2000. Number of steps to finish: 20. Loss: 52.74614715576172 Reward: -10.0\nAgent: sacd_agent . Episode 518/2000. Number of steps to finish: 20. Loss: 53.1181640625 Reward: -12.0\nAgent: sacd_agent . Episode 519/2000. Number of steps to finish: 20. Loss: 59.19496154785156 Reward: -8.0\nAgent: sacd_agent . Episode 520/2000. Number of steps to finish: 17. Loss: 48.39185333251953 Reward: -5.0\nAgent: sacd_agent . Episode 521/2000. Number of steps to finish: 20. Loss: 52.48112869262695 Reward: -10.0\nAgent: sacd_agent . Episode 522/2000. Number of steps to finish: 20. Loss: 59.7034797668457 Reward: -10.0\nAgent: sacd_agent . Episode 523/2000. Number of steps to finish: 17. Loss: 50.21314239501953 Reward: -5.0\nAgent: sacd_agent . Episode 524/2000. Number of steps to finish: 15. Loss: 44.149635314941406 Reward: -3.0\nAgent: sacd_agent . Episode 525/2000. Number of steps to finish: 9. Loss: 24.707683563232422 Reward: 3.0\nAgent: sacd_agent . Episode 526/2000. Number of steps to finish: 15. Loss: 38.509368896484375 Reward: -3.0\nAgent: sacd_agent . Episode 527/2000. Number of steps to finish: 20. Loss: 59.886390686035156 Reward: -12.0\nAgent: sacd_agent . Episode 528/2000. Number of steps to finish: 20. Loss: 57.157161712646484 Reward: -8.0\nAgent: sacd_agent . Episode 529/2000. Number of steps to finish: 20. Loss: 57.960121154785156 Reward: -12.0\nAgent: sacd_agent . Episode 530/2000. Number of steps to finish: 20. Loss: 60.26328659057617 Reward: -8.0\nAgent: sacd_agent . Episode 531/2000. Number of steps to finish: 20. Loss: 50.93374252319336 Reward: -10.0\nAgent: sacd_agent . Episode 532/2000. Number of steps to finish: 20. Loss: 58.30571746826172 Reward: -10.0\nAgent: sacd_agent . Episode 533/2000. Number of steps to finish: 20. Loss: 51.40138626098633 Reward: -12.0\nAgent: sacd_agent . Episode 534/2000. Number of steps to finish: 20. Loss: 55.46121597290039 Reward: -10.0\nAgent: sacd_agent . Episode 535/2000. Number of steps to finish: 19. Loss: 50.51972579956055 Reward: -7.0\nAgent: sacd_agent . Episode 536/2000. Number of steps to finish: 20. Loss: 63.24024963378906 Reward: -10.0\nAgent: sacd_agent . Episode 537/2000. Number of steps to finish: 19. Loss: 56.46131134033203 Reward: -7.0\nAgent: sacd_agent . Episode 538/2000. Number of steps to finish: 18. Loss: 59.3015022277832 Reward: -6.0\nAgent: sacd_agent . Episode 539/2000. Number of steps to finish: 19. Loss: 64.00044250488281 Reward: -7.0\nAgent: sacd_agent . Episode 540/2000. Number of steps to finish: 20. Loss: 56.419349670410156 Reward: -10.0\nAgent: sacd_agent . Episode 541/2000. Number of steps to finish: 15. Loss: 45.44502639770508 Reward: -3.0\nAgent: sacd_agent . Episode 542/2000. Number of steps to finish: 20. Loss: 60.847923278808594 Reward: -12.0\nAgent: sacd_agent . Episode 543/2000. Number of steps to finish: 20. Loss: 62.288753509521484 Reward: -8.0\nAgent: sacd_agent . Episode 544/2000. Number of steps to finish: 20. Loss: 63.79576873779297 Reward: -10.0\nAgent: sacd_agent . Episode 545/2000. Number of steps to finish: 20. Loss: 68.19613647460938 Reward: -10.0\nAgent: sacd_agent . Episode 546/2000. Number of steps to finish: 19. Loss: 57.79737091064453 Reward: -7.0\nAgent: sacd_agent . Episode 547/2000. Number of steps to finish: 18. Loss: 62.335479736328125 Reward: -6.0\nAgent: sacd_agent . Episode 548/2000. Number of steps to finish: 20. Loss: 58.268856048583984 Reward: -10.0\nAgent: sacd_agent . Episode 549/2000. Number of steps to finish: 18. Loss: 61.674171447753906 Reward: -6.0\nAgent: sacd_agent . Episode 550/2000. Number of steps to finish: 20. Loss: 60.088050842285156 Reward: -10.0\nAgent: sacd_agent . Episode 551/2000. Number of steps to finish: 20. Loss: 67.21908569335938 Reward: -12.0\nAgent: sacd_agent . Episode 552/2000. Number of steps to finish: 15. Loss: 47.933963775634766 Reward: -3.0\nAgent: sacd_agent . Episode 553/2000. Number of steps to finish: 18. Loss: 56.96307373046875 Reward: -6.0\nAgent: sacd_agent . Episode 554/2000. Number of steps to finish: 16. Loss: 49.07989501953125 Reward: -4.0\nAgent: sacd_agent . Episode 555/2000. Number of steps to finish: 20. Loss: 70.48159790039062 Reward: -10.0\nAgent: sacd_agent . Episode 556/2000. Number of steps to finish: 17. Loss: 53.5864372253418 Reward: -5.0\nAgent: sacd_agent . Episode 557/2000. Number of steps to finish: 20. Loss: 62.36252212524414 Reward: -10.0\nAgent: sacd_agent . Episode 558/2000. Number of steps to finish: 17. Loss: 55.44441604614258 Reward: -5.0\nAgent: sacd_agent . Episode 559/2000. Number of steps to finish: 12. Loss: 35.76243209838867 Reward: 0.0\nAgent: sacd_agent . Episode 560/2000. Number of steps to finish: 14. Loss: 43.63901138305664 Reward: -2.0\nAgent: sacd_agent . Episode 561/2000. Number of steps to finish: 20. Loss: 64.20088195800781 Reward: -12.0\nAgent: sacd_agent . Episode 562/2000. Number of steps to finish: 19. Loss: 64.55166625976562 Reward: -7.0\nAgent: sacd_agent . Episode 563/2000. Number of steps to finish: 20. Loss: 66.59825897216797 Reward: -10.0\nAgent: sacd_agent . Episode 564/2000. Number of steps to finish: 15. Loss: 52.22470474243164 Reward: -3.0\nAgent: sacd_agent . Episode 565/2000. Number of steps to finish: 15. Loss: 48.24161148071289 Reward: -3.0\nAgent: sacd_agent . Episode 566/2000. Number of steps to finish: 20. Loss: 64.51298522949219 Reward: -12.0\nAgent: sacd_agent . Episode 567/2000. Number of steps to finish: 20. Loss: 69.59589385986328 Reward: -12.0\nAgent: sacd_agent . Episode 568/2000. Number of steps to finish: 11. Loss: 40.503780364990234 Reward: 1.0\nAgent: sacd_agent . Episode 569/2000. Number of steps to finish: 20. Loss: 68.96533966064453 Reward: -14.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 570/2000. Number of steps to finish: 14. Loss: 41.89027404785156 Reward: -2.0\nAgent: sacd_agent . Episode 571/2000. Number of steps to finish: 20. Loss: 67.0359115600586 Reward: -10.0\nAgent: sacd_agent . Episode 572/2000. Number of steps to finish: 20. Loss: 66.8360595703125 Reward: -14.0\nAgent: sacd_agent . Episode 573/2000. Number of steps to finish: 19. Loss: 63.98932647705078 Reward: -7.0\nAgent: sacd_agent . Episode 574/2000. Number of steps to finish: 20. Loss: 69.25785827636719 Reward: -10.0\nAgent: sacd_agent . Episode 575/2000. Number of steps to finish: 19. Loss: 68.6243896484375 Reward: -7.0\nAgent: sacd_agent . Episode 576/2000. Number of steps to finish: 9. Loss: 37.583988189697266 Reward: 3.0\nAgent: sacd_agent . Episode 577/2000. Number of steps to finish: 20. Loss: 65.10277557373047 Reward: -10.0\nAgent: sacd_agent . Episode 578/2000. Number of steps to finish: 20. Loss: 69.01419067382812 Reward: -10.0\nAgent: sacd_agent . Episode 579/2000. Number of steps to finish: 16. Loss: 61.238677978515625 Reward: -4.0\nAgent: sacd_agent . Episode 580/2000. Number of steps to finish: 19. Loss: 59.97815704345703 Reward: -7.0\nAgent: sacd_agent . Episode 581/2000. Number of steps to finish: 10. Loss: 33.605133056640625 Reward: 2.0\nAgent: sacd_agent . Episode 582/2000. Number of steps to finish: 20. Loss: 58.38671112060547 Reward: -12.0\nAgent: sacd_agent . Episode 583/2000. Number of steps to finish: 18. Loss: 54.16217041015625 Reward: -6.0\nAgent: sacd_agent . Episode 584/2000. Number of steps to finish: 14. Loss: 55.014495849609375 Reward: -2.0\nAgent: sacd_agent . Episode 585/2000. Number of steps to finish: 20. Loss: 75.78460693359375 Reward: -12.0\nAgent: sacd_agent . Episode 586/2000. Number of steps to finish: 20. Loss: 66.72721862792969 Reward: -14.0\nAgent: sacd_agent . Episode 587/2000. Number of steps to finish: 15. Loss: 47.784324645996094 Reward: -3.0\nAgent: sacd_agent . Episode 588/2000. Number of steps to finish: 20. Loss: 78.45169830322266 Reward: -12.0\nAgent: sacd_agent . Episode 589/2000. Number of steps to finish: 9. Loss: 33.798545837402344 Reward: 3.0\nAgent: sacd_agent . Episode 590/2000. Number of steps to finish: 16. Loss: 54.5562629699707 Reward: -4.0\nAgent: sacd_agent . Episode 591/2000. Number of steps to finish: 20. Loss: 70.18209075927734 Reward: -10.0\nAgent: sacd_agent . Episode 592/2000. Number of steps to finish: 20. Loss: 73.07915496826172 Reward: -8.0\nAgent: sacd_agent . Episode 593/2000. Number of steps to finish: 18. Loss: 61.38591384887695 Reward: -6.0\nAgent: sacd_agent . Episode 594/2000. Number of steps to finish: 20. Loss: 78.99800109863281 Reward: -12.0\nAgent: sacd_agent . Episode 595/2000. Number of steps to finish: 20. Loss: 77.52528381347656 Reward: -8.0\nAgent: sacd_agent . Episode 596/2000. Number of steps to finish: 20. Loss: 73.18148040771484 Reward: -14.0\nAgent: sacd_agent . Episode 597/2000. Number of steps to finish: 13. Loss: 47.066707611083984 Reward: -1.0\nAgent: sacd_agent . Episode 598/2000. Number of steps to finish: 15. Loss: 62.099815368652344 Reward: -3.0\nAgent: sacd_agent . Episode 599/2000. Number of steps to finish: 8. Loss: 25.857145309448242 Reward: 4.0\nAgent: sacd_agent . Episode 600/2000. Number of steps to finish: 15. Loss: 50.60696792602539 Reward: -3.0\nAgent: sacd_agent . Episode 601/2000. Number of steps to finish: 20. Loss: 78.58576202392578 Reward: -12.0\nAgent: sacd_agent . Episode 602/2000. Number of steps to finish: 20. Loss: 74.88848876953125 Reward: -14.0\nAgent: sacd_agent . Episode 603/2000. Number of steps to finish: 20. Loss: 75.02304077148438 Reward: -10.0\nAgent: sacd_agent . Episode 604/2000. Number of steps to finish: 20. Loss: 85.52741241455078 Reward: -12.0\nAgent: sacd_agent . Episode 605/2000. Number of steps to finish: 20. Loss: 87.82990264892578 Reward: -12.0\nAgent: sacd_agent . Episode 606/2000. Number of steps to finish: 20. Loss: 82.18379211425781 Reward: -14.0\nAgent: sacd_agent . Episode 607/2000. Number of steps to finish: 20. Loss: 67.76832580566406 Reward: -12.0\nAgent: sacd_agent . Episode 608/2000. Number of steps to finish: 17. Loss: 75.90262603759766 Reward: -5.0\nAgent: sacd_agent . Episode 609/2000. Number of steps to finish: 17. Loss: 62.099884033203125 Reward: -5.0\nAgent: sacd_agent . Episode 610/2000. Number of steps to finish: 20. Loss: 84.06864166259766 Reward: -14.0\nAgent: sacd_agent . Episode 611/2000. Number of steps to finish: 19. Loss: 77.62820434570312 Reward: -7.0\nAgent: sacd_agent . Episode 612/2000. Number of steps to finish: 20. Loss: 68.46730041503906 Reward: -10.0\nAgent: sacd_agent . Episode 613/2000. Number of steps to finish: 20. Loss: 69.31890106201172 Reward: -12.0\nAgent: sacd_agent . Episode 614/2000. Number of steps to finish: 18. Loss: 71.3765869140625 Reward: -6.0\nAgent: sacd_agent . Episode 615/2000. Number of steps to finish: 20. Loss: 76.95259857177734 Reward: -12.0\nAgent: sacd_agent . Episode 616/2000. Number of steps to finish: 20. Loss: 82.37811279296875 Reward: -14.0\nAgent: sacd_agent . Episode 617/2000. Number of steps to finish: 19. Loss: 75.89382934570312 Reward: -7.0\nAgent: sacd_agent . Episode 618/2000. Number of steps to finish: 16. Loss: 64.22575378417969 Reward: -4.0\nAgent: sacd_agent . Episode 619/2000. Number of steps to finish: 9. Loss: 33.058006286621094 Reward: 3.0\nAgent: sacd_agent . Episode 620/2000. Number of steps to finish: 20. Loss: 81.41352844238281 Reward: -12.0\nAgent: sacd_agent . Episode 621/2000. Number of steps to finish: 18. Loss: 70.93620300292969 Reward: -6.0\nAgent: sacd_agent . Episode 622/2000. Number of steps to finish: 20. Loss: 73.5600814819336 Reward: -12.0\nAgent: sacd_agent . Episode 623/2000. Number of steps to finish: 18. Loss: 69.42565155029297 Reward: -6.0\nAgent: sacd_agent . Episode 624/2000. Number of steps to finish: 11. Loss: 50.63885498046875 Reward: 1.0\nAgent: sacd_agent . Episode 625/2000. Number of steps to finish: 20. Loss: 82.90145874023438 Reward: -10.0\nAgent: sacd_agent . Episode 626/2000. Number of steps to finish: 18. Loss: 66.50363159179688 Reward: -6.0\nAgent: sacd_agent . Episode 627/2000. Number of steps to finish: 20. Loss: 85.83765411376953 Reward: -8.0\nAgent: sacd_agent . Episode 628/2000. Number of steps to finish: 20. Loss: 86.66728973388672 Reward: -10.0\nAgent: sacd_agent . Episode 629/2000. Number of steps to finish: 16. Loss: 71.1154556274414 Reward: -4.0\nAgent: sacd_agent . Episode 630/2000. Number of steps to finish: 19. Loss: 79.16280364990234 Reward: -7.0\nAgent: sacd_agent . Episode 631/2000. Number of steps to finish: 18. Loss: 74.60894775390625 Reward: -6.0\nAgent: sacd_agent . Episode 632/2000. Number of steps to finish: 20. Loss: 91.20634460449219 Reward: -10.0\nAgent: sacd_agent . Episode 633/2000. Number of steps to finish: 8. Loss: 31.132137298583984 Reward: 4.0\nAgent: sacd_agent . Episode 634/2000. Number of steps to finish: 20. Loss: 84.37724304199219 Reward: -12.0\nAgent: sacd_agent . Episode 635/2000. Number of steps to finish: 20. Loss: 99.06273651123047 Reward: -10.0\nAgent: sacd_agent . Episode 636/2000. Number of steps to finish: 20. Loss: 84.1697769165039 Reward: -12.0\nAgent: sacd_agent . Episode 637/2000. Number of steps to finish: 20. Loss: 81.75523376464844 Reward: -14.0\nAgent: sacd_agent . Episode 638/2000. Number of steps to finish: 20. Loss: 75.4823989868164 Reward: -10.0\nAgent: sacd_agent . Episode 639/2000. Number of steps to finish: 16. Loss: 68.71721649169922 Reward: -4.0\nAgent: sacd_agent . Episode 640/2000. Number of steps to finish: 14. Loss: 57.71928787231445 Reward: -2.0\nAgent: sacd_agent . Episode 641/2000. Number of steps to finish: 19. Loss: 77.4698486328125 Reward: -7.0\nAgent: sacd_agent . Episode 642/2000. Number of steps to finish: 20. Loss: 93.47037506103516 Reward: -10.0\nAgent: sacd_agent . Episode 643/2000. Number of steps to finish: 20. Loss: 89.64105987548828 Reward: -14.0\nAgent: sacd_agent . Episode 644/2000. Number of steps to finish: 20. Loss: 82.12323760986328 Reward: -12.0\nAgent: sacd_agent . Episode 645/2000. Number of steps to finish: 17. Loss: 72.61994934082031 Reward: -5.0\nAgent: sacd_agent . Episode 646/2000. Number of steps to finish: 20. Loss: 75.66156005859375 Reward: -12.0\nAgent: sacd_agent . Episode 647/2000. Number of steps to finish: 11. Loss: 38.1451530456543 Reward: 1.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 648/2000. Number of steps to finish: 17. Loss: 81.73129272460938 Reward: -5.0\nAgent: sacd_agent . Episode 649/2000. Number of steps to finish: 20. Loss: 81.40934753417969 Reward: -10.0\nAgent: sacd_agent . Episode 650/2000. Number of steps to finish: 9. Loss: 40.53513717651367 Reward: 3.0\nAgent: sacd_agent . Episode 651/2000. Number of steps to finish: 20. Loss: 93.71880340576172 Reward: -10.0\nAgent: sacd_agent . Episode 652/2000. Number of steps to finish: 20. Loss: 86.23228454589844 Reward: -10.0\nAgent: sacd_agent . Episode 653/2000. Number of steps to finish: 17. Loss: 68.99053955078125 Reward: -5.0\nAgent: sacd_agent . Episode 654/2000. Number of steps to finish: 20. Loss: 75.46084594726562 Reward: -14.0\nAgent: sacd_agent . Episode 655/2000. Number of steps to finish: 14. Loss: 70.55413818359375 Reward: -2.0\nAgent: sacd_agent . Episode 656/2000. Number of steps to finish: 13. Loss: 51.133052825927734 Reward: -1.0\nAgent: sacd_agent . Episode 657/2000. Number of steps to finish: 20. Loss: 93.18565368652344 Reward: -12.0\nAgent: sacd_agent . Episode 658/2000. Number of steps to finish: 15. Loss: 65.26834106445312 Reward: -3.0\nAgent: sacd_agent . Episode 659/2000. Number of steps to finish: 14. Loss: 57.00758361816406 Reward: -2.0\nAgent: sacd_agent . Episode 660/2000. Number of steps to finish: 20. Loss: 88.73759460449219 Reward: -10.0\nAgent: sacd_agent . Episode 661/2000. Number of steps to finish: 20. Loss: 90.18531799316406 Reward: -10.0\nAgent: sacd_agent . Episode 662/2000. Number of steps to finish: 20. Loss: 95.62030029296875 Reward: -10.0\nAgent: sacd_agent . Episode 663/2000. Number of steps to finish: 10. Loss: 41.631134033203125 Reward: 2.0\nAgent: sacd_agent . Episode 664/2000. Number of steps to finish: 20. Loss: 82.96438598632812 Reward: -10.0\nAgent: sacd_agent . Episode 665/2000. Number of steps to finish: 11. Loss: 47.543495178222656 Reward: 1.0\nAgent: sacd_agent . Episode 666/2000. Number of steps to finish: 20. Loss: 89.30679321289062 Reward: -12.0\nAgent: sacd_agent . Episode 667/2000. Number of steps to finish: 16. Loss: 71.54103088378906 Reward: -4.0\nAgent: sacd_agent . Episode 668/2000. Number of steps to finish: 20. Loss: 81.99952697753906 Reward: -12.0\nAgent: sacd_agent . Episode 669/2000. Number of steps to finish: 16. Loss: 74.36122131347656 Reward: -4.0\nAgent: sacd_agent . Episode 670/2000. Number of steps to finish: 20. Loss: 99.39835357666016 Reward: -10.0\nAgent: sacd_agent . Episode 671/2000. Number of steps to finish: 19. Loss: 86.46601104736328 Reward: -7.0\nAgent: sacd_agent . Episode 672/2000. Number of steps to finish: 10. Loss: 51.125099182128906 Reward: 2.0\nAgent: sacd_agent . Episode 673/2000. Number of steps to finish: 14. Loss: 70.29534912109375 Reward: -2.0\nAgent: sacd_agent . Episode 674/2000. Number of steps to finish: 20. Loss: 101.7381820678711 Reward: -12.0\nAgent: sacd_agent . Episode 675/2000. Number of steps to finish: 13. Loss: 70.05595397949219 Reward: -1.0\nAgent: sacd_agent . Episode 676/2000. Number of steps to finish: 20. Loss: 87.04238891601562 Reward: -8.0\nAgent: sacd_agent . Episode 677/2000. Number of steps to finish: 12. Loss: 60.02939987182617 Reward: 0.0\nAgent: sacd_agent . Episode 678/2000. Number of steps to finish: 13. Loss: 55.43359375 Reward: -1.0\nAgent: sacd_agent . Episode 679/2000. Number of steps to finish: 20. Loss: 88.394287109375 Reward: -10.0\nAgent: sacd_agent . Episode 680/2000. Number of steps to finish: 7. Loss: 34.24816131591797 Reward: 5.0\nAgent: sacd_agent . Episode 681/2000. Number of steps to finish: 20. Loss: 89.15048217773438 Reward: -12.0\nAgent: sacd_agent . Episode 682/2000. Number of steps to finish: 16. Loss: 73.0567398071289 Reward: -4.0\nAgent: sacd_agent . Episode 683/2000. Number of steps to finish: 17. Loss: 77.18856048583984 Reward: -5.0\nAgent: sacd_agent . Episode 684/2000. Number of steps to finish: 13. Loss: 66.64654541015625 Reward: -1.0\nAgent: sacd_agent . Episode 685/2000. Number of steps to finish: 11. Loss: 60.886619567871094 Reward: 1.0\nAgent: sacd_agent . Episode 686/2000. Number of steps to finish: 20. Loss: 92.46401977539062 Reward: -12.0\nAgent: sacd_agent . Episode 687/2000. Number of steps to finish: 20. Loss: 93.5599365234375 Reward: -10.0\nAgent: sacd_agent . Episode 688/2000. Number of steps to finish: 12. Loss: 66.16111755371094 Reward: 0.0\nAgent: sacd_agent . Episode 689/2000. Number of steps to finish: 20. Loss: 86.28316497802734 Reward: -14.0\nAgent: sacd_agent . Episode 690/2000. Number of steps to finish: 16. Loss: 78.04671478271484 Reward: -4.0\nAgent: sacd_agent . Episode 691/2000. Number of steps to finish: 15. Loss: 68.16596221923828 Reward: -3.0\nAgent: sacd_agent . Episode 692/2000. Number of steps to finish: 20. Loss: 104.46906280517578 Reward: -8.0\nAgent: sacd_agent . Episode 693/2000. Number of steps to finish: 19. Loss: 86.63531494140625 Reward: -7.0\nAgent: sacd_agent . Episode 694/2000. Number of steps to finish: 20. Loss: 106.6086196899414 Reward: -14.0\nAgent: sacd_agent . Episode 695/2000. Number of steps to finish: 20. Loss: 100.40845489501953 Reward: -10.0\nAgent: sacd_agent . Episode 696/2000. Number of steps to finish: 16. Loss: 86.74663543701172 Reward: -4.0\nAgent: sacd_agent . Episode 697/2000. Number of steps to finish: 20. Loss: 103.83491516113281 Reward: -10.0\nAgent: sacd_agent . Episode 698/2000. Number of steps to finish: 20. Loss: 113.57537841796875 Reward: -12.0\nAgent: sacd_agent . Episode 699/2000. Number of steps to finish: 9. Loss: 48.60343933105469 Reward: 3.0\nAgent: sacd_agent . Episode 700/2000. Number of steps to finish: 20. Loss: 92.05350494384766 Reward: -8.0\nAgent: sacd_agent . Episode 701/2000. Number of steps to finish: 20. Loss: 88.03792572021484 Reward: -10.0\nAgent: sacd_agent . Episode 702/2000. Number of steps to finish: 18. Loss: 87.1976547241211 Reward: -6.0\nAgent: sacd_agent . Episode 703/2000. Number of steps to finish: 15. Loss: 73.84697723388672 Reward: -3.0\nAgent: sacd_agent . Episode 704/2000. Number of steps to finish: 16. Loss: 87.89506530761719 Reward: -4.0\nAgent: sacd_agent . Episode 705/2000. Number of steps to finish: 20. Loss: 119.95584106445312 Reward: -12.0\nAgent: sacd_agent . Episode 706/2000. Number of steps to finish: 12. Loss: 65.58794403076172 Reward: 0.0\nAgent: sacd_agent . Episode 707/2000. Number of steps to finish: 14. Loss: 64.75640869140625 Reward: -2.0\nAgent: sacd_agent . Episode 708/2000. Number of steps to finish: 20. Loss: 98.0087661743164 Reward: -12.0\nAgent: sacd_agent . Episode 709/2000. Number of steps to finish: 15. Loss: 71.9999771118164 Reward: -3.0\nAgent: sacd_agent . Episode 710/2000. Number of steps to finish: 20. Loss: 112.99275207519531 Reward: -10.0\nAgent: sacd_agent . Episode 711/2000. Number of steps to finish: 20. Loss: 102.58987426757812 Reward: -12.0\nAgent: sacd_agent . Episode 712/2000. Number of steps to finish: 16. Loss: 84.67475128173828 Reward: -4.0\nAgent: sacd_agent . Episode 713/2000. Number of steps to finish: 20. Loss: 99.77310943603516 Reward: -10.0\nAgent: sacd_agent . Episode 714/2000. Number of steps to finish: 18. Loss: 98.82401275634766 Reward: -6.0\nAgent: sacd_agent . Episode 715/2000. Number of steps to finish: 20. Loss: 104.0347900390625 Reward: -12.0\nAgent: sacd_agent . Episode 716/2000. Number of steps to finish: 20. Loss: 110.41342163085938 Reward: -14.0\nAgent: sacd_agent . Episode 717/2000. Number of steps to finish: 20. Loss: 95.16665649414062 Reward: -10.0\nAgent: sacd_agent . Episode 718/2000. Number of steps to finish: 20. Loss: 94.98788452148438 Reward: -16.0\nAgent: sacd_agent . Episode 719/2000. Number of steps to finish: 20. Loss: 102.77490234375 Reward: -12.0\nAgent: sacd_agent . Episode 720/2000. Number of steps to finish: 20. Loss: 122.56848907470703 Reward: -12.0\nAgent: sacd_agent . Episode 721/2000. Number of steps to finish: 18. Loss: 99.71691131591797 Reward: -6.0\nAgent: sacd_agent . Episode 722/2000. Number of steps to finish: 17. Loss: 89.77334594726562 Reward: -5.0\nAgent: sacd_agent . Episode 723/2000. Number of steps to finish: 15. Loss: 78.62973022460938 Reward: -3.0\nAgent: sacd_agent . Episode 724/2000. Number of steps to finish: 20. Loss: 114.79707336425781 Reward: -10.0\nAgent: sacd_agent . Episode 725/2000. Number of steps to finish: 20. Loss: 100.15707397460938 Reward: -10.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 726/2000. Number of steps to finish: 20. Loss: 105.05607604980469 Reward: -10.0\nAgent: sacd_agent . Episode 727/2000. Number of steps to finish: 20. Loss: 115.25420379638672 Reward: -10.0\nAgent: sacd_agent . Episode 728/2000. Number of steps to finish: 20. Loss: 112.61668395996094 Reward: -10.0\nAgent: sacd_agent . Episode 729/2000. Number of steps to finish: 15. Loss: 86.19487762451172 Reward: -3.0\nAgent: sacd_agent . Episode 730/2000. Number of steps to finish: 20. Loss: 114.71015167236328 Reward: -8.0\nAgent: sacd_agent . Episode 731/2000. Number of steps to finish: 16. Loss: 88.38363647460938 Reward: -4.0\nAgent: sacd_agent . Episode 732/2000. Number of steps to finish: 20. Loss: 118.58646392822266 Reward: -16.0\nAgent: sacd_agent . Episode 733/2000. Number of steps to finish: 20. Loss: 97.43482971191406 Reward: -14.0\nAgent: sacd_agent . Episode 734/2000. Number of steps to finish: 20. Loss: 115.66897583007812 Reward: -12.0\nAgent: sacd_agent . Episode 735/2000. Number of steps to finish: 18. Loss: 95.56257629394531 Reward: -6.0\nAgent: sacd_agent . Episode 736/2000. Number of steps to finish: 20. Loss: 103.25102233886719 Reward: -10.0\nAgent: sacd_agent . Episode 737/2000. Number of steps to finish: 16. Loss: 78.10186767578125 Reward: -4.0\nAgent: sacd_agent . Episode 738/2000. Number of steps to finish: 15. Loss: 89.81778717041016 Reward: -3.0\nAgent: sacd_agent . Episode 739/2000. Number of steps to finish: 18. Loss: 106.15858459472656 Reward: -6.0\nAgent: sacd_agent . Episode 740/2000. Number of steps to finish: 17. Loss: 95.1734848022461 Reward: -5.0\nAgent: sacd_agent . Episode 741/2000. Number of steps to finish: 20. Loss: 108.78811645507812 Reward: -14.0\nAgent: sacd_agent . Episode 742/2000. Number of steps to finish: 20. Loss: 118.4061279296875 Reward: -8.0\nAgent: sacd_agent . Episode 743/2000. Number of steps to finish: 20. Loss: 114.1863021850586 Reward: -12.0\nAgent: sacd_agent . Episode 744/2000. Number of steps to finish: 9. Loss: 51.885215759277344 Reward: 3.0\nAgent: sacd_agent . Episode 745/2000. Number of steps to finish: 14. Loss: 83.22071075439453 Reward: -2.0\nAgent: sacd_agent . Episode 746/2000. Number of steps to finish: 20. Loss: 97.20069885253906 Reward: -10.0\nAgent: sacd_agent . Episode 747/2000. Number of steps to finish: 19. Loss: 114.11783599853516 Reward: -7.0\nAgent: sacd_agent . Episode 748/2000. Number of steps to finish: 20. Loss: 110.84934997558594 Reward: -12.0\nAgent: sacd_agent . Episode 749/2000. Number of steps to finish: 9. Loss: 50.95576477050781 Reward: 3.0\nAgent: sacd_agent . Episode 750/2000. Number of steps to finish: 20. Loss: 114.15408325195312 Reward: -10.0\nAgent: sacd_agent . Episode 751/2000. Number of steps to finish: 15. Loss: 92.28848266601562 Reward: -3.0\nAgent: sacd_agent . Episode 752/2000. Number of steps to finish: 16. Loss: 100.57144165039062 Reward: -4.0\nAgent: sacd_agent . Episode 753/2000. Number of steps to finish: 20. Loss: 113.3956527709961 Reward: -8.0\nAgent: sacd_agent . Episode 754/2000. Number of steps to finish: 20. Loss: 114.8288345336914 Reward: -10.0\nAgent: sacd_agent . Episode 755/2000. Number of steps to finish: 20. Loss: 107.51464080810547 Reward: -12.0\nAgent: sacd_agent . Episode 756/2000. Number of steps to finish: 20. Loss: 110.43272399902344 Reward: -10.0\nAgent: sacd_agent . Episode 757/2000. Number of steps to finish: 12. Loss: 71.63931274414062 Reward: 0.0\nAgent: sacd_agent . Episode 758/2000. Number of steps to finish: 20. Loss: 119.50390625 Reward: -14.0\nAgent: sacd_agent . Episode 759/2000. Number of steps to finish: 13. Loss: 74.22883605957031 Reward: -1.0\nAgent: sacd_agent . Episode 760/2000. Number of steps to finish: 20. Loss: 120.51460266113281 Reward: -10.0\nAgent: sacd_agent . Episode 761/2000. Number of steps to finish: 20. Loss: 118.96533966064453 Reward: -10.0\nAgent: sacd_agent . Episode 762/2000. Number of steps to finish: 20. Loss: 124.18618774414062 Reward: -10.0\nAgent: sacd_agent . Episode 763/2000. Number of steps to finish: 19. Loss: 110.32332611083984 Reward: -7.0\nAgent: sacd_agent . Episode 764/2000. Number of steps to finish: 20. Loss: 118.64507293701172 Reward: -12.0\nAgent: sacd_agent . Episode 765/2000. Number of steps to finish: 20. Loss: 113.24928283691406 Reward: -10.0\nAgent: sacd_agent . Episode 766/2000. Number of steps to finish: 20. Loss: 117.15203094482422 Reward: -12.0\nAgent: sacd_agent . Episode 767/2000. Number of steps to finish: 16. Loss: 85.47749328613281 Reward: -4.0\nAgent: sacd_agent . Episode 768/2000. Number of steps to finish: 11. Loss: 66.50662994384766 Reward: 1.0\nAgent: sacd_agent . Episode 769/2000. Number of steps to finish: 20. Loss: 142.85818481445312 Reward: -12.0\nAgent: sacd_agent . Episode 770/2000. Number of steps to finish: 14. Loss: 82.26972198486328 Reward: -2.0\nAgent: sacd_agent . Episode 771/2000. Number of steps to finish: 20. Loss: 128.09988403320312 Reward: -12.0\nAgent: sacd_agent . Episode 772/2000. Number of steps to finish: 12. Loss: 79.00199127197266 Reward: 0.0\nAgent: sacd_agent . Episode 773/2000. Number of steps to finish: 20. Loss: 114.12977600097656 Reward: -12.0\nAgent: sacd_agent . Episode 774/2000. Number of steps to finish: 20. Loss: 131.52777099609375 Reward: -10.0\nAgent: sacd_agent . Episode 775/2000. Number of steps to finish: 15. Loss: 89.87435150146484 Reward: -3.0\nAgent: sacd_agent . Episode 776/2000. Number of steps to finish: 20. Loss: 119.84207153320312 Reward: -14.0\nAgent: sacd_agent . Episode 777/2000. Number of steps to finish: 13. Loss: 75.76268768310547 Reward: -1.0\nAgent: sacd_agent . Episode 778/2000. Number of steps to finish: 20. Loss: 130.6514129638672 Reward: -12.0\nAgent: sacd_agent . Episode 779/2000. Number of steps to finish: 18. Loss: 97.80403900146484 Reward: -6.0\nAgent: sacd_agent . Episode 780/2000. Number of steps to finish: 20. Loss: 126.54109954833984 Reward: -14.0\nAgent: sacd_agent . Episode 781/2000. Number of steps to finish: 20. Loss: 123.64187622070312 Reward: -12.0\nAgent: sacd_agent . Episode 782/2000. Number of steps to finish: 20. Loss: 125.0936050415039 Reward: -8.0\nAgent: sacd_agent . Episode 783/2000. Number of steps to finish: 17. Loss: 115.65503692626953 Reward: -5.0\nAgent: sacd_agent . Episode 784/2000. Number of steps to finish: 20. Loss: 129.48480224609375 Reward: -10.0\nAgent: sacd_agent . Episode 785/2000. Number of steps to finish: 16. Loss: 101.4177474975586 Reward: -4.0\nAgent: sacd_agent . Episode 786/2000. Number of steps to finish: 20. Loss: 130.14093017578125 Reward: -8.0\nAgent: sacd_agent . Episode 787/2000. Number of steps to finish: 20. Loss: 132.47616577148438 Reward: -12.0\nAgent: sacd_agent . Episode 788/2000. Number of steps to finish: 20. Loss: 115.02508544921875 Reward: -8.0\nAgent: sacd_agent . Episode 789/2000. Number of steps to finish: 15. Loss: 92.66649627685547 Reward: -3.0\nAgent: sacd_agent . Episode 790/2000. Number of steps to finish: 19. Loss: 123.50062561035156 Reward: -7.0\nAgent: sacd_agent . Episode 791/2000. Number of steps to finish: 14. Loss: 99.6842269897461 Reward: -2.0\nAgent: sacd_agent . Episode 792/2000. Number of steps to finish: 13. Loss: 83.61542510986328 Reward: -1.0\nAgent: sacd_agent . Episode 793/2000. Number of steps to finish: 20. Loss: 132.78143310546875 Reward: -8.0\nAgent: sacd_agent . Episode 794/2000. Number of steps to finish: 19. Loss: 128.5248260498047 Reward: -7.0\nAgent: sacd_agent . Episode 795/2000. Number of steps to finish: 16. Loss: 118.88668823242188 Reward: -4.0\nAgent: sacd_agent . Episode 796/2000. Number of steps to finish: 20. Loss: 122.44914245605469 Reward: -8.0\nAgent: sacd_agent . Episode 797/2000. Number of steps to finish: 8. Loss: 46.28553771972656 Reward: 4.0\nAgent: sacd_agent . Episode 798/2000. Number of steps to finish: 19. Loss: 118.0125732421875 Reward: -7.0\nAgent: sacd_agent . Episode 799/2000. Number of steps to finish: 20. Loss: 133.07737731933594 Reward: -12.0\nAgent: sacd_agent . Episode 800/2000. Number of steps to finish: 20. Loss: 129.88572692871094 Reward: -8.0\nAgent: sacd_agent . Episode 801/2000. Number of steps to finish: 16. Loss: 111.42163848876953 Reward: -4.0\nAgent: sacd_agent . Episode 802/2000. Number of steps to finish: 20. Loss: 133.9140167236328 Reward: -10.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 803/2000. Number of steps to finish: 18. Loss: 110.65345764160156 Reward: -6.0\nAgent: sacd_agent . Episode 804/2000. Number of steps to finish: 20. Loss: 113.30935668945312 Reward: -12.0\nAgent: sacd_agent . Episode 805/2000. Number of steps to finish: 14. Loss: 83.64583587646484 Reward: -2.0\nAgent: sacd_agent . Episode 806/2000. Number of steps to finish: 19. Loss: 134.78961181640625 Reward: -7.0\nAgent: sacd_agent . Episode 807/2000. Number of steps to finish: 20. Loss: 142.4204864501953 Reward: -16.0\nAgent: sacd_agent . Episode 808/2000. Number of steps to finish: 19. Loss: 134.8755645751953 Reward: -7.0\nAgent: sacd_agent . Episode 809/2000. Number of steps to finish: 20. Loss: 145.4323272705078 Reward: -10.0\nAgent: sacd_agent . Episode 810/2000. Number of steps to finish: 19. Loss: 125.48081970214844 Reward: -7.0\nAgent: sacd_agent . Episode 811/2000. Number of steps to finish: 18. Loss: 124.54042053222656 Reward: -6.0\nAgent: sacd_agent . Episode 812/2000. Number of steps to finish: 20. Loss: 141.3315887451172 Reward: -12.0\nAgent: sacd_agent . Episode 813/2000. Number of steps to finish: 11. Loss: 73.36212921142578 Reward: 1.0\nAgent: sacd_agent . Episode 814/2000. Number of steps to finish: 8. Loss: 56.629661560058594 Reward: 4.0\nAgent: sacd_agent . Episode 815/2000. Number of steps to finish: 20. Loss: 137.36782836914062 Reward: -8.0\nAgent: sacd_agent . Episode 816/2000. Number of steps to finish: 18. Loss: 122.71744537353516 Reward: -6.0\nAgent: sacd_agent . Episode 817/2000. Number of steps to finish: 20. Loss: 138.44468688964844 Reward: -12.0\nAgent: sacd_agent . Episode 818/2000. Number of steps to finish: 20. Loss: 146.89794921875 Reward: -8.0\nAgent: sacd_agent . Episode 819/2000. Number of steps to finish: 13. Loss: 97.51461029052734 Reward: -1.0\nAgent: sacd_agent . Episode 820/2000. Number of steps to finish: 20. Loss: 110.47627258300781 Reward: -8.0\nAgent: sacd_agent . Episode 821/2000. Number of steps to finish: 14. Loss: 93.86016845703125 Reward: -2.0\nAgent: sacd_agent . Episode 822/2000. Number of steps to finish: 20. Loss: 125.11123657226562 Reward: -10.0\nAgent: sacd_agent . Episode 823/2000. Number of steps to finish: 19. Loss: 143.145751953125 Reward: -7.0\nAgent: sacd_agent . Episode 824/2000. Number of steps to finish: 20. Loss: 143.93826293945312 Reward: -10.0\nAgent: sacd_agent . Episode 825/2000. Number of steps to finish: 20. Loss: 149.9025115966797 Reward: -16.0\nAgent: sacd_agent . Episode 826/2000. Number of steps to finish: 20. Loss: 119.79717254638672 Reward: -10.0\nAgent: sacd_agent . Episode 827/2000. Number of steps to finish: 17. Loss: 119.86774444580078 Reward: -5.0\nAgent: sacd_agent . Episode 828/2000. Number of steps to finish: 14. Loss: 96.42132568359375 Reward: -2.0\nAgent: sacd_agent . Episode 829/2000. Number of steps to finish: 20. Loss: 131.912841796875 Reward: -10.0\nAgent: sacd_agent . Episode 830/2000. Number of steps to finish: 13. Loss: 96.73039245605469 Reward: -1.0\nAgent: sacd_agent . Episode 831/2000. Number of steps to finish: 20. Loss: 148.526123046875 Reward: -10.0\nAgent: sacd_agent . Episode 832/2000. Number of steps to finish: 20. Loss: 138.05105590820312 Reward: -16.0\nAgent: sacd_agent . Episode 833/2000. Number of steps to finish: 20. Loss: 112.43106842041016 Reward: -10.0\nAgent: sacd_agent . Episode 834/2000. Number of steps to finish: 20. Loss: 123.72632598876953 Reward: -12.0\nAgent: sacd_agent . Episode 835/2000. Number of steps to finish: 9. Loss: 67.05927276611328 Reward: 3.0\nAgent: sacd_agent . Episode 836/2000. Number of steps to finish: 13. Loss: 91.02765655517578 Reward: -1.0\nAgent: sacd_agent . Episode 837/2000. Number of steps to finish: 20. Loss: 146.84725952148438 Reward: -10.0\nAgent: sacd_agent . Episode 838/2000. Number of steps to finish: 20. Loss: 153.6627197265625 Reward: -10.0\nAgent: sacd_agent . Episode 839/2000. Number of steps to finish: 20. Loss: 150.501953125 Reward: -14.0\nAgent: sacd_agent . Episode 840/2000. Number of steps to finish: 16. Loss: 120.09320831298828 Reward: -4.0\nAgent: sacd_agent . Episode 841/2000. Number of steps to finish: 20. Loss: 154.60153198242188 Reward: -10.0\nAgent: sacd_agent . Episode 842/2000. Number of steps to finish: 10. Loss: 85.85826110839844 Reward: 2.0\nAgent: sacd_agent . Episode 843/2000. Number of steps to finish: 20. Loss: 148.49948120117188 Reward: -10.0\nAgent: sacd_agent . Episode 844/2000. Number of steps to finish: 20. Loss: 128.3177490234375 Reward: -14.0\nAgent: sacd_agent . Episode 845/2000. Number of steps to finish: 18. Loss: 123.45445251464844 Reward: -6.0\nAgent: sacd_agent . Episode 846/2000. Number of steps to finish: 15. Loss: 113.90611267089844 Reward: -3.0\nAgent: sacd_agent . Episode 847/2000. Number of steps to finish: 20. Loss: 157.60606384277344 Reward: -12.0\nAgent: sacd_agent . Episode 848/2000. Number of steps to finish: 8. Loss: 57.86760711669922 Reward: 4.0\nAgent: sacd_agent . Episode 849/2000. Number of steps to finish: 20. Loss: 135.36119079589844 Reward: -10.0\nAgent: sacd_agent . Episode 850/2000. Number of steps to finish: 20. Loss: 168.17919921875 Reward: -10.0\nAgent: sacd_agent . Episode 851/2000. Number of steps to finish: 11. Loss: 75.96337127685547 Reward: 1.0\nAgent: sacd_agent . Episode 852/2000. Number of steps to finish: 15. Loss: 105.74790954589844 Reward: -3.0\nAgent: sacd_agent . Episode 853/2000. Number of steps to finish: 15. Loss: 109.15292358398438 Reward: -3.0\nAgent: sacd_agent . Episode 854/2000. Number of steps to finish: 18. Loss: 132.23287963867188 Reward: -6.0\nAgent: sacd_agent . Episode 855/2000. Number of steps to finish: 15. Loss: 126.34432983398438 Reward: -3.0\nAgent: sacd_agent . Episode 856/2000. Number of steps to finish: 20. Loss: 128.06185913085938 Reward: -14.0\nAgent: sacd_agent . Episode 857/2000. Number of steps to finish: 20. Loss: 147.09014892578125 Reward: -10.0\nAgent: sacd_agent . Episode 858/2000. Number of steps to finish: 20. Loss: 168.8970184326172 Reward: -10.0\nAgent: sacd_agent . Episode 859/2000. Number of steps to finish: 14. Loss: 103.8616714477539 Reward: -2.0\nAgent: sacd_agent . Episode 860/2000. Number of steps to finish: 20. Loss: 159.42640686035156 Reward: -8.0\nAgent: sacd_agent . Episode 861/2000. Number of steps to finish: 12. Loss: 94.25492858886719 Reward: 0.0\nAgent: sacd_agent . Episode 862/2000. Number of steps to finish: 20. Loss: 137.81666564941406 Reward: -10.0\nAgent: sacd_agent . Episode 863/2000. Number of steps to finish: 20. Loss: 146.9217071533203 Reward: -10.0\nAgent: sacd_agent . Episode 864/2000. Number of steps to finish: 18. Loss: 133.01295471191406 Reward: -6.0\nAgent: sacd_agent . Episode 865/2000. Number of steps to finish: 16. Loss: 116.0302734375 Reward: -4.0\nAgent: sacd_agent . Episode 866/2000. Number of steps to finish: 15. Loss: 132.6654815673828 Reward: -3.0\nAgent: sacd_agent . Episode 867/2000. Number of steps to finish: 15. Loss: 113.9064712524414 Reward: -3.0\nAgent: sacd_agent . Episode 868/2000. Number of steps to finish: 8. Loss: 58.58739471435547 Reward: 4.0\nAgent: sacd_agent . Episode 869/2000. Number of steps to finish: 20. Loss: 148.43798828125 Reward: -10.0\nAgent: sacd_agent . Episode 870/2000. Number of steps to finish: 12. Loss: 93.50232696533203 Reward: 0.0\nAgent: sacd_agent . Episode 871/2000. Number of steps to finish: 20. Loss: 160.83245849609375 Reward: -10.0\nAgent: sacd_agent . Episode 872/2000. Number of steps to finish: 20. Loss: 163.48696899414062 Reward: -10.0\nAgent: sacd_agent . Episode 873/2000. Number of steps to finish: 12. Loss: 94.91088104248047 Reward: 0.0\nAgent: sacd_agent . Episode 874/2000. Number of steps to finish: 16. Loss: 122.69502258300781 Reward: -4.0\nAgent: sacd_agent . Episode 875/2000. Number of steps to finish: 9. Loss: 79.92543029785156 Reward: 3.0\nAgent: sacd_agent . Episode 876/2000. Number of steps to finish: 20. Loss: 151.48451232910156 Reward: -10.0\nAgent: sacd_agent . Episode 877/2000. Number of steps to finish: 20. Loss: 163.00311279296875 Reward: -12.0\nAgent: sacd_agent . Episode 878/2000. Number of steps to finish: 9. Loss: 71.69682312011719 Reward: 3.0\nAgent: sacd_agent . Episode 879/2000. Number of steps to finish: 14. Loss: 98.04696655273438 Reward: -2.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 880/2000. Number of steps to finish: 20. Loss: 155.433837890625 Reward: -14.0\nAgent: sacd_agent . Episode 881/2000. Number of steps to finish: 7. Loss: 60.69189453125 Reward: 5.0\nAgent: sacd_agent . Episode 882/2000. Number of steps to finish: 15. Loss: 114.37269592285156 Reward: -3.0\nAgent: sacd_agent . Episode 883/2000. Number of steps to finish: 15. Loss: 111.0091323852539 Reward: -3.0\nAgent: sacd_agent . Episode 884/2000. Number of steps to finish: 17. Loss: 133.8705596923828 Reward: -5.0\nAgent: sacd_agent . Episode 885/2000. Number of steps to finish: 11. Loss: 97.11354064941406 Reward: 1.0\nAgent: sacd_agent . Episode 886/2000. Number of steps to finish: 20. Loss: 159.72885131835938 Reward: -10.0\nAgent: sacd_agent . Episode 887/2000. Number of steps to finish: 11. Loss: 86.09056854248047 Reward: 1.0\nAgent: sacd_agent . Episode 888/2000. Number of steps to finish: 15. Loss: 129.668212890625 Reward: -3.0\nAgent: sacd_agent . Episode 889/2000. Number of steps to finish: 12. Loss: 97.514892578125 Reward: 0.0\nAgent: sacd_agent . Episode 890/2000. Number of steps to finish: 13. Loss: 105.25656127929688 Reward: -1.0\nAgent: sacd_agent . Episode 891/2000. Number of steps to finish: 14. Loss: 101.46232604980469 Reward: -2.0\nAgent: sacd_agent . Episode 892/2000. Number of steps to finish: 18. Loss: 132.2962646484375 Reward: -6.0\nAgent: sacd_agent . Episode 893/2000. Number of steps to finish: 20. Loss: 163.89344787597656 Reward: -12.0\nAgent: sacd_agent . Episode 894/2000. Number of steps to finish: 20. Loss: 169.42303466796875 Reward: -12.0\nAgent: sacd_agent . Episode 895/2000. Number of steps to finish: 16. Loss: 146.19943237304688 Reward: -4.0\nAgent: sacd_agent . Episode 896/2000. Number of steps to finish: 20. Loss: 148.06724548339844 Reward: -10.0\nAgent: sacd_agent . Episode 897/2000. Number of steps to finish: 20. Loss: 182.7269744873047 Reward: -14.0\nAgent: sacd_agent . Episode 898/2000. Number of steps to finish: 15. Loss: 105.37238311767578 Reward: -3.0\nAgent: sacd_agent . Episode 899/2000. Number of steps to finish: 17. Loss: 126.94023132324219 Reward: -5.0\nAgent: sacd_agent . Episode 900/2000. Number of steps to finish: 12. Loss: 114.17697143554688 Reward: 0.0\nAgent: sacd_agent . Episode 901/2000. Number of steps to finish: 14. Loss: 131.4027862548828 Reward: -2.0\nAgent: sacd_agent . Episode 902/2000. Number of steps to finish: 20. Loss: 177.30667114257812 Reward: -14.0\nAgent: sacd_agent . Episode 903/2000. Number of steps to finish: 18. Loss: 151.3820343017578 Reward: -6.0\nAgent: sacd_agent . Episode 904/2000. Number of steps to finish: 12. Loss: 100.86907196044922 Reward: 0.0\nAgent: sacd_agent . Episode 905/2000. Number of steps to finish: 8. Loss: 60.522483825683594 Reward: 4.0\nAgent: sacd_agent . Episode 906/2000. Number of steps to finish: 12. Loss: 97.62004852294922 Reward: 0.0\nAgent: sacd_agent . Episode 907/2000. Number of steps to finish: 20. Loss: 166.60926818847656 Reward: -12.0\nAgent: sacd_agent . Episode 908/2000. Number of steps to finish: 20. Loss: 177.03672790527344 Reward: -12.0\nAgent: sacd_agent . Episode 909/2000. Number of steps to finish: 13. Loss: 91.98592376708984 Reward: -1.0\nAgent: sacd_agent . Episode 910/2000. Number of steps to finish: 10. Loss: 85.21621704101562 Reward: 2.0\nAgent: sacd_agent . Episode 911/2000. Number of steps to finish: 19. Loss: 187.68280029296875 Reward: -7.0\nAgent: sacd_agent . Episode 912/2000. Number of steps to finish: 13. Loss: 119.38103485107422 Reward: -1.0\nAgent: sacd_agent . Episode 913/2000. Number of steps to finish: 20. Loss: 163.43621826171875 Reward: -12.0\nAgent: sacd_agent . Episode 914/2000. Number of steps to finish: 18. Loss: 149.67169189453125 Reward: -6.0\nAgent: sacd_agent . Episode 915/2000. Number of steps to finish: 18. Loss: 147.06951904296875 Reward: -6.0\nAgent: sacd_agent . Episode 916/2000. Number of steps to finish: 20. Loss: 168.510986328125 Reward: -14.0\nAgent: sacd_agent . Episode 917/2000. Number of steps to finish: 20. Loss: 155.950927734375 Reward: -12.0\nAgent: sacd_agent . Episode 918/2000. Number of steps to finish: 18. Loss: 155.50823974609375 Reward: -6.0\nAgent: sacd_agent . Episode 919/2000. Number of steps to finish: 17. Loss: 157.6970977783203 Reward: -5.0\nAgent: sacd_agent . Episode 920/2000. Number of steps to finish: 18. Loss: 150.76670837402344 Reward: -6.0\nAgent: sacd_agent . Episode 921/2000. Number of steps to finish: 18. Loss: 169.6745147705078 Reward: -6.0\nAgent: sacd_agent . Episode 922/2000. Number of steps to finish: 17. Loss: 140.19566345214844 Reward: -5.0\nAgent: sacd_agent . Episode 923/2000. Number of steps to finish: 20. Loss: 190.58807373046875 Reward: -10.0\nAgent: sacd_agent . Episode 924/2000. Number of steps to finish: 18. Loss: 171.26292419433594 Reward: -6.0\nAgent: sacd_agent . Episode 925/2000. Number of steps to finish: 13. Loss: 98.26167297363281 Reward: -1.0\nAgent: sacd_agent . Episode 926/2000. Number of steps to finish: 20. Loss: 186.0435791015625 Reward: -10.0\nAgent: sacd_agent . Episode 927/2000. Number of steps to finish: 20. Loss: 179.46780395507812 Reward: -12.0\nAgent: sacd_agent . Episode 928/2000. Number of steps to finish: 20. Loss: 174.090576171875 Reward: -10.0\nAgent: sacd_agent . Episode 929/2000. Number of steps to finish: 17. Loss: 138.05152893066406 Reward: -5.0\nAgent: sacd_agent . Episode 930/2000. Number of steps to finish: 16. Loss: 144.6610565185547 Reward: -4.0\nAgent: sacd_agent . Episode 931/2000. Number of steps to finish: 9. Loss: 79.047119140625 Reward: 3.0\nAgent: sacd_agent . Episode 932/2000. Number of steps to finish: 12. Loss: 99.15394592285156 Reward: 0.0\nAgent: sacd_agent . Episode 933/2000. Number of steps to finish: 12. Loss: 94.72270202636719 Reward: 0.0\nAgent: sacd_agent . Episode 934/2000. Number of steps to finish: 11. Loss: 116.35171508789062 Reward: 1.0\nAgent: sacd_agent . Episode 935/2000. Number of steps to finish: 12. Loss: 88.30946350097656 Reward: 0.0\nAgent: sacd_agent . Episode 936/2000. Number of steps to finish: 20. Loss: 181.22279357910156 Reward: -12.0\nAgent: sacd_agent . Episode 937/2000. Number of steps to finish: 20. Loss: 189.25819396972656 Reward: -10.0\nAgent: sacd_agent . Episode 938/2000. Number of steps to finish: 20. Loss: 167.79232788085938 Reward: -10.0\nAgent: sacd_agent . Episode 939/2000. Number of steps to finish: 20. Loss: 190.09817504882812 Reward: -14.0\nAgent: sacd_agent . Episode 940/2000. Number of steps to finish: 12. Loss: 105.80014038085938 Reward: 0.0\nAgent: sacd_agent . Episode 941/2000. Number of steps to finish: 20. Loss: 177.87725830078125 Reward: -10.0\nAgent: sacd_agent . Episode 942/2000. Number of steps to finish: 11. Loss: 100.6615982055664 Reward: 1.0\nAgent: sacd_agent . Episode 943/2000. Number of steps to finish: 12. Loss: 120.66288757324219 Reward: 0.0\nAgent: sacd_agent . Episode 944/2000. Number of steps to finish: 20. Loss: 164.9136962890625 Reward: -10.0\nAgent: sacd_agent . Episode 945/2000. Number of steps to finish: 14. Loss: 131.28805541992188 Reward: -2.0\nAgent: sacd_agent . Episode 946/2000. Number of steps to finish: 20. Loss: 185.04315185546875 Reward: -16.0\nAgent: sacd_agent . Episode 947/2000. Number of steps to finish: 16. Loss: 135.88442993164062 Reward: -4.0\nAgent: sacd_agent . Episode 948/2000. Number of steps to finish: 12. Loss: 122.51747131347656 Reward: 0.0\nAgent: sacd_agent . Episode 949/2000. Number of steps to finish: 20. Loss: 161.19866943359375 Reward: -14.0\nAgent: sacd_agent . Episode 950/2000. Number of steps to finish: 20. Loss: 186.9704132080078 Reward: -10.0\nAgent: sacd_agent . Episode 951/2000. Number of steps to finish: 20. Loss: 192.10543823242188 Reward: -12.0\nAgent: sacd_agent . Episode 952/2000. Number of steps to finish: 14. Loss: 111.3982162475586 Reward: -2.0\nAgent: sacd_agent . Episode 953/2000. Number of steps to finish: 18. Loss: 169.51222229003906 Reward: -6.0\nAgent: sacd_agent . Episode 954/2000. Number of steps to finish: 19. Loss: 159.68421936035156 Reward: -7.0\nAgent: sacd_agent . Episode 955/2000. Number of steps to finish: 20. Loss: 204.61737060546875 Reward: -10.0\nAgent: sacd_agent . Episode 956/2000. Number of steps to finish: 20. Loss: 183.713623046875 Reward: -10.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 957/2000. Number of steps to finish: 20. Loss: 198.51129150390625 Reward: -10.0\nAgent: sacd_agent . Episode 958/2000. Number of steps to finish: 20. Loss: 183.764404296875 Reward: -12.0\nAgent: sacd_agent . Episode 959/2000. Number of steps to finish: 17. Loss: 177.24545288085938 Reward: -5.0\nAgent: sacd_agent . Episode 960/2000. Number of steps to finish: 19. Loss: 183.56382751464844 Reward: -7.0\nAgent: sacd_agent . Episode 961/2000. Number of steps to finish: 20. Loss: 170.41458129882812 Reward: -10.0\nAgent: sacd_agent . Episode 962/2000. Number of steps to finish: 20. Loss: 202.67776489257812 Reward: -8.0\nAgent: sacd_agent . Episode 963/2000. Number of steps to finish: 13. Loss: 123.22079467773438 Reward: -1.0\nAgent: sacd_agent . Episode 964/2000. Number of steps to finish: 16. Loss: 163.57342529296875 Reward: -4.0\nAgent: sacd_agent . Episode 965/2000. Number of steps to finish: 19. Loss: 187.35189819335938 Reward: -7.0\nAgent: sacd_agent . Episode 966/2000. Number of steps to finish: 20. Loss: 205.88980102539062 Reward: -8.0\nAgent: sacd_agent . Episode 967/2000. Number of steps to finish: 20. Loss: 173.959716796875 Reward: -12.0\nAgent: sacd_agent . Episode 968/2000. Number of steps to finish: 20. Loss: 188.3060760498047 Reward: -10.0\nAgent: sacd_agent . Episode 969/2000. Number of steps to finish: 12. Loss: 108.42391204833984 Reward: 0.0\nAgent: sacd_agent . Episode 970/2000. Number of steps to finish: 20. Loss: 178.057373046875 Reward: -10.0\nAgent: sacd_agent . Episode 971/2000. Number of steps to finish: 10. Loss: 89.6864242553711 Reward: 2.0\nAgent: sacd_agent . Episode 972/2000. Number of steps to finish: 20. Loss: 168.26605224609375 Reward: -14.0\nAgent: sacd_agent . Episode 973/2000. Number of steps to finish: 17. Loss: 172.87322998046875 Reward: -5.0\nAgent: sacd_agent . Episode 974/2000. Number of steps to finish: 16. Loss: 143.80868530273438 Reward: -4.0\nAgent: sacd_agent . Episode 975/2000. Number of steps to finish: 7. Loss: 55.10366439819336 Reward: 5.0\nAgent: sacd_agent . Episode 976/2000. Number of steps to finish: 9. Loss: 86.71713256835938 Reward: 3.0\nAgent: sacd_agent . Episode 977/2000. Number of steps to finish: 13. Loss: 118.9866714477539 Reward: -1.0\nAgent: sacd_agent . Episode 978/2000. Number of steps to finish: 20. Loss: 205.60494995117188 Reward: -14.0\nAgent: sacd_agent . Episode 979/2000. Number of steps to finish: 13. Loss: 144.53665161132812 Reward: -1.0\nAgent: sacd_agent . Episode 980/2000. Number of steps to finish: 10. Loss: 94.11506652832031 Reward: 2.0\nAgent: sacd_agent . Episode 981/2000. Number of steps to finish: 20. Loss: 197.34100341796875 Reward: -10.0\nAgent: sacd_agent . Episode 982/2000. Number of steps to finish: 9. Loss: 101.2847900390625 Reward: 3.0\nAgent: sacd_agent . Episode 983/2000. Number of steps to finish: 20. Loss: 207.79244995117188 Reward: -12.0\nAgent: sacd_agent . Episode 984/2000. Number of steps to finish: 16. Loss: 153.57037353515625 Reward: -4.0\nAgent: sacd_agent . Episode 985/2000. Number of steps to finish: 20. Loss: 162.8246612548828 Reward: -8.0\nAgent: sacd_agent . Episode 986/2000. Number of steps to finish: 20. Loss: 204.62203979492188 Reward: -10.0\nAgent: sacd_agent . Episode 987/2000. Number of steps to finish: 20. Loss: 172.39268493652344 Reward: -10.0\nAgent: sacd_agent . Episode 988/2000. Number of steps to finish: 10. Loss: 124.20272827148438 Reward: 2.0\nAgent: sacd_agent . Episode 989/2000. Number of steps to finish: 20. Loss: 206.59230041503906 Reward: -10.0\nAgent: sacd_agent . Episode 990/2000. Number of steps to finish: 20. Loss: 216.7445831298828 Reward: -12.0\nAgent: sacd_agent . Episode 991/2000. Number of steps to finish: 20. Loss: 223.07887268066406 Reward: -12.0\nAgent: sacd_agent . Episode 992/2000. Number of steps to finish: 17. Loss: 188.01022338867188 Reward: -5.0\nAgent: sacd_agent . Episode 993/2000. Number of steps to finish: 20. Loss: 195.00555419921875 Reward: -10.0\nAgent: sacd_agent . Episode 994/2000. Number of steps to finish: 20. Loss: 175.4518280029297 Reward: -16.0\nAgent: sacd_agent . Episode 995/2000. Number of steps to finish: 17. Loss: 168.9041290283203 Reward: -5.0\nAgent: sacd_agent . Episode 996/2000. Number of steps to finish: 20. Loss: 202.66758728027344 Reward: -8.0\nAgent: sacd_agent . Episode 997/2000. Number of steps to finish: 20. Loss: 194.76084899902344 Reward: -10.0\nAgent: sacd_agent . Episode 998/2000. Number of steps to finish: 20. Loss: 199.349609375 Reward: -10.0\nAgent: sacd_agent . Episode 999/2000. Number of steps to finish: 17. Loss: 156.24874877929688 Reward: -5.0\nAgent: sacd_agent . Episode 1000/2000. Number of steps to finish: 20. Loss: 193.90884399414062 Reward: -14.0\nAgent: sacd_agent . Episode 1001/2000. Number of steps to finish: 20. Loss: 215.05801391601562 Reward: -10.0\nAgent: sacd_agent . Episode 1002/2000. Number of steps to finish: 18. Loss: 167.59095764160156 Reward: -6.0\nAgent: sacd_agent . Episode 1003/2000. Number of steps to finish: 20. Loss: 201.71498107910156 Reward: -12.0\nAgent: sacd_agent . Episode 1004/2000. Number of steps to finish: 20. Loss: 220.51043701171875 Reward: -10.0\nAgent: sacd_agent . Episode 1005/2000. Number of steps to finish: 20. Loss: 204.9748077392578 Reward: -10.0\nAgent: sacd_agent . Episode 1006/2000. Number of steps to finish: 16. Loss: 162.22169494628906 Reward: -4.0\nAgent: sacd_agent . Episode 1007/2000. Number of steps to finish: 20. Loss: 177.89195251464844 Reward: -8.0\nAgent: sacd_agent . Episode 1008/2000. Number of steps to finish: 14. Loss: 154.20919799804688 Reward: -2.0\nAgent: sacd_agent . Episode 1009/2000. Number of steps to finish: 20. Loss: 220.6288299560547 Reward: -10.0\nAgent: sacd_agent . Episode 1010/2000. Number of steps to finish: 20. Loss: 212.96853637695312 Reward: -10.0\nAgent: sacd_agent . Episode 1011/2000. Number of steps to finish: 20. Loss: 208.24600219726562 Reward: -10.0\nAgent: sacd_agent . Episode 1012/2000. Number of steps to finish: 19. Loss: 217.0561065673828 Reward: -7.0\nAgent: sacd_agent . Episode 1013/2000. Number of steps to finish: 20. Loss: 207.66244506835938 Reward: -8.0\nAgent: sacd_agent . Episode 1014/2000. Number of steps to finish: 13. Loss: 137.11903381347656 Reward: -1.0\nAgent: sacd_agent . Episode 1015/2000. Number of steps to finish: 20. Loss: 219.91879272460938 Reward: -10.0\nAgent: sacd_agent . Episode 1016/2000. Number of steps to finish: 9. Loss: 92.31599426269531 Reward: 3.0\nAgent: sacd_agent . Episode 1017/2000. Number of steps to finish: 20. Loss: 230.57054138183594 Reward: -16.0\nAgent: sacd_agent . Episode 1018/2000. Number of steps to finish: 20. Loss: 201.46681213378906 Reward: -10.0\nAgent: sacd_agent . Episode 1019/2000. Number of steps to finish: 19. Loss: 197.7545166015625 Reward: -7.0\nAgent: sacd_agent . Episode 1020/2000. Number of steps to finish: 12. Loss: 133.0109100341797 Reward: 0.0\nAgent: sacd_agent . Episode 1021/2000. Number of steps to finish: 20. Loss: 214.2484588623047 Reward: -10.0\nAgent: sacd_agent . Episode 1022/2000. Number of steps to finish: 17. Loss: 172.6299285888672 Reward: -5.0\nAgent: sacd_agent . Episode 1023/2000. Number of steps to finish: 20. Loss: 204.0161590576172 Reward: -10.0\nAgent: sacd_agent . Episode 1024/2000. Number of steps to finish: 20. Loss: 207.65126037597656 Reward: -12.0\nAgent: sacd_agent . Episode 1025/2000. Number of steps to finish: 20. Loss: 208.75257873535156 Reward: -14.0\nAgent: sacd_agent . Episode 1026/2000. Number of steps to finish: 17. Loss: 175.32199096679688 Reward: -5.0\nAgent: sacd_agent . Episode 1027/2000. Number of steps to finish: 20. Loss: 237.15206909179688 Reward: -14.0\nAgent: sacd_agent . Episode 1028/2000. Number of steps to finish: 20. Loss: 219.7288360595703 Reward: -10.0\nAgent: sacd_agent . Episode 1029/2000. Number of steps to finish: 20. Loss: 192.6133575439453 Reward: -10.0\nAgent: sacd_agent . Episode 1030/2000. Number of steps to finish: 17. Loss: 178.24838256835938 Reward: -5.0\nAgent: sacd_agent . Episode 1031/2000. Number of steps to finish: 13. Loss: 126.92224884033203 Reward: -1.0\nAgent: sacd_agent . Episode 1032/2000. Number of steps to finish: 20. Loss: 200.8935089111328 Reward: -16.0\nAgent: sacd_agent . Episode 1033/2000. Number of steps to finish: 14. Loss: 144.95458984375 Reward: -2.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 1034/2000. Number of steps to finish: 20. Loss: 200.32693481445312 Reward: -12.0\nAgent: sacd_agent . Episode 1035/2000. Number of steps to finish: 18. Loss: 199.0359344482422 Reward: -6.0\nAgent: sacd_agent . Episode 1036/2000. Number of steps to finish: 20. Loss: 237.2479705810547 Reward: -10.0\nAgent: sacd_agent . Episode 1037/2000. Number of steps to finish: 20. Loss: 196.67115783691406 Reward: -10.0\nAgent: sacd_agent . Episode 1038/2000. Number of steps to finish: 18. Loss: 203.9076690673828 Reward: -6.0\nAgent: sacd_agent . Episode 1039/2000. Number of steps to finish: 18. Loss: 215.06031799316406 Reward: -6.0\nAgent: sacd_agent . Episode 1040/2000. Number of steps to finish: 20. Loss: 213.13380432128906 Reward: -10.0\nAgent: sacd_agent . Episode 1041/2000. Number of steps to finish: 20. Loss: 220.91407775878906 Reward: -14.0\nAgent: sacd_agent . Episode 1042/2000. Number of steps to finish: 20. Loss: 207.485107421875 Reward: -12.0\nAgent: sacd_agent . Episode 1043/2000. Number of steps to finish: 14. Loss: 158.65243530273438 Reward: -2.0\nAgent: sacd_agent . Episode 1044/2000. Number of steps to finish: 9. Loss: 97.4430160522461 Reward: 3.0\nAgent: sacd_agent . Episode 1045/2000. Number of steps to finish: 20. Loss: 213.88107299804688 Reward: -10.0\nAgent: sacd_agent . Episode 1046/2000. Number of steps to finish: 14. Loss: 151.57809448242188 Reward: -2.0\nAgent: sacd_agent . Episode 1047/2000. Number of steps to finish: 11. Loss: 114.91482543945312 Reward: 1.0\nAgent: sacd_agent . Episode 1048/2000. Number of steps to finish: 15. Loss: 151.2698211669922 Reward: -3.0\nAgent: sacd_agent . Episode 1049/2000. Number of steps to finish: 20. Loss: 245.92417907714844 Reward: -14.0\nAgent: sacd_agent . Episode 1050/2000. Number of steps to finish: 20. Loss: 252.21847534179688 Reward: -12.0\nAgent: sacd_agent . Episode 1051/2000. Number of steps to finish: 14. Loss: 145.69049072265625 Reward: -2.0\nAgent: sacd_agent . Episode 1052/2000. Number of steps to finish: 15. Loss: 148.1446990966797 Reward: -3.0\nAgent: sacd_agent . Episode 1053/2000. Number of steps to finish: 20. Loss: 219.56785583496094 Reward: -8.0\nAgent: sacd_agent . Episode 1054/2000. Number of steps to finish: 14. Loss: 175.73683166503906 Reward: -2.0\nAgent: sacd_agent . Episode 1055/2000. Number of steps to finish: 9. Loss: 89.7023696899414 Reward: 3.0\nAgent: sacd_agent . Episode 1056/2000. Number of steps to finish: 20. Loss: 246.1565399169922 Reward: -14.0\nAgent: sacd_agent . Episode 1057/2000. Number of steps to finish: 15. Loss: 143.56524658203125 Reward: -3.0\nAgent: sacd_agent . Episode 1058/2000. Number of steps to finish: 19. Loss: 209.35292053222656 Reward: -7.0\nAgent: sacd_agent . Episode 1059/2000. Number of steps to finish: 16. Loss: 173.90609741210938 Reward: -4.0\nAgent: sacd_agent . Episode 1060/2000. Number of steps to finish: 9. Loss: 100.67208862304688 Reward: 3.0\nAgent: sacd_agent . Episode 1061/2000. Number of steps to finish: 10. Loss: 112.54669952392578 Reward: 2.0\nAgent: sacd_agent . Episode 1062/2000. Number of steps to finish: 20. Loss: 224.45631408691406 Reward: -12.0\nAgent: sacd_agent . Episode 1063/2000. Number of steps to finish: 9. Loss: 109.98126983642578 Reward: 3.0\nAgent: sacd_agent . Episode 1064/2000. Number of steps to finish: 11. Loss: 122.83013153076172 Reward: 1.0\nAgent: sacd_agent . Episode 1065/2000. Number of steps to finish: 16. Loss: 150.51406860351562 Reward: -4.0\nAgent: sacd_agent . Episode 1066/2000. Number of steps to finish: 20. Loss: 258.8028259277344 Reward: -14.0\nAgent: sacd_agent . Episode 1067/2000. Number of steps to finish: 17. Loss: 195.15757751464844 Reward: -5.0\nAgent: sacd_agent . Episode 1068/2000. Number of steps to finish: 9. Loss: 106.9372329711914 Reward: 3.0\nAgent: sacd_agent . Episode 1069/2000. Number of steps to finish: 12. Loss: 141.7476043701172 Reward: 0.0\nAgent: sacd_agent . Episode 1070/2000. Number of steps to finish: 20. Loss: 239.5797882080078 Reward: -14.0\nAgent: sacd_agent . Episode 1071/2000. Number of steps to finish: 20. Loss: 252.19137573242188 Reward: -10.0\nAgent: sacd_agent . Episode 1072/2000. Number of steps to finish: 12. Loss: 136.1879119873047 Reward: 0.0\nAgent: sacd_agent . Episode 1073/2000. Number of steps to finish: 16. Loss: 203.67315673828125 Reward: -4.0\nAgent: sacd_agent . Episode 1074/2000. Number of steps to finish: 20. Loss: 226.80421447753906 Reward: -10.0\nAgent: sacd_agent . Episode 1075/2000. Number of steps to finish: 18. Loss: 202.56825256347656 Reward: -6.0\nAgent: sacd_agent . Episode 1076/2000. Number of steps to finish: 11. Loss: 142.4375762939453 Reward: 1.0\nAgent: sacd_agent . Episode 1077/2000. Number of steps to finish: 20. Loss: 229.8379669189453 Reward: -10.0\nAgent: sacd_agent . Episode 1078/2000. Number of steps to finish: 12. Loss: 156.67144775390625 Reward: 0.0\nAgent: sacd_agent . Episode 1079/2000. Number of steps to finish: 20. Loss: 222.78724670410156 Reward: -12.0\nAgent: sacd_agent . Episode 1080/2000. Number of steps to finish: 12. Loss: 145.09365844726562 Reward: 0.0\nAgent: sacd_agent . Episode 1081/2000. Number of steps to finish: 19. Loss: 228.3070526123047 Reward: -7.0\nAgent: sacd_agent . Episode 1082/2000. Number of steps to finish: 20. Loss: 220.12509155273438 Reward: -12.0\nAgent: sacd_agent . Episode 1083/2000. Number of steps to finish: 20. Loss: 252.32766723632812 Reward: -12.0\nAgent: sacd_agent . Episode 1084/2000. Number of steps to finish: 20. Loss: 203.4959259033203 Reward: -10.0\nAgent: sacd_agent . Episode 1085/2000. Number of steps to finish: 15. Loss: 166.172119140625 Reward: -3.0\nAgent: sacd_agent . Episode 1086/2000. Number of steps to finish: 9. Loss: 108.75889587402344 Reward: 3.0\nAgent: sacd_agent . Episode 1087/2000. Number of steps to finish: 20. Loss: 237.41375732421875 Reward: -10.0\nAgent: sacd_agent . Episode 1088/2000. Number of steps to finish: 10. Loss: 136.7671661376953 Reward: 2.0\nAgent: sacd_agent . Episode 1089/2000. Number of steps to finish: 20. Loss: 215.26890563964844 Reward: -12.0\nAgent: sacd_agent . Episode 1090/2000. Number of steps to finish: 9. Loss: 119.84358215332031 Reward: 3.0\nAgent: sacd_agent . Episode 1091/2000. Number of steps to finish: 12. Loss: 144.34381103515625 Reward: 0.0\nAgent: sacd_agent . Episode 1092/2000. Number of steps to finish: 20. Loss: 243.602783203125 Reward: -10.0\nAgent: sacd_agent . Episode 1093/2000. Number of steps to finish: 19. Loss: 238.27401733398438 Reward: -7.0\nAgent: sacd_agent . Episode 1094/2000. Number of steps to finish: 20. Loss: 214.97525024414062 Reward: -8.0\nAgent: sacd_agent . Episode 1095/2000. Number of steps to finish: 13. Loss: 169.37730407714844 Reward: -1.0\nAgent: sacd_agent . Episode 1096/2000. Number of steps to finish: 12. Loss: 160.94094848632812 Reward: 0.0\nAgent: sacd_agent . Episode 1097/2000. Number of steps to finish: 17. Loss: 230.58181762695312 Reward: -5.0\nAgent: sacd_agent . Episode 1098/2000. Number of steps to finish: 20. Loss: 240.76199340820312 Reward: -10.0\nAgent: sacd_agent . Episode 1099/2000. Number of steps to finish: 20. Loss: 248.97946166992188 Reward: -8.0\nAgent: sacd_agent . Episode 1100/2000. Number of steps to finish: 12. Loss: 164.92868041992188 Reward: 0.0\nAgent: sacd_agent . Episode 1101/2000. Number of steps to finish: 19. Loss: 183.42051696777344 Reward: -7.0\nAgent: sacd_agent . Episode 1102/2000. Number of steps to finish: 17. Loss: 187.25648498535156 Reward: -5.0\nAgent: sacd_agent . Episode 1103/2000. Number of steps to finish: 17. Loss: 177.23114013671875 Reward: -5.0\nAgent: sacd_agent . Episode 1104/2000. Number of steps to finish: 20. Loss: 281.5615234375 Reward: -14.0\nAgent: sacd_agent . Episode 1105/2000. Number of steps to finish: 20. Loss: 249.98422241210938 Reward: -8.0\nAgent: sacd_agent . Episode 1106/2000. Number of steps to finish: 20. Loss: 246.12310791015625 Reward: -8.0\nAgent: sacd_agent . Episode 1107/2000. Number of steps to finish: 20. Loss: 254.5071258544922 Reward: -8.0\nAgent: sacd_agent . Episode 1108/2000. Number of steps to finish: 15. Loss: 184.40145874023438 Reward: -3.0\nAgent: sacd_agent . Episode 1109/2000. Number of steps to finish: 20. Loss: 274.1202087402344 Reward: -10.0\nAgent: sacd_agent . Episode 1110/2000. Number of steps to finish: 20. Loss: 235.03671264648438 Reward: -10.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 1111/2000. Number of steps to finish: 20. Loss: 263.9681396484375 Reward: -10.0\nAgent: sacd_agent . Episode 1112/2000. Number of steps to finish: 20. Loss: 235.9845733642578 Reward: -14.0\nAgent: sacd_agent . Episode 1113/2000. Number of steps to finish: 20. Loss: 271.0592041015625 Reward: -10.0\nAgent: sacd_agent . Episode 1114/2000. Number of steps to finish: 18. Loss: 238.7799072265625 Reward: -6.0\nAgent: sacd_agent . Episode 1115/2000. Number of steps to finish: 16. Loss: 181.17945861816406 Reward: -4.0\nAgent: sacd_agent . Episode 1116/2000. Number of steps to finish: 19. Loss: 231.2689971923828 Reward: -7.0\nAgent: sacd_agent . Episode 1117/2000. Number of steps to finish: 14. Loss: 170.12615966796875 Reward: -2.0\nAgent: sacd_agent . Episode 1118/2000. Number of steps to finish: 17. Loss: 208.4442138671875 Reward: -5.0\nAgent: sacd_agent . Episode 1119/2000. Number of steps to finish: 20. Loss: 313.2825622558594 Reward: -10.0\nAgent: sacd_agent . Episode 1120/2000. Number of steps to finish: 20. Loss: 266.111328125 Reward: -10.0\nAgent: sacd_agent . Episode 1121/2000. Number of steps to finish: 20. Loss: 288.2707824707031 Reward: -10.0\nAgent: sacd_agent . Episode 1122/2000. Number of steps to finish: 16. Loss: 202.38462829589844 Reward: -4.0\nAgent: sacd_agent . Episode 1123/2000. Number of steps to finish: 17. Loss: 203.83853149414062 Reward: -5.0\nAgent: sacd_agent . Episode 1124/2000. Number of steps to finish: 20. Loss: 241.64532470703125 Reward: -14.0\nAgent: sacd_agent . Episode 1125/2000. Number of steps to finish: 11. Loss: 157.67308044433594 Reward: 1.0\nAgent: sacd_agent . Episode 1126/2000. Number of steps to finish: 20. Loss: 268.6153564453125 Reward: -8.0\nAgent: sacd_agent . Episode 1127/2000. Number of steps to finish: 16. Loss: 225.71697998046875 Reward: -4.0\nAgent: sacd_agent . Episode 1128/2000. Number of steps to finish: 9. Loss: 115.38523864746094 Reward: 3.0\nAgent: sacd_agent . Episode 1129/2000. Number of steps to finish: 20. Loss: 295.5968017578125 Reward: -10.0\nAgent: sacd_agent . Episode 1130/2000. Number of steps to finish: 14. Loss: 181.6821746826172 Reward: -2.0\nAgent: sacd_agent . Episode 1131/2000. Number of steps to finish: 18. Loss: 210.92535400390625 Reward: -6.0\nAgent: sacd_agent . Episode 1132/2000. Number of steps to finish: 17. Loss: 244.03536987304688 Reward: -5.0\nAgent: sacd_agent . Episode 1133/2000. Number of steps to finish: 8. Loss: 102.77429962158203 Reward: 4.0\nAgent: sacd_agent . Episode 1134/2000. Number of steps to finish: 20. Loss: 227.74388122558594 Reward: -8.0\nAgent: sacd_agent . Episode 1135/2000. Number of steps to finish: 19. Loss: 218.3639373779297 Reward: -7.0\nAgent: sacd_agent . Episode 1136/2000. Number of steps to finish: 19. Loss: 246.9344482421875 Reward: -7.0\nAgent: sacd_agent . Episode 1137/2000. Number of steps to finish: 20. Loss: 254.5338592529297 Reward: -12.0\nAgent: sacd_agent . Episode 1138/2000. Number of steps to finish: 8. Loss: 115.91973114013672 Reward: 4.0\nAgent: sacd_agent . Episode 1139/2000. Number of steps to finish: 13. Loss: 198.57887268066406 Reward: -1.0\nAgent: sacd_agent . Episode 1140/2000. Number of steps to finish: 17. Loss: 249.4692840576172 Reward: -5.0\nAgent: sacd_agent . Episode 1141/2000. Number of steps to finish: 20. Loss: 257.25555419921875 Reward: -10.0\nAgent: sacd_agent . Episode 1142/2000. Number of steps to finish: 20. Loss: 266.0733337402344 Reward: -12.0\nAgent: sacd_agent . Episode 1143/2000. Number of steps to finish: 20. Loss: 252.95489501953125 Reward: -10.0\nAgent: sacd_agent . Episode 1144/2000. Number of steps to finish: 20. Loss: 271.10247802734375 Reward: -12.0\nAgent: sacd_agent . Episode 1145/2000. Number of steps to finish: 20. Loss: 278.7044677734375 Reward: -10.0\nAgent: sacd_agent . Episode 1146/2000. Number of steps to finish: 20. Loss: 281.9737243652344 Reward: -14.0\nAgent: sacd_agent . Episode 1147/2000. Number of steps to finish: 12. Loss: 180.6074676513672 Reward: 0.0\nAgent: sacd_agent . Episode 1148/2000. Number of steps to finish: 20. Loss: 292.18817138671875 Reward: -12.0\nAgent: sacd_agent . Episode 1149/2000. Number of steps to finish: 20. Loss: 255.72720336914062 Reward: -12.0\nAgent: sacd_agent . Episode 1150/2000. Number of steps to finish: 20. Loss: 252.91343688964844 Reward: -12.0\nAgent: sacd_agent . Episode 1151/2000. Number of steps to finish: 20. Loss: 271.7387390136719 Reward: -12.0\nAgent: sacd_agent . Episode 1152/2000. Number of steps to finish: 20. Loss: 256.39715576171875 Reward: -12.0\nAgent: sacd_agent . Episode 1153/2000. Number of steps to finish: 20. Loss: 257.23358154296875 Reward: -12.0\nAgent: sacd_agent . Episode 1154/2000. Number of steps to finish: 16. Loss: 214.6488494873047 Reward: -4.0\nAgent: sacd_agent . Episode 1155/2000. Number of steps to finish: 20. Loss: 287.2691345214844 Reward: -10.0\nAgent: sacd_agent . Episode 1156/2000. Number of steps to finish: 20. Loss: 310.8806457519531 Reward: -12.0\nAgent: sacd_agent . Episode 1157/2000. Number of steps to finish: 17. Loss: 218.99386596679688 Reward: -5.0\nAgent: sacd_agent . Episode 1158/2000. Number of steps to finish: 11. Loss: 135.78659057617188 Reward: 1.0\nAgent: sacd_agent . Episode 1159/2000. Number of steps to finish: 20. Loss: 285.3968200683594 Reward: -18.0\nAgent: sacd_agent . Episode 1160/2000. Number of steps to finish: 8. Loss: 104.93655395507812 Reward: 4.0\nAgent: sacd_agent . Episode 1161/2000. Number of steps to finish: 15. Loss: 204.913330078125 Reward: -3.0\nAgent: sacd_agent . Episode 1162/2000. Number of steps to finish: 20. Loss: 287.3666076660156 Reward: -12.0\nAgent: sacd_agent . Episode 1163/2000. Number of steps to finish: 13. Loss: 170.76312255859375 Reward: -1.0\nAgent: sacd_agent . Episode 1164/2000. Number of steps to finish: 20. Loss: 279.55157470703125 Reward: -10.0\nAgent: sacd_agent . Episode 1165/2000. Number of steps to finish: 17. Loss: 224.9620361328125 Reward: -5.0\nAgent: sacd_agent . Episode 1166/2000. Number of steps to finish: 11. Loss: 138.45553588867188 Reward: 1.0\nAgent: sacd_agent . Episode 1167/2000. Number of steps to finish: 13. Loss: 183.0729522705078 Reward: -1.0\nAgent: sacd_agent . Episode 1168/2000. Number of steps to finish: 19. Loss: 258.10430908203125 Reward: -7.0\nAgent: sacd_agent . Episode 1169/2000. Number of steps to finish: 18. Loss: 223.66444396972656 Reward: -6.0\nAgent: sacd_agent . Episode 1170/2000. Number of steps to finish: 19. Loss: 307.64361572265625 Reward: -7.0\nAgent: sacd_agent . Episode 1171/2000. Number of steps to finish: 14. Loss: 194.94288635253906 Reward: -2.0\nAgent: sacd_agent . Episode 1172/2000. Number of steps to finish: 20. Loss: 251.91563415527344 Reward: -12.0\nAgent: sacd_agent . Episode 1173/2000. Number of steps to finish: 8. Loss: 112.59649658203125 Reward: 4.0\nAgent: sacd_agent . Episode 1174/2000. Number of steps to finish: 11. Loss: 140.34762573242188 Reward: 1.0\nAgent: sacd_agent . Episode 1175/2000. Number of steps to finish: 11. Loss: 154.64370727539062 Reward: 1.0\nAgent: sacd_agent . Episode 1176/2000. Number of steps to finish: 20. Loss: 223.0271759033203 Reward: -10.0\nAgent: sacd_agent . Episode 1177/2000. Number of steps to finish: 20. Loss: 279.51507568359375 Reward: -10.0\nAgent: sacd_agent . Episode 1178/2000. Number of steps to finish: 18. Loss: 226.82144165039062 Reward: -6.0\nAgent: sacd_agent . Episode 1179/2000. Number of steps to finish: 20. Loss: 284.1280517578125 Reward: -14.0\nAgent: sacd_agent . Episode 1180/2000. Number of steps to finish: 9. Loss: 117.73445129394531 Reward: 3.0\nAgent: sacd_agent . Episode 1181/2000. Number of steps to finish: 15. Loss: 199.3959197998047 Reward: -3.0\nAgent: sacd_agent . Episode 1182/2000. Number of steps to finish: 20. Loss: 305.9273681640625 Reward: -10.0\nAgent: sacd_agent . Episode 1183/2000. Number of steps to finish: 20. Loss: 271.0669860839844 Reward: -12.0\nAgent: sacd_agent . Episode 1184/2000. Number of steps to finish: 20. Loss: 274.5277404785156 Reward: -10.0\nAgent: sacd_agent . Episode 1185/2000. Number of steps to finish: 20. Loss: 308.6726989746094 Reward: -14.0\nAgent: sacd_agent . Episode 1186/2000. Number of steps to finish: 20. Loss: 256.32745361328125 Reward: -8.0\nAgent: sacd_agent . Episode 1187/2000. Number of steps to finish: 14. Loss: 187.2598419189453 Reward: -2.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 1188/2000. Number of steps to finish: 20. Loss: 276.40625 Reward: -14.0\nAgent: sacd_agent . Episode 1189/2000. Number of steps to finish: 20. Loss: 310.60089111328125 Reward: -14.0\nAgent: sacd_agent . Episode 1190/2000. Number of steps to finish: 19. Loss: 251.36268615722656 Reward: -7.0\nAgent: sacd_agent . Episode 1191/2000. Number of steps to finish: 20. Loss: 294.8547058105469 Reward: -12.0\nAgent: sacd_agent . Episode 1192/2000. Number of steps to finish: 16. Loss: 237.47537231445312 Reward: -4.0\nAgent: sacd_agent . Episode 1193/2000. Number of steps to finish: 16. Loss: 281.1026611328125 Reward: -4.0\nAgent: sacd_agent . Episode 1194/2000. Number of steps to finish: 20. Loss: 311.4430847167969 Reward: -10.0\nAgent: sacd_agent . Episode 1195/2000. Number of steps to finish: 20. Loss: 290.2864685058594 Reward: -8.0\nAgent: sacd_agent . Episode 1196/2000. Number of steps to finish: 20. Loss: 294.5580139160156 Reward: -10.0\nAgent: sacd_agent . Episode 1197/2000. Number of steps to finish: 13. Loss: 195.16217041015625 Reward: -1.0\nAgent: sacd_agent . Episode 1198/2000. Number of steps to finish: 17. Loss: 243.68321228027344 Reward: -5.0\nAgent: sacd_agent . Episode 1199/2000. Number of steps to finish: 20. Loss: 275.42041015625 Reward: -10.0\nAgent: sacd_agent . Episode 1200/2000. Number of steps to finish: 13. Loss: 202.20364379882812 Reward: -1.0\nAgent: sacd_agent . Episode 1201/2000. Number of steps to finish: 20. Loss: 277.4662170410156 Reward: -10.0\nAgent: sacd_agent . Episode 1202/2000. Number of steps to finish: 20. Loss: 320.0666198730469 Reward: -10.0\nAgent: sacd_agent . Episode 1203/2000. Number of steps to finish: 13. Loss: 182.8783721923828 Reward: -1.0\nAgent: sacd_agent . Episode 1204/2000. Number of steps to finish: 20. Loss: 254.16384887695312 Reward: -10.0\nAgent: sacd_agent . Episode 1205/2000. Number of steps to finish: 20. Loss: 303.823974609375 Reward: -10.0\nAgent: sacd_agent . Episode 1206/2000. Number of steps to finish: 19. Loss: 251.5872344970703 Reward: -7.0\nAgent: sacd_agent . Episode 1207/2000. Number of steps to finish: 20. Loss: 292.9615173339844 Reward: -10.0\nAgent: sacd_agent . Episode 1208/2000. Number of steps to finish: 13. Loss: 176.175048828125 Reward: -1.0\nAgent: sacd_agent . Episode 1209/2000. Number of steps to finish: 20. Loss: 343.48114013671875 Reward: -10.0\nAgent: sacd_agent . Episode 1210/2000. Number of steps to finish: 18. Loss: 255.7296142578125 Reward: -6.0\nAgent: sacd_agent . Episode 1211/2000. Number of steps to finish: 11. Loss: 147.1239471435547 Reward: 1.0\nAgent: sacd_agent . Episode 1212/2000. Number of steps to finish: 20. Loss: 298.1479797363281 Reward: -12.0\nAgent: sacd_agent . Episode 1213/2000. Number of steps to finish: 9. Loss: 119.7353515625 Reward: 3.0\nAgent: sacd_agent . Episode 1214/2000. Number of steps to finish: 20. Loss: 310.5387878417969 Reward: -14.0\nAgent: sacd_agent . Episode 1215/2000. Number of steps to finish: 18. Loss: 260.4830627441406 Reward: -6.0\nAgent: sacd_agent . Episode 1216/2000. Number of steps to finish: 20. Loss: 302.7271728515625 Reward: -16.0\nAgent: sacd_agent . Episode 1217/2000. Number of steps to finish: 20. Loss: 300.7541809082031 Reward: -8.0\nAgent: sacd_agent . Episode 1218/2000. Number of steps to finish: 19. Loss: 280.25213623046875 Reward: -7.0\nAgent: sacd_agent . Episode 1219/2000. Number of steps to finish: 17. Loss: 299.71319580078125 Reward: -5.0\nAgent: sacd_agent . Episode 1220/2000. Number of steps to finish: 20. Loss: 273.4557800292969 Reward: -10.0\nAgent: sacd_agent . Episode 1221/2000. Number of steps to finish: 13. Loss: 202.1962127685547 Reward: -1.0\nAgent: sacd_agent . Episode 1222/2000. Number of steps to finish: 20. Loss: 321.05584716796875 Reward: -12.0\nAgent: sacd_agent . Episode 1223/2000. Number of steps to finish: 18. Loss: 264.3536682128906 Reward: -6.0\nAgent: sacd_agent . Episode 1224/2000. Number of steps to finish: 13. Loss: 207.45864868164062 Reward: -1.0\nAgent: sacd_agent . Episode 1225/2000. Number of steps to finish: 19. Loss: 281.98187255859375 Reward: -7.0\nAgent: sacd_agent . Episode 1226/2000. Number of steps to finish: 13. Loss: 197.33172607421875 Reward: -1.0\nAgent: sacd_agent . Episode 1227/2000. Number of steps to finish: 17. Loss: 290.2962341308594 Reward: -5.0\nAgent: sacd_agent . Episode 1228/2000. Number of steps to finish: 15. Loss: 235.02886962890625 Reward: -3.0\nAgent: sacd_agent . Episode 1229/2000. Number of steps to finish: 20. Loss: 310.78338623046875 Reward: -12.0\nAgent: sacd_agent . Episode 1230/2000. Number of steps to finish: 20. Loss: 311.24468994140625 Reward: -12.0\nAgent: sacd_agent . Episode 1231/2000. Number of steps to finish: 18. Loss: 291.2134094238281 Reward: -6.0\nAgent: sacd_agent . Episode 1232/2000. Number of steps to finish: 12. Loss: 168.7910919189453 Reward: 0.0\nAgent: sacd_agent . Episode 1233/2000. Number of steps to finish: 20. Loss: 349.8368225097656 Reward: -14.0\nAgent: sacd_agent . Episode 1234/2000. Number of steps to finish: 20. Loss: 344.41827392578125 Reward: -10.0\nAgent: sacd_agent . Episode 1235/2000. Number of steps to finish: 20. Loss: 319.89105224609375 Reward: -10.0\nAgent: sacd_agent . Episode 1236/2000. Number of steps to finish: 14. Loss: 221.864013671875 Reward: -2.0\nAgent: sacd_agent . Episode 1237/2000. Number of steps to finish: 17. Loss: 243.5697021484375 Reward: -5.0\nAgent: sacd_agent . Episode 1238/2000. Number of steps to finish: 18. Loss: 276.3090515136719 Reward: -6.0\nAgent: sacd_agent . Episode 1239/2000. Number of steps to finish: 20. Loss: 310.11767578125 Reward: -12.0\nAgent: sacd_agent . Episode 1240/2000. Number of steps to finish: 16. Loss: 234.15850830078125 Reward: -4.0\nAgent: sacd_agent . Episode 1241/2000. Number of steps to finish: 20. Loss: 279.1271667480469 Reward: -14.0\nAgent: sacd_agent . Episode 1242/2000. Number of steps to finish: 20. Loss: 272.038818359375 Reward: -8.0\nAgent: sacd_agent . Episode 1243/2000. Number of steps to finish: 20. Loss: 304.4653015136719 Reward: -10.0\nAgent: sacd_agent . Episode 1244/2000. Number of steps to finish: 11. Loss: 193.27845764160156 Reward: 1.0\nAgent: sacd_agent . Episode 1245/2000. Number of steps to finish: 20. Loss: 328.307861328125 Reward: -10.0\nAgent: sacd_agent . Episode 1246/2000. Number of steps to finish: 12. Loss: 188.45669555664062 Reward: 0.0\nAgent: sacd_agent . Episode 1247/2000. Number of steps to finish: 15. Loss: 202.65603637695312 Reward: -3.0\nAgent: sacd_agent . Episode 1248/2000. Number of steps to finish: 20. Loss: 302.12225341796875 Reward: -14.0\nAgent: sacd_agent . Episode 1249/2000. Number of steps to finish: 16. Loss: 247.06195068359375 Reward: -4.0\nAgent: sacd_agent . Episode 1250/2000. Number of steps to finish: 20. Loss: 303.63494873046875 Reward: -14.0\nAgent: sacd_agent . Episode 1251/2000. Number of steps to finish: 20. Loss: 316.13372802734375 Reward: -12.0\nAgent: sacd_agent . Episode 1252/2000. Number of steps to finish: 19. Loss: 294.2756652832031 Reward: -7.0\nAgent: sacd_agent . Episode 1253/2000. Number of steps to finish: 20. Loss: 298.48284912109375 Reward: -10.0\nAgent: sacd_agent . Episode 1254/2000. Number of steps to finish: 20. Loss: 327.637451171875 Reward: -14.0\nAgent: sacd_agent . Episode 1255/2000. Number of steps to finish: 11. Loss: 156.87083435058594 Reward: 1.0\nAgent: sacd_agent . Episode 1256/2000. Number of steps to finish: 12. Loss: 195.5405731201172 Reward: 0.0\nAgent: sacd_agent . Episode 1257/2000. Number of steps to finish: 20. Loss: 364.8526916503906 Reward: -12.0\nAgent: sacd_agent . Episode 1258/2000. Number of steps to finish: 18. Loss: 265.8859558105469 Reward: -6.0\nAgent: sacd_agent . Episode 1259/2000. Number of steps to finish: 9. Loss: 134.53643798828125 Reward: 3.0\nAgent: sacd_agent . Episode 1260/2000. Number of steps to finish: 13. Loss: 190.97216796875 Reward: -1.0\nAgent: sacd_agent . Episode 1261/2000. Number of steps to finish: 12. Loss: 175.2445068359375 Reward: 0.0\nAgent: sacd_agent . Episode 1262/2000. Number of steps to finish: 17. Loss: 294.9720458984375 Reward: -5.0\nAgent: sacd_agent . Episode 1263/2000. Number of steps to finish: 11. Loss: 191.86624145507812 Reward: 1.0\nAgent: sacd_agent . Episode 1264/2000. Number of steps to finish: 20. Loss: 353.5554504394531 Reward: -12.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 1265/2000. Number of steps to finish: 20. Loss: 303.43548583984375 Reward: -16.0\nAgent: sacd_agent . Episode 1266/2000. Number of steps to finish: 11. Loss: 187.91720581054688 Reward: 1.0\nAgent: sacd_agent . Episode 1267/2000. Number of steps to finish: 11. Loss: 164.7624969482422 Reward: 1.0\nAgent: sacd_agent . Episode 1268/2000. Number of steps to finish: 20. Loss: 343.0595397949219 Reward: -8.0\nAgent: sacd_agent . Episode 1269/2000. Number of steps to finish: 9. Loss: 124.41193389892578 Reward: 3.0\nAgent: sacd_agent . Episode 1270/2000. Number of steps to finish: 20. Loss: 327.7594909667969 Reward: -8.0\nAgent: sacd_agent . Episode 1271/2000. Number of steps to finish: 20. Loss: 286.8105773925781 Reward: -16.0\nAgent: sacd_agent . Episode 1272/2000. Number of steps to finish: 12. Loss: 202.98313903808594 Reward: 0.0\nAgent: sacd_agent . Episode 1273/2000. Number of steps to finish: 20. Loss: 344.9091796875 Reward: -12.0\nAgent: sacd_agent . Episode 1274/2000. Number of steps to finish: 17. Loss: 318.8959045410156 Reward: -5.0\nAgent: sacd_agent . Episode 1275/2000. Number of steps to finish: 20. Loss: 340.2488098144531 Reward: -12.0\nAgent: sacd_agent . Episode 1276/2000. Number of steps to finish: 20. Loss: 284.24798583984375 Reward: -10.0\nAgent: sacd_agent . Episode 1277/2000. Number of steps to finish: 20. Loss: 306.6963195800781 Reward: -10.0\nAgent: sacd_agent . Episode 1278/2000. Number of steps to finish: 15. Loss: 256.097900390625 Reward: -3.0\nAgent: sacd_agent . Episode 1279/2000. Number of steps to finish: 10. Loss: 174.2552032470703 Reward: 2.0\nAgent: sacd_agent . Episode 1280/2000. Number of steps to finish: 20. Loss: 334.9327087402344 Reward: -10.0\nAgent: sacd_agent . Episode 1281/2000. Number of steps to finish: 20. Loss: 326.1258239746094 Reward: -12.0\nAgent: sacd_agent . Episode 1282/2000. Number of steps to finish: 20. Loss: 325.7826843261719 Reward: -14.0\nAgent: sacd_agent . Episode 1283/2000. Number of steps to finish: 15. Loss: 264.19073486328125 Reward: -3.0\nAgent: sacd_agent . Episode 1284/2000. Number of steps to finish: 20. Loss: 363.6634826660156 Reward: -8.0\nAgent: sacd_agent . Episode 1285/2000. Number of steps to finish: 20. Loss: 328.0013122558594 Reward: -14.0\nAgent: sacd_agent . Episode 1286/2000. Number of steps to finish: 20. Loss: 327.1724548339844 Reward: -10.0\nAgent: sacd_agent . Episode 1287/2000. Number of steps to finish: 13. Loss: 214.07745361328125 Reward: -1.0\nAgent: sacd_agent . Episode 1288/2000. Number of steps to finish: 20. Loss: 361.57501220703125 Reward: -12.0\nAgent: sacd_agent . Episode 1289/2000. Number of steps to finish: 8. Loss: 120.27963256835938 Reward: 4.0\nAgent: sacd_agent . Episode 1290/2000. Number of steps to finish: 17. Loss: 294.3311767578125 Reward: -5.0\nAgent: sacd_agent . Episode 1291/2000. Number of steps to finish: 10. Loss: 158.00930786132812 Reward: 2.0\nAgent: sacd_agent . Episode 1292/2000. Number of steps to finish: 20. Loss: 313.1174011230469 Reward: -14.0\nAgent: sacd_agent . Episode 1293/2000. Number of steps to finish: 20. Loss: 366.55029296875 Reward: -10.0\nAgent: sacd_agent . Episode 1294/2000. Number of steps to finish: 19. Loss: 319.5666809082031 Reward: -7.0\nAgent: sacd_agent . Episode 1295/2000. Number of steps to finish: 16. Loss: 237.68731689453125 Reward: -4.0\nAgent: sacd_agent . Episode 1296/2000. Number of steps to finish: 20. Loss: 354.4443664550781 Reward: -10.0\nAgent: sacd_agent . Episode 1297/2000. Number of steps to finish: 20. Loss: 325.11810302734375 Reward: -14.0\nAgent: sacd_agent . Episode 1298/2000. Number of steps to finish: 14. Loss: 238.90664672851562 Reward: -2.0\nAgent: sacd_agent . Episode 1299/2000. Number of steps to finish: 20. Loss: 290.3404541015625 Reward: -10.0\nAgent: sacd_agent . Episode 1300/2000. Number of steps to finish: 7. Loss: 114.1905517578125 Reward: 5.0\nAgent: sacd_agent . Episode 1301/2000. Number of steps to finish: 12. Loss: 206.58590698242188 Reward: 0.0\nAgent: sacd_agent . Episode 1302/2000. Number of steps to finish: 12. Loss: 195.6263427734375 Reward: 0.0\nAgent: sacd_agent . Episode 1303/2000. Number of steps to finish: 20. Loss: 372.5976867675781 Reward: -10.0\nAgent: sacd_agent . Episode 1304/2000. Number of steps to finish: 20. Loss: 346.7431640625 Reward: -8.0\nAgent: sacd_agent . Episode 1305/2000. Number of steps to finish: 20. Loss: 311.6951904296875 Reward: -10.0\nAgent: sacd_agent . Episode 1306/2000. Number of steps to finish: 16. Loss: 304.7659912109375 Reward: -4.0\nAgent: sacd_agent . Episode 1307/2000. Number of steps to finish: 18. Loss: 288.2330322265625 Reward: -6.0\nAgent: sacd_agent . Episode 1308/2000. Number of steps to finish: 20. Loss: 383.3741455078125 Reward: -12.0\nAgent: sacd_agent . Episode 1309/2000. Number of steps to finish: 14. Loss: 248.52285766601562 Reward: -2.0\nAgent: sacd_agent . Episode 1310/2000. Number of steps to finish: 12. Loss: 247.61500549316406 Reward: 0.0\nAgent: sacd_agent . Episode 1311/2000. Number of steps to finish: 20. Loss: 334.2478332519531 Reward: -10.0\nAgent: sacd_agent . Episode 1312/2000. Number of steps to finish: 12. Loss: 226.25868225097656 Reward: 0.0\nAgent: sacd_agent . Episode 1313/2000. Number of steps to finish: 14. Loss: 279.00543212890625 Reward: -2.0\nAgent: sacd_agent . Episode 1314/2000. Number of steps to finish: 20. Loss: 329.2239990234375 Reward: -10.0\nAgent: sacd_agent . Episode 1315/2000. Number of steps to finish: 20. Loss: 364.2391662597656 Reward: -10.0\nAgent: sacd_agent . Episode 1316/2000. Number of steps to finish: 20. Loss: 308.0062561035156 Reward: -12.0\nAgent: sacd_agent . Episode 1317/2000. Number of steps to finish: 18. Loss: 304.59027099609375 Reward: -6.0\nAgent: sacd_agent . Episode 1318/2000. Number of steps to finish: 20. Loss: 364.0235290527344 Reward: -12.0\nAgent: sacd_agent . Episode 1319/2000. Number of steps to finish: 8. Loss: 130.93698120117188 Reward: 4.0\nAgent: sacd_agent . Episode 1320/2000. Number of steps to finish: 16. Loss: 270.0971374511719 Reward: -4.0\nAgent: sacd_agent . Episode 1321/2000. Number of steps to finish: 8. Loss: 145.6481170654297 Reward: 4.0\nAgent: sacd_agent . Episode 1322/2000. Number of steps to finish: 19. Loss: 367.3737487792969 Reward: -7.0\nAgent: sacd_agent . Episode 1323/2000. Number of steps to finish: 20. Loss: 357.4612121582031 Reward: -10.0\nAgent: sacd_agent . Episode 1324/2000. Number of steps to finish: 16. Loss: 269.04803466796875 Reward: -4.0\nAgent: sacd_agent . Episode 1325/2000. Number of steps to finish: 20. Loss: 364.424560546875 Reward: -10.0\nAgent: sacd_agent . Episode 1326/2000. Number of steps to finish: 20. Loss: 340.0950622558594 Reward: -12.0\nAgent: sacd_agent . Episode 1327/2000. Number of steps to finish: 17. Loss: 313.3384704589844 Reward: -5.0\nAgent: sacd_agent . Episode 1328/2000. Number of steps to finish: 20. Loss: 365.67169189453125 Reward: -16.0\nAgent: sacd_agent . Episode 1329/2000. Number of steps to finish: 20. Loss: 313.8187561035156 Reward: -12.0\nAgent: sacd_agent . Episode 1330/2000. Number of steps to finish: 16. Loss: 258.9557800292969 Reward: -4.0\nAgent: sacd_agent . Episode 1331/2000. Number of steps to finish: 20. Loss: 346.6597900390625 Reward: -10.0\nAgent: sacd_agent . Episode 1332/2000. Number of steps to finish: 15. Loss: 268.5302734375 Reward: -3.0\nAgent: sacd_agent . Episode 1333/2000. Number of steps to finish: 20. Loss: 364.61090087890625 Reward: -8.0\nAgent: sacd_agent . Episode 1334/2000. Number of steps to finish: 14. Loss: 206.63169860839844 Reward: -2.0\nAgent: sacd_agent . Episode 1335/2000. Number of steps to finish: 20. Loss: 344.0764465332031 Reward: -12.0\nAgent: sacd_agent . Episode 1336/2000. Number of steps to finish: 15. Loss: 234.68453979492188 Reward: -3.0\nAgent: sacd_agent . Episode 1337/2000. Number of steps to finish: 20. Loss: 418.47918701171875 Reward: -10.0\nAgent: sacd_agent . Episode 1338/2000. Number of steps to finish: 17. Loss: 280.0279541015625 Reward: -5.0\nAgent: sacd_agent . Episode 1339/2000. Number of steps to finish: 14. Loss: 268.6483459472656 Reward: -2.0\nAgent: sacd_agent . Episode 1340/2000. Number of steps to finish: 20. Loss: 356.4183349609375 Reward: -10.0\nAgent: sacd_agent . Episode 1341/2000. Number of steps to finish: 16. Loss: 297.139404296875 Reward: -4.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 1342/2000. Number of steps to finish: 19. Loss: 304.1859436035156 Reward: -7.0\nAgent: sacd_agent . Episode 1343/2000. Number of steps to finish: 20. Loss: 372.20574951171875 Reward: -10.0\nAgent: sacd_agent . Episode 1344/2000. Number of steps to finish: 20. Loss: 366.19189453125 Reward: -10.0\nAgent: sacd_agent . Episode 1345/2000. Number of steps to finish: 16. Loss: 282.6368713378906 Reward: -4.0\nAgent: sacd_agent . Episode 1346/2000. Number of steps to finish: 14. Loss: 265.02679443359375 Reward: -2.0\nAgent: sacd_agent . Episode 1347/2000. Number of steps to finish: 9. Loss: 172.49647521972656 Reward: 3.0\nAgent: sacd_agent . Episode 1348/2000. Number of steps to finish: 10. Loss: 179.54383850097656 Reward: 2.0\nAgent: sacd_agent . Episode 1349/2000. Number of steps to finish: 20. Loss: 310.058837890625 Reward: -14.0\nAgent: sacd_agent . Episode 1350/2000. Number of steps to finish: 20. Loss: 363.4700012207031 Reward: -14.0\nAgent: sacd_agent . Episode 1351/2000. Number of steps to finish: 19. Loss: 356.7785339355469 Reward: -7.0\nAgent: sacd_agent . Episode 1352/2000. Number of steps to finish: 20. Loss: 360.3497009277344 Reward: -10.0\nAgent: sacd_agent . Episode 1353/2000. Number of steps to finish: 19. Loss: 332.1912841796875 Reward: -7.0\nAgent: sacd_agent . Episode 1354/2000. Number of steps to finish: 20. Loss: 368.7015686035156 Reward: -10.0\nAgent: sacd_agent . Episode 1355/2000. Number of steps to finish: 10. Loss: 178.86941528320312 Reward: 2.0\nAgent: sacd_agent . Episode 1356/2000. Number of steps to finish: 15. Loss: 276.9130554199219 Reward: -3.0\nAgent: sacd_agent . Episode 1357/2000. Number of steps to finish: 12. Loss: 208.19029235839844 Reward: 0.0\nAgent: sacd_agent . Episode 1358/2000. Number of steps to finish: 20. Loss: 375.1896667480469 Reward: -12.0\nAgent: sacd_agent . Episode 1359/2000. Number of steps to finish: 20. Loss: 343.5477294921875 Reward: -8.0\nAgent: sacd_agent . Episode 1360/2000. Number of steps to finish: 16. Loss: 305.3972473144531 Reward: -4.0\nAgent: sacd_agent . Episode 1361/2000. Number of steps to finish: 19. Loss: 323.4792785644531 Reward: -7.0\nAgent: sacd_agent . Episode 1362/2000. Number of steps to finish: 20. Loss: 399.522216796875 Reward: -10.0\nAgent: sacd_agent . Episode 1363/2000. Number of steps to finish: 20. Loss: 305.7323913574219 Reward: -10.0\nAgent: sacd_agent . Episode 1364/2000. Number of steps to finish: 12. Loss: 213.84957885742188 Reward: 0.0\nAgent: sacd_agent . Episode 1365/2000. Number of steps to finish: 15. Loss: 278.4744873046875 Reward: -3.0\nAgent: sacd_agent . Episode 1366/2000. Number of steps to finish: 20. Loss: 383.0841369628906 Reward: -16.0\nAgent: sacd_agent . Episode 1367/2000. Number of steps to finish: 15. Loss: 318.17498779296875 Reward: -3.0\nAgent: sacd_agent . Episode 1368/2000. Number of steps to finish: 20. Loss: 341.4333801269531 Reward: -10.0\nAgent: sacd_agent . Episode 1369/2000. Number of steps to finish: 20. Loss: 398.8656005859375 Reward: -10.0\nAgent: sacd_agent . Episode 1370/2000. Number of steps to finish: 14. Loss: 250.6403350830078 Reward: -2.0\nAgent: sacd_agent . Episode 1371/2000. Number of steps to finish: 20. Loss: 352.837646484375 Reward: -8.0\nAgent: sacd_agent . Episode 1372/2000. Number of steps to finish: 20. Loss: 327.50732421875 Reward: -12.0\nAgent: sacd_agent . Episode 1373/2000. Number of steps to finish: 12. Loss: 222.79150390625 Reward: 0.0\nAgent: sacd_agent . Episode 1374/2000. Number of steps to finish: 20. Loss: 346.8194885253906 Reward: -10.0\nAgent: sacd_agent . Episode 1375/2000. Number of steps to finish: 13. Loss: 236.6921844482422 Reward: -1.0\nAgent: sacd_agent . Episode 1376/2000. Number of steps to finish: 17. Loss: 329.99835205078125 Reward: -5.0\nAgent: sacd_agent . Episode 1377/2000. Number of steps to finish: 20. Loss: 384.47418212890625 Reward: -10.0\nAgent: sacd_agent . Episode 1378/2000. Number of steps to finish: 13. Loss: 252.120849609375 Reward: -1.0\nAgent: sacd_agent . Episode 1379/2000. Number of steps to finish: 20. Loss: 363.8746643066406 Reward: -14.0\nAgent: sacd_agent . Episode 1380/2000. Number of steps to finish: 20. Loss: 365.71588134765625 Reward: -10.0\nAgent: sacd_agent . Episode 1381/2000. Number of steps to finish: 10. Loss: 201.32818603515625 Reward: 2.0\nAgent: sacd_agent . Episode 1382/2000. Number of steps to finish: 20. Loss: 399.6995849609375 Reward: -12.0\nAgent: sacd_agent . Episode 1383/2000. Number of steps to finish: 19. Loss: 348.5137023925781 Reward: -7.0\nAgent: sacd_agent . Episode 1384/2000. Number of steps to finish: 20. Loss: 342.8936462402344 Reward: -14.0\nAgent: sacd_agent . Episode 1385/2000. Number of steps to finish: 9. Loss: 166.16004943847656 Reward: 3.0\nAgent: sacd_agent . Episode 1386/2000. Number of steps to finish: 20. Loss: 376.1043701171875 Reward: -12.0\nAgent: sacd_agent . Episode 1387/2000. Number of steps to finish: 20. Loss: 388.2890625 Reward: -10.0\nAgent: sacd_agent . Episode 1388/2000. Number of steps to finish: 16. Loss: 272.14117431640625 Reward: -4.0\nAgent: sacd_agent . Episode 1389/2000. Number of steps to finish: 16. Loss: 280.4771728515625 Reward: -4.0\nAgent: sacd_agent . Episode 1390/2000. Number of steps to finish: 20. Loss: 413.1855773925781 Reward: -14.0\nAgent: sacd_agent . Episode 1391/2000. Number of steps to finish: 10. Loss: 175.67465209960938 Reward: 2.0\nAgent: sacd_agent . Episode 1392/2000. Number of steps to finish: 20. Loss: 394.3017578125 Reward: -10.0\nAgent: sacd_agent . Episode 1393/2000. Number of steps to finish: 20. Loss: 361.4291076660156 Reward: -12.0\nAgent: sacd_agent . Episode 1394/2000. Number of steps to finish: 20. Loss: 333.6848449707031 Reward: -10.0\nAgent: sacd_agent . Episode 1395/2000. Number of steps to finish: 20. Loss: 364.71380615234375 Reward: -14.0\nAgent: sacd_agent . Episode 1396/2000. Number of steps to finish: 20. Loss: 371.555908203125 Reward: -10.0\nAgent: sacd_agent . Episode 1397/2000. Number of steps to finish: 16. Loss: 282.656982421875 Reward: -4.0\nAgent: sacd_agent . Episode 1398/2000. Number of steps to finish: 13. Loss: 240.046875 Reward: -1.0\nAgent: sacd_agent . Episode 1399/2000. Number of steps to finish: 15. Loss: 314.61273193359375 Reward: -3.0\nAgent: sacd_agent . Episode 1400/2000. Number of steps to finish: 20. Loss: 425.9972229003906 Reward: -8.0\nAgent: sacd_agent . Episode 1401/2000. Number of steps to finish: 14. Loss: 323.0416564941406 Reward: -2.0\nAgent: sacd_agent . Episode 1402/2000. Number of steps to finish: 13. Loss: 276.12469482421875 Reward: -1.0\nAgent: sacd_agent . Episode 1403/2000. Number of steps to finish: 10. Loss: 229.78004455566406 Reward: 2.0\nAgent: sacd_agent . Episode 1404/2000. Number of steps to finish: 20. Loss: 392.5225830078125 Reward: -14.0\nAgent: sacd_agent . Episode 1405/2000. Number of steps to finish: 15. Loss: 303.8377685546875 Reward: -3.0\nAgent: sacd_agent . Episode 1406/2000. Number of steps to finish: 19. Loss: 367.3687744140625 Reward: -7.0\nAgent: sacd_agent . Episode 1407/2000. Number of steps to finish: 8. Loss: 166.38168334960938 Reward: 4.0\nAgent: sacd_agent . Episode 1408/2000. Number of steps to finish: 20. Loss: 407.1036682128906 Reward: -10.0\nAgent: sacd_agent . Episode 1409/2000. Number of steps to finish: 20. Loss: 397.2922668457031 Reward: -10.0\nAgent: sacd_agent . Episode 1410/2000. Number of steps to finish: 11. Loss: 201.29214477539062 Reward: 1.0\nAgent: sacd_agent . Episode 1411/2000. Number of steps to finish: 20. Loss: 410.512451171875 Reward: -10.0\nAgent: sacd_agent . Episode 1412/2000. Number of steps to finish: 9. Loss: 152.5249481201172 Reward: 3.0\nAgent: sacd_agent . Episode 1413/2000. Number of steps to finish: 7. Loss: 152.47264099121094 Reward: 5.0\nAgent: sacd_agent . Episode 1414/2000. Number of steps to finish: 20. Loss: 370.76104736328125 Reward: -10.0\nAgent: sacd_agent . Episode 1415/2000. Number of steps to finish: 20. Loss: 378.13616943359375 Reward: -10.0\nAgent: sacd_agent . Episode 1416/2000. Number of steps to finish: 14. Loss: 288.4373779296875 Reward: -2.0\nAgent: sacd_agent . Episode 1417/2000. Number of steps to finish: 20. Loss: 386.8153381347656 Reward: -10.0\nAgent: sacd_agent . Episode 1418/2000. Number of steps to finish: 20. Loss: 409.67108154296875 Reward: -10.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 1419/2000. Number of steps to finish: 16. Loss: 297.3390808105469 Reward: -4.0\nAgent: sacd_agent . Episode 1420/2000. Number of steps to finish: 20. Loss: 385.0867004394531 Reward: -16.0\nAgent: sacd_agent . Episode 1421/2000. Number of steps to finish: 17. Loss: 329.376220703125 Reward: -5.0\nAgent: sacd_agent . Episode 1422/2000. Number of steps to finish: 20. Loss: 427.60955810546875 Reward: -10.0\nAgent: sacd_agent . Episode 1423/2000. Number of steps to finish: 20. Loss: 382.50177001953125 Reward: -16.0\nAgent: sacd_agent . Episode 1424/2000. Number of steps to finish: 20. Loss: 403.3656311035156 Reward: -10.0\nAgent: sacd_agent . Episode 1425/2000. Number of steps to finish: 20. Loss: 462.8736572265625 Reward: -12.0\nAgent: sacd_agent . Episode 1426/2000. Number of steps to finish: 11. Loss: 178.9377899169922 Reward: 1.0\nAgent: sacd_agent . Episode 1427/2000. Number of steps to finish: 20. Loss: 340.015625 Reward: -12.0\nAgent: sacd_agent . Episode 1428/2000. Number of steps to finish: 19. Loss: 447.2995300292969 Reward: -7.0\nAgent: sacd_agent . Episode 1429/2000. Number of steps to finish: 20. Loss: 394.7316589355469 Reward: -10.0\nAgent: sacd_agent . Episode 1430/2000. Number of steps to finish: 10. Loss: 248.59910583496094 Reward: 2.0\nAgent: sacd_agent . Episode 1431/2000. Number of steps to finish: 15. Loss: 332.1761779785156 Reward: -3.0\nAgent: sacd_agent . Episode 1432/2000. Number of steps to finish: 11. Loss: 291.43377685546875 Reward: 1.0\nAgent: sacd_agent . Episode 1433/2000. Number of steps to finish: 14. Loss: 294.9228820800781 Reward: -2.0\nAgent: sacd_agent . Episode 1434/2000. Number of steps to finish: 20. Loss: 398.8460998535156 Reward: -10.0\nAgent: sacd_agent . Episode 1435/2000. Number of steps to finish: 18. Loss: 396.517578125 Reward: -6.0\nAgent: sacd_agent . Episode 1436/2000. Number of steps to finish: 20. Loss: 423.5863037109375 Reward: -10.0\nAgent: sacd_agent . Episode 1437/2000. Number of steps to finish: 12. Loss: 227.47589111328125 Reward: 0.0\nAgent: sacd_agent . Episode 1438/2000. Number of steps to finish: 20. Loss: 457.8568420410156 Reward: -10.0\nAgent: sacd_agent . Episode 1439/2000. Number of steps to finish: 17. Loss: 351.3853759765625 Reward: -5.0\nAgent: sacd_agent . Episode 1440/2000. Number of steps to finish: 14. Loss: 292.2303466796875 Reward: -2.0\nAgent: sacd_agent . Episode 1441/2000. Number of steps to finish: 20. Loss: 419.5997619628906 Reward: -12.0\nAgent: sacd_agent . Episode 1442/2000. Number of steps to finish: 13. Loss: 231.75054931640625 Reward: -1.0\nAgent: sacd_agent . Episode 1443/2000. Number of steps to finish: 20. Loss: 411.30419921875 Reward: -10.0\nAgent: sacd_agent . Episode 1444/2000. Number of steps to finish: 17. Loss: 375.1861877441406 Reward: -5.0\nAgent: sacd_agent . Episode 1445/2000. Number of steps to finish: 18. Loss: 356.1424865722656 Reward: -6.0\nAgent: sacd_agent . Episode 1446/2000. Number of steps to finish: 17. Loss: 348.4570007324219 Reward: -5.0\nAgent: sacd_agent . Episode 1447/2000. Number of steps to finish: 19. Loss: 394.2757263183594 Reward: -7.0\nAgent: sacd_agent . Episode 1448/2000. Number of steps to finish: 20. Loss: 411.0323486328125 Reward: -14.0\nAgent: sacd_agent . Episode 1449/2000. Number of steps to finish: 16. Loss: 314.7986755371094 Reward: -4.0\nAgent: sacd_agent . Episode 1450/2000. Number of steps to finish: 12. Loss: 275.129638671875 Reward: 0.0\nAgent: sacd_agent . Episode 1451/2000. Number of steps to finish: 15. Loss: 282.3360290527344 Reward: -3.0\nAgent: sacd_agent . Episode 1452/2000. Number of steps to finish: 12. Loss: 280.96380615234375 Reward: 0.0\nAgent: sacd_agent . Episode 1453/2000. Number of steps to finish: 20. Loss: 388.3056640625 Reward: -10.0\nAgent: sacd_agent . Episode 1454/2000. Number of steps to finish: 17. Loss: 366.2366027832031 Reward: -5.0\nAgent: sacd_agent . Episode 1455/2000. Number of steps to finish: 13. Loss: 239.54811096191406 Reward: -1.0\nAgent: sacd_agent . Episode 1456/2000. Number of steps to finish: 20. Loss: 380.76519775390625 Reward: -8.0\nAgent: sacd_agent . Episode 1457/2000. Number of steps to finish: 20. Loss: 443.574462890625 Reward: -10.0\nAgent: sacd_agent . Episode 1458/2000. Number of steps to finish: 20. Loss: 421.7958984375 Reward: -12.0\nAgent: sacd_agent . Episode 1459/2000. Number of steps to finish: 18. Loss: 411.8172302246094 Reward: -6.0\nAgent: sacd_agent . Episode 1460/2000. Number of steps to finish: 20. Loss: 469.17572021484375 Reward: -10.0\nAgent: sacd_agent . Episode 1461/2000. Number of steps to finish: 19. Loss: 426.9089660644531 Reward: -7.0\nAgent: sacd_agent . Episode 1462/2000. Number of steps to finish: 20. Loss: 421.7821960449219 Reward: -10.0\nAgent: sacd_agent . Episode 1463/2000. Number of steps to finish: 20. Loss: 373.8198547363281 Reward: -10.0\nAgent: sacd_agent . Episode 1464/2000. Number of steps to finish: 17. Loss: 372.7884216308594 Reward: -5.0\nAgent: sacd_agent . Episode 1465/2000. Number of steps to finish: 20. Loss: 429.04962158203125 Reward: -12.0\nAgent: sacd_agent . Episode 1466/2000. Number of steps to finish: 13. Loss: 255.76040649414062 Reward: -1.0\nAgent: sacd_agent . Episode 1467/2000. Number of steps to finish: 18. Loss: 371.06671142578125 Reward: -6.0\nAgent: sacd_agent . Episode 1468/2000. Number of steps to finish: 20. Loss: 439.82598876953125 Reward: -10.0\nAgent: sacd_agent . Episode 1469/2000. Number of steps to finish: 20. Loss: 400.53875732421875 Reward: -10.0\nAgent: sacd_agent . Episode 1470/2000. Number of steps to finish: 20. Loss: 407.28875732421875 Reward: -10.0\nAgent: sacd_agent . Episode 1471/2000. Number of steps to finish: 20. Loss: 479.1912841796875 Reward: -8.0\nAgent: sacd_agent . Episode 1472/2000. Number of steps to finish: 15. Loss: 282.4890441894531 Reward: -3.0\nAgent: sacd_agent . Episode 1473/2000. Number of steps to finish: 20. Loss: 424.6892395019531 Reward: -16.0\nAgent: sacd_agent . Episode 1474/2000. Number of steps to finish: 20. Loss: 456.396240234375 Reward: -10.0\nAgent: sacd_agent . Episode 1475/2000. Number of steps to finish: 13. Loss: 269.0566101074219 Reward: -1.0\nAgent: sacd_agent . Episode 1476/2000. Number of steps to finish: 20. Loss: 402.1466064453125 Reward: -10.0\nAgent: sacd_agent . Episode 1477/2000. Number of steps to finish: 15. Loss: 350.40899658203125 Reward: -3.0\nAgent: sacd_agent . Episode 1478/2000. Number of steps to finish: 19. Loss: 390.07574462890625 Reward: -7.0\nAgent: sacd_agent . Episode 1479/2000. Number of steps to finish: 17. Loss: 413.68695068359375 Reward: -5.0\nAgent: sacd_agent . Episode 1480/2000. Number of steps to finish: 19. Loss: 425.6572570800781 Reward: -7.0\nAgent: sacd_agent . Episode 1481/2000. Number of steps to finish: 11. Loss: 237.58433532714844 Reward: 1.0\nAgent: sacd_agent . Episode 1482/2000. Number of steps to finish: 11. Loss: 243.31085205078125 Reward: 1.0\nAgent: sacd_agent . Episode 1483/2000. Number of steps to finish: 20. Loss: 420.01556396484375 Reward: -12.0\nAgent: sacd_agent . Episode 1484/2000. Number of steps to finish: 20. Loss: 406.5959167480469 Reward: -10.0\nAgent: sacd_agent . Episode 1485/2000. Number of steps to finish: 20. Loss: 444.1543884277344 Reward: -12.0\nAgent: sacd_agent . Episode 1486/2000. Number of steps to finish: 12. Loss: 301.3934326171875 Reward: 0.0\nAgent: sacd_agent . Episode 1487/2000. Number of steps to finish: 20. Loss: 450.7452392578125 Reward: -14.0\nAgent: sacd_agent . Episode 1488/2000. Number of steps to finish: 20. Loss: 428.08612060546875 Reward: -8.0\nAgent: sacd_agent . Episode 1489/2000. Number of steps to finish: 14. Loss: 343.2364196777344 Reward: -2.0\nAgent: sacd_agent . Episode 1490/2000. Number of steps to finish: 11. Loss: 242.22120666503906 Reward: 1.0\nAgent: sacd_agent . Episode 1491/2000. Number of steps to finish: 20. Loss: 472.98260498046875 Reward: -10.0\nAgent: sacd_agent . Episode 1492/2000. Number of steps to finish: 19. Loss: 414.7128601074219 Reward: -7.0\nAgent: sacd_agent . Episode 1493/2000. Number of steps to finish: 20. Loss: 421.3561096191406 Reward: -14.0\nAgent: sacd_agent . Episode 1494/2000. Number of steps to finish: 11. Loss: 239.52899169921875 Reward: 1.0\nAgent: sacd_agent . Episode 1495/2000. Number of steps to finish: 19. Loss: 461.87384033203125 Reward: -7.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 1496/2000. Number of steps to finish: 20. Loss: 384.9562683105469 Reward: -12.0\nAgent: sacd_agent . Episode 1497/2000. Number of steps to finish: 14. Loss: 318.9195251464844 Reward: -2.0\nAgent: sacd_agent . Episode 1498/2000. Number of steps to finish: 9. Loss: 225.06118774414062 Reward: 3.0\nAgent: sacd_agent . Episode 1499/2000. Number of steps to finish: 12. Loss: 261.2102355957031 Reward: 0.0\nAgent: sacd_agent . Episode 1500/2000. Number of steps to finish: 20. Loss: 471.41485595703125 Reward: -12.0\nAgent: sacd_agent . Episode 1501/2000. Number of steps to finish: 20. Loss: 461.6719665527344 Reward: -10.0\nAgent: sacd_agent . Episode 1502/2000. Number of steps to finish: 20. Loss: 440.4685363769531 Reward: -10.0\nAgent: sacd_agent . Episode 1503/2000. Number of steps to finish: 20. Loss: 442.24920654296875 Reward: -18.0\nAgent: sacd_agent . Episode 1504/2000. Number of steps to finish: 20. Loss: 427.55572509765625 Reward: -10.0\nAgent: sacd_agent . Episode 1505/2000. Number of steps to finish: 20. Loss: 435.39434814453125 Reward: -10.0\nAgent: sacd_agent . Episode 1506/2000. Number of steps to finish: 20. Loss: 412.4540710449219 Reward: -10.0\nAgent: sacd_agent . Episode 1507/2000. Number of steps to finish: 20. Loss: 423.0339660644531 Reward: -14.0\nAgent: sacd_agent . Episode 1508/2000. Number of steps to finish: 20. Loss: 415.238525390625 Reward: -14.0\nAgent: sacd_agent . Episode 1509/2000. Number of steps to finish: 10. Loss: 230.20355224609375 Reward: 2.0\nAgent: sacd_agent . Episode 1510/2000. Number of steps to finish: 20. Loss: 439.75250244140625 Reward: -10.0\nAgent: sacd_agent . Episode 1511/2000. Number of steps to finish: 15. Loss: 311.82696533203125 Reward: -3.0\nAgent: sacd_agent . Episode 1512/2000. Number of steps to finish: 20. Loss: 404.5914306640625 Reward: -12.0\nAgent: sacd_agent . Episode 1513/2000. Number of steps to finish: 18. Loss: 385.49285888671875 Reward: -6.0\nAgent: sacd_agent . Episode 1514/2000. Number of steps to finish: 14. Loss: 331.82568359375 Reward: -2.0\nAgent: sacd_agent . Episode 1515/2000. Number of steps to finish: 14. Loss: 293.50341796875 Reward: -2.0\nAgent: sacd_agent . Episode 1516/2000. Number of steps to finish: 20. Loss: 433.7635803222656 Reward: -8.0\nAgent: sacd_agent . Episode 1517/2000. Number of steps to finish: 13. Loss: 301.010986328125 Reward: -1.0\nAgent: sacd_agent . Episode 1518/2000. Number of steps to finish: 20. Loss: 428.46148681640625 Reward: -12.0\nAgent: sacd_agent . Episode 1519/2000. Number of steps to finish: 20. Loss: 445.454833984375 Reward: -10.0\nAgent: sacd_agent . Episode 1520/2000. Number of steps to finish: 19. Loss: 407.3852844238281 Reward: -7.0\nAgent: sacd_agent . Episode 1521/2000. Number of steps to finish: 20. Loss: 434.3954772949219 Reward: -14.0\nAgent: sacd_agent . Episode 1522/2000. Number of steps to finish: 10. Loss: 234.32415771484375 Reward: 2.0\nAgent: sacd_agent . Episode 1523/2000. Number of steps to finish: 20. Loss: 453.69744873046875 Reward: -10.0\nAgent: sacd_agent . Episode 1524/2000. Number of steps to finish: 12. Loss: 216.13938903808594 Reward: 0.0\nAgent: sacd_agent . Episode 1525/2000. Number of steps to finish: 20. Loss: 467.96075439453125 Reward: -16.0\nAgent: sacd_agent . Episode 1526/2000. Number of steps to finish: 11. Loss: 217.52896118164062 Reward: 1.0\nAgent: sacd_agent . Episode 1527/2000. Number of steps to finish: 20. Loss: 397.4695739746094 Reward: -14.0\nAgent: sacd_agent . Episode 1528/2000. Number of steps to finish: 20. Loss: 480.18505859375 Reward: -14.0\nAgent: sacd_agent . Episode 1529/2000. Number of steps to finish: 17. Loss: 368.6497497558594 Reward: -5.0\nAgent: sacd_agent . Episode 1530/2000. Number of steps to finish: 20. Loss: 473.8212585449219 Reward: -8.0\nAgent: sacd_agent . Episode 1531/2000. Number of steps to finish: 15. Loss: 307.9851379394531 Reward: -3.0\nAgent: sacd_agent . Episode 1532/2000. Number of steps to finish: 20. Loss: 416.6015625 Reward: -8.0\nAgent: sacd_agent . Episode 1533/2000. Number of steps to finish: 16. Loss: 416.98583984375 Reward: -4.0\nAgent: sacd_agent . Episode 1534/2000. Number of steps to finish: 20. Loss: 477.28631591796875 Reward: -12.0\nAgent: sacd_agent . Episode 1535/2000. Number of steps to finish: 20. Loss: 426.18280029296875 Reward: -12.0\nAgent: sacd_agent . Episode 1536/2000. Number of steps to finish: 16. Loss: 369.5712890625 Reward: -4.0\nAgent: sacd_agent . Episode 1537/2000. Number of steps to finish: 20. Loss: 437.6371154785156 Reward: -10.0\nAgent: sacd_agent . Episode 1538/2000. Number of steps to finish: 20. Loss: 475.7207336425781 Reward: -10.0\nAgent: sacd_agent . Episode 1539/2000. Number of steps to finish: 20. Loss: 515.0040283203125 Reward: -12.0\nAgent: sacd_agent . Episode 1540/2000. Number of steps to finish: 19. Loss: 467.3516540527344 Reward: -7.0\nAgent: sacd_agent . Episode 1541/2000. Number of steps to finish: 20. Loss: 401.2408752441406 Reward: -10.0\nAgent: sacd_agent . Episode 1542/2000. Number of steps to finish: 16. Loss: 427.7242736816406 Reward: -4.0\nAgent: sacd_agent . Episode 1543/2000. Number of steps to finish: 20. Loss: 471.3353271484375 Reward: -10.0\nAgent: sacd_agent . Episode 1544/2000. Number of steps to finish: 20. Loss: 442.3462219238281 Reward: -10.0\nAgent: sacd_agent . Episode 1545/2000. Number of steps to finish: 20. Loss: 514.3524169921875 Reward: -10.0\nAgent: sacd_agent . Episode 1546/2000. Number of steps to finish: 20. Loss: 501.0618591308594 Reward: -10.0\nAgent: sacd_agent . Episode 1547/2000. Number of steps to finish: 20. Loss: 493.5465087890625 Reward: -12.0\nAgent: sacd_agent . Episode 1548/2000. Number of steps to finish: 16. Loss: 396.23046875 Reward: -4.0\nAgent: sacd_agent . Episode 1549/2000. Number of steps to finish: 8. Loss: 167.6146240234375 Reward: 4.0\nAgent: sacd_agent . Episode 1550/2000. Number of steps to finish: 14. Loss: 322.5771179199219 Reward: -2.0\nAgent: sacd_agent . Episode 1551/2000. Number of steps to finish: 15. Loss: 354.125244140625 Reward: -3.0\nAgent: sacd_agent . Episode 1552/2000. Number of steps to finish: 17. Loss: 365.0912170410156 Reward: -5.0\nAgent: sacd_agent . Episode 1553/2000. Number of steps to finish: 18. Loss: 431.84063720703125 Reward: -6.0\nAgent: sacd_agent . Episode 1554/2000. Number of steps to finish: 20. Loss: 514.8126220703125 Reward: -10.0\nAgent: sacd_agent . Episode 1555/2000. Number of steps to finish: 20. Loss: 470.1477966308594 Reward: -14.0\nAgent: sacd_agent . Episode 1556/2000. Number of steps to finish: 20. Loss: 496.96185302734375 Reward: -10.0\nAgent: sacd_agent . Episode 1557/2000. Number of steps to finish: 20. Loss: 467.2826232910156 Reward: -8.0\nAgent: sacd_agent . Episode 1558/2000. Number of steps to finish: 20. Loss: 479.35345458984375 Reward: -10.0\nAgent: sacd_agent . Episode 1559/2000. Number of steps to finish: 17. Loss: 439.5856628417969 Reward: -5.0\nAgent: sacd_agent . Episode 1560/2000. Number of steps to finish: 20. Loss: 523.556396484375 Reward: -10.0\nAgent: sacd_agent . Episode 1561/2000. Number of steps to finish: 20. Loss: 542.7579345703125 Reward: -10.0\nAgent: sacd_agent . Episode 1562/2000. Number of steps to finish: 20. Loss: 507.4661865234375 Reward: -8.0\nAgent: sacd_agent . Episode 1563/2000. Number of steps to finish: 20. Loss: 521.4671630859375 Reward: -10.0\nAgent: sacd_agent . Episode 1564/2000. Number of steps to finish: 20. Loss: 457.3546447753906 Reward: -12.0\nAgent: sacd_agent . Episode 1565/2000. Number of steps to finish: 14. Loss: 376.2478332519531 Reward: -2.0\nAgent: sacd_agent . Episode 1566/2000. Number of steps to finish: 20. Loss: 546.9730224609375 Reward: -14.0\nAgent: sacd_agent . Episode 1567/2000. Number of steps to finish: 20. Loss: 526.8819580078125 Reward: -10.0\nAgent: sacd_agent . Episode 1568/2000. Number of steps to finish: 20. Loss: 486.2928771972656 Reward: -10.0\nAgent: sacd_agent . Episode 1569/2000. Number of steps to finish: 20. Loss: 460.93585205078125 Reward: -10.0\nAgent: sacd_agent . Episode 1570/2000. Number of steps to finish: 20. Loss: 499.4325256347656 Reward: -16.0\nAgent: sacd_agent . Episode 1571/2000. Number of steps to finish: 20. Loss: 455.906494140625 Reward: -14.0\nAgent: sacd_agent . Episode 1572/2000. Number of steps to finish: 20. Loss: 473.81878662109375 Reward: -10.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 1573/2000. Number of steps to finish: 12. Loss: 292.218505859375 Reward: 0.0\nAgent: sacd_agent . Episode 1574/2000. Number of steps to finish: 20. Loss: 471.33624267578125 Reward: -10.0\nAgent: sacd_agent . Episode 1575/2000. Number of steps to finish: 20. Loss: 488.2610168457031 Reward: -10.0\nAgent: sacd_agent . Episode 1576/2000. Number of steps to finish: 7. Loss: 155.38539123535156 Reward: 5.0\nAgent: sacd_agent . Episode 1577/2000. Number of steps to finish: 20. Loss: 489.81695556640625 Reward: -10.0\nAgent: sacd_agent . Episode 1578/2000. Number of steps to finish: 20. Loss: 478.69415283203125 Reward: -14.0\nAgent: sacd_agent . Episode 1579/2000. Number of steps to finish: 20. Loss: 453.11724853515625 Reward: -18.0\nAgent: sacd_agent . Episode 1580/2000. Number of steps to finish: 14. Loss: 346.4185791015625 Reward: -2.0\nAgent: sacd_agent . Episode 1581/2000. Number of steps to finish: 19. Loss: 494.428955078125 Reward: -7.0\nAgent: sacd_agent . Episode 1582/2000. Number of steps to finish: 19. Loss: 455.6759033203125 Reward: -7.0\nAgent: sacd_agent . Episode 1583/2000. Number of steps to finish: 20. Loss: 422.1128845214844 Reward: -10.0\nAgent: sacd_agent . Episode 1584/2000. Number of steps to finish: 14. Loss: 330.71966552734375 Reward: -2.0\nAgent: sacd_agent . Episode 1585/2000. Number of steps to finish: 20. Loss: 465.1134948730469 Reward: -10.0\nAgent: sacd_agent . Episode 1586/2000. Number of steps to finish: 19. Loss: 468.346435546875 Reward: -7.0\nAgent: sacd_agent . Episode 1587/2000. Number of steps to finish: 15. Loss: 386.7315673828125 Reward: -3.0\nAgent: sacd_agent . Episode 1588/2000. Number of steps to finish: 20. Loss: 544.2564086914062 Reward: -10.0\nAgent: sacd_agent . Episode 1589/2000. Number of steps to finish: 20. Loss: 486.66546630859375 Reward: -18.0\nAgent: sacd_agent . Episode 1590/2000. Number of steps to finish: 20. Loss: 486.3719177246094 Reward: -18.0\nAgent: sacd_agent . Episode 1591/2000. Number of steps to finish: 20. Loss: 483.28656005859375 Reward: -12.0\nAgent: sacd_agent . Episode 1592/2000. Number of steps to finish: 12. Loss: 246.41043090820312 Reward: 0.0\nAgent: sacd_agent . Episode 1593/2000. Number of steps to finish: 9. Loss: 192.51806640625 Reward: 3.0\nAgent: sacd_agent . Episode 1594/2000. Number of steps to finish: 20. Loss: 508.78662109375 Reward: -12.0\nAgent: sacd_agent . Episode 1595/2000. Number of steps to finish: 20. Loss: 519.7516479492188 Reward: -14.0\nAgent: sacd_agent . Episode 1596/2000. Number of steps to finish: 12. Loss: 302.6026916503906 Reward: 0.0\nAgent: sacd_agent . Episode 1597/2000. Number of steps to finish: 14. Loss: 388.8403625488281 Reward: -2.0\nAgent: sacd_agent . Episode 1598/2000. Number of steps to finish: 11. Loss: 315.8517761230469 Reward: 1.0\nAgent: sacd_agent . Episode 1599/2000. Number of steps to finish: 19. Loss: 451.1844787597656 Reward: -7.0\nAgent: sacd_agent . Episode 1600/2000. Number of steps to finish: 20. Loss: 471.3005676269531 Reward: -14.0\nAgent: sacd_agent . Episode 1601/2000. Number of steps to finish: 20. Loss: 519.8469848632812 Reward: -8.0\nAgent: sacd_agent . Episode 1602/2000. Number of steps to finish: 13. Loss: 330.96484375 Reward: -1.0\nAgent: sacd_agent . Episode 1603/2000. Number of steps to finish: 19. Loss: 465.67718505859375 Reward: -7.0\nAgent: sacd_agent . Episode 1604/2000. Number of steps to finish: 20. Loss: 505.7217712402344 Reward: -10.0\nAgent: sacd_agent . Episode 1605/2000. Number of steps to finish: 20. Loss: 515.9920043945312 Reward: -8.0\nAgent: sacd_agent . Episode 1606/2000. Number of steps to finish: 13. Loss: 357.8601989746094 Reward: -1.0\nAgent: sacd_agent . Episode 1607/2000. Number of steps to finish: 16. Loss: 387.972900390625 Reward: -4.0\nAgent: sacd_agent . Episode 1608/2000. Number of steps to finish: 15. Loss: 360.2015380859375 Reward: -3.0\nAgent: sacd_agent . Episode 1609/2000. Number of steps to finish: 20. Loss: 548.612060546875 Reward: -10.0\nAgent: sacd_agent . Episode 1610/2000. Number of steps to finish: 19. Loss: 453.8172607421875 Reward: -7.0\nAgent: sacd_agent . Episode 1611/2000. Number of steps to finish: 14. Loss: 311.2164001464844 Reward: -2.0\nAgent: sacd_agent . Episode 1612/2000. Number of steps to finish: 20. Loss: 506.1172180175781 Reward: -14.0\nAgent: sacd_agent . Episode 1613/2000. Number of steps to finish: 20. Loss: 509.85491943359375 Reward: -10.0\nAgent: sacd_agent . Episode 1614/2000. Number of steps to finish: 20. Loss: 522.8534545898438 Reward: -8.0\nAgent: sacd_agent . Episode 1615/2000. Number of steps to finish: 11. Loss: 266.16180419921875 Reward: 1.0\nAgent: sacd_agent . Episode 1616/2000. Number of steps to finish: 15. Loss: 409.59783935546875 Reward: -3.0\nAgent: sacd_agent . Episode 1617/2000. Number of steps to finish: 17. Loss: 394.210693359375 Reward: -5.0\nAgent: sacd_agent . Episode 1618/2000. Number of steps to finish: 9. Loss: 270.5522155761719 Reward: 3.0\nAgent: sacd_agent . Episode 1619/2000. Number of steps to finish: 14. Loss: 370.55255126953125 Reward: -2.0\nAgent: sacd_agent . Episode 1620/2000. Number of steps to finish: 11. Loss: 336.71282958984375 Reward: 1.0\nAgent: sacd_agent . Episode 1621/2000. Number of steps to finish: 19. Loss: 502.18499755859375 Reward: -7.0\nAgent: sacd_agent . Episode 1622/2000. Number of steps to finish: 20. Loss: 548.460205078125 Reward: -12.0\nAgent: sacd_agent . Episode 1623/2000. Number of steps to finish: 20. Loss: 507.9507141113281 Reward: -12.0\nAgent: sacd_agent . Episode 1624/2000. Number of steps to finish: 20. Loss: 546.03515625 Reward: -10.0\nAgent: sacd_agent . Episode 1625/2000. Number of steps to finish: 20. Loss: 439.8317565917969 Reward: -10.0\nAgent: sacd_agent . Episode 1626/2000. Number of steps to finish: 9. Loss: 227.3490753173828 Reward: 3.0\nAgent: sacd_agent . Episode 1627/2000. Number of steps to finish: 13. Loss: 334.49127197265625 Reward: -1.0\nAgent: sacd_agent . Episode 1628/2000. Number of steps to finish: 20. Loss: 572.5457763671875 Reward: -14.0\nAgent: sacd_agent . Episode 1629/2000. Number of steps to finish: 20. Loss: 492.7454528808594 Reward: -10.0\nAgent: sacd_agent . Episode 1630/2000. Number of steps to finish: 20. Loss: 480.3970031738281 Reward: -12.0\nAgent: sacd_agent . Episode 1631/2000. Number of steps to finish: 20. Loss: 606.7379760742188 Reward: -16.0\nAgent: sacd_agent . Episode 1632/2000. Number of steps to finish: 11. Loss: 322.1573486328125 Reward: 1.0\nAgent: sacd_agent . Episode 1633/2000. Number of steps to finish: 16. Loss: 386.0075378417969 Reward: -4.0\nAgent: sacd_agent . Episode 1634/2000. Number of steps to finish: 17. Loss: 463.5536804199219 Reward: -5.0\nAgent: sacd_agent . Episode 1635/2000. Number of steps to finish: 11. Loss: 327.44830322265625 Reward: 1.0\nAgent: sacd_agent . Episode 1636/2000. Number of steps to finish: 19. Loss: 475.329833984375 Reward: -7.0\nAgent: sacd_agent . Episode 1637/2000. Number of steps to finish: 20. Loss: 522.6117553710938 Reward: -10.0\nAgent: sacd_agent . Episode 1638/2000. Number of steps to finish: 20. Loss: 488.0624084472656 Reward: -10.0\nAgent: sacd_agent . Episode 1639/2000. Number of steps to finish: 11. Loss: 294.2443542480469 Reward: 1.0\nAgent: sacd_agent . Episode 1640/2000. Number of steps to finish: 20. Loss: 535.574951171875 Reward: -12.0\nAgent: sacd_agent . Episode 1641/2000. Number of steps to finish: 13. Loss: 372.4609069824219 Reward: -1.0\nAgent: sacd_agent . Episode 1642/2000. Number of steps to finish: 20. Loss: 531.4265747070312 Reward: -10.0\nAgent: sacd_agent . Episode 1643/2000. Number of steps to finish: 10. Loss: 229.2107696533203 Reward: 2.0\nAgent: sacd_agent . Episode 1644/2000. Number of steps to finish: 18. Loss: 453.83392333984375 Reward: -6.0\nAgent: sacd_agent . Episode 1645/2000. Number of steps to finish: 13. Loss: 330.6472473144531 Reward: -1.0\nAgent: sacd_agent . Episode 1646/2000. Number of steps to finish: 20. Loss: 550.7717895507812 Reward: -12.0\nAgent: sacd_agent . Episode 1647/2000. Number of steps to finish: 20. Loss: 517.87548828125 Reward: -12.0\nAgent: sacd_agent . Episode 1648/2000. Number of steps to finish: 20. Loss: 492.7373352050781 Reward: -10.0\nAgent: sacd_agent . Episode 1649/2000. Number of steps to finish: 19. Loss: 515.95654296875 Reward: -7.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 1650/2000. Number of steps to finish: 16. Loss: 450.12646484375 Reward: -4.0\nAgent: sacd_agent . Episode 1651/2000. Number of steps to finish: 16. Loss: 432.11785888671875 Reward: -4.0\nAgent: sacd_agent . Episode 1652/2000. Number of steps to finish: 20. Loss: 451.88226318359375 Reward: -10.0\nAgent: sacd_agent . Episode 1653/2000. Number of steps to finish: 19. Loss: 509.1649475097656 Reward: -7.0\nAgent: sacd_agent . Episode 1654/2000. Number of steps to finish: 20. Loss: 491.8281555175781 Reward: -12.0\nAgent: sacd_agent . Episode 1655/2000. Number of steps to finish: 20. Loss: 583.3777465820312 Reward: -14.0\nAgent: sacd_agent . Episode 1656/2000. Number of steps to finish: 20. Loss: 592.46728515625 Reward: -14.0\nAgent: sacd_agent . Episode 1657/2000. Number of steps to finish: 19. Loss: 478.7373352050781 Reward: -7.0\nAgent: sacd_agent . Episode 1658/2000. Number of steps to finish: 20. Loss: 563.9088134765625 Reward: -10.0\nAgent: sacd_agent . Episode 1659/2000. Number of steps to finish: 12. Loss: 313.63421630859375 Reward: 0.0\nAgent: sacd_agent . Episode 1660/2000. Number of steps to finish: 16. Loss: 424.1080627441406 Reward: -4.0\nAgent: sacd_agent . Episode 1661/2000. Number of steps to finish: 20. Loss: 484.4787292480469 Reward: -12.0\nAgent: sacd_agent . Episode 1662/2000. Number of steps to finish: 20. Loss: 567.3232421875 Reward: -14.0\nAgent: sacd_agent . Episode 1663/2000. Number of steps to finish: 19. Loss: 529.6467895507812 Reward: -7.0\nAgent: sacd_agent . Episode 1664/2000. Number of steps to finish: 14. Loss: 380.39276123046875 Reward: -2.0\nAgent: sacd_agent . Episode 1665/2000. Number of steps to finish: 8. Loss: 211.25936889648438 Reward: 4.0\nAgent: sacd_agent . Episode 1666/2000. Number of steps to finish: 19. Loss: 555.402099609375 Reward: -7.0\nAgent: sacd_agent . Episode 1667/2000. Number of steps to finish: 20. Loss: 513.6456298828125 Reward: -10.0\nAgent: sacd_agent . Episode 1668/2000. Number of steps to finish: 14. Loss: 361.66839599609375 Reward: -2.0\nAgent: sacd_agent . Episode 1669/2000. Number of steps to finish: 13. Loss: 275.49639892578125 Reward: -1.0\nAgent: sacd_agent . Episode 1670/2000. Number of steps to finish: 18. Loss: 552.2161254882812 Reward: -6.0\nAgent: sacd_agent . Episode 1671/2000. Number of steps to finish: 11. Loss: 332.4476623535156 Reward: 1.0\nAgent: sacd_agent . Episode 1672/2000. Number of steps to finish: 19. Loss: 560.0136108398438 Reward: -7.0\nAgent: sacd_agent . Episode 1673/2000. Number of steps to finish: 17. Loss: 503.94195556640625 Reward: -5.0\nAgent: sacd_agent . Episode 1674/2000. Number of steps to finish: 20. Loss: 596.7366943359375 Reward: -12.0\nAgent: sacd_agent . Episode 1675/2000. Number of steps to finish: 14. Loss: 312.27294921875 Reward: -2.0\nAgent: sacd_agent . Episode 1676/2000. Number of steps to finish: 12. Loss: 308.82421875 Reward: 0.0\nAgent: sacd_agent . Episode 1677/2000. Number of steps to finish: 20. Loss: 539.7122192382812 Reward: -12.0\nAgent: sacd_agent . Episode 1678/2000. Number of steps to finish: 20. Loss: 519.3331298828125 Reward: -12.0\nAgent: sacd_agent . Episode 1679/2000. Number of steps to finish: 20. Loss: 533.4285888671875 Reward: -10.0\nAgent: sacd_agent . Episode 1680/2000. Number of steps to finish: 20. Loss: 490.8555908203125 Reward: -10.0\nAgent: sacd_agent . Episode 1681/2000. Number of steps to finish: 20. Loss: 526.5783081054688 Reward: -10.0\nAgent: sacd_agent . Episode 1682/2000. Number of steps to finish: 20. Loss: 532.6838989257812 Reward: -10.0\nAgent: sacd_agent . Episode 1683/2000. Number of steps to finish: 10. Loss: 266.0799865722656 Reward: 2.0\nAgent: sacd_agent . Episode 1684/2000. Number of steps to finish: 20. Loss: 521.1277465820312 Reward: -14.0\nAgent: sacd_agent . Episode 1685/2000. Number of steps to finish: 20. Loss: 512.271728515625 Reward: -8.0\nAgent: sacd_agent . Episode 1686/2000. Number of steps to finish: 14. Loss: 330.0938720703125 Reward: -2.0\nAgent: sacd_agent . Episode 1687/2000. Number of steps to finish: 12. Loss: 338.35162353515625 Reward: 0.0\nAgent: sacd_agent . Episode 1688/2000. Number of steps to finish: 20. Loss: 611.4970703125 Reward: -10.0\nAgent: sacd_agent . Episode 1689/2000. Number of steps to finish: 20. Loss: 569.4014892578125 Reward: -10.0\nAgent: sacd_agent . Episode 1690/2000. Number of steps to finish: 20. Loss: 570.6087646484375 Reward: -14.0\nAgent: sacd_agent . Episode 1691/2000. Number of steps to finish: 20. Loss: 626.4417114257812 Reward: -18.0\nAgent: sacd_agent . Episode 1692/2000. Number of steps to finish: 20. Loss: 568.1315307617188 Reward: -10.0\nAgent: sacd_agent . Episode 1693/2000. Number of steps to finish: 20. Loss: 532.7421875 Reward: -10.0\nAgent: sacd_agent . Episode 1694/2000. Number of steps to finish: 20. Loss: 595.9335327148438 Reward: -12.0\nAgent: sacd_agent . Episode 1695/2000. Number of steps to finish: 20. Loss: 566.989501953125 Reward: -14.0\nAgent: sacd_agent . Episode 1696/2000. Number of steps to finish: 17. Loss: 517.4631958007812 Reward: -5.0\nAgent: sacd_agent . Episode 1697/2000. Number of steps to finish: 20. Loss: 552.8048095703125 Reward: -10.0\nAgent: sacd_agent . Episode 1698/2000. Number of steps to finish: 20. Loss: 592.2943115234375 Reward: -10.0\nAgent: sacd_agent . Episode 1699/2000. Number of steps to finish: 20. Loss: 605.1624755859375 Reward: -14.0\nAgent: sacd_agent . Episode 1700/2000. Number of steps to finish: 15. Loss: 405.2525634765625 Reward: -3.0\nAgent: sacd_agent . Episode 1701/2000. Number of steps to finish: 19. Loss: 508.036376953125 Reward: -7.0\nAgent: sacd_agent . Episode 1702/2000. Number of steps to finish: 12. Loss: 324.2120056152344 Reward: 0.0\nAgent: sacd_agent . Episode 1703/2000. Number of steps to finish: 20. Loss: 617.1321411132812 Reward: -12.0\nAgent: sacd_agent . Episode 1704/2000. Number of steps to finish: 20. Loss: 565.5264282226562 Reward: -10.0\nAgent: sacd_agent . Episode 1705/2000. Number of steps to finish: 14. Loss: 421.01031494140625 Reward: -2.0\nAgent: sacd_agent . Episode 1706/2000. Number of steps to finish: 18. Loss: 524.5148315429688 Reward: -6.0\nAgent: sacd_agent . Episode 1707/2000. Number of steps to finish: 19. Loss: 528.0032348632812 Reward: -7.0\nAgent: sacd_agent . Episode 1708/2000. Number of steps to finish: 14. Loss: 430.08380126953125 Reward: -2.0\nAgent: sacd_agent . Episode 1709/2000. Number of steps to finish: 20. Loss: 533.7545776367188 Reward: -10.0\nAgent: sacd_agent . Episode 1710/2000. Number of steps to finish: 20. Loss: 604.8207397460938 Reward: -14.0\nAgent: sacd_agent . Episode 1711/2000. Number of steps to finish: 18. Loss: 548.7424926757812 Reward: -6.0\nAgent: sacd_agent . Episode 1712/2000. Number of steps to finish: 20. Loss: 606.6298217773438 Reward: -8.0\nAgent: sacd_agent . Episode 1713/2000. Number of steps to finish: 20. Loss: 591.4838256835938 Reward: -12.0\nAgent: sacd_agent . Episode 1714/2000. Number of steps to finish: 20. Loss: 620.8036499023438 Reward: -12.0\nAgent: sacd_agent . Episode 1715/2000. Number of steps to finish: 16. Loss: 488.9097900390625 Reward: -4.0\nAgent: sacd_agent . Episode 1716/2000. Number of steps to finish: 20. Loss: 531.188232421875 Reward: -8.0\nAgent: sacd_agent . Episode 1717/2000. Number of steps to finish: 20. Loss: 602.9957275390625 Reward: -10.0\nAgent: sacd_agent . Episode 1718/2000. Number of steps to finish: 15. Loss: 447.68902587890625 Reward: -3.0\nAgent: sacd_agent . Episode 1719/2000. Number of steps to finish: 15. Loss: 395.5743103027344 Reward: -3.0\nAgent: sacd_agent . Episode 1720/2000. Number of steps to finish: 20. Loss: 629.544189453125 Reward: -12.0\nAgent: sacd_agent . Episode 1721/2000. Number of steps to finish: 17. Loss: 472.5350341796875 Reward: -5.0\nAgent: sacd_agent . Episode 1722/2000. Number of steps to finish: 10. Loss: 238.1456298828125 Reward: 2.0\nAgent: sacd_agent . Episode 1723/2000. Number of steps to finish: 20. Loss: 538.5880126953125 Reward: -10.0\nAgent: sacd_agent . Episode 1724/2000. Number of steps to finish: 15. Loss: 453.49267578125 Reward: -3.0\nAgent: sacd_agent . Episode 1725/2000. Number of steps to finish: 20. Loss: 503.7003479003906 Reward: -8.0\nAgent: sacd_agent . Episode 1726/2000. Number of steps to finish: 20. Loss: 543.7838745117188 Reward: -10.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 1727/2000. Number of steps to finish: 20. Loss: 508.07037353515625 Reward: -14.0\nAgent: sacd_agent . Episode 1728/2000. Number of steps to finish: 19. Loss: 530.1054077148438 Reward: -7.0\nAgent: sacd_agent . Episode 1729/2000. Number of steps to finish: 15. Loss: 440.7633056640625 Reward: -3.0\nAgent: sacd_agent . Episode 1730/2000. Number of steps to finish: 8. Loss: 277.5341491699219 Reward: 4.0\nAgent: sacd_agent . Episode 1731/2000. Number of steps to finish: 12. Loss: 406.02056884765625 Reward: 0.0\nAgent: sacd_agent . Episode 1732/2000. Number of steps to finish: 20. Loss: 567.210693359375 Reward: -10.0\nAgent: sacd_agent . Episode 1733/2000. Number of steps to finish: 14. Loss: 395.31097412109375 Reward: -2.0\nAgent: sacd_agent . Episode 1734/2000. Number of steps to finish: 20. Loss: 628.54345703125 Reward: -10.0\nAgent: sacd_agent . Episode 1735/2000. Number of steps to finish: 15. Loss: 428.4020690917969 Reward: -3.0\nAgent: sacd_agent . Episode 1736/2000. Number of steps to finish: 13. Loss: 325.87725830078125 Reward: -1.0\nAgent: sacd_agent . Episode 1737/2000. Number of steps to finish: 17. Loss: 496.3679504394531 Reward: -5.0\nAgent: sacd_agent . Episode 1738/2000. Number of steps to finish: 11. Loss: 312.3223571777344 Reward: 1.0\nAgent: sacd_agent . Episode 1739/2000. Number of steps to finish: 15. Loss: 426.85015869140625 Reward: -3.0\nAgent: sacd_agent . Episode 1740/2000. Number of steps to finish: 10. Loss: 279.6146240234375 Reward: 2.0\nAgent: sacd_agent . Episode 1741/2000. Number of steps to finish: 10. Loss: 247.05410766601562 Reward: 2.0\nAgent: sacd_agent . Episode 1742/2000. Number of steps to finish: 17. Loss: 537.2164306640625 Reward: -5.0\nAgent: sacd_agent . Episode 1743/2000. Number of steps to finish: 19. Loss: 520.5549926757812 Reward: -7.0\nAgent: sacd_agent . Episode 1744/2000. Number of steps to finish: 11. Loss: 328.0030212402344 Reward: 1.0\nAgent: sacd_agent . Episode 1745/2000. Number of steps to finish: 10. Loss: 309.59417724609375 Reward: 2.0\nAgent: sacd_agent . Episode 1746/2000. Number of steps to finish: 20. Loss: 541.779052734375 Reward: -16.0\nAgent: sacd_agent . Episode 1747/2000. Number of steps to finish: 20. Loss: 572.650390625 Reward: -12.0\nAgent: sacd_agent . Episode 1748/2000. Number of steps to finish: 20. Loss: 601.2967529296875 Reward: -16.0\nAgent: sacd_agent . Episode 1749/2000. Number of steps to finish: 20. Loss: 645.5791625976562 Reward: -12.0\nAgent: sacd_agent . Episode 1750/2000. Number of steps to finish: 12. Loss: 371.3335876464844 Reward: 0.0\nAgent: sacd_agent . Episode 1751/2000. Number of steps to finish: 20. Loss: 524.1403198242188 Reward: -10.0\nAgent: sacd_agent . Episode 1752/2000. Number of steps to finish: 20. Loss: 570.36962890625 Reward: -10.0\nAgent: sacd_agent . Episode 1753/2000. Number of steps to finish: 17. Loss: 531.5448608398438 Reward: -5.0\nAgent: sacd_agent . Episode 1754/2000. Number of steps to finish: 20. Loss: 583.740234375 Reward: -14.0\nAgent: sacd_agent . Episode 1755/2000. Number of steps to finish: 17. Loss: 527.717529296875 Reward: -5.0\nAgent: sacd_agent . Episode 1756/2000. Number of steps to finish: 20. Loss: 604.6146240234375 Reward: -20.0\nAgent: sacd_agent . Episode 1757/2000. Number of steps to finish: 20. Loss: 602.5432739257812 Reward: -10.0\nAgent: sacd_agent . Episode 1758/2000. Number of steps to finish: 12. Loss: 362.69384765625 Reward: 0.0\nAgent: sacd_agent . Episode 1759/2000. Number of steps to finish: 20. Loss: 643.3340454101562 Reward: -10.0\nAgent: sacd_agent . Episode 1760/2000. Number of steps to finish: 13. Loss: 423.94354248046875 Reward: -1.0\nAgent: sacd_agent . Episode 1761/2000. Number of steps to finish: 20. Loss: 632.8609008789062 Reward: -10.0\nAgent: sacd_agent . Episode 1762/2000. Number of steps to finish: 20. Loss: 574.7892456054688 Reward: -18.0\nAgent: sacd_agent . Episode 1763/2000. Number of steps to finish: 20. Loss: 647.8502807617188 Reward: -14.0\nAgent: sacd_agent . Episode 1764/2000. Number of steps to finish: 20. Loss: 673.0894165039062 Reward: -12.0\nAgent: sacd_agent . Episode 1765/2000. Number of steps to finish: 20. Loss: 626.0108642578125 Reward: -10.0\nAgent: sacd_agent . Episode 1766/2000. Number of steps to finish: 12. Loss: 317.717529296875 Reward: 0.0\nAgent: sacd_agent . Episode 1767/2000. Number of steps to finish: 20. Loss: 563.743408203125 Reward: -14.0\nAgent: sacd_agent . Episode 1768/2000. Number of steps to finish: 18. Loss: 519.7981567382812 Reward: -6.0\nAgent: sacd_agent . Episode 1769/2000. Number of steps to finish: 20. Loss: 594.8646850585938 Reward: -10.0\nAgent: sacd_agent . Episode 1770/2000. Number of steps to finish: 20. Loss: 603.306884765625 Reward: -12.0\nAgent: sacd_agent . Episode 1771/2000. Number of steps to finish: 19. Loss: 563.9312744140625 Reward: -7.0\nAgent: sacd_agent . Episode 1772/2000. Number of steps to finish: 20. Loss: 600.58642578125 Reward: -12.0\nAgent: sacd_agent . Episode 1773/2000. Number of steps to finish: 17. Loss: 542.8737182617188 Reward: -5.0\nAgent: sacd_agent . Episode 1774/2000. Number of steps to finish: 20. Loss: 686.3051147460938 Reward: -12.0\nAgent: sacd_agent . Episode 1775/2000. Number of steps to finish: 20. Loss: 621.6842651367188 Reward: -14.0\nAgent: sacd_agent . Episode 1776/2000. Number of steps to finish: 19. Loss: 618.4312744140625 Reward: -7.0\nAgent: sacd_agent . Episode 1777/2000. Number of steps to finish: 13. Loss: 390.5934753417969 Reward: -1.0\nAgent: sacd_agent . Episode 1778/2000. Number of steps to finish: 19. Loss: 561.5177001953125 Reward: -7.0\nAgent: sacd_agent . Episode 1779/2000. Number of steps to finish: 20. Loss: 684.1903686523438 Reward: -8.0\nAgent: sacd_agent . Episode 1780/2000. Number of steps to finish: 20. Loss: 556.9502563476562 Reward: -12.0\nAgent: sacd_agent . Episode 1781/2000. Number of steps to finish: 20. Loss: 646.0447387695312 Reward: -18.0\nAgent: sacd_agent . Episode 1782/2000. Number of steps to finish: 20. Loss: 599.7115478515625 Reward: -10.0\nAgent: sacd_agent . Episode 1783/2000. Number of steps to finish: 20. Loss: 552.475830078125 Reward: -12.0\nAgent: sacd_agent . Episode 1784/2000. Number of steps to finish: 20. Loss: 629.6109619140625 Reward: -8.0\nAgent: sacd_agent . Episode 1785/2000. Number of steps to finish: 15. Loss: 494.306640625 Reward: -3.0\nAgent: sacd_agent . Episode 1786/2000. Number of steps to finish: 12. Loss: 342.39990234375 Reward: 0.0\nAgent: sacd_agent . Episode 1787/2000. Number of steps to finish: 20. Loss: 726.2428588867188 Reward: -14.0\nAgent: sacd_agent . Episode 1788/2000. Number of steps to finish: 20. Loss: 653.9027099609375 Reward: -10.0\nAgent: sacd_agent . Episode 1789/2000. Number of steps to finish: 14. Loss: 439.8731384277344 Reward: -2.0\nAgent: sacd_agent . Episode 1790/2000. Number of steps to finish: 17. Loss: 516.7037353515625 Reward: -5.0\nAgent: sacd_agent . Episode 1791/2000. Number of steps to finish: 14. Loss: 474.74566650390625 Reward: -2.0\nAgent: sacd_agent . Episode 1792/2000. Number of steps to finish: 20. Loss: 629.304931640625 Reward: -14.0\nAgent: sacd_agent . Episode 1793/2000. Number of steps to finish: 20. Loss: 609.4713745117188 Reward: -18.0\nAgent: sacd_agent . Episode 1794/2000. Number of steps to finish: 20. Loss: 594.5228881835938 Reward: -10.0\nAgent: sacd_agent . Episode 1795/2000. Number of steps to finish: 10. Loss: 281.4650573730469 Reward: 2.0\nAgent: sacd_agent . Episode 1796/2000. Number of steps to finish: 13. Loss: 398.3378601074219 Reward: -1.0\nAgent: sacd_agent . Episode 1797/2000. Number of steps to finish: 20. Loss: 571.9700927734375 Reward: -12.0\nAgent: sacd_agent . Episode 1798/2000. Number of steps to finish: 20. Loss: 719.1657104492188 Reward: -14.0\nAgent: sacd_agent . Episode 1799/2000. Number of steps to finish: 20. Loss: 602.53759765625 Reward: -12.0\nAgent: sacd_agent . Episode 1800/2000. Number of steps to finish: 20. Loss: 636.3292236328125 Reward: -10.0\nAgent: sacd_agent . Episode 1801/2000. Number of steps to finish: 18. Loss: 586.9794921875 Reward: -6.0\nAgent: sacd_agent . Episode 1802/2000. Number of steps to finish: 20. Loss: 592.7960205078125 Reward: -10.0\nAgent: sacd_agent . Episode 1803/2000. Number of steps to finish: 20. Loss: 649.5240478515625 Reward: -12.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 1804/2000. Number of steps to finish: 12. Loss: 419.3766174316406 Reward: 0.0\nAgent: sacd_agent . Episode 1805/2000. Number of steps to finish: 20. Loss: 591.0408935546875 Reward: -12.0\nAgent: sacd_agent . Episode 1806/2000. Number of steps to finish: 20. Loss: 538.0970458984375 Reward: -12.0\nAgent: sacd_agent . Episode 1807/2000. Number of steps to finish: 18. Loss: 598.7472534179688 Reward: -6.0\nAgent: sacd_agent . Episode 1808/2000. Number of steps to finish: 20. Loss: 582.65869140625 Reward: -16.0\nAgent: sacd_agent . Episode 1809/2000. Number of steps to finish: 20. Loss: 661.3568725585938 Reward: -10.0\nAgent: sacd_agent . Episode 1810/2000. Number of steps to finish: 20. Loss: 654.4780883789062 Reward: -12.0\nAgent: sacd_agent . Episode 1811/2000. Number of steps to finish: 20. Loss: 646.214599609375 Reward: -14.0\nAgent: sacd_agent . Episode 1812/2000. Number of steps to finish: 20. Loss: 637.3893432617188 Reward: -14.0\nAgent: sacd_agent . Episode 1813/2000. Number of steps to finish: 20. Loss: 679.1918334960938 Reward: -18.0\nAgent: sacd_agent . Episode 1814/2000. Number of steps to finish: 12. Loss: 362.3243103027344 Reward: 0.0\nAgent: sacd_agent . Episode 1815/2000. Number of steps to finish: 13. Loss: 439.53887939453125 Reward: -1.0\nAgent: sacd_agent . Episode 1816/2000. Number of steps to finish: 20. Loss: 656.7642211914062 Reward: -10.0\nAgent: sacd_agent . Episode 1817/2000. Number of steps to finish: 14. Loss: 405.3233337402344 Reward: -2.0\nAgent: sacd_agent . Episode 1818/2000. Number of steps to finish: 20. Loss: 605.9646606445312 Reward: -10.0\nAgent: sacd_agent . Episode 1819/2000. Number of steps to finish: 19. Loss: 648.6569213867188 Reward: -7.0\nAgent: sacd_agent . Episode 1820/2000. Number of steps to finish: 20. Loss: 661.0691528320312 Reward: -10.0\nAgent: sacd_agent . Episode 1821/2000. Number of steps to finish: 20. Loss: 652.6991577148438 Reward: -10.0\nAgent: sacd_agent . Episode 1822/2000. Number of steps to finish: 20. Loss: 738.8311157226562 Reward: -16.0\nAgent: sacd_agent . Episode 1823/2000. Number of steps to finish: 11. Loss: 310.05908203125 Reward: 1.0\nAgent: sacd_agent . Episode 1824/2000. Number of steps to finish: 15. Loss: 500.0417785644531 Reward: -3.0\nAgent: sacd_agent . Episode 1825/2000. Number of steps to finish: 20. Loss: 540.4465942382812 Reward: -18.0\nAgent: sacd_agent . Episode 1826/2000. Number of steps to finish: 11. Loss: 382.4725036621094 Reward: 1.0\nAgent: sacd_agent . Episode 1827/2000. Number of steps to finish: 20. Loss: 740.8851928710938 Reward: -10.0\nAgent: sacd_agent . Episode 1828/2000. Number of steps to finish: 20. Loss: 654.3660278320312 Reward: -12.0\nAgent: sacd_agent . Episode 1829/2000. Number of steps to finish: 20. Loss: 641.6976928710938 Reward: -8.0\nAgent: sacd_agent . Episode 1830/2000. Number of steps to finish: 20. Loss: 670.8588256835938 Reward: -16.0\nAgent: sacd_agent . Episode 1831/2000. Number of steps to finish: 9. Loss: 265.1530456542969 Reward: 3.0\nAgent: sacd_agent . Episode 1832/2000. Number of steps to finish: 13. Loss: 464.0667724609375 Reward: -1.0\nAgent: sacd_agent . Episode 1833/2000. Number of steps to finish: 20. Loss: 593.4347534179688 Reward: -14.0\nAgent: sacd_agent . Episode 1834/2000. Number of steps to finish: 18. Loss: 586.3193359375 Reward: -6.0\nAgent: sacd_agent . Episode 1835/2000. Number of steps to finish: 20. Loss: 689.2570190429688 Reward: -10.0\nAgent: sacd_agent . Episode 1836/2000. Number of steps to finish: 12. Loss: 377.0057067871094 Reward: 0.0\nAgent: sacd_agent . Episode 1837/2000. Number of steps to finish: 10. Loss: 281.4371032714844 Reward: 2.0\nAgent: sacd_agent . Episode 1838/2000. Number of steps to finish: 20. Loss: 621.12158203125 Reward: -18.0\nAgent: sacd_agent . Episode 1839/2000. Number of steps to finish: 20. Loss: 597.1887817382812 Reward: -14.0\nAgent: sacd_agent . Episode 1840/2000. Number of steps to finish: 10. Loss: 326.8506164550781 Reward: 2.0\nAgent: sacd_agent . Episode 1841/2000. Number of steps to finish: 15. Loss: 492.8781433105469 Reward: -3.0\nAgent: sacd_agent . Episode 1842/2000. Number of steps to finish: 20. Loss: 704.5256958007812 Reward: -12.0\nAgent: sacd_agent . Episode 1843/2000. Number of steps to finish: 12. Loss: 452.00494384765625 Reward: 0.0\nAgent: sacd_agent . Episode 1844/2000. Number of steps to finish: 13. Loss: 382.3643493652344 Reward: -1.0\nAgent: sacd_agent . Episode 1845/2000. Number of steps to finish: 13. Loss: 465.03814697265625 Reward: -1.0\nAgent: sacd_agent . Episode 1846/2000. Number of steps to finish: 20. Loss: 645.6498413085938 Reward: -12.0\nAgent: sacd_agent . Episode 1847/2000. Number of steps to finish: 10. Loss: 357.3397521972656 Reward: 2.0\nAgent: sacd_agent . Episode 1848/2000. Number of steps to finish: 11. Loss: 380.37677001953125 Reward: 1.0\nAgent: sacd_agent . Episode 1849/2000. Number of steps to finish: 20. Loss: 753.812255859375 Reward: -12.0\nAgent: sacd_agent . Episode 1850/2000. Number of steps to finish: 9. Loss: 320.75946044921875 Reward: 3.0\nAgent: sacd_agent . Episode 1851/2000. Number of steps to finish: 18. Loss: 646.07177734375 Reward: -6.0\nAgent: sacd_agent . Episode 1852/2000. Number of steps to finish: 7. Loss: 232.5390167236328 Reward: 5.0\nAgent: sacd_agent . Episode 1853/2000. Number of steps to finish: 20. Loss: 710.8765869140625 Reward: -10.0\nAgent: sacd_agent . Episode 1854/2000. Number of steps to finish: 16. Loss: 528.548095703125 Reward: -4.0\nAgent: sacd_agent . Episode 1855/2000. Number of steps to finish: 20. Loss: 675.6561889648438 Reward: -12.0\nAgent: sacd_agent . Episode 1856/2000. Number of steps to finish: 20. Loss: 663.7833862304688 Reward: -12.0\nAgent: sacd_agent . Episode 1857/2000. Number of steps to finish: 20. Loss: 734.3474731445312 Reward: -14.0\nAgent: sacd_agent . Episode 1858/2000. Number of steps to finish: 20. Loss: 637.02587890625 Reward: -12.0\nAgent: sacd_agent . Episode 1859/2000. Number of steps to finish: 20. Loss: 693.152099609375 Reward: -10.0\nAgent: sacd_agent . Episode 1860/2000. Number of steps to finish: 14. Loss: 484.7806701660156 Reward: -2.0\nAgent: sacd_agent . Episode 1861/2000. Number of steps to finish: 20. Loss: 762.1405639648438 Reward: -8.0\nAgent: sacd_agent . Episode 1862/2000. Number of steps to finish: 15. Loss: 516.965576171875 Reward: -3.0\nAgent: sacd_agent . Episode 1863/2000. Number of steps to finish: 12. Loss: 446.9508972167969 Reward: 0.0\nAgent: sacd_agent . Episode 1864/2000. Number of steps to finish: 16. Loss: 533.3111572265625 Reward: -4.0\nAgent: sacd_agent . Episode 1865/2000. Number of steps to finish: 11. Loss: 331.1720275878906 Reward: 1.0\nAgent: sacd_agent . Episode 1866/2000. Number of steps to finish: 20. Loss: 714.4126586914062 Reward: -12.0\nAgent: sacd_agent . Episode 1867/2000. Number of steps to finish: 20. Loss: 703.7528076171875 Reward: -8.0\nAgent: sacd_agent . Episode 1868/2000. Number of steps to finish: 20. Loss: 662.840087890625 Reward: -16.0\nAgent: sacd_agent . Episode 1869/2000. Number of steps to finish: 20. Loss: 696.1287231445312 Reward: -14.0\nAgent: sacd_agent . Episode 1870/2000. Number of steps to finish: 17. Loss: 598.0917358398438 Reward: -5.0\nAgent: sacd_agent . Episode 1871/2000. Number of steps to finish: 16. Loss: 624.3271484375 Reward: -4.0\nAgent: sacd_agent . Episode 1872/2000. Number of steps to finish: 17. Loss: 536.513427734375 Reward: -5.0\nAgent: sacd_agent . Episode 1873/2000. Number of steps to finish: 12. Loss: 494.92254638671875 Reward: 0.0\nAgent: sacd_agent . Episode 1874/2000. Number of steps to finish: 20. Loss: 728.1476440429688 Reward: -10.0\nAgent: sacd_agent . Episode 1875/2000. Number of steps to finish: 16. Loss: 626.5425415039062 Reward: -4.0\nAgent: sacd_agent . Episode 1876/2000. Number of steps to finish: 20. Loss: 702.0897216796875 Reward: -12.0\nAgent: sacd_agent . Episode 1877/2000. Number of steps to finish: 20. Loss: 611.5140380859375 Reward: -14.0\nAgent: sacd_agent . Episode 1878/2000. Number of steps to finish: 15. Loss: 563.71826171875 Reward: -3.0\nAgent: sacd_agent . Episode 1879/2000. Number of steps to finish: 20. Loss: 631.9882202148438 Reward: -10.0\nAgent: sacd_agent . Episode 1880/2000. Number of steps to finish: 11. Loss: 410.393798828125 Reward: 1.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 1881/2000. Number of steps to finish: 20. Loss: 672.4093017578125 Reward: -10.0\nAgent: sacd_agent . Episode 1882/2000. Number of steps to finish: 13. Loss: 488.8382568359375 Reward: -1.0\nAgent: sacd_agent . Episode 1883/2000. Number of steps to finish: 9. Loss: 340.47955322265625 Reward: 3.0\nAgent: sacd_agent . Episode 1884/2000. Number of steps to finish: 20. Loss: 578.2789916992188 Reward: -12.0\nAgent: sacd_agent . Episode 1885/2000. Number of steps to finish: 20. Loss: 620.1774291992188 Reward: -10.0\nAgent: sacd_agent . Episode 1886/2000. Number of steps to finish: 20. Loss: 622.1619262695312 Reward: -16.0\nAgent: sacd_agent . Episode 1887/2000. Number of steps to finish: 20. Loss: 727.0828247070312 Reward: -10.0\nAgent: sacd_agent . Episode 1888/2000. Number of steps to finish: 20. Loss: 767.2169799804688 Reward: -10.0\nAgent: sacd_agent . Episode 1889/2000. Number of steps to finish: 15. Loss: 550.0263671875 Reward: -3.0\nAgent: sacd_agent . Episode 1890/2000. Number of steps to finish: 20. Loss: 667.39990234375 Reward: -14.0\nAgent: sacd_agent . Episode 1891/2000. Number of steps to finish: 20. Loss: 667.6666870117188 Reward: -8.0\nAgent: sacd_agent . Episode 1892/2000. Number of steps to finish: 20. Loss: 766.5511474609375 Reward: -14.0\nAgent: sacd_agent . Episode 1893/2000. Number of steps to finish: 18. Loss: 713.9057006835938 Reward: -6.0\nAgent: sacd_agent . Episode 1894/2000. Number of steps to finish: 18. Loss: 628.7163696289062 Reward: -6.0\nAgent: sacd_agent . Episode 1895/2000. Number of steps to finish: 13. Loss: 465.68682861328125 Reward: -1.0\nAgent: sacd_agent . Episode 1896/2000. Number of steps to finish: 20. Loss: 695.5254516601562 Reward: -12.0\nAgent: sacd_agent . Episode 1897/2000. Number of steps to finish: 20. Loss: 679.3954467773438 Reward: -12.0\nAgent: sacd_agent . Episode 1898/2000. Number of steps to finish: 16. Loss: 582.556396484375 Reward: -4.0\nAgent: sacd_agent . Episode 1899/2000. Number of steps to finish: 20. Loss: 697.1484375 Reward: -12.0\nAgent: sacd_agent . Episode 1900/2000. Number of steps to finish: 20. Loss: 774.4705810546875 Reward: -8.0\nAgent: sacd_agent . Episode 1901/2000. Number of steps to finish: 20. Loss: 855.2296142578125 Reward: -12.0\nAgent: sacd_agent . Episode 1902/2000. Number of steps to finish: 10. Loss: 355.42706298828125 Reward: 2.0\nAgent: sacd_agent . Episode 1903/2000. Number of steps to finish: 20. Loss: 783.6751708984375 Reward: -14.0\nAgent: sacd_agent . Episode 1904/2000. Number of steps to finish: 20. Loss: 756.138427734375 Reward: -12.0\nAgent: sacd_agent . Episode 1905/2000. Number of steps to finish: 18. Loss: 604.4459228515625 Reward: -6.0\nAgent: sacd_agent . Episode 1906/2000. Number of steps to finish: 20. Loss: 794.400146484375 Reward: -10.0\nAgent: sacd_agent . Episode 1907/2000. Number of steps to finish: 20. Loss: 768.02197265625 Reward: -12.0\nAgent: sacd_agent . Episode 1908/2000. Number of steps to finish: 15. Loss: 512.2727661132812 Reward: -3.0\nAgent: sacd_agent . Episode 1909/2000. Number of steps to finish: 15. Loss: 517.7454833984375 Reward: -3.0\nAgent: sacd_agent . Episode 1910/2000. Number of steps to finish: 20. Loss: 780.5955810546875 Reward: -16.0\nAgent: sacd_agent . Episode 1911/2000. Number of steps to finish: 18. Loss: 683.3629150390625 Reward: -6.0\nAgent: sacd_agent . Episode 1912/2000. Number of steps to finish: 20. Loss: 721.51904296875 Reward: -10.0\nAgent: sacd_agent . Episode 1913/2000. Number of steps to finish: 13. Loss: 421.3484802246094 Reward: -1.0\nAgent: sacd_agent . Episode 1914/2000. Number of steps to finish: 20. Loss: 667.1909790039062 Reward: -10.0\nAgent: sacd_agent . Episode 1915/2000. Number of steps to finish: 14. Loss: 505.7071838378906 Reward: -2.0\nAgent: sacd_agent . Episode 1916/2000. Number of steps to finish: 16. Loss: 605.7883911132812 Reward: -4.0\nAgent: sacd_agent . Episode 1917/2000. Number of steps to finish: 20. Loss: 674.3946533203125 Reward: -12.0\nAgent: sacd_agent . Episode 1918/2000. Number of steps to finish: 16. Loss: 548.1837158203125 Reward: -4.0\nAgent: sacd_agent . Episode 1919/2000. Number of steps to finish: 20. Loss: 801.1891479492188 Reward: -14.0\nAgent: sacd_agent . Episode 1920/2000. Number of steps to finish: 20. Loss: 686.2269897460938 Reward: -14.0\nAgent: sacd_agent . Episode 1921/2000. Number of steps to finish: 20. Loss: 738.0866088867188 Reward: -10.0\nAgent: sacd_agent . Episode 1922/2000. Number of steps to finish: 20. Loss: 731.1076049804688 Reward: -10.0\nAgent: sacd_agent . Episode 1923/2000. Number of steps to finish: 20. Loss: 725.0712280273438 Reward: -10.0\nAgent: sacd_agent . Episode 1924/2000. Number of steps to finish: 15. Loss: 556.2131958007812 Reward: -3.0\nAgent: sacd_agent . Episode 1925/2000. Number of steps to finish: 20. Loss: 730.39794921875 Reward: -10.0\nAgent: sacd_agent . Episode 1926/2000. Number of steps to finish: 20. Loss: 692.6845703125 Reward: -20.0\nAgent: sacd_agent . Episode 1927/2000. Number of steps to finish: 13. Loss: 447.8968811035156 Reward: -1.0\nAgent: sacd_agent . Episode 1928/2000. Number of steps to finish: 20. Loss: 821.77490234375 Reward: -12.0\nAgent: sacd_agent . Episode 1929/2000. Number of steps to finish: 20. Loss: 757.8533935546875 Reward: -10.0\nAgent: sacd_agent . Episode 1930/2000. Number of steps to finish: 18. Loss: 636.411865234375 Reward: -6.0\nAgent: sacd_agent . Episode 1931/2000. Number of steps to finish: 19. Loss: 662.1351928710938 Reward: -7.0\nAgent: sacd_agent . Episode 1932/2000. Number of steps to finish: 13. Loss: 455.5299987792969 Reward: -1.0\nAgent: sacd_agent . Episode 1933/2000. Number of steps to finish: 20. Loss: 662.2623901367188 Reward: -14.0\nAgent: sacd_agent . Episode 1934/2000. Number of steps to finish: 20. Loss: 720.916015625 Reward: -14.0\nAgent: sacd_agent . Episode 1935/2000. Number of steps to finish: 14. Loss: 498.7717590332031 Reward: -2.0\nAgent: sacd_agent . Episode 1936/2000. Number of steps to finish: 20. Loss: 749.326904296875 Reward: -12.0\nAgent: sacd_agent . Episode 1937/2000. Number of steps to finish: 19. Loss: 657.5432739257812 Reward: -7.0\nAgent: sacd_agent . Episode 1938/2000. Number of steps to finish: 17. Loss: 661.36181640625 Reward: -5.0\nAgent: sacd_agent . Episode 1939/2000. Number of steps to finish: 14. Loss: 530.154541015625 Reward: -2.0\nAgent: sacd_agent . Episode 1940/2000. Number of steps to finish: 20. Loss: 826.8570556640625 Reward: -18.0\nAgent: sacd_agent . Episode 1941/2000. Number of steps to finish: 20. Loss: 733.343505859375 Reward: -12.0\nAgent: sacd_agent . Episode 1942/2000. Number of steps to finish: 20. Loss: 784.9199829101562 Reward: -10.0\nAgent: sacd_agent . Episode 1943/2000. Number of steps to finish: 20. Loss: 720.2738647460938 Reward: -10.0\nAgent: sacd_agent . Episode 1944/2000. Number of steps to finish: 20. Loss: 710.6383056640625 Reward: -10.0\nAgent: sacd_agent . Episode 1945/2000. Number of steps to finish: 20. Loss: 744.2313232421875 Reward: -10.0\nAgent: sacd_agent . Episode 1946/2000. Number of steps to finish: 20. Loss: 787.1302490234375 Reward: -10.0\nAgent: sacd_agent . Episode 1947/2000. Number of steps to finish: 20. Loss: 773.8226928710938 Reward: -14.0\nAgent: sacd_agent . Episode 1948/2000. Number of steps to finish: 10. Loss: 332.1990051269531 Reward: 2.0\nAgent: sacd_agent . Episode 1949/2000. Number of steps to finish: 12. Loss: 459.66314697265625 Reward: 0.0\nAgent: sacd_agent . Episode 1950/2000. Number of steps to finish: 20. Loss: 690.8148193359375 Reward: -10.0\nAgent: sacd_agent . Episode 1951/2000. Number of steps to finish: 20. Loss: 725.8416748046875 Reward: -14.0\nAgent: sacd_agent . Episode 1952/2000. Number of steps to finish: 9. Loss: 333.88433837890625 Reward: 3.0\nAgent: sacd_agent . Episode 1953/2000. Number of steps to finish: 19. Loss: 714.20263671875 Reward: -7.0\nAgent: sacd_agent . Episode 1954/2000. Number of steps to finish: 20. Loss: 849.1888427734375 Reward: -8.0\nAgent: sacd_agent . Episode 1955/2000. Number of steps to finish: 20. Loss: 758.2544555664062 Reward: -12.0\nAgent: sacd_agent . Episode 1956/2000. Number of steps to finish: 20. Loss: 739.027099609375 Reward: -10.0\nAgent: sacd_agent . Episode 1957/2000. Number of steps to finish: 14. Loss: 505.543701171875 Reward: -2.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: sacd_agent . Episode 1958/2000. Number of steps to finish: 20. Loss: 752.3407592773438 Reward: -10.0\nAgent: sacd_agent . Episode 1959/2000. Number of steps to finish: 20. Loss: 775.5552978515625 Reward: -12.0\nAgent: sacd_agent . Episode 1960/2000. Number of steps to finish: 9. Loss: 371.26885986328125 Reward: 3.0\nAgent: sacd_agent . Episode 1961/2000. Number of steps to finish: 20. Loss: 738.4630737304688 Reward: -10.0\nAgent: sacd_agent . Episode 1962/2000. Number of steps to finish: 13. Loss: 519.7545166015625 Reward: -1.0\nAgent: sacd_agent . Episode 1963/2000. Number of steps to finish: 20. Loss: 734.4758911132812 Reward: -16.0\nAgent: sacd_agent . Episode 1964/2000. Number of steps to finish: 20. Loss: 649.5870361328125 Reward: -10.0\nAgent: sacd_agent . Episode 1965/2000. Number of steps to finish: 20. Loss: 748.3004150390625 Reward: -10.0\nAgent: sacd_agent . Episode 1966/2000. Number of steps to finish: 15. Loss: 522.2303466796875 Reward: -3.0\nAgent: sacd_agent . Episode 1967/2000. Number of steps to finish: 15. Loss: 569.953125 Reward: -3.0\nAgent: sacd_agent . Episode 1968/2000. Number of steps to finish: 20. Loss: 782.79443359375 Reward: -10.0\nAgent: sacd_agent . Episode 1969/2000. Number of steps to finish: 20. Loss: 708.4857788085938 Reward: -14.0\nAgent: sacd_agent . Episode 1970/2000. Number of steps to finish: 10. Loss: 360.7882080078125 Reward: 2.0\nAgent: sacd_agent . Episode 1971/2000. Number of steps to finish: 20. Loss: 774.0534057617188 Reward: -12.0\nAgent: sacd_agent . Episode 1972/2000. Number of steps to finish: 20. Loss: 719.7459106445312 Reward: -10.0\nAgent: sacd_agent . Episode 1973/2000. Number of steps to finish: 12. Loss: 491.9906005859375 Reward: 0.0\nAgent: sacd_agent . Episode 1974/2000. Number of steps to finish: 20. Loss: 797.5318603515625 Reward: -18.0\nAgent: sacd_agent . Episode 1975/2000. Number of steps to finish: 12. Loss: 474.0914001464844 Reward: 0.0\nAgent: sacd_agent . Episode 1976/2000. Number of steps to finish: 10. Loss: 442.556884765625 Reward: 2.0\nAgent: sacd_agent . Episode 1977/2000. Number of steps to finish: 15. Loss: 578.70361328125 Reward: -3.0\nAgent: sacd_agent . Episode 1978/2000. Number of steps to finish: 14. Loss: 582.6165161132812 Reward: -2.0\nAgent: sacd_agent . Episode 1979/2000. Number of steps to finish: 10. Loss: 451.9717102050781 Reward: 2.0\nAgent: sacd_agent . Episode 1980/2000. Number of steps to finish: 20. Loss: 742.4308471679688 Reward: -12.0\nAgent: sacd_agent . Episode 1981/2000. Number of steps to finish: 20. Loss: 822.391357421875 Reward: -12.0\nAgent: sacd_agent . Episode 1982/2000. Number of steps to finish: 7. Loss: 271.0357666015625 Reward: 5.0\nAgent: sacd_agent . Episode 1983/2000. Number of steps to finish: 17. Loss: 698.7050170898438 Reward: -5.0\nAgent: sacd_agent . Episode 1984/2000. Number of steps to finish: 20. Loss: 754.6536865234375 Reward: -12.0\nAgent: sacd_agent . Episode 1985/2000. Number of steps to finish: 13. Loss: 569.27099609375 Reward: -1.0\nAgent: sacd_agent . Episode 1986/2000. Number of steps to finish: 18. Loss: 640.1678466796875 Reward: -6.0\nAgent: sacd_agent . Episode 1987/2000. Number of steps to finish: 20. Loss: 769.9349975585938 Reward: -10.0\nAgent: sacd_agent . Episode 1988/2000. Number of steps to finish: 20. Loss: 761.1341552734375 Reward: -12.0\nAgent: sacd_agent . Episode 1989/2000. Number of steps to finish: 16. Loss: 704.6622314453125 Reward: -4.0\nAgent: sacd_agent . Episode 1990/2000. Number of steps to finish: 20. Loss: 759.8125 Reward: -10.0\nAgent: sacd_agent . Episode 1991/2000. Number of steps to finish: 12. Loss: 497.9544982910156 Reward: 0.0\nAgent: sacd_agent . Episode 1992/2000. Number of steps to finish: 20. Loss: 826.4559326171875 Reward: -10.0\nAgent: sacd_agent . Episode 1993/2000. Number of steps to finish: 12. Loss: 469.7745666503906 Reward: 0.0\nAgent: sacd_agent . Episode 1994/2000. Number of steps to finish: 20. Loss: 759.3067016601562 Reward: -10.0\nAgent: sacd_agent . Episode 1995/2000. Number of steps to finish: 19. Loss: 762.4769287109375 Reward: -7.0\nAgent: sacd_agent . Episode 1996/2000. Number of steps to finish: 20. Loss: 847.3998413085938 Reward: -10.0\nAgent: sacd_agent . Episode 1997/2000. Number of steps to finish: 20. Loss: 827.7449340820312 Reward: -10.0\nAgent: sacd_agent . Episode 1998/2000. Number of steps to finish: 20. Loss: 889.1830444335938 Reward: -14.0\nAgent: sacd_agent . Episode 1999/2000. Number of steps to finish: 13. Loss: 471.740234375 Reward: -1.0\nAgent: sacd_agent . Episode 2000/2000. Number of steps to finish: 20. Loss: 748.2807006835938 Reward: -10.0\n########## battleship_baseline is running ##########\nAgent: battleship_baseline . Episode 0/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 2/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 3/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 4/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 5/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 6/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 7/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 8/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 9/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 10/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 11/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 12/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 13/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 14/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 15/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 16/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 17/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 18/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 19/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 20/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 21/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 22/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 23/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 24/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 25/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 26/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 27/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 28/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 29/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 30/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 31/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 32/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 33/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 34/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 35/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 36/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 37/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 38/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 39/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 40/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 41/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 42/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 43/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 44/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 45/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 46/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 47/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 48/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 49/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 50/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 51/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 52/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 53/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 54/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 55/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 56/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 57/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 58/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 59/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 60/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 61/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 62/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 63/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 64/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 65/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 66/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 67/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 68/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 69/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 70/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 71/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 72/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 73/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 74/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 75/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 76/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 77/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 78/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 79/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 80/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 81/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 82/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 83/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 84/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 85/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 86/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 87/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 88/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 89/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 90/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 91/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 92/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 93/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 94/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 95/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 96/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 97/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 98/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 99/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 100/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 101/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 102/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 103/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 104/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 105/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 106/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 107/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 108/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 109/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 110/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 111/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 112/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 113/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 114/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 115/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 116/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 117/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 118/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 119/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 120/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 121/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 122/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 123/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 124/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 125/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 126/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 127/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 128/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 129/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 130/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 131/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 132/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 133/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 134/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 135/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 136/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 137/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 138/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 139/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 140/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 141/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 142/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 143/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 144/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 145/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 146/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 147/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 148/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 149/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 150/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 151/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 152/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 153/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 154/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 155/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 156/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 157/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 158/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 159/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 160/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 161/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 162/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 163/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 164/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 165/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 166/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 167/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 168/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 169/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 170/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 171/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 172/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 173/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 174/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 175/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 176/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 177/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 178/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 179/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 180/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 181/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 182/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 183/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 184/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 185/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 186/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 187/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 188/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 189/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 190/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 191/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 192/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 193/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 194/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 195/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 196/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 197/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 198/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 199/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 200/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 201/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 202/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 203/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 204/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 205/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 206/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 207/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 208/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 209/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 210/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 211/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 212/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 213/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 214/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 215/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 216/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 217/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 218/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 219/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 220/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 221/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 222/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 223/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 224/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 225/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 226/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 227/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 228/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 229/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 230/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 231/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 232/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 233/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 234/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 235/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 236/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 237/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 238/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 239/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 240/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 241/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 242/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 243/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 244/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 245/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 246/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 247/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 248/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 249/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 250/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 251/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 252/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 253/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 254/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 255/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 256/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 257/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 258/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 259/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 260/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 261/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 262/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 263/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 264/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 265/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 266/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 267/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 268/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 269/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 270/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 271/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 272/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 273/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 274/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 275/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 276/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 277/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 278/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 279/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 280/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 281/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 282/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 283/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 284/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 285/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 286/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 287/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 288/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 289/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 290/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 291/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 292/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 293/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 294/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 295/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 296/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 297/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 298/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 299/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 300/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 301/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 302/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 303/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 304/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 305/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 306/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 307/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 308/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 309/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 310/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 311/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 312/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 313/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 314/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 315/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 316/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 317/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 318/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 319/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 320/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 321/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 322/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 323/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 324/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 325/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 326/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 327/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 328/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 329/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 330/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 331/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 332/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 333/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 334/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 335/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 336/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 337/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 338/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 339/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 340/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 341/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 342/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 343/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 344/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 345/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 346/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 347/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 348/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 349/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 350/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 351/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 352/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 353/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 354/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 355/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 356/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 357/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 358/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 359/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 360/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 361/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 362/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 363/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 364/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 365/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 366/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 367/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 368/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 369/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 370/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 371/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 372/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 373/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 374/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 375/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 376/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 377/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 378/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 379/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 380/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 381/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 382/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 383/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 384/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 385/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 386/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 387/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 388/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 389/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 390/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 391/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 392/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 393/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 394/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 395/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 396/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 397/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 398/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 399/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 400/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 401/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 402/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 403/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 404/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 405/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 406/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 407/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 408/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 409/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 410/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 411/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 412/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 413/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 414/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 415/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 416/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 417/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 418/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 419/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 420/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 421/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 422/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 423/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 424/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 425/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 426/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 427/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 428/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 429/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 430/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 431/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 432/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 433/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 434/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 435/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 436/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 437/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 438/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 439/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 440/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 441/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 442/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 443/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 444/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 445/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 446/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 447/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 448/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 449/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 450/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 451/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 452/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 453/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 454/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 455/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 456/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 457/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 458/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 459/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 460/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 461/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 462/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 463/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 464/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 465/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 466/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 467/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 468/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 469/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 470/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 471/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 472/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 473/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 474/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 475/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 476/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 477/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 478/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 479/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 480/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 481/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 482/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 483/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 484/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 485/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 486/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 487/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 488/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 489/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 490/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 491/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 492/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 493/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 494/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 495/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 496/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 497/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 498/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 499/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 500/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 501/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 502/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 503/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 504/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 505/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 506/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 507/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 508/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 509/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 510/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 511/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 512/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 513/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 514/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 515/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 516/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 517/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 518/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 519/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 520/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 521/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 522/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 523/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 524/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 525/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 526/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 527/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 528/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 529/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 530/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 531/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 532/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 533/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 534/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 535/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 536/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 537/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 538/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 539/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 540/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 541/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 542/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 543/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 544/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 545/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 546/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 547/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 548/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 549/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 550/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 551/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 552/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 553/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 554/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 555/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 556/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 557/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 558/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 559/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 560/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 561/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 562/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 563/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 564/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 565/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 566/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 567/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 568/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 569/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 570/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 571/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 572/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 573/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 574/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 575/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 576/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 577/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 578/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 579/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 580/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 581/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 582/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 583/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 584/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 585/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 586/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 587/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 588/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 589/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 590/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 591/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 592/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 593/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 594/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 595/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 596/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 597/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 598/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 599/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 600/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 601/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 602/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 603/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 604/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 605/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 606/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 607/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 608/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 609/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 610/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 611/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 612/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 613/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 614/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 615/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 616/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 617/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 618/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 619/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 620/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 621/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 622/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 623/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 624/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 625/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 626/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 627/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 628/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 629/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 630/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 631/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 632/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 633/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 634/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 635/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 636/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 637/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 638/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 639/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 640/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 641/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 642/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 643/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 644/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 645/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 646/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 647/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 648/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 649/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 650/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 651/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 652/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 653/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 654/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 655/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 656/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 657/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 658/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 659/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 660/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 661/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 662/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 663/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 664/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 665/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 666/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 667/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 668/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 669/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 670/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 671/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 672/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 673/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 674/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 675/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 676/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 677/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 678/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 679/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 680/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 681/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 682/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 683/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 684/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 685/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 686/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 687/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 688/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 689/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 690/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 691/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 692/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 693/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 694/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 695/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 696/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 697/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 698/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 699/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 700/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 701/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 702/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 703/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 704/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 705/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 706/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 707/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 708/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 709/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 710/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 711/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 712/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 713/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 714/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 715/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 716/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 717/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 718/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 719/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 720/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 721/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 722/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 723/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 724/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 725/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 726/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 727/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 728/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 729/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 730/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 731/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 732/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 733/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 734/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 735/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 736/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 737/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 738/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 739/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 740/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 741/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 742/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 743/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 744/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 745/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 746/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 747/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 748/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 749/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 750/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 751/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 752/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 753/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 754/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 755/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 756/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 757/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 758/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 759/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 760/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 761/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 762/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 763/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 764/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 765/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 766/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 767/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 768/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 769/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 770/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 771/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 772/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 773/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 774/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 775/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 776/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 777/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 778/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 779/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 780/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 781/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 782/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 783/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 784/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 785/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 786/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 787/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 788/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 789/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 790/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 791/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 792/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 793/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 794/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 795/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 796/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 797/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 798/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 799/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 800/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 801/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 802/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 803/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 804/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 805/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 806/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 807/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 808/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 809/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 810/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 811/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 812/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 813/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 814/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 815/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 816/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 817/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 818/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 819/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 820/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 821/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 822/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 823/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 824/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 825/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 826/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 827/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 828/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 829/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 830/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 831/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 832/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 833/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 834/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 835/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 836/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 837/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 838/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 839/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 840/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 841/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 842/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 843/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 844/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 845/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 846/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 847/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 848/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 849/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 850/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 851/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 852/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 853/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 854/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 855/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 856/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 857/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 858/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 859/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 860/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 861/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 862/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 863/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 864/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 865/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 866/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 867/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 868/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 869/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 870/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 871/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 872/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 873/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 874/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 875/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 876/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 877/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 878/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 879/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 880/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 881/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 882/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 883/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 884/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 885/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 886/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 887/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 888/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 889/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 890/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 891/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 892/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 893/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 894/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 895/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 896/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 897/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 898/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 899/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 900/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 901/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 902/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 903/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 904/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 905/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 906/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 907/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 908/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 909/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 910/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 911/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 912/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 913/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 914/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 915/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 916/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 917/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 918/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 919/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 920/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 921/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 922/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 923/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 924/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 925/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 926/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 927/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 928/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 929/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 930/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 931/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 932/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 933/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 934/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 935/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 936/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 937/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 938/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 939/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 940/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 941/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 942/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 943/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 944/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 945/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 946/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 947/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 948/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 949/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 950/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 951/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 952/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 953/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 954/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 955/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 956/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 957/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 958/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 959/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 960/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 961/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 962/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 963/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 964/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 965/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 966/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 967/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 968/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 969/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 970/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 971/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 972/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 973/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 974/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 975/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 976/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 977/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 978/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 979/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 980/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 981/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 982/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 983/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 984/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 985/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 986/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 987/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 988/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 989/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 990/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 991/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 992/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 993/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 994/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 995/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 996/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 997/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 998/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 999/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1000/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1001/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1002/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1003/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1004/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1005/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1006/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1007/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1008/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1009/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1010/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1011/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1012/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1013/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1014/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 1015/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1016/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1017/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1018/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1019/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1020/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1021/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1022/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1023/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1024/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1025/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1026/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1027/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1028/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1029/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1030/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1031/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1032/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1033/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1034/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1035/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1036/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1037/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1038/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1039/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1040/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1041/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1042/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1043/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1044/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1045/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1046/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1047/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1048/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1049/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1050/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1051/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1052/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1053/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1054/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1055/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1056/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1057/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1058/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1059/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1060/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1061/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1062/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1063/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1064/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1065/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1066/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1067/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1068/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1069/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1070/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1071/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1072/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1073/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1074/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1075/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1076/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1077/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1078/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1079/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1080/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1081/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1082/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1083/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1084/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1085/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1086/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1087/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1088/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1089/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1090/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1091/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1092/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1093/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1094/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1095/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1096/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1097/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1098/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1099/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1100/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1101/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1102/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1103/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1104/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1105/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1106/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1107/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1108/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1109/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1110/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1111/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1112/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 1113/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1114/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1115/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1116/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1117/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1118/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1119/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1120/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1121/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1122/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1123/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1124/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1125/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1126/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1127/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1128/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1129/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1130/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1131/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1132/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1133/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1134/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1135/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1136/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1137/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1138/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1139/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1140/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1141/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1142/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1143/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1144/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1145/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1146/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1147/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1148/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1149/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1150/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1151/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1152/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1153/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1154/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1155/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1156/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1157/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1158/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1159/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1160/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1161/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1162/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1163/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1164/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1165/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1166/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1167/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1168/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1169/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1170/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1171/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1172/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1173/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1174/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1175/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1176/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1177/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1178/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1179/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1180/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1181/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1182/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1183/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1184/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1185/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1186/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1187/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1188/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1189/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1190/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1191/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1192/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1193/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1194/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1195/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 1196/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1197/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1198/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1199/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1200/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1201/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1202/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1203/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1204/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1205/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1206/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1207/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1208/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1209/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1210/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1211/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1212/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1213/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1214/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1215/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1216/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1217/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1218/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1219/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1220/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1221/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1222/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1223/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1224/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1225/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1226/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1227/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1228/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1229/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1230/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1231/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1232/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1233/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1234/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1235/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1236/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1237/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1238/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1239/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1240/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1241/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 1242/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1243/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1244/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1245/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1246/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1247/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1248/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1249/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1250/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1251/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1252/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1253/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1254/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1255/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1256/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1257/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1258/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1259/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1260/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1261/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1262/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1263/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1264/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1265/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1266/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1267/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1268/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1269/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1270/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1271/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1272/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1273/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1274/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1275/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1276/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1277/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1278/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1279/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 1280/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1281/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1282/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1283/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1284/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1285/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 1286/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1287/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1288/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1289/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1290/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1291/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1292/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1293/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1294/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1295/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1296/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1297/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1298/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1299/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1300/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1301/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1302/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1303/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1304/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1305/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1306/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1307/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1308/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1309/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1310/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1311/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1312/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1313/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1314/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1315/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1316/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1317/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1318/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1319/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1320/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1321/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1322/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1323/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1324/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1325/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1326/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1327/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1328/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1329/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1330/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1331/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1332/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1333/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1334/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1335/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1336/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1337/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1338/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 1339/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1340/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1341/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1342/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1343/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1344/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1345/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1346/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1347/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 1348/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1349/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1350/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1351/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1352/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1353/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1354/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1355/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1356/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1357/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1358/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1359/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1360/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1361/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1362/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 1363/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1364/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1365/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1366/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1367/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1368/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1369/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1370/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1371/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1372/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1373/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1374/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1375/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1376/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1377/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1378/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1379/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1380/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1381/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1382/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1383/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1384/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1385/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1386/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1387/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1388/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1389/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1390/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1391/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1392/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1393/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1394/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1395/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1396/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1397/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1398/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1399/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1400/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1401/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1402/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1403/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1404/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1405/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1406/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1407/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1408/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1409/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1410/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1411/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1412/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1413/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1414/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1415/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1416/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1417/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1418/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1419/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1420/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1421/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1422/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 1423/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1424/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1425/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1426/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1427/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1428/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1429/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1430/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1431/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1432/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1433/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1434/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1435/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1436/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1437/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1438/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1439/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1440/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1441/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1442/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1443/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1444/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 1445/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1446/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1447/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1448/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1449/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1450/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1451/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1452/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1453/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1454/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1455/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1456/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1457/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1458/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1459/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1460/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1461/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1462/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1463/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1464/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1465/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1466/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1467/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1468/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1469/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1470/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1471/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1472/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1473/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1474/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 1475/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1476/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1477/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1478/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 1479/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1480/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1481/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1482/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1483/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 1484/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1485/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1486/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1487/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1488/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1489/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1490/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1491/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1492/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1493/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1494/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1495/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1496/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1497/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1498/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1499/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1500/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1501/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1502/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1503/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1504/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1505/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1506/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1507/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1508/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1509/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1510/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1511/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1512/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1513/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1514/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1515/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1516/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1517/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1518/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1519/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1520/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1521/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1522/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1523/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1524/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1525/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1526/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1527/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1528/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1529/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1530/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1531/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1532/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1533/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1534/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1535/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 1536/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1537/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1538/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1539/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1540/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 1541/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1542/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1543/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1544/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1545/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 1546/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1547/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1548/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1549/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1550/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1551/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1552/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1553/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1554/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1555/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1556/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1557/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1558/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1559/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1560/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1561/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1562/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1563/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1564/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1565/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1566/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1567/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1568/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1569/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1570/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1571/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1572/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1573/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1574/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1575/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1576/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1577/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1578/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1579/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1580/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1581/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1582/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1583/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1584/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1585/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1586/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1587/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1588/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1589/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1590/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1591/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1592/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1593/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1594/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1595/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1596/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1597/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1598/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1599/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1600/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1601/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1602/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1603/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1604/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1605/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1606/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1607/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1608/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1609/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1610/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1611/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1612/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1613/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1614/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1615/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1616/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1617/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1618/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1619/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1620/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1621/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1622/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1623/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1624/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1625/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1626/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1627/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1628/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1629/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1630/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1631/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1632/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1633/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1634/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1635/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1636/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1637/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1638/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1639/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1640/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1641/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1642/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1643/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1644/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 1645/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1646/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1647/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1648/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1649/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1650/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1651/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1652/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1653/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1654/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1655/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1656/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1657/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1658/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1659/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1660/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1661/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1662/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1663/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1664/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1665/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1666/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1667/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1668/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1669/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1670/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1671/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1672/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1673/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1674/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1675/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1676/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1677/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1678/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1679/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1680/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1681/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1682/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1683/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1684/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1685/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1686/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1687/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1688/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1689/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1690/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1691/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1692/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1693/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1694/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1695/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1696/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 1697/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1698/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1699/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1700/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1701/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1702/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1703/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1704/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1705/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1706/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1707/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1708/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1709/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1710/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1711/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1712/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1713/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1714/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1715/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1716/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1717/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1718/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1719/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1720/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1721/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1722/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1723/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1724/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1725/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1726/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1727/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1728/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1729/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1730/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 1731/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1732/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1733/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1734/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1735/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1736/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1737/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1738/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 1739/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1740/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1741/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1742/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1743/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1744/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1745/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1746/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1747/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1748/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1749/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1750/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1751/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1752/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1753/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1754/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1755/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1756/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1757/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1758/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1759/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1760/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1761/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1762/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1763/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1764/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1765/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1766/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1767/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1768/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1769/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1770/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1771/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1772/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1773/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1774/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1775/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1776/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1777/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1778/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1779/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1780/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1781/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1782/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1783/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1784/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1785/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1786/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1787/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1788/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1789/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1790/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1791/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1792/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1793/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1794/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1795/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1796/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1797/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1798/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1799/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1800/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1801/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1802/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1803/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1804/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1805/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1806/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1807/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1808/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1809/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1810/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1811/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1812/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1813/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1814/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1815/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1816/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 1817/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1818/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1819/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1820/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1821/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1822/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1823/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1824/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1825/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1826/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1827/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1828/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1829/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1830/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1831/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1832/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1833/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1834/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1835/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1836/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 1837/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1838/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1839/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1840/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1841/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1842/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1843/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1844/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1845/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1846/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1847/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1848/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1849/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1850/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1851/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1852/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1853/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1854/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1855/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1856/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1857/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1858/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1859/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1860/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1861/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1862/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1863/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1864/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1865/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1866/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1867/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1868/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1869/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1870/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1871/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1872/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1873/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1874/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1875/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1876/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1877/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1878/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1879/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1880/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1881/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1882/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1883/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1884/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1885/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1886/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1887/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1888/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1889/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1890/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1891/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1892/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1893/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1894/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1895/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1896/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1897/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1898/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1899/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1900/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1901/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1902/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1903/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1904/2000. Number of steps to finish: 6. Loss: 0.0 Reward: 6.0\nAgent: battleship_baseline . Episode 1905/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1906/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1907/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1908/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1909/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1910/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1911/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1912/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1913/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1914/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1915/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1916/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1917/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1918/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1919/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1920/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1921/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1922/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1923/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1924/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1925/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1926/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1927/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1928/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1929/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1930/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1931/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1932/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1933/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1934/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1935/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: battleship_baseline . Episode 1936/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1937/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1938/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1939/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1940/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1941/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1942/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1943/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1944/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1945/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1946/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1947/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1948/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1949/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1950/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1951/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1952/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1953/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1954/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1955/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1956/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1957/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1958/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1959/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1960/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1961/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1962/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1963/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1964/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1965/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1966/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1967/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: battleship_baseline . Episode 1968/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1969/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1970/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1971/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1972/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1973/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1974/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1975/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1976/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1977/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1978/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1979/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1980/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1981/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1982/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1983/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1984/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1985/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1986/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1987/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1988/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1989/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1990/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1991/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1992/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: battleship_baseline . Episode 1993/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1994/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: battleship_baseline . Episode 1995/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1996/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: battleship_baseline . Episode 1997/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 1998/2000. Number of steps to finish: 8. Loss: 0.0 Reward: 4.0\nAgent: battleship_baseline . Episode 1999/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: battleship_baseline . Episode 2000/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\n########## random_agent is running ##########\nAgent: random_agent . Episode 0/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 2/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 3/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 4/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 5/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 6/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 7/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 8/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 9/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 10/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 11/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 12/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 13/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 14/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 15/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: random_agent . Episode 16/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 17/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 18/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 19/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 20/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 21/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 22/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 23/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 24/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 25/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 26/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 27/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 28/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 29/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 30/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 31/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 32/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 33/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 34/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 35/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 36/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 37/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 38/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 39/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 40/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 41/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 42/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 43/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 44/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 45/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 46/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 47/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 48/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -18.0\nAgent: random_agent . Episode 49/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 50/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 51/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 52/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 53/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 54/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 55/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 56/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 57/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 58/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 59/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 60/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 61/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 62/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 63/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 64/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 65/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 66/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 67/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 68/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 69/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 70/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: random_agent . Episode 71/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 72/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 73/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 74/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 75/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 76/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 77/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 78/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 79/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 80/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 81/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 82/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 83/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 84/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 85/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 86/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 87/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 88/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 89/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 90/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 91/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 92/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 93/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 94/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 95/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 96/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 97/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 98/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 99/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 100/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 101/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 102/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 103/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 104/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 105/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -18.0\nAgent: random_agent . Episode 106/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 107/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 108/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 109/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 110/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: random_agent . Episode 111/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 112/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 113/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 114/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 115/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 116/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 117/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 118/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 119/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 120/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 121/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 122/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 123/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 124/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 125/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 126/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 127/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 128/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 129/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 130/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 131/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 132/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 133/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 134/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 135/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 136/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 137/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 138/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 139/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 140/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 141/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 142/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 143/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 144/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 145/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 146/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 147/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 148/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 149/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 150/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 151/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 152/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 153/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 154/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 155/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 156/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 157/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 158/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 159/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 160/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 161/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 162/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 163/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 164/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 165/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 166/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 167/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 168/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 169/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 170/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -18.0\nAgent: random_agent . Episode 171/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 172/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 173/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 174/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 175/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 176/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 177/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 178/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 179/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 180/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 181/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 182/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 183/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 184/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 185/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: random_agent . Episode 186/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 187/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 188/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 189/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 190/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 191/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 192/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 193/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 194/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 195/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 196/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 197/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 198/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 199/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 200/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 201/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 202/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 203/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 204/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 205/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 206/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 207/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 208/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 209/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 210/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 211/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 212/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 213/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 214/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 215/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 216/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 217/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 218/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 219/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 220/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 221/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 222/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 223/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 224/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 225/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: random_agent . Episode 226/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 227/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 228/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 229/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 230/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 231/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 232/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 233/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 234/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 235/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 236/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 237/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 238/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 239/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 240/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 241/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 242/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 243/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 244/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 245/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 246/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 247/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 248/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 249/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 250/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 251/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 252/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 253/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 254/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 255/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 256/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 257/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 258/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 259/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 260/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 261/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 262/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 263/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 264/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 265/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 266/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 267/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 268/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 269/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 270/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 271/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 272/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 273/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 274/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 275/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 276/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 277/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 278/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 279/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 280/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 281/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 282/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 283/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 284/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 285/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 286/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 287/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 288/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 289/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 290/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 291/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 292/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 293/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 294/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 295/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 296/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 297/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 298/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 299/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 300/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 301/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 302/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 303/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -18.0\nAgent: random_agent . Episode 304/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 305/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 306/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 307/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 308/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 309/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 310/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 311/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 312/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 313/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 314/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 315/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 316/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 317/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 318/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 319/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 320/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 321/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 322/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 323/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 324/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 325/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 326/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 327/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 328/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 329/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 330/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 331/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 332/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 333/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 334/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 335/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 336/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 337/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 338/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 339/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 340/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 341/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 342/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 343/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 344/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 345/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 346/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 347/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 348/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 349/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 350/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 351/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 352/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 353/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 354/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 355/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 356/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 357/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 358/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 359/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 360/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 361/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 362/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 363/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 364/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 365/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 366/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 367/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 368/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 369/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 370/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 371/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 372/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 373/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 374/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: random_agent . Episode 375/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 376/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -18.0\nAgent: random_agent . Episode 377/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 378/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 379/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 380/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 381/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 382/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 383/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 384/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 385/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 386/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 387/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 388/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 389/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 390/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 391/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 392/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 393/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 394/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 395/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 396/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 397/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 398/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 399/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 400/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 401/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 402/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 403/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 404/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 405/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 406/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 407/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -18.0\nAgent: random_agent . Episode 408/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 409/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 410/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 411/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 412/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 413/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 414/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 415/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 416/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 417/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 418/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 419/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 420/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 421/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 422/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 423/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 424/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 425/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 426/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 427/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 428/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 429/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 430/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 431/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 432/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 433/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 434/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 435/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 436/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 437/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 438/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 439/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 440/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 441/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 442/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 443/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 444/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 445/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 446/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 447/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 448/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 449/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 450/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 451/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 452/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 453/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 454/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 455/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 456/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 457/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 458/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 459/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 460/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 461/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 462/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 463/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 464/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 465/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 466/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 467/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 468/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 469/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 470/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 471/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 472/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 473/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 474/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 475/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 476/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 477/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 478/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 479/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 480/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 481/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 482/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 483/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 484/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 485/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 486/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 487/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 488/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 489/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 490/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 491/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 492/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 493/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 494/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 495/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 496/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 497/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 498/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 499/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 500/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 501/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 502/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 503/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 504/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 505/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 506/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 507/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 508/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 509/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 510/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 511/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 512/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 513/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 514/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 515/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 516/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 517/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 518/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 519/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 520/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 521/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -18.0\nAgent: random_agent . Episode 522/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 523/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 524/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 525/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 526/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 527/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 528/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 529/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 530/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 531/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 532/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: random_agent . Episode 533/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 534/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 535/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 536/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 537/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 538/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 539/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 540/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 541/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 542/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 543/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 544/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 545/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 546/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 547/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 548/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 549/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 550/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 551/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 552/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 553/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 554/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 555/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 556/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 557/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 558/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 559/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 560/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 561/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 562/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 563/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 564/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 565/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 566/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 567/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 568/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 569/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 570/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 571/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 572/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 573/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 574/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 575/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 576/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 577/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 578/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 579/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 580/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 581/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 582/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 583/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 584/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 585/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 586/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 587/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 588/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 589/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 590/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 591/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: random_agent . Episode 592/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 593/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 594/2000. Number of steps to finish: 7. Loss: 0.0 Reward: 5.0\nAgent: random_agent . Episode 595/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 596/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 597/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 598/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 599/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 600/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 601/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 602/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 603/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 604/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 605/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 606/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 607/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 608/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 609/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 610/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 611/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 612/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 613/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 614/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 615/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 616/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 617/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 618/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 619/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 620/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 621/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 622/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 623/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 624/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 625/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 626/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 627/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 628/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 629/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 630/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 631/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 632/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 633/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 634/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 635/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 636/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 637/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 638/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 639/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 640/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 641/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 642/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 643/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 644/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 645/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 646/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 647/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 648/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 649/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 650/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 651/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 652/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 653/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 654/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 655/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -18.0\nAgent: random_agent . Episode 656/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 657/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 658/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 659/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 660/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 661/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 662/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 663/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 664/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 665/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 666/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 667/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 668/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 669/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 670/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 671/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 672/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 673/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 674/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 675/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 676/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 677/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 678/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 679/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 680/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 681/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 682/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 683/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 684/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 685/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 686/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 687/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 688/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 689/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 690/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 691/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 692/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 693/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 694/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 695/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 696/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 697/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 698/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 699/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 700/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 701/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 702/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 703/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 704/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 705/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 706/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 707/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 708/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 709/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 710/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 711/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 712/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 713/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 714/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 715/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 716/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 717/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 718/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 719/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 720/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 721/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 722/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 723/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 724/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 725/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 726/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 727/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 728/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 729/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 730/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 731/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 732/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 733/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 734/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 735/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 736/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 737/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 738/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 739/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 740/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 741/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 742/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 743/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 744/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 745/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 746/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 747/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 748/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 749/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 750/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 751/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 752/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 753/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: random_agent . Episode 754/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 755/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 756/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 757/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 758/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 759/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 760/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 761/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 762/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 763/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 764/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 765/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 766/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 767/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 768/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 769/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 770/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 771/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 772/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 773/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 774/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 775/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 776/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 777/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 778/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 779/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 780/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 781/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 782/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 783/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 784/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 785/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 786/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 787/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 788/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 789/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 790/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 791/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 792/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 793/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 794/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 795/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 796/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 797/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 798/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 799/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 800/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 801/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 802/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 803/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 804/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 805/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 806/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -18.0\nAgent: random_agent . Episode 807/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 808/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 809/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 810/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 811/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 812/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 813/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 814/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 815/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 816/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 817/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 818/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 819/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 820/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 821/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 822/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 823/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 824/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 825/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 826/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 827/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 828/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 829/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 830/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 831/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 832/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 833/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 834/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 835/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 836/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 837/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 838/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 839/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 840/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 841/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 842/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 843/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 844/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 845/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 846/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 847/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 848/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 849/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 850/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 851/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 852/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 853/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 854/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 855/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 856/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 857/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 858/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 859/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 860/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 861/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 862/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 863/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 864/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 865/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 866/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 867/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 868/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 869/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 870/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 871/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 872/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 873/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 874/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 875/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 876/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 877/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 878/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 879/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 880/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 881/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 882/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 883/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 884/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 885/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 886/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 887/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 888/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 889/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 890/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 891/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 892/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 893/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 894/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 895/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 896/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 897/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 898/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 899/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 900/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 901/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 902/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 903/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 904/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 905/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 906/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 907/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 908/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 909/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -18.0\nAgent: random_agent . Episode 910/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 911/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 912/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 913/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 914/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: random_agent . Episode 915/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 916/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 917/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 918/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 919/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 920/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 921/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 922/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 923/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 924/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 925/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 926/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: random_agent . Episode 927/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 928/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 929/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 930/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 931/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 932/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 933/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 934/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 935/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 936/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 937/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 938/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 939/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 940/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 941/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 942/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 943/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 944/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 945/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 946/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 947/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 948/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 949/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 950/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -18.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 951/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 952/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 953/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 954/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 955/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 956/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 957/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 958/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 959/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 960/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 961/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 962/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 963/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 964/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 965/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 966/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 967/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 968/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 969/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 970/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 971/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 972/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 973/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 974/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 975/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 976/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 977/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 978/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 979/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 980/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -18.0\nAgent: random_agent . Episode 981/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 982/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 983/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 984/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 985/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 986/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 987/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: random_agent . Episode 988/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 989/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 990/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 991/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 992/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 993/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 994/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 995/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 996/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 997/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 998/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 999/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1000/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1001/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1002/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1003/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1004/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1005/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1006/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1007/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1008/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1009/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1010/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1011/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1012/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1013/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1014/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1015/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1016/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1017/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1018/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1019/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1020/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1021/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1022/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1023/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1024/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1025/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1026/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1027/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1028/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1029/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1030/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1031/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1032/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1033/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1034/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 1035/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1036/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1037/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1038/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1039/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1040/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 1041/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1042/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1043/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1044/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1045/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1046/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1047/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1048/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1049/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: random_agent . Episode 1050/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1051/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1052/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1053/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1054/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1055/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1056/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1057/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1058/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1059/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1060/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1061/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1062/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 1063/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1064/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1065/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1066/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1067/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1068/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1069/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1070/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1071/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1072/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1073/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1074/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1075/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1076/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1077/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1078/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1079/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 1080/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1081/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1082/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1083/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1084/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1085/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1086/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1087/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 1088/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 1089/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1090/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: random_agent . Episode 1091/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1092/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 1093/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1094/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1095/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1096/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1097/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1098/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1099/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1100/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1101/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1102/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1103/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1104/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 1105/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1106/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1107/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1108/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 1109/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1110/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1111/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1112/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1113/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1114/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1115/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1116/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1117/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1118/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1119/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1120/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 1121/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1122/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1123/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1124/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1125/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1126/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 1127/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 1128/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1129/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1130/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1131/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1132/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1133/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1134/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1135/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1136/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1137/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1138/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1139/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1140/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1141/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1142/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1143/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1144/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1145/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1146/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1147/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1148/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1149/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1150/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1151/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1152/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1153/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1154/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1155/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1156/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1157/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1158/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1159/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1160/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1161/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1162/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1163/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1164/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1165/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1166/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1167/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 1168/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1169/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1170/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1171/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1172/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1173/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1174/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 1175/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1176/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1177/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1178/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1179/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1180/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1181/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1182/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1183/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1184/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1185/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1186/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1187/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1188/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1189/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1190/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1191/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1192/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1193/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1194/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 1195/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1196/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1197/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1198/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1199/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1200/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1201/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1202/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1203/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1204/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1205/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1206/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1207/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1208/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1209/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1210/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1211/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1212/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 1213/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1214/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1215/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 1216/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1217/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1218/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 1219/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1220/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1221/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1222/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1223/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1224/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1225/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1226/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1227/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1228/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1229/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1230/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1231/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1232/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1233/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -18.0\nAgent: random_agent . Episode 1234/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: random_agent . Episode 1235/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1236/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1237/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1238/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1239/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1240/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1241/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 1242/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1243/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1244/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1245/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1246/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1247/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 1248/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1249/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1250/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1251/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1252/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1253/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1254/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1255/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1256/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1257/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1258/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1259/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1260/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1261/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1262/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1263/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 1264/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1265/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1266/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1267/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1268/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1269/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1270/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1271/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1272/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1273/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1274/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1275/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1276/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1277/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 1278/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1279/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1280/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1281/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1282/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1283/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1284/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1285/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1286/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1287/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1288/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1289/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1290/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1291/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1292/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1293/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1294/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1295/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1296/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1297/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1298/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1299/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1300/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1301/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1302/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1303/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1304/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 1305/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1306/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1307/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1308/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1309/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1310/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1311/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1312/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 1313/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1314/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 1315/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1316/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1317/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1318/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1319/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1320/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1321/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1322/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1323/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 1324/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1325/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1326/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1327/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1328/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1329/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1330/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1331/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1332/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1333/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1334/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1335/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1336/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1337/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 1338/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1339/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1340/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1341/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1342/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1343/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1344/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1345/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1346/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1347/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1348/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1349/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1350/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1351/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1352/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1353/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1354/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1355/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1356/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1357/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1358/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1359/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1360/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1361/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 1362/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1363/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1364/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1365/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1366/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1367/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1368/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1369/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1370/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1371/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1372/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 1373/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1374/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1375/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1376/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1377/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 1378/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1379/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1380/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1381/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1382/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1383/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1384/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1385/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1386/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1387/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1388/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1389/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1390/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1391/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1392/2000. Number of steps to finish: 10. Loss: 0.0 Reward: 2.0\nAgent: random_agent . Episode 1393/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1394/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1395/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 1396/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1397/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1398/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 1399/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1400/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1401/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 1402/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1403/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1404/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1405/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 1406/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1407/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1408/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1409/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 1410/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1411/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1412/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1413/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1414/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1415/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1416/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1417/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1418/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1419/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1420/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1421/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1422/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1423/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1424/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 1425/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1426/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1427/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1428/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1429/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1430/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1431/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1432/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1433/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1434/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1435/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1436/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1437/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1438/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1439/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1440/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 1441/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1442/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1443/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1444/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1445/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1446/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1447/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1448/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1449/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1450/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1451/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1452/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1453/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1454/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1455/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 1456/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1457/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1458/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1459/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1460/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1461/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1462/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1463/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1464/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1465/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1466/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1467/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1468/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1469/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 1470/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1471/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1472/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1473/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1474/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1475/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1476/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1477/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1478/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1479/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1480/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1481/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1482/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: random_agent . Episode 1483/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1484/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1485/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1486/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1487/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1488/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1489/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1490/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 1491/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1492/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1493/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1494/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1495/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1496/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1497/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1498/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1499/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1500/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1501/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1502/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1503/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1504/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1505/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1506/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1507/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1508/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 1509/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1510/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1511/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1512/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1513/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1514/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1515/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1516/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1517/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1518/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1519/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1520/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1521/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1522/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1523/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1524/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1525/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1526/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1527/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1528/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1529/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1530/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1531/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1532/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1533/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1534/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1535/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1536/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 1537/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1538/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1539/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1540/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1541/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1542/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1543/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1544/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1545/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1546/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1547/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1548/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1549/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1550/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1551/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1552/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1553/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1554/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1555/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1556/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1557/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1558/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1559/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1560/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1561/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1562/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1563/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1564/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1565/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1566/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1567/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1568/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 1569/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 1570/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1571/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1572/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1573/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1574/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1575/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1576/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1577/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1578/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 1579/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1580/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1581/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1582/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 1583/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1584/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1585/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1586/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 1587/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1588/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1589/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1590/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1591/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 1592/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1593/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1594/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1595/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1596/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1597/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1598/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1599/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1600/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1601/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1602/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1603/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1604/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1605/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1606/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1607/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1608/2000. Number of steps to finish: 9. Loss: 0.0 Reward: 3.0\nAgent: random_agent . Episode 1609/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1610/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1611/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1612/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1613/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1614/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1615/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 1616/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1617/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1618/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1619/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1620/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1621/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1622/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1623/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1624/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1625/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1626/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1627/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1628/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1629/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1630/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1631/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 1632/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1633/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1634/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1635/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1636/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1637/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1638/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1639/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1640/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1641/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1642/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1643/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1644/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1645/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1646/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1647/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1648/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1649/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1650/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 1651/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1652/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1653/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1654/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1655/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1656/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1657/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1658/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1659/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1660/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1661/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1662/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1663/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1664/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1665/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 1666/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1667/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1668/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1669/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1670/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 1671/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1672/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1673/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1674/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -18.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 1675/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 1676/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1677/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1678/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1679/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1680/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1681/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1682/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1683/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1684/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1685/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1686/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1687/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1688/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1689/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1690/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1691/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1692/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1693/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 1694/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1695/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1696/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1697/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1698/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1699/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1700/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1701/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1702/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1703/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1704/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1705/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1706/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1707/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1708/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1709/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1710/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1711/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 1712/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 1713/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1714/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1715/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1716/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1717/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1718/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1719/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1720/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1721/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1722/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1723/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1724/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1725/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1726/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1727/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1728/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1729/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1730/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1731/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1732/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1733/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1734/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1735/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1736/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1737/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1738/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1739/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1740/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 1741/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1742/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1743/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1744/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1745/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1746/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1747/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1748/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1749/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1750/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1751/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1752/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1753/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1754/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1755/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1756/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1757/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 1758/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1759/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1760/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1761/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1762/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 1763/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1764/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 1765/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1766/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 1767/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1768/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1769/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1770/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1771/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1772/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1773/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1774/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1775/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1776/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1777/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1778/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1779/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1780/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1781/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1782/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 1783/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1784/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1785/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1786/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -18.0\nAgent: random_agent . Episode 1787/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1788/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1789/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1790/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1791/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1792/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1793/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1794/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1795/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1796/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1797/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1798/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1799/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1800/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1801/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1802/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1803/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1804/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1805/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1806/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1807/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1808/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1809/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1810/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1811/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1812/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1813/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1814/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1815/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1816/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 1817/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1818/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1819/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1820/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1821/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 1822/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 1823/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1824/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1825/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1826/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1827/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1828/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: random_agent . Episode 1829/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1830/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1831/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1832/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1833/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1834/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1835/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1836/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1837/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1838/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1839/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1840/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 1841/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1842/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1843/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1844/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1845/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1846/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1847/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1848/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1849/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1850/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 1851/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 1852/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1853/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1854/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1855/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1856/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1857/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 1858/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1859/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1860/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1861/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1862/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: random_agent . Episode 1863/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1864/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 1865/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1866/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 1867/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1868/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1869/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1870/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1871/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1872/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1873/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1874/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1875/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1876/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1877/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1878/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1879/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1880/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1881/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1882/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1883/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1884/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1885/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1886/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1887/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1888/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1889/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1890/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1891/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1892/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -18.0\nAgent: random_agent . Episode 1893/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1894/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1895/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -18.0\nAgent: random_agent . Episode 1896/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1897/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1898/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1899/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1900/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1901/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1902/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1903/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1904/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1905/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1906/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1907/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1908/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1909/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1910/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1911/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1912/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1913/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1914/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1915/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1916/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1917/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1918/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 1919/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1920/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 1921/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1922/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1923/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\nAgent: random_agent . Episode 1924/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1925/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1926/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1927/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1928/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1929/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1930/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1931/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1932/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1933/2000. Number of steps to finish: 13. Loss: 0.0 Reward: -1.0\nAgent: random_agent . Episode 1934/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1935/2000. Number of steps to finish: 12. Loss: 0.0 Reward: 0.0\nAgent: random_agent . Episode 1936/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1937/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1938/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1939/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1940/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1941/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1942/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1943/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1944/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1945/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1946/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1947/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1948/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1949/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1950/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1951/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\n","name":"stdout"},{"output_type":"stream","text":"Agent: random_agent . Episode 1952/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1953/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1954/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1955/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1956/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1957/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1958/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1959/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1960/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1961/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1962/2000. Number of steps to finish: 16. Loss: 0.0 Reward: -4.0\nAgent: random_agent . Episode 1963/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1964/2000. Number of steps to finish: 14. Loss: 0.0 Reward: -2.0\nAgent: random_agent . Episode 1965/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -8.0\nAgent: random_agent . Episode 1966/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1967/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1968/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1969/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1970/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1971/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1972/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -16.0\nAgent: random_agent . Episode 1973/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1974/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1975/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1976/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1977/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1978/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1979/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1980/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1981/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1982/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1983/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1984/2000. Number of steps to finish: 17. Loss: 0.0 Reward: -5.0\nAgent: random_agent . Episode 1985/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1986/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1987/2000. Number of steps to finish: 19. Loss: 0.0 Reward: -7.0\nAgent: random_agent . Episode 1988/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1989/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1990/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1991/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1992/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 1993/2000. Number of steps to finish: 11. Loss: 0.0 Reward: 1.0\nAgent: random_agent . Episode 1994/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1995/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -12.0\nAgent: random_agent . Episode 1996/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1997/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -14.0\nAgent: random_agent . Episode 1998/2000. Number of steps to finish: 18. Loss: 0.0 Reward: -6.0\nAgent: random_agent . Episode 1999/2000. Number of steps to finish: 20. Loss: 0.0 Reward: -10.0\nAgent: random_agent . Episode 2000/2000. Number of steps to finish: 15. Loss: 0.0 Reward: -3.0\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# PLOTTING"},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nresults.append(agent_random.list_rewards)\nresults.append(agent_ddqn.list_rewards)\nresults.append(agent_sacd.list_rewards)\nresults.append(agent_battleship_baseline.list_rewards)\nplot_results(results, rolling = 30, download=False)","execution_count":62,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}